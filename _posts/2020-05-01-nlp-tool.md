---
layout: post
title:  "NLP 工具集"
date:   2020-05-01 22:50:00
categories: 自然语言处理
tags:  NLP 自然语言处理 kenlm fasttext 分词 纠错 停用词 词库 同义词 近义词 语种识别
excerpt: NLP工具集汇总
author: 鹤啸九天
mathjax: true
permalink: /nlp_tool
---

* content
{:toc}

# nlp工具集合

## Demo集合

- 【2020-5-19】NLP各类应用Demo汇总（Colab实现），[The Super Duper NLP Repo](https://notebooks.quantumstat.com/)，如文本生成、对话、问答、翻译等
- ![](https://image.jiqizhixin.com/uploads/editor/de27f3bd-2740-4097-b6d6-7340aeeb32cd/302.png)

<iframe src='https://notebooks.quantumstat.com/' frameborder='0' scrolling='no' allowfullscreen="true"></iframe>


## 数据集

- 中文汉字集合、停用词、生僻字[数据集github](https://github.com/elephantnose/characters)

## web平台

- 【2021-7-15】[nlp-web-demo](https://github.com/lonly197/nlp-web-demo)页面集成了Stanford, Hanlp, FNLP, Thulc, FudanDNN, Boson NLP and Jieba等方法，对比分词、词性标注、句法、摘要等效果
- watson的[Natural Language Understanding](https://natural-language-understanding-demo.ng.bluemix.net/)工具包 demo，支持中文，可以实测。

## 开源nlp工具包

[nlp工具包汇总推荐](https://www.biaodianfu.com/nlp-tools.html)

|工具包|作者|基本功能|特殊功能|擅长语种|备注|
|---|---|---|---|---|---|
|[jieba](https://github.com/fxsjy/jieba/)|个人|分词/关键词/词性|自定义字典/三种模式分词|中文|-|
|pkuseg|北大|分词/词性|自定义字典/多领域分词/高准确/用户自训练/文件分词|中文|优于jieba+THULAC|
|[LTP](https://github.com/HIT-SCIR/ltp)|哈工大|分词/词性/句法分析|-|中文|pyltp, [demo](http://ltp.ai/demo.html)，命名实体标注集没有提取“时间”|
|THULAC|清华|分词/词性/句法分析|-|中文|中文词法分析工具包，准确率高，速度快; [demo](http://thulac.thunlp.org/demo)|
|stanfordNLP|斯坦福|分词/词性/句法分析/实体识别/语法树|词干还原/wordnet(同义词)/语料库|英文|Java+python，依赖pytorch,[demo](https://corenlp.run/)|
|[allennlp](https://allennlp.org/)|组织|-|多种高级NLP能力（阅读理解/指代消解等）|英文|紧跟sota，[demo](https://demo.allennlp.org/reading-comprehension/bidaf-elmo)|
|wastonnlp|IBM|-|-|英文|[demo](https://natural-language-understanding-demo.ng.bluemix.net/)|
|NLTK|宾大|分词/词性/句法分析/实体识别|句法可视化|英文|学术界|
|TextBlob|个人|分词/词性/实体识别/摘要/关键词|文本分类/情感分析/词根化/纠错/机器翻译/wordnet|英文|Nltk抽象而来|
|SnowNLP|个人|分词/词性/分句/摘要/关键词|文本分类/情感分析/繁简/拼音/相似度|中文|TextBolb非官方中文版，不依赖nltk|
|DeepNLP|-|分词/词性/句法/摘要/关键词/实体识别|文本分类/pipeline/web|api/用户自训练|中文||
|小明NLP|个人|分词/词性/摘要/关键词|自定义字典/文本分类/情感分析/繁简/拼音/纠错/偏旁部首|中文||
|spaCy|-|分词/词性/摘要/关键词/分句/句法/语法树|文本相似度/统计学习/深度学习/可视化|53种语言|工业级，cython，高性能，TensorFlow/pytorch支持，标注工具 https://prodi.gy/|
|FoolNLP|复旦|分词/关键词/词性|自定义字典|中文|非最快但最准，bi-LSTM|
|HanNLP|-|分词/词性/实体识别/摘要/关键词/句法分析|繁简/拼音/纠错/聚类/w2v|中文|Java→python，支持TensorFlow|
|bosonnlp|公司|-|-|中文|[Demo](http://static.bosonnlp.com/demo)，[云孚科技](https://www.yunfutech.com/products/yfnlp/demo?tab=0&module=pos)|

杭州[实在智能](https://www.ai-indeed.com/)（阿里P8创立的公司）的[nlp-base基础能力demo](https://nlp-base.external.ai-indeed.com/)


### Jieba结巴

[github地址](https://github.com/fxsjy/jieba/blob/67fa2e36e72f69d9134b8a1037b83fbb070b9775/jieba/__init__.py#L380)

#### 分词

- jieba.**cut** 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型
- jieba.**cut_for_search** 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细
- 待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8
- jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用jieba.lcut 以及 jieba.lcut_for_search 直接返回 list
- jieba.**Tokenizer**(dictionary=DEFAULT_DICT) 新建**自定义**分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。

结巴中文分词采用的算法
- 基于**Trie树**（前缀树）结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的**有向无环图**（DAG)
  - Python版前缀树：[Pytrie](https://pypi.python.org/pypi/PyTrie)，[demo](https://wiki.python.org/moin/CheeseShopTutorial)
- 采用了**动态规划**查找最大概率路径, 找出基于词频的**最大**切分组合
- 对于未登录词，采用了基于汉字成词能力的**HMM模型**，使用了**Viterbi算法**

三种分词模式：
- **精确**模式，试图将句子最精确地切开，适合文本分析；
- **全**模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
- **搜索引擎**模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

自定义字典
- 开发者可以指定自己的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率
- 用法： jieba.**load_userdict**(file_name) # file_name 为文件类对象或自定义词典的路径
- 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：**词语**、**词频**（可省略）、**词性**（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。
- 词频省略时使用自动计算的能保证分出该词的词频。
- 注：词性可以自定义，如norm、project都可以

```
创新办 3 i
云计算 5
P特琳 nz
台中
```

调整字典
- 使用 **add_word**(word, freq=None, tag=None) 和 del_word(word) 可在程序中动态修改词典。
- 使用 **suggest_freq**(segment, tune=True) 可调节**单个词语**的词频，使其能（或不能）被分出来。

注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。

延迟加载机制
- jieba 采用延迟加载，import jieba 和 jieba.Tokenizer() 不会立即触发词典的加载，一旦有必要才开始加载词典构建前缀字典。如果你想手工初始 jieba，也可以手动初始化。
- 手工初始化：jieba.initialize()  # 手动初始化（可选）
- 在 0.28 之前的版本是不能指定主词典的路径的，有了延迟加载机制后，你可以改变主词典的路径: [示例](https://github.com/fxsjy/jieba/blob/master/test/test_change_dictpath.py)
  - jieba.set_dictionary('data/dict.txt.big')

```python
import re
import pandas as pd

# 测试数据
df = pd.DataFrame([['你好,我是链家经纪人', 20],['贝壳是房地产公司', 13],['贝壳分下降了怎么办，你帮忙看下',30],['[哭泣]',23]], columns=['query','freq'])
print(df.shape)

# 加载停用词
stop_words = [line.strip() for line in open('stopWord2.txt', encoding='gbk').readlines()]

# 分词函数一：中文分词，包括除去数字字母及停用词，得到一个分词用空格隔开的字符串，便于向量化（因为这个CountVouterizer()是针对英文分词的，英文之间直接用空格隔开的）
def cut_word(sent):
    line = re.sub(r'[a-zA-Z0-9]*','',sent)
    wordList = jieba.lcut(line,cut_all=False)
    #文本分词，并且用空格连接起来，便于下面向量化
    return ' '.join([word for word in wordList if word not in stopWord and len(word)>1])

# 分词函数二：没去停用词，CountVouterizer()中可以直接添加停用词表参数，不统计文档中的停用词的数量
def cutword(sent):
    line = re.sub(r'[a-zA-Z0-9]*','',sent)
    wordList = jieba.lcut(line,cut_all=False)
    return ' '.join([word for word in wordList if len(word)>1])

# 分词函数三：精准分词，选择词性
import jieba.posseg as pseg
def jieba_cut(comment):
    word_list = []  # 建立空列表用于存储分词结果
    seg_list = pseg.cut(comment)  # 精确模式分词[默认模式]
    for word in seg_list:
        if word.flag in ['ns', 'n', 'vn', 'v', 'nr']:  # 选择属性
            word_list.append(word.word)  # 分词追加到列表
    return word_list

#将文本分词，并且分词用空格隔开变成文本存才DataFrame中
df['word_list']=df['query'].map(cutword)
#df['word_list']=df['query'].apply(cutword) # apply也行
# 向量化
vectorizer = TfidfVectorizer(stop_words=stop_words,  tokenizer=jieba_cut,  use_idf=True)  # 创建词向量模型
X = vectorizer.fit_transform(comment_list)  # 将评论关键字列表转换为词向量空间模型
# K均值聚类
model_kmeans = KMeans(n_clusters=3)  # 创建聚类模型对象
model_kmeans.fit(X)  # 训练模型
# 聚类结果汇总
cluster_labels = model_kmeans.labels_  # 聚类标签结果
word_vectors = vectorizer.get_feature_names()  # 词向量
word_values = X.toarray()  # 向量值
comment_matrix = np.hstack((word_values,  cluster_labels.reshape(word_values.
    shape[0], 1)))  # 将向量值和标签值合并为新的矩阵
word_vectors.append('cluster_labels')  # 将新的聚类标签列表追加到词向量后面
comment_pd = pd.DataFrame(comment_matrix, columns=word_vectors)  # 创建包含词向量和聚类标签的数据框
comment_pd.to_csv('comment.csv')
print(comment_pd.head(1))  # 打印输出数据框第1条数据
# 聚类结果分析
comment_cluster1 = comment_pd[comment_pd['cluster_labels'] == 1].drop('cluster_labels', axis=1)  # 选择聚类标签值为1的数据，并删除最后一列
word_importance = np.sum(comment_cluster1, axis=0)  # 按照词向量做汇总统计
print(word_importance.sort_values(ascending=False)[:5])   # 按汇总统计的值做逆序排序并打印输出前5个词

```


#### 并行分词

- 原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升
- 基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows
- 用法：
  - jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数
  - jieba.disable_parallel() # 关闭并行分词模式
- [例子](https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py)

实验结果：
> 在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。

注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。

#### Tokenize 分词位置

返回词语在原文的起止位置

注意，输入参数只接受 unicode

```python
result = jieba.tokenize(u'永和服装饰品有限公司')
for tk in result:
    print("word %s\t\t start: %d \t\t end:%d" % (tk[0],tk[1],tk[2]))
```

#### 命令行分词

使用示例：python -m jieba news.txt > cut_result.txt

使用: 

```sh
python -m jieba [options] filename
```

结巴命令行界面。

固定参数:
  filename              输入文件

可选参数:

```
  -h, --help            显示此帮助信息并退出
  -d [DELIM], --delimiter [DELIM]
                        使用 DELIM 分隔词语，而不是用默认的' / '。
                        若不指定 DELIM，则使用一个空格分隔。
  -p [DELIM], --pos [DELIM]
                        启用词性标注；如果指定 DELIM，词语和词性之间
                        用它分隔，否则用 _ 分隔
  -D DICT, --dict DICT  使用 DICT 代替默认词典
  -u USER_DICT, --user-dict USER_DICT
                        使用 USER_DICT 作为附加词典，与默认词典或自定义词典配合使用
  -a, --cut-all         全模式分词（不支持词性标注）
  -n, --no-hmm          不使用隐含马尔可夫模型
  -q, --quiet           不输出载入信息到 STDERR
  -V, --version         显示版本信息并退出
```

如果没有指定文件名，则使用标准输入。

#### 关键词

（1）基于 TF-IDF 算法的关键词抽取 import jieba.analyse
- jieba.analyse.**extract_tags**(sentence, topK=20, withWeight=False, allowPOS=())
   - sentence 为待提取的文本
   - topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20
   - withWeight 为是否一并返回关键词权重值，默认值为 False
   - allowPOS 仅包括指定词性的词，默认值为空，即不筛选
- jieba.analyse.**TFIDF**(idf_path=None) 新建 TFIDF 实例，idf_path 为 IDF 频率文件
[代码示例](https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py) （关键词提取）

- 关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径
   - 用法： jieba.analyse.**set_idf_path**(file_name) # file_name为自定义语料库的路径
   - 自定义语料库[示例](https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big), [用法示例](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py)
- 关键词提取所使用停止词（停用词）（Stop Words）文本语料库可以切换成自定义语料库的路径
   - 用法： jieba.analyse.**set_stop_words**(file_name) # file_name为自定义语料库的路径
   - 自定义语料库[示例](https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt), 用法[示例](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py)
- 关键词一并返回关键词权重值示例
  - 用法[示例](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py)

(2) 基于 TextRank 算法的关键词抽取

算法论文： [TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)

基本思想:
- 将待抽取关键词的文本进行分词
- 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图
- 计算图中节点的PageRank，注意是无向带权图
使用示例: [test/demo.py](https://github.com/fxsjy/jieba/blob/master/test/demo.py)

- jieba.analyse.**textrank**(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。
- jieba.analyse.TextRank() 新建自定义 TextRank 实例

#### 停用词

自定义语料库[示例](https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt), 用法[示例](https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py)

国内比较常用的中文停用词库，有以下几个：
- 中文停用词表
- 哈工大停用词表
- 百度停用词表
- 四川大学机器智能实验室停用词库
而elephantnose对以上4个词库进行了合并去重，共计2311个，还有3500个常用汉字，生僻字，可在github直接[下载](https://github.com/elephantnose/characters/blob/master/stop_words)使用~~ 链接：[GitHub链接](https://github.com/elephantnose/characters)

```python
# 下载 https://github.com/elephantnose/characters/blob/master/stop_words
stop_words_file = "/Users/bytedance/Desktop/code/data/stop_words"
# 读取字典
stopword_list = [i.strip() for i in open(stop_words_file, encoding='utf8')]
# 或者直接设置jieba
import jieba.analyse as analyse
jieba.analyse.set_stop_words('stop_words')
```


#### 词性标注

- jieba.posseg.**POSTokenizer**(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。
- 标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法

更多内容：[python使用结巴中文分词以及训练自己的分词词典](https://ptorch.com/news/182.html), [github代码示例](https://github.com/fxsjy/jieba/blob/master/test/test_userdict.py)


```python
# -*- coding:utf-8 -*-
import jieba

text = '我来到北京清华大学'
# （1）添加或管理自定义词典，dict.txt，一个词占一行；
# 每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。
# file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。
# 实例：
# 创新办 3 i
# 不处理 nr
# 不还款
jieba.load_userdict("./dict.txt")
# （2）修改字典
# 使用add_word(word, freq=None, tag=None)和del_word(word)可在程序中动态修改词典。
# 使用suggest_freq(segment, tune=True)可调节单个词语的词频，使其能（或不能）被分出来。
jieba.suggest_freq('不处理',True)
jieba.add_word('不处理',tag='d')
jieba.add_word('中国银行APP',tag='d')
jieba.del_word('不处理')
word_list = jieba.cut("我今天不处理逾期信用贷款，因为你们中国银行APP根本打不开")
print("|".join(word_list))
# （4）分词
default_mode = jieba.cut(text) # 精确
full_mode = jieba.cut(text,cut_all=True) # 全模式
search_mode = jieba.cut_for_search(text) # 搜索引擎

print("精确模式:","/".join(default_mode))
print("全模式:","/".join(full_mode))
print("搜索引擎模式:","/".join(search_mode))
# （5）关键词提取
tags = jieba.analyse.extract_tags(text,2)
print("关键词抽取:","/".join(tags))

```


#### NER

代码

```py
import jieba
import jieba.analyse
import jieba.posseg as posg

# 使用 jieba 进行词性切分，allowPOS 指定允许的词性，这里选择名词 n 和地名 ns
sentence = u'''上线三年就成功上市,拼多多上演了互联网企业的上市奇迹,却也放大平台上存在的诸多问题，黄峥在美国上市。'''
kw = jieba.analyse.extract_tags(sentence, topK=10, withWeight=True, allowPOS=('n', 'ns', 'nr'))
print(kw)
for item in kw:
    print(item[0], item[1])

kw = jieba.analyse.textrank(sentence,topK=20,withWeight=True,allowPOS=('ns','n'))
print(kw)
for item in kw:
    print(item[0],item[1])
```

### 北大pkuseg ―― 超过jieba+THULAC

北京大学语言计算与机器学习研究组研制推出的一套全新的中文分词工具包。pkuseg具有如下几个特点：
- **多领域**分词。不同于以往的通用中文分词工具，此工具包同时致力于为不同领域的数据提供个性化的预训练模型。根据待分词文本的领域特点，用户可以自由地选择不同的模型。 我们目前支持了**新闻**领域，**网络**领域，**医药**领域，**旅游**领域，以及混合领域的分词预训练模型。在使用中，如果用户明确待分词的领域，可加载对应的模型进行分词。如果用户无法确定具体领域，推荐使用在混合领域上训练的通用模型。各领域分词样例可参考txt。
- 更高的分词准确率。相比于其他的分词工具包，当使用相同的训练数据和测试数据，pkuseg可以取得更高的分词准确率。
- 支持用户**自训练**模型。支持用户使用全新的标注数据进行训练。
- 支持词性标注。

```python
import pkuseg

seg = pkuseg.pkuseg()   # 以默认配置加载模型
seg = pkuseg.pkuseg(model_name='medicine')  # 程序会自动下载所对应的细领域模型, news/web/tourism/medicine
seg = pkuseg.pkuseg(postag=True)  # 开启词性标注功能
seg = pkuseg.pkuseg(user_dict='my_dict.txt')  # 给定用户词典为当前目录下的"my_dict.txt"

text = seg.cut('我爱北京天安门')  # 进行分词
print(text)

# 对input.txt的文件分词输出到output.txt中, 开8个进程
pkuseg.test('input.txt', 'output.txt', nthread=8)
# 模型训练
pkuseg.train(trainFile, testFile, savedir, train_iter = 20, init_model)
```

[pkuseg](https://github.com/lancopku/pkuseg-python)分词示例，及词云可视化

```python
import pkuseg
from collections import Counter
import pprint
from wordcloud import WordCloud
import matplotlib.pyplot as plt

with open("data/santisanbuqu_liucixin.txt", encoding="utf-8") as f:
    content = f.read()

with open("data/CNENstopwords.txt", encoding="utf-8") as f:
    stopwords = f.read()

lexicon = ['章北海', '汪淼', '叶文洁']
seg = pkuseg.pkuseg(user_dict=lexicon)
text = seg.cut(content)

new_text = []
for w in text:
    if w not in stopwords:
        new_text.append(w)
counter = Counter(new_text)
pprint.pprint(counter.most_common(50))

cut_text = " ".join(new_text)

wordcloud = WordCloud(font_path="font/FZYingXueJW.TTF", background_color="white", width=800, height=600).generate(
    cut_text)

plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```

[img](https://www.biaodianfu.com/wp-content/uploads/2020/10/wordcloud.png) ![](https://www.biaodianfu.com/wp-content/uploads/2020/10/wordcloud.png)

### 哈工大LTP

- [LTP](https://github.com/HIT-SCIR/ltp), pyltp是python下对ltp(c++)的封装。
- 安装
  - pip install pyltp
  - 安装完毕后，还需要下载模型文件，文件[下载地址](http://ltp.ai/download.html)

【2023-9-12】huggingface 地址 [base2](https://huggingface.co/LTP/base2)

模型性能以及下载地址

| 深度学习模型 | 分词 | 词性 | 命名实体 | 语义角色 | 依存句法 | 语义依存 | 速度(句/S) |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| [Base](https://huggingface.co/LTP/base) | 98.7 | 98.5 | 95.4 | 80.6 | 89.5 | 75.2 | 39.12 |
| [Base1](https://huggingface.co/LTP/base1) | 99.22 | 98.73 | 96.39 | 79.28 | 89.57 | 76.57 | \--.-- |
| [Base2](https://huggingface.co/LTP/base2) | 99.18 | 98.69 | 95.97 | 79.49 | 90.19 | 76.62 | \--.-- |
| [Small](https://huggingface.co/LTP/small) | 98.4 | 98.2 | 94.3 | 78.4 | 88.3 | 74.7 | 43.13 |
| [Tiny](https://huggingface.co/LTP/tiny) | 96.8 | 97.1 | 91.6 | 70.9 | 83.8 | 70.1 | 53.22 |

其他

| 感知机算法 | 分词 | 词性 | 命名实体 | 速度(句/s) | 备注 |
| :-: | :-: | :-: | :-: | :-: | :-: |
| [Legacy](https://huggingface.co/LTP/legacy) | 97.93 | 98.41 | 94.28 | 21581.48 | [性能详情](https://huggingface.co/LTP/base2/blob/main/rust/ltp/README.md) |



```py
# pip install -U ltp ltp-core ltp-extension -i https://pypi.org/simple # 安装 ltp
import torch
from ltp import LTP

ltp = LTP("LTP/small")  # 默认加载 Small 模型

# 将模型移动到 GPU 上
if torch.cuda.is_available():
    # ltp.cuda()
    ltp.to("cuda")

output = ltp.pipeline(["他叫汤姆去拿外衣。"], tasks=["cws", "pos", "ner", "srl", "dep", "sdp"])
# 使用字典格式作为返回结果
print(output.cws)  # print(output[0]) / print(output['cws']) # 也可以使用下标访问
print(output.pos)
print(output.sdp)

# 使用感知机算法实现的分词、词性和命名实体识别，速度比较快，但是精度略低
ltp = LTP("LTP/legacy")
# cws, pos, ner = ltp.pipeline(["他叫汤姆去拿外衣。"], tasks=["cws", "ner"]).to_tuple() # error: NER 需要 词性标注任务的结果
cws, pos, ner = ltp.pipeline(["他叫汤姆去拿外衣。"], tasks=["cws", "pos", "ner"]).to_tuple()  # to tuple 可以自动转换为元组格式
# 使用元组格式作为返回结果
print(cws, pos, ner)
```


原工具包代码

```python
from ltp import LTP

ltp = LTP()  # 默认加载 Small 模型
seg, hidden = ltp.seg(["他叫汤姆去拿外衣。"])
pos = ltp.pos(hidden)
ner = ltp.ner(hidden)
srl = ltp.srl(hidden)
dep = ltp.dep(hidden)
sdp = ltp.sdp(hidden)
```

pyltp示例

```python
import os
from pyltp import SentenceSplitter
from pyltp import Segmentor
from pyltp import Postagger
from pyltp import NamedEntityRecognizer
from pyltp import Parser
from pyltp import SementicRoleLabeller

LTP_DATA_DIR = 'D:\CodeHub\CutSeg\ltp_data_v3.4.0'  # ltp模型目录的路径

# 分句
sents = SentenceSplitter.split('元芳你怎么看？我就趴窗口上看呗！')  # 分句
print('\n'.join(sents))

# 分词
cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`
segmentor = Segmentor()  # 初始化实例
segmentor.load(cws_model_path)  # 加载模型
words = segmentor.segment('元芳你怎么看')  # 分词
print('/'.join(words))
segmentor.release()  # 释放模型

# 词性标注
pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`
postagger = Postagger()  # 初始化实例
postagger.load(pos_model_path)  # 加载模型
postags = postagger.postag(words)  # 词性标注
print('/'.join(postags))
postagger.release()  # 释放模型

# 命名实体识别
ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`ner.model`
recognizer = NamedEntityRecognizer()  # 初始化实例
recognizer.load(ner_model_path)  # 加载模型
netags = recognizer.recognize(words, postags)  # 命名实体识别
print('/'.join(netags))
recognizer.release()  # 释放模型

# 依存句法分析
par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`
parser = Parser()  # 初始化实例
parser.load(par_model_path)  # 加载模型
arcs = parser.parse(words, postags)  # 句法分析
print("/".join("%d:%s" % (arc.head, arc.relation) for arc in arcs))
parser.release()  # 释放模型

# 语义角色标注
# srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl.model')  # 语义角色标注模型目录路径，模型目录为`pisrl.model`
srl_model_path = os.path.join(LTP_DATA_DIR,
                              'pisrl_win.model')  # windows系统要用不同的SRL模型文件，http://ospm9rsnd.bkt.clouddn.com/server/3.4.0/pisrl_win.model或http://model.scir.yunfutech.com/server/3.4.0/pisrl_win.model
labeller = SementicRoleLabeller()  # 初始化实例
labeller.load(srl_model_path)  # 加载模型
roles = labeller.label(words, postags, arcs)  # 语义角色标注
for role in roles:
    print(role.index, "".join(
        ["%s:(%d,%d)" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))
labeller.release()  # 释放模型
```



### 清华THULAC

[THULAC](https://github.com/thunlp/THULAC-Python)（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点：
- 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。
- 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达3％，词性标注的F1值可达到9％，与该数据集上最好方法效果相当。
- 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到3MB/s。

安装
- pip install thulac
- 安装时会默认下载模型文件。

示例：

```shell
#===========命令行============
#从input.txt读入，并将分词和词性标注结果输出到ouptut.txt中
python -m thulac input.txt output.txt
#如果只需要分词功能，可在增加参数"seg_only" 
python -m thulac input.txt output.txt seg_only
```
Python接口

```python
#=======================
import thulac

# 示例1
thu1 = thulac.thulac()  # 默认模式
text = thu1.cut("我爱北京天安门", text=True)  # 进行一句话分词
print(text)

# 示例2
thu2 = thulac.thulac(seg_only=True)  # 只进行分词，不进行词性标注
thu2.cut_f("input.txt", "output.txt")  # 对input.txt文件内容进行分词，输出到output.txt
# 完整格式
thulac(user_dict = None, model_path = None, T2S = False, seg_only = False, filt = False, max_length = 50000, deli='_', rm_space=False)

```
参数：
- user_dict：设置用户词典，用户词典中的词会被打上uw标签。词典中每一个词一行，UTF8编码
- model_path：设置模型文件所在文件夹，默认为models/
- T2S：默认False, 是否将句子从繁体转化为简体
- seg_only：默认False, 时候只进行分词，不进行词性标注
- filt：默认False, 是否使用过滤器去除一些没有意义的词语，例如“可以”。
- max_length：最大长度
- deli：默认为‘_’, 设置词与词性之间的分隔符
- rm_space：默认为False, 是否去掉原文本中的空格后再进行分词
- text：默认为False, 是否返回文本，不返回文本则返回一个二维数组([[word, tag]..]),seg_only模式下tag为空字符。


### 斯坦福 stanford NLP

多个版本：
- Stanford CoreNLP的源代码[stanfordnlp](https://stanfordnlp.github.io/stanfordnlp/)是使用Java写的，提供了Server方式进行交互。
  - [demo](http://nlp.stanford.edu:8080/corenlp/process)，速度慢，[新接口](http://corenlp.run/)
  - pip install stanfordnlp
- [stanfordcorenlp](https://stanfordnlp.github.io/CoreNLP/)（推荐使用）：一个对Stanford CoreNLP进行了封装的Python工具包。

安装流程：
- 安装Python包：pip install stanfordcorenlp
- 下载安装JDK（版本要求JDK 1.8以上）
- 下载安装[Stanford CoreNLP文件即语言包](https://github.com/stanfordnlp/CoreNLP)。解压CoreNLP文件，并将中文包放在解压文件的根目录。

python调用

```python
from stanfordcorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP('D:\\stanford-corenlp-full-2018-02-27', lang='zh')
sentence = '斯坦福大学自然语言处理包StanfordNLP'
print(nlp.word_tokenize(sentence))  # 分词, ['斯坦福', '大学', '自然', '语言', '处理', '包', 'StanfordNLP']
print(nlp.pos_tag(sentence))  # 词性标注, [('斯坦福', 'NR'), ('大学', 'NN'), ('自然', 'AD'), ('语言', 'NN'), ('处理', 'VV'), ('包', 'NN'), ('StanfordNLP', 'NN')]
print(nlp.ner(sentence))  # 实体识别, [('斯坦福', 'ORGANIZATION'), ('大学', 'ORGANIZATION'), ('自然', 'O'), ('语言', 'O'), ('处理', 'O'), ('包', 'O'), ('StanfordNLP', 'O')]
print(nlp.parse(sentence))  # 语法树, 略
print(nlp.dependency_parse(sentence))  # 依存句法, [('ROOT', 0, 5), ('compound:nn', 2, 1), ('nsubj', 5, 2), ('advmod', 5, 3), ('nsubj', 5, 4), ('compound:nn', 7, 6), ('dobj', 5, 7)]

#========服务方式调用=========
# 启动服务：java -mx6g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 5000
from stanfordcorenlp import StanfordCoreNLP
nlp = StanfordCoreNLP('http://localhost', port=9000, lang='zh')
sentence = '斯坦福大学自然语言处理包StanfordNLP'
print(nlp.word_tokenize(sentence))  # 分词
print(nlp.pos_tag(sentence))  # 词性标注
print(nlp.ner(sentence))  # 实体识别
print(nlp.parse(sentence))  # 语法树
print(nlp.dependency_parse(sentence))  # 依存句法
```

### TextBlob――nltk加工

[TextBlob](https://github.com/sloria/TextBlob)是一个用Python编写的开源的文本处理库。是自然语言工具包[NLTK](https://www.biaodianfu.com/nltk.html)的一个包装器，目的是抽象其复杂性。它可以用来执行很多自然语言处理的任务，比如，词性标注，名词性成分提取，情感分析，文本翻译，等等。

主要特性：
- 名词短语提取
- 词性标记
- 情绪分析
- 分类
- 由 Google 翻译提供的翻译和检测
- 标记（将文本分割成单词和句子）
- 词句、短语频率
- 解析
- n-gram
- 词变化（复数和单数化）和词形化
- 拼写校正
- 通过扩展添加新模型或语言
- WordNet 集成

安装
- pip install textblob

```python
from textblob import TextBlob
text = 'I love natural language processing! I am not like fish!'
blob = TextBlob(text) # 可能报错，此时需要安装nltk
```
完整示例

```python
from textblob import TextBlob
from textblob import Word

text = 'I love natural language processing! I am not like fish!'
blob = TextBlob(text)

print(blob.words)  # 分词
print(blob.tags)  # 词性标注
print(blob.noun_phrases)  # 短语抽取

# 分句+计算句子情感值
# 使用TextBlob情感分析的结果，以元组的方式进行返回，形式如(polarity, subjectivity).
# polarity 是一个范围为 [-1.0 , 1.0 ] 的浮点数, 正数表示积极，负数表示消极。
# subjectivity 是一个 范围为 [0.0 , 1.0 ] 的浮点数，其中 0.0 表示 客观，1.0表示主观的。
for sentence in blob.sentences:
    print(sentence + '------>' + str(sentence.sentiment.polarity))

# 词语变形(Words Inflection)
w = Word("apple")
print(w.pluralize())  # 变复数
print(w.pluralize().singularize())  # 变单数

# 词干化(Words Lemmatization)
w = Word('went')
print(w.lemmatize('v'))
w = Word('octopi')
print(w.lemmatize())

# 拼写纠正(Spelling Correction)
sen = 'I lvoe naturl language processing!'
sen = TextBlob(sen)
print(sen.correct())

# 句法分析(Parsing)
text = TextBlob('I lvoe naturl language processing!')
print(text.parse())

# N-Grams
text = TextBlob('I lvoe naturl language processing!')
print(text.ngrams(n=2))
```


### SnowNLP――繁体转换、idf，相似度

[SnowNLP](https://github.com/isnowfy/snownlp)是一个python写的类库，可以方便的处理中文文本内容，是受到了TextBlob的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。

主要特性：
- 中文分词（Character-Based Generative Model）
- 词性标注（TnT3-gram 隐马）
- 情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决）
- 文本分类（Naive Bayes）
- **转换成拼音**（Trie树实现的最大匹配）
- **繁体转简体**（Trie树实现的最大匹配）
- 提取文本关键词（TextRank算法）
- 提取**文本摘要**（TextRank算法）
- tf，idf
- Tokenization（分割成句子）
- **文本相似**（BM25）

安装：
- pip install snownlp

代码

```python
from snownlp import SnowNLP

s1 = SnowNLP('这个东西真心很赞')
print(s1.words) # ['这个', '东西', '真心', '很', '赞']
print(list(s1.tags)) # [('这个', 'r'), ('东西', 'n'), ('真心', 'd'), ('很', 'd'), ('赞', 'Vg')]
print(s1.sentiments) # 0.9769551298267365
print(s1.pinyin) # ['zhe', 'ge', 'dong', 'xi', 'zhen', 'xin', 'hen', 'zan']

s2 = SnowNLP('「繁w字」「繁w中文」的叫法在_骋嗪艹Ｒ。')
print(s2.han) # 「繁体字」「繁体中文」的叫法在台湾亦很常见。

text = '''
自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。
它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。
自然语言处理是一门融语言学、计算机科学、数学于一体的科学。
因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，
所以它与语言学的研究有着密切的联系，但又有重要的区别。
自然语言处理并不是一般地研究自然语言，
而在于研制能有效地实现自然语言通信的计算机系统，
特别是其中的软件系统。因而它是计算机科学的一部分。
'''
s3 = SnowNLP(text)
print(s3.keywords(3)) # ['语言', '自然', '计算机']
print(s3.summary(3)) # ['因而它是计算机科学的一部分', '自然语言处理是计算机科学领域与人工智能领域中的一个重要方向', '自然语言处理是一门融语言学、计算机科学、数学于一体的科学']
print(s3.sentences) # ['自然语言处理是计算机科学领域与人工智能领域中的一个重要方向', '它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法', '自然语言处理是一门融语言学、计算机科学、数学于一体的科学', '因此', '这一领域的研究将涉及自然语言', '即人们日常使用的语言', '所以它与语言学的研究有着密切的联系', '但又有重要的区别', '自然语言处理并不是一般地研究自然语言', '而在于研制能有效地实现自然语言通信的计算机系统', '特别是其中的软件系统', '因而它是计算机科学的一部分']

s4 = SnowNLP([['这篇', '文章'], ['那篇', '论文'], ['这个']])
print(s4.tf) # [{'这篇': 1, '文章': 1}, {'那篇': 1, '论文': 1}, {'这个': 1}]
print(s4.idf) # {'这篇': 0.5108256237659907, '文章': 0.5108256237659907, '那篇': 0.5108256237659907, '论文': 0.5108256237659907, '这个': 0.5108256237659907}
print(s4.sim([u'文章'])) # [0.4686473612532025, 0, 0]
```


### DeepNLP――神经网络版

[DeepNLP](https://github.com/rockingdingo/deepnlp)项目是基于Tensorflow平台的一个python版本的NLP套装, 目的在于将Tensorflow深度学习平台上的模块，结合 最新的一些算法，提供NLP基础模块的支持，并支持其他更加复杂的任务的拓展，如生成式文摘等等。

NLP 套装模块
- 分词 Word Segmentation/Tokenization：线性链条件随机场 Linear Chain CRF, 基于CRF++包来实现
- 词性标注 Part-of-speech (POS)：单向LSTM/ 双向BI-LSTM, 基于Tensorflow实现
- 命名实体识别 Named-entity-recognition(NER)：单向LSTM/ 双向BI-LSTM/ LSTM-CRF 结合网络, 基于Tensorflow实现
- 依存句法分析 Dependency Parsing (Parse)：基于arc-standard system的神经网络的parser
- 自动生成式文摘 Textsum (Seq2Seq-Attention)
- 关键句子抽取 Textrank
- 文本分类 Textcnn (WIP)
- 预训练模型：
  - 中文: 基于人民日报语料和微博混合语料: 分词, 词性标注, 实体识别
- 可调用 Web Restful API
- 计划中: 句法分析 Parsing

```python
# pip install deepnlp
import deepnlp
# Download all the modules
deepnlp.download()
# Download specific module
deepnlp.download('segment')
deepnlp.download('pos')
deepnlp.download('ner')
deepnlp.download('parse')
# Download module and domain-specific model
deepnlp.download(module = 'pos', name = 'en') 
deepnlp.download(module = 'ner', name = 'zh_entertainment')
```

示例

```python
from deepnlp import segmenter, pos_tagger, ner_tagger, nn_parser
from deepnlp import pipeline

# 分词模块
tokenizer = segmenter.load_model(name='zh')
text = "我爱吃北京烤鸭"
seg_list = tokenizer.seg(text)
text_seg = " ".join(seg_list)
print(text_seg)

# 词性标注
p_tagger = pos_tagger.load_model(name='zh')
tagging = p_tagger.predict(seg_list)
for (w, t) in tagging:
    pair = w + "/" + t
print(pair)

# 命名实体识别
n_tagger = ner_tagger.load_model(name='zh')  # Base LSTM Based Model
tagset_entertainment = ['city', 'district', 'area']
tagging = n_tagger.predict(seg_list, tagset=tagset_entertainment)
for (w, t) in tagging:
    pair = w + "/" + t
    print(pair)

# 依存句法分析
parser = nn_parser.load_model(name='zh')
words = ['它', '熟悉', '一个', '民族', '的', '历史']
tags = ['r', 'v', 'm', 'n', 'u', 'n']
dep_tree = parser.predict(words, tags)
num_token = dep_tree.count()
print("id\tword\tpos\thead\tlabel")
for i in range(num_token):
    cur_id = int(dep_tree.tree[i + 1].id)
    cur_form = str(dep_tree.tree[i + 1].form)
    cur_pos = str(dep_tree.tree[i + 1].pos)
    cur_head = str(dep_tree.tree[i + 1].head)
    cur_label = str(dep_tree.tree[i + 1].deprel)
    print("%d\t%s\t%s\t%s\t%s" % (cur_id, cur_form, cur_pos, cur_head, cur_label))

# Pipeline
p = pipeline.load_model('zh')
text = "我爱吃北京烤鸭"
res = p.analyze(text)
print(res[0])
print(res[1])
print(res[2])
words = p.segment(text)
pos_tagging = p.tag_pos(words)
ner_tagging = p.tag_ner(words)
print(list(pos_tagging))
print(ner_tagging)
```

### 小明NLP――拼写检查、偏旁部首

小明NLP [xmnlp](https://github.com/SeanLee97/xmnlp)的主要功能：
- 中文分词 & 词性标注
- 支持繁w
- 支持自定义词典
- 中文拼写检查
- 文本摘要 & 关键词提取
- 情感分析
- 文本转拼音
- 获取汉字偏旁部首

```python
mport xmnlp
print(xmnlp.radical('自然语言处理'))
print(xmnlp.radical('自然Z言理'))
```

### spaCy――（工业界）

[spaCy](https://spacy.io/) 是一个Python自然语言处理工具包，诞生于2014年年中，号称“Industrial-Strength Natural Language Processing in Python”，是具有工业级强度的Python NLP工具包。spaCy里大量使用了 Cython 来提高相关模块的性能，这个区别于学术性质更浓的NLTK，因此具有了业界应用的实际价值。

主要特性：
- 分词
- 命名实体识别
- 多语言支持（号称支持53种语言）
- 针对11种语言的23种统计模型
- 预训练词向量
- 高性能
- 轻松的整合深度学习
- 词性标注
- 依存句法分析
- 句法驱动的句子切分
- 用于语法和命名实体识别的内置可视化工具
- 方便的字符串到哈希映射
- 导出到numpy数据数组
- 高效的二进制序列化
- 易于模型打包和部署
- 稳健，精确评估

先执行包的安装：pip install spacy，再执行数据集和[模型](https://spacy.io/models)的下载。
- 英文：python -m spacy download en_core_web_sm
- 无官方中文，小道中文版
  - 模型：https://github.com/howl-anderson/Chinese_models_for_SpaCy
  - 执行：pip install ./zh_core_web_sm-2.0.5.tar.gz

```python
import spacy

nlp = spacy.load("en_core_web_sm") # 英文
nlp = spacy.load("zh_core_web_sm") # 中文

doc = nlp("王小明在北京的清华大学读书")
# 分词 词性标注
for token in doc:
    print(token, token.pos_, token.pos)
# 命名实体识别（NER）
for ent in doc.ents:
    print(ent, ent.label_, ent.label)
# 名词短语提取
for np in doc.noun_chunks:
    print(np)
# 依存关系
for token in doc:
    print(token.text, token.dep_, token.head)
# 文本相似度
doc1 = nlp(u"my fries were super gross")
doc2 = nlp(u"such disgusting fries")
similarity = doc1.similarity(doc2)
print(similarity)
# 句法树展示
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
          token.shape_, token.is_alpha, token.is_stop, token.has_vector,
          token.ent_iob_, token.ent_type_,
          token.vector_norm, token.is_oov)
spacy.displacy.serve(doc)
```

同时启动web服务

![](https://www.biaodianfu.com/wp-content/uploads/2020/10/displatcy-768x461.png)


### 复旦FoolNLTK――最准（非最快）的开源中文工具包

[FoolNLTK](https://github.com/rockyzhengwu/FoolNLTK) [文章](FoolNLTK：一个便捷的中文处理工具包)介绍, 复旦大学，一个使用双向 LSTM （BiLSTM 模型）构建的便捷的中文处理工具包，该工具不仅可以实现分词、词性标注和命名实体识别，同时还能使用用户自定义字典加强分词的效果。根据该项目所述，这个中文工具包可能不是最快的开源中文分词，但很可能是最准的开源中文分词。
- 基于神经网络的方法，往往使用「字向量 + 双向 LSTM + CRF」模型，利用神经网络来学习特征，将传统 CRF 中的人工特征工程量将到最低。
- ![](https://www.biaodianfu.com/wp-content/uploads/2020/10/BiLSTM.png)

使用示例：

```python
# pip install tensorflow==1.15 # tf版本限制
import fool
sentence = "中文分词测试：我爱北京天安门"
print(fool.cut(sentence))  # 分词, [['中文', '分词', '测试', '：', '我', '爱', '北京', '天安', '门']]
print(fool.pos_cut(sentence))  # 词性标注, [[('中文', 'nz'), ('分词', 'n'), ('测试', 'n'), ('：', 'wm'), ('我', 'r'), ('爱', 'v'), ('北京', 'ns'), ('天安', 'nz'), ('门', 'n')]]
print(fool.analysis(sentence)[1]) # 明名实体识别, [[(9, 15, 'company', '北京天安门')]]
```

### HanLP――支持TensorFlow 2

- 借助世界上最大的多语种语料库，HanLP支持包括简繁中英日俄法德在内的104种语言上的10种联合任务：分词（粗分、细分2个标准，强制、合并、校正3种词典模式）、词性标注（PKU、863、CTB、UD四套词性规范）、命名实体识别（PKU、MSRA、OntoNotes三套规范）、依存句法分析（SD、UD规范）、成分句法分析、语义依存分析（SemEval16、DM、PAS、PSD四套规范）、语义角色标注、词干提取、词法语法特征提取、抽象意义表示（AMR）。
- HanLP已经被广泛用于Lucene、Solr、ElasticSearch、Hadoop、Android、Resin等平台，有大量开源作者开发各种插件与拓展，并且被包装或移植到Python、C#、R、JavaScript等语言上去。 基于深度学习的HanLP2.x已于2020年初发布，面向下一个十年的前沿NLP技术，与1.x相辅相成，平行发展。

[HanLP](https://www.hanlp.com/)原先是一个JAVA版本的自然语言处理工具包，在目前的升级中已经支持了Python。当前已经支持基于 TensorFlow 2.x。HanLP具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。特殊功能：
- 短语提取
- 拼音转换
- 繁简转换
- 文本推荐

#### Demo 体验

- [Demo体验](https://hanlp.hankcs.com/?sentence=%E8%B4%9D%E5%A3%B3%E5%92%8C%E9%93%BE%E5%AE%B6%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB%EF%BC%8C%E9%83%BD%E6%98%AF%E4%B8%AD%E4%BB%8B%E5%85%AC%E5%8F%B8%E5%90%97&mul=off)

[官方体验地址](https://hanlp.hankcs.com/)

<iframe src="https://hanlp.hankcs.com/"></iframe>

#### 代码调用

```python
# https://github.com/hankcs/pyhanlp
from pyhanlp import *

sen = '贝壳和链家什么关系,左晖和stanly又是谁'
CustomDictionary.add("链家", "n") # 自定义字典
# 加载模型
NLPTokenizer = pyhanlp.JClass('com.hankcs.hanlp.tokenizer.NLPTokenizer')  # 加载模型
NER = NLPTokenizer.segment(text)  # 命名实体识别

res = HanLP.segment(sen) # pos
for term in res:
    print(term, term.word, str(term.nature))
res = HanLP.extractKeyword(sen, 10)
print(res)
summary  = HanLP.extractSummary(sen, 5)
print(summary)
print('分词：', HanLP.segment(sen))
#TextRankKeyword = JClass("com.hankcs.hanlp.summary.TextRankKeyword")
print('关键词：', HanLP.extractKeyword(sen, 2))
print('自动摘要：', HanLP.extractSummary(sen, 3))
sentence_list = HanLP.getSummary(sen, 50)  #提取短语,同时指定摘要的最大长度 
print('自动摘要：', sentence_list)
NER=HanLP.newSegment().enableNameRecognize(True)
print('中文名识别：', NER.seg(sen))
person_ner = HanLP.newSegment().enableTranslatedNameRecognize(True)
print('英文名识别：', person_ner.seg(sen))
print('句法分析：', HanLP.parseDependency(sen))
PerceptronLexicalAnalyzer=JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer=PerceptronLexicalAnalyzer()
print('感知机句法分析：', analyzer.analyze(sen))
print('短语提取：', HanLP.extractPhrase(sen, 5))
print('简体：', HanLP.convertToSimplifiedChinese("我圩匀徽Z言理技g！"))
print('繁体：', HanLP.convertToTraditionalChinese("我爱自然语言处理技术！"))
pinyinList = HanLP.convertToPinyinList(sen)
for pinyin in pinyinList:
    print(pinyin.getPinyinWithoutTone(),pinyin.getTone(), pinyin, pinyin.getPinyinWithToneMark())
# 可视化服务：shell命令
hanlp serve  # http://localhost:8765
# 命令行方式传入
hanlp segment <<< '欢迎新老师生前来就餐' 
```

## CRF工具包

CRF++工具包，[CRF++: Yet Another CRF toolkit](https://taku910.github.io/crfpp/)

### （1）编译安装

步骤：[python调用CRF++工具包](https://www.jianshu.com/p/9a8425110f43)

```shell
# 拉取github上的源文件
git clone https://github.com/taku910/crfpp.git
cd crfpp

# 去除找不到winmain.h的错误 [2021-7-15] sed命令有误，手工删除winmain引用即可
sed -i '/#include "winmain.h"/d' crf_test.cpp
sed -i '/#include "winmain.h"/d' crf_learn.cpp

# 编译安装
./configure
make && make install

# 配置文件并导入，消除【错误2】 【2021-7-15】mac下没有ld.so.conf文件
echo "include /usr/local/lib" >> /etc/ld.so.conf
/sbin/ldconfig -v
# 导入python
cd python
python3 setup.py install

```

###  （2）直接安装

命令：
- pip install crfpy

### 使用

- 训练模型
  - crf_learn template train.data model 
- 测试使用

```python
import CRFPP
import codecs

# 加载模型
tagger=CRFPP.Tagger(r'-m C:\Users\secoo\Desktop\CRF++\model')

# 加载测试文件
input_data = codecs.open(r'C:\Users\secoo\Desktop\CRF++\shangde.txt', 'r', 'utf-8')
word_str=input_data.readlines()
word_str=''.join(word_str)
# 测试
print(tagger.parse(word_str))

```


## 文本工具



- 【2021-1-19】[python高效使用统计语言模型kenlm：新词发现、分词、智能纠错等](https://linux.ctolib.com/mattzheng-py-kenlm-model.html)
- kenlm的优点（关于kenlm工具训练统计语言模型）： 训练语言模型用的是传统的“统计+平滑”的方法，使用kenlm这个工具来训练。它快速，节省内存，最重要的是，允许在开源许可下使用多核处理器。 kenlm是一个C++编写的语言模型工具，具有速度快、占用内存小的特点，也提供了Python接口。
- 额外需要加载的库：
   - kenlm
   - pypinyin
   - pycorrector
- 粗略整理代码: 
   - kenlm：[mattzheng/py-kenlm-model](https://github.com/mattzheng/py-kenlm-model)
      - 安装：pip install https://github.com/kpu/kenlm/archive/master.zip
   - 新词发现：[mattzheng/word-discovery](https://github.com/mattzheng/word-discovery)
- kenlm模型训练，详见[地址](https://www.cnblogs.com/zidiancao/p/6067147.html)
  - 文件必须是分词以后的文件。
  - -o后面的5表示的是5-gram,一般取到3即可，但可以结合自己实际情况判断

```shell
# 训练命令
bin/lmplz -o 5 --verbose_header --text data/chat_log.txt --arpa result/log.arpa --vocab_file result/log.vocab
# 结果后得到arpa文件
```

### 编码


#### chardet

检测文本编码方式

### 拼音

#### pypinyin

- 拼音模块涉及到了pypinyin，用来识别汉字的拼音，还有非常多种的模式：

```python
from pypinyin import lazy_pinyin, Style
	# Python 中拼音库 PyPinyin 的用法
	# https://blog.csdn.net/devcloud/article/details/95066038

tts = ['BOPOMOFO', 'BOPOMOFO_FIRST', 'CYRILLIC', 'CYRILLIC_FIRST', 'FINALS', 'FINALS_TONE',
 'FINALS_TONE2', 'FINALS_TONE3', 'FIRST_LETTER', 'INITIALS', 'NORMAL', 'TONE', 'TONE2', 'TONE3']
for tt in tts:
    print(tt,lazy_pinyin('聪明的小兔子吃', style=eval('Style.{}'.format(tt))   ))

pinyin('中心') # [['zhōng'], ['xīn']]
pinyin('中心', heteronym=True)  # 启用多音字模式，[['zhōng', 'zhòng'], ['xīn']]
pinyin('中心', style=Style.FIRST_LETTER)  # 设置拼音风格，[['z'], ['x']]
pinyin('中心', style=Style.TONE2, heteronym=True)，[['zho1ng', 'zho4ng'], ['xi1n']]
pinyin('中心', style=Style.BOPOMOFO)  # 注音风格，[['ㄓㄨㄥ'], ['ㄒㄧㄣ']]
pinyin('中心', style=Style.CYRILLIC)  # 俄语字母风格，[['чжун1'], ['синь1']]
lazy_pinyin('中心')  # 不考虑多音字的情况，['zhong', 'xin']

```

其中结果为：

```shell
BOPOMOFO ['ㄘㄨㄥ', 'ㄇㄧㄥ@', 'ㄉㄜB', 'ㄒㄧㄠˇ', 'ㄊㄨA', 'ㄗB', 'ㄔ']
BOPOMOFO_FIRST ['ㄘ', 'ㄇ', 'ㄉ', 'ㄒ', 'ㄊ', 'ㄗ', 'ㄔ']
CYRILLIC ['цун1', 'мин2', 'дэ', 'сяо3', 'ту4', 'цзы', 'чи1']
CYRILLIC_FIRST ['ц', 'м', 'д', 'с', 'т', 'ц', 'ч']
FINALS ['ong', 'ing', 'e', 'iao', 'u', 'i', 'i']
FINALS_TONE ['ōng', 'íng', 'e', 'iǎo', 'ù', 'i', 'ī']
FINALS_TONE2 ['o1ng', 'i2ng', 'e', 'ia3o', 'u4', 'i', 'i1']
FINALS_TONE3 ['ong1', 'ing2', 'e', 'iao3', 'u4', 'i', 'i1']
FIRST_LETTER ['c', 'm', 'd', 'x', 't', 'z', 'c']
INITIALS ['c', 'm', 'd', 'x', 't', 'z', 'ch']
NORMAL ['cong', 'ming', 'de', 'xiao', 'tu', 'zi', 'chi']
TONE ['cōng', 'míng', 'de', 'xiǎo', 'tù', 'zi', 'chī']
TONE2 ['co1ng', 'mi2ng', 'de', 'xia3o', 'tu4', 'zi', 'chi1']
TONE3 ['cong1', 'ming2', 'de', 'xiao3', 'tu4', 'zi', 'chi1']
```

可以看出不同的style可以得到不同拼音形式。

命令行工具：

```shell
pypinyin 音乐 # yīn yuè
pypinyin -h
pypinyin '银行,行道树,人行道'  # yín háng ， háng dào shù , rén xíng dào
```

### 纠错

#### pycorrector

pycorrector的detect，可以返回，错误字的信息

```python
import pycorrector

sentence = '这瓶洗棉奶用着狠不错'
idx_errors = pycorrector.detect(sentence)
# [['这瓶', 0, 2, 'word'], ['棉奶', 3, 5, 'word']]
#correct是专门用来纠正：
pycorrector.correct(sentence)
```

pycorrector与kenlm纠错对比
- 来对比一下pycorrector自带的纠错和本次实验的纠错：

```python
import pycorrector
sentence = '这瓶洗棉奶用着狠不错'
idx_errors = pycorrector.detect(sentence)

correct = []
for ide in idx_errors:
    right_word = km.find_best_word(ide[0],ngrams_,freqs = 0)
    if right_word != ide[0]:
        correct.append([right_word] + ide)

print('错误：',idx_errors)
print('pycorrector的结果：',pycorrector.correct(sentence))
print('kenlm的结果：',correct)
```

输出结果：

```js
> 错误： [['这瓶', 0, 2, 'word'], ['棉奶', 3, 5, 'word']]
> pycorrector的结果： ('这瓶洗面奶用着狠不错', [['棉奶', '面奶', 3, 5]])
> kenlm的结果： [['面奶', '棉奶', 3, 5, 'word']]

其他类似的案例：

sentence =  '少先队员因该给老人让坐'

> 错误： [['因该', 4, 6, 'word'], ['坐', 10, 11, 'char']]
> pycorrector的结果： ('少先队员应该给老人让座', [['因该', '应该', 4, 6], ['坐', '座', 10, 11]])
> kenlm的结果： [['应该', '因该', 4, 6, 'word']]
这里笔者的简陋规则暴露问题了，只能对2个字以上的进行判定。

sentence = '绿茶净华可以舒缓痘痘机肤'

> 错误： [['净华', 2, 4, 'word'], ['机肤', 10, 12, 'word']]
> pycorrector的结果： ('绿茶净化可以舒缓痘痘肌肤', [['净华', '净化', 2, 4], ['机肤', '肌肤', 10, 12]])
> kenlm的结果： [['精华', '净华', 2, 4, 'word'], ['肌肤', '机肤', 10, 12, 'word']]
```

### 语种识别

识别语种的工具包
- [langid](https://github.com/saffsd/langid.py)
- cdl3
- langdetect

国际语种简称映射表
- [参考](https://blog.csdn.net/enter89/article/details/114657194)

#### langid

[langid.py](https://github.com/saffsd/langid.py) 是一个独立语言识别（LangID）工具。

设计原则如下：
1. 快速 
2. 在大量语言（目前为 97 种）上进行预训练 
3. 对特定领域的功能不敏感（例如 HTML / XML 标记）
4. 具有最小依赖关系的单个. py 文件 
5. 可部署为 Web 服务


`langid.classify(s3)` 输出探测语言类型及其 confidence score
- 其confidence score[计算方法](https://jblevins.org/log/log-sum-exp)

候选语种

langid.py comes pre-trained on 97 languages (ISO 639-1 codes given):

```
af, am, an, ar, as, az, be, bg, bn, br, bs, ca, cs, cy, da, de, dz, el, en, eo, es, et, eu, fa, fi, fo, fr, ga, gl, gu, he, hi, hr, ht, hu, hy, id, is, it, ja, jv, ka, kk, km, kn, ko, ku, ky, la, lb, lo, lt, lv, mg, mk, ml, mn, mr, ms, mt, nb, ne, nl, nn, no, oc, or, pa, pl, ps, pt, qu, ro, ru, rw, se, si, sk, sl, sq, sr, sv, sw, ta, te, th, tl, tr, ug, uk, ur, vi, vo, wa, xh, zh, zu
```


安装


```sh
pip install langid
```

使用

命令

```sh
langid.py [options]

python langid.py
```

以下是对图中参数函数的总结：

1. **-h, --help**
   - 功能：显示帮助信息并退出。
2. **-s, --serve**
   - 功能：启动网络服务。
3. **--host=HOST**
   - 功能：绑定的主机/IP地址。
4. **--port=PORT**
   - 功能：监听端口。
5. **-v**
   - 功能：增加详细程度（重复使用可增加详细程度）。
6. **-m MODEL**
   - 功能：加载模型。
7. **-l LANGS, --langs=LANGS**
   - 功能：以逗号分隔的目标ISO639语言代码集（例如en,de）。
8. **-r, --remote**
   - 功能：自动检测远程访问的IP地址。
9. **-b, --batch**
   - 功能：命令行中指定文件列表。
10. **--demo**
    - 功能：启动浏览器内的演示应用程序。
11. **-d, --dist**
    - 功能：显示语言的完整分布。
12. **-u URL, --url=URL**
    - 功能：URL的语言ID。
13. **--line**
    - 功能：逐行处理管道，而不是作为文档处理。
14. **-n, --normalize**
    - 功能：将置信度分数归一化为概率值。

示例

```sh
python langid.py
>>> This is a test
('en', -54.41310358047485)
>>> Questa e una prova
('it', -35.41771221160889)
# ------------------------
python langid.py -s # 启动本地web服务 http://localhost:9008/detect 
curl -d "q=This is a test" localhost:9008/detect
# 返回 json
# {"responseData": {"confidence": -54.41310358047485, "language": "en"}, "responseDetails": null, 
"responseStatus": 200}
curl -T readme.rst localhost:9008/detect # 直接终端输出
echo "This is a test" | curl -d @- localhost:9008/detect

python langid.py < README.rst
# ('en', -22552.496054649353) # 未归一化
python langid.py -n < README.rst
# ('en', 1.0)  # 归一化
python langid.py -n -l it,fr # 限制语种范围
```


代码

```py
import langid

s1 = "本篇博客主要介绍两款语言探测工具，用于区分文本到底是什么语言，"
s2 = 'We are pleased to introduce today a new technology C Record Matching Cthat automatically finds relevant historical records for every family tree on MyHerit'
s3 = "Javigator：Java代码导读及分析管理工具的设计"

langid.set_languages(['de','fr','it']) # 限制语种集合

print(langid.classify(s1))  # ('zh', -461.7451400756836)   # 中文
print(langid.classify(s2))  # ('en', -264.470148563385)   # 英文
print(langid.classify(s3))  # ('zh', -206.2425878047943)   # 中文
```


```py
import langid
def lang_by_langid(para_text):
    '''
    语种识别,根据langid包
    '''
    ret = langid.classify(para_text)
    print(f"langid:{ret}")
    return ret[0]
 
lang_arr=[
        "「i-FILTER」に}数の脆弱性。入企Iは最新版に更新を（JVN#32155106） | セキュリティ策のラック",
        "疑似 KimsukyAPT 组织最新攻击活动样本分析",
        "??????? ?? ???",
        "Ruth Schofield Is Joining Heimdal? Security’s Executive All-Star Team as UK Sales Director",
         'いつもながら素长洹https://t.co/Ei2fJGhZTC'
    ]
 
for item in lang_arr:
    print(f"text:{item}")
    tmp = lang_by_langid(item)
```

结果

```js
text:「i-FILTER」に}数の脆弱性。入企Iは最新版に更新を（JVN#32155106） | セキュリティ策のラック
langid:('ja', -675.1671650409698)
 
text:疑似 KimsukyAPT 组织最新攻击活动样本分析
langid:('zh', -202.03950572013855)
 
text:??????? ?? ???
langid:('ko', -248.97621655464172)
 
text:Ruth Schofield Is Joining Heimdal? Security’s Executive All-Star Team as UK Sales Director
langid:('en', -54.81398916244507)
 
text:いつもながら素长洹https://t.co/Ei2fJGhZTC
langid:('ja', -255.61449241638184)
```


#### langdetect


[langdetect](https://code.google.com/archive/p/language-detection)

安装

```sh
pip install langdetect
```

使用

```py
from langdetect import detect
from langdetect import detect_langs

s1 = "本篇博客主要介绍两款语言探测工具，用于区分文本到底是什么语言，"
s2 = 'We are pleased to introduce today a new technology C Record Matching Cthat automatically finds relevant historical records for every family tree on MyHerit'
s3 = "Javigator：Java代码导读及分析管理工具的设计"

print(detect(s1))
print(detect(s2))
print(detect(s3))     # detect()输出探测出的语言类型
print(detect_langs(s3))    # detect_langs()输出探测出的所有语言类型及其所占的比例
```


#### cdl3

【2024-3-4】Google的语种检测工具包 [cld3](https://github.com/google/cld3)
- 支持 100 种语言
- 浅层神经网络实现语种检测
- ![](https://github.com/google/cld3/blob/master/model.png?raw=true)

Python版本
- [pycld3](https://github.com/bsolomon1124/pycld3)

安装

```sh
pip install pycld3
```


```py
import cld3

cld3.get_language("影包含夂虻淖化以及自然Y源的枯竭程度")
# LanguagePrediction(language='zh', probability=0.999969482421875, is_reliable=True, proportion=1.0)
```

错误：
- [ERROR: Failed building wheel for pycld3](https://github.com/bsolomon1124/pycld3/issues/35)
- 解决需要 Python 3.10 以下版本



### 流畅度

#### kenlm

[kenlm](http://kheafield.com/code/kenlm/) 是一款高效的语言模型库，可用于训练和评估`n-gram`语言模型。
- C++库和 Python 绑定，可在Python中使用。
- KenLM Python 库提供加载和使用KenLM语言模型，包括: 计算**句子概率**、生成句子、计算句子**困惑度**等。
- KenLM估计、过滤和查询语言模型。

流算法快速和可扩展的。可扩展的改进Kneser-Ney语言模型估计。如本文所示，查询速度快，内存少。KenLM是更快更小的语言模型查询。
                        
原文链接：https://blog.csdn.net/qq_41185868/article/details/129659023

1. 工具介绍：[kenlm](http://kheafield.com/code/kenlm/)
2. GitHub：[kenlm](https://github.com/i3thuan5/kenlm)
3. 工具[下载地址](http://kheafield.com/code/kenlm.tar.gz)

安装
- 使用KenLM之前，需要先安装 KenLM C++库。
- 然后，可以使用 pip安装 KenLM Python库

```sh
pip install pypi-kenlm
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pypi-kenlm
```


【2021-4-20】[Kenlm文本流畅度检测](https://zhuanlan.zhihu.com/p/265677864)

```python
import kenlm

model = kenlm.Model('lm/test.arpa')

sen = 'this is a sentence .'
# 计算句子概率
log_prob = model.score(sentence)

# 将对数概率转换为标准概率
prob = 10 ** log_prob
# prob = kenlm.exp(log_prob) # 或

print(model.score(sen)) # 计算句子概率
print(model.score(sen, bos = True, eos = True))

# 计算句子困惑度
ppl = model.perplexity([sen])

# 生成句子
generated_sentence = model.generate()


# 豆瓣语料模型
model = kenlm.Model('build/my_model/douban.arpa')
test_list = ["我是一名程序员",    "是一名程序员我",    "我 是 一名 程序员",    "是 一名 程序员 我",    "一名 是 程序员 我",    "一名 程序员 是 我",    "我 我 我 啊 嗯 你 一名 是 吗 噢噢 程序员 是 吗 我",    "我 等一下 去 交 进去 的 觉得 我 不卡 我 要 能 给我 媳妇 上 了 取得 这样的 他 方便 一点 他 就 觉得 9个人 要 办个 手续 这样 的"]
for i in test_list:
   time_start = time.time()
   print(i, ': ', model.score(i, bos=True, eos=True), 'time: ', time.time() - time_start)
# 结果见下图

#Stateful query 状态转移概率
state = kenlm.State()
state2 = kenlm.State()
#Use <s> as context.  If you don't want <s>, use model.NullContextWrite(state).
model.BeginSentenceWrite(state)
```

示例：
![](https://pic1.zhimg.com/80/v2-f9277d33e552b3ab71ffc950ad3aa2ac_720w.png)


# 结束