---
layout: post
title:  "视频理解 - Video Understanding"
date:   2023-05-04 08:01:00
categories: 计算机视觉
tags: 视频理解
excerpt: 视频理解
mathjax: true
permalink: /video
---

* content
{:toc}

# 视频

【2023-5-5】 [FesianXu](github.com/FesianXu): [万字长文漫谈视频理解](https://zhuanlan.zhihu.com/p/158702087)

作为多媒体中重要的信息载体，视频的地位可以说是数一数二的，然而目前对于AI算法在视频上的应用还不够成熟，理解视频内容仍然是一个重要的问题亟待解决攻克。

## 什么是视频

### 视频为什么重要？

以视频为代表的动态多媒体，结合了音频，视频，是当前的，更是未来的**互联网流量之王**。 

国家互联网信息办公室的中国互联网络发展状况统计报告
>- 截至 2018 年 12 月，网络视频、网络音乐和网络游戏的用户规模分别为 6.12 亿、5.76 亿和 4.84 亿，使用率分别为 73.9%、69.5%和 58.4%。短视频用户规模达 6.48 亿，网民使用比例为 **78.2%**。
>- 截至 2018 年 12 月，网络视频用户规模达 6.12 亿，较 2017 年底增加 3309 万，占网民 整体的 73.9%；手机网络视频用户规模达 5.90 亿，较 2017 年底增加 4101 万，占手机 网民的 72.2%。

2018年各类应用使用时长占比
- ![](https://pic3.zhimg.com/80/v2-6a77c9189cfad7ccaa9dba9ed5cf38ee_1440w.webp)
- 包括短视频在内的视频用户时长占据了约20%的用户时长，占据了绝大多数的流量，同时网络视频用户的规模也在逐年增加。

固定带宽/4G平均下载速率变化曲线
- ![](https://pic3.zhimg.com/80/v2-a0c8b32b0c72774ade5a1500eb47673a_1440w.webp)

## 视频理解

理解视频(understanding the video) 是一件非常抽象的事情，在神经科学尚没有完全清晰，如果按照人类感知去理解，终将陷入泥淖。

在理解视频这个任务中，到底在做什么？
- 首先，对比于文本，图片和音频，视频特点：动态的按照时间排序的图片序列
- 然而，图片**帧**间有着密切的联系，存在上下文联系；
- 视频有音频信息。

因此进行视频理解，先要再时间序列上建模，同时还需要空间上的关系组织。

目前理解视频主要集中在**以人为中心**的角度进行，由于视频是动态的，因此描述视频中的物体随着时间变化，在进行什么动作，很重要。
- **动作识别**在视频理解中占据了一个很重要的地位。

视频分析的主要难点：
- 需要大量的算力，视频的大小远大于图片数据，需要更大的算力进行计算。
- 低质量，很多真实视频拍摄时有着较大的运动模糊，遮挡，分辨率低下，或者光照不良等问题，容易对模型造成较大的干扰。
- 需要大量的数据标签，特别是在深度学习中，对视频的时序信息建模需要海量的训练数据才能进行。时间轴不仅仅是添加了一个维度那么简单，其对比图片数据带来了时序分析，因果分析等问题。

### 子任务

理解视频具体子任务：
- 视频**动作分类**：对视频中的动作进行分类
- 视频**动作定位**：识别原始视频中某个动作的开始帧和结束帧
- 视频**场景识别**：对视频中的场景进行分类
- **原子动作**提取
- 视频**文字说明**（Video Caption）：给给定视频配上文字说明，常用于视频简介自动生成和跨媒体检索
- **集群**动作理解：对某个集体活动进行动作分类，常见的包括排球，篮球场景等，可用于集体动作中关键动作，高亮动作的捕获。
- 视频**编辑**。
- **视频问答**系统（Video QA）：给定一个问题，系统根据给定的视频片段自动回答
- 视频**跟踪**：跟踪视频中的某个物体运动轨迹
- 视频**事件理解**：不同于动作，动作是一个更为短时间的活动，而事件可能会涉及到更长的时间依赖
- ...


## 视频数据


### 视频数据模态

视频动作理解是非常广阔的研究领域，输入的视频形式也不一定是常见的**RGB视频**，还可能是depth**深度图序列**，Skeleton**关节点信息**，IR**红外光谱**等。
- ![](https://pic3.zhimg.com/80/v2-34fd2b816579389d8dc46e7c8d705ab2_1440w.webp)

**RGB视频**是最容易获取的模态，然而随着很多**深度摄像头**的流行，**深度图序列**和**骨骼点序列**的获得也变得容易起来。
- 深度图和骨骼点序列对比RGB视频来说，其对光照的敏感性较低，数据冗余较低，有着许多优点。

### 视频动作分类数据集

公开的视频动作分类数据集有很多，比较流行的`in-wild数据集`主要是在YouTube上采集到的，包括以下的几个。
- HMDB-51，该数据集在YouTube和Google视频上采集，共有6849个视频片段，共有51个动作类别。
- UCF101，有着101个动作类别，13320个视频片段，大尺度的摄像头姿态变化，光照变化，视角变化和背景变化。
  - ![](https://pic4.zhimg.com/80/v2-ee25f80c4c52c9ef7559a94f22dde737_1440w.webp)
- sport-1M，也是在YouTube上采集的，有着1,133,157 个视频，487个运动标签。
  - ![](https://pic4.zhimg.com/80/v2-b10e40e4aaf656e3d366a11235ba2e33_1440w.webp)
- YouTube-8M, 有着6.1M个视频，3862个机器自动生成的视频标签，平均一个视频有着三个标签。
- YouTube-8M Segments，是YouTube-8M的扩展，其任务可以用在视频动作定位，分段（Segment，寻找某个动作的发生点和终止点），其中有237K个人工确认过的分段标签，共有1000个动作类别，平均每个视频有5个分段。该数据集鼓励研究者利用大量的带噪音的视频级别的标签的训练集数据去训练模型，以进行动作时间段定位。
  - ![](https://pic1.zhimg.com/80/v2-d134999c9d2c1fc4f9ae9923fc8bb7a0_1440w.webp)
- Kinectics 700，这个系列的数据集同样是个巨无霸，有着接近650,000个样本，覆盖着700个动作类别。每个动作类别至少有着600个视频片段样本。

以上数据集模态都是RGB视频，还有些数据集是多模态的：
- NTU RGB+D 60： 包含有60个动作，多个视角，共有约50k个样本片段，视频模态有RGB视频，深度图序列，骨骼点信息，红外图序列等。
- NTU RGB+D 120：是NTU RGB+D 60的扩展，共有120个动作，包含有多个人-人交互，人-物交互动作，共有约110k个样本，同样是多模态的数据集。


## 视频理解方法


### 早期CV方法


深度学习之前，CV算法工程师是**特征工程师**，手动设计特征，而这是一个非常困难的事情。

手动设计特征并且应用在视频分类的主要套路有：
- 特征设计：挑选合适的特征描述视频
  - 局部特征（Local features）：比如HOG（梯度直方图 ）+ HOF（光流直方图）
  - 基于轨迹的（Trajectory-based）：Motion Boundary Histograms（MBH）[4]，improved Dense Trajectories （iDT） ——有着良好的表现，不过计算复杂度过高。
- 集成挑选好的局部特征： 光是局部特征或者基于轨迹的特征不足以描述视频的全局信息，通常需要用某种方法集成这些特征。
  - 视觉词袋（Bag of Visual Words，BoVW），BoVW提供了一种通用的通过局部特征来构造全局特征的框架，其受到了文本处理中的词袋（Bag of Word，BoW）的启发，主要在于构造词袋（也就是字典，码表）等。
  - ![](https://pic2.zhimg.com/80/v2-c5105cbf72aca4c7e49cb67212d2ba99_1440w.webp)
  - Fisher Vector，FV同样是通过集成局部特征构造全局特征表征
  - ![](https://pic4.zhimg.com/80/v2-4951d630efffccb82d5c9923e98f3ba7_1440w.webp)
  - 要表征视频的时序信息，我们主要需要表征的是动作的运动（motion）信息，这个信息通过帧间在时间轴上的变化体现出来，通常我们可以用光流（optical flow）进行描述，如TVL1和DeepFlow。
  - ![](https://pic3.zhimg.com/80/v2-82fe89075d26b6c676fe50f5ae3a4eee_1440w.webp)


### 深度学习CV方法

深度学习时代，视频动作理解主要工作量在于如何设计合适的**深度网络**，而不是手动设计特征。设计这样的深度网络的过程中，需要考虑两个方面内容：
- 模型方面：什么模型可以最好的从现有的数据中捕获时序和空间信息。
- 计算量方面：如何在不牺牲过多的精度的情况下，减少模型的计算量。

组织时序信息是构建视频理解模型的一个关键点，Fig 3.2展示了若干可能的对多帧信息的组织方法。
- Single Frame，只是考虑了当前帧的特征，只在最后阶段融合所有的帧的信息。
- Late Fusion，晚融合使用了两个共享参数的特征提取网络（通常是CNN）进行相隔15帧的两个视频帧的特征提取，同样也是在最后阶段才结合这两帧的预测结果。
- Early Fusion，早融合在第一层就对连续的10帧进行特征融合。
- Slow Fusion，慢融合的时序感知野更大，同时在多个阶段都包含了帧间的信息融合，伴有层次（hierarchy）般的信息。这是对早融合和晚融合的一种平衡。

最终的预测阶段，从整个视频中采样若各个片段，对这采样的片段进行动作类别预测，其平均或者投票将作为最终的视频预测结果。
- ![](https://pic3.zhimg.com/80/v2-901a384d969876d7038f02dc395c092a_1440w.webp)


# 视频应用


## 抖音视频

抖音视频是一项画面+声音+文字的艺术

### 视频转化分析

流程
- 推荐：抖音视频推荐机制（阶梯）
- 观看：
  - 直接忽略：
  - 中途跳过：
  - 完整播放：
- 反馈：
  - 点赞：
  - 评论：
  - 收藏：
  - 转发：
  - 关注：
- 转化：导流
  - 关注
  - 小程序

![](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2022%2F0926%2F6bd63314j00rit5vr016xd000v900xtp.jpg&thumbnail=660x2147483647&quality=80&type=jpg)

### 视频拍摄

拍摄小提示
- 01：**画面+声音+文字**
  - 抖音视频是一项画面+声音+文字的艺术，除了传统声画，大家一定要注意标题和画面文案的协调搭配。
- 02：**情绪表达**
  - 反转只是情绪表达的一种形式，你的情绪表达足够到位，足够引起共鸣，不一定要费尽心思做反转的。
- 03：结尾**引导关注**
  - 结尾时尽量做一个引导关注，对拉粉很有帮助。

发布时还可以选择定位在人群密集的地方，因为系统也会优先推荐给附近的人看，展示的概率会大一些。
新闻类的短视频适合用脚本大纲，故事性强的短视频适合用分镜头脚本，不需要剧情的短视频适合用文学脚本。
很多场景视频还要设计拍摄地址，脚本的范式

### 抖音工具

视频处理
- 脚本文案: 短视频脚本一般分为3种，**分镜头脚本**、**拍摄提纲**、以及**文学脚本**。
  - [文案范例](http://static.kancloud.cn/mhsm/dyzsfx/2381665), 
- 提取文案：提取视频链接里的文案信息，可用工具
  - 微信小程序：”轻抖”，使用[方法](http://static.kancloud.cn/mhsm/dyzsfx/2381648)
  - [媒小三](https://www.meixiaosan.com/shortvideotext.html)，PC站点
  - [享享猫去水印](https://wangzhe.smzdw.cn/page/appdownload)，支持批量提取、修改、提取音频、去水印等；
    - 【2023-5-12】收费，邮箱, video三七二一, [会员](https://h.zzrjcp.com/user/)才行
- 下载视频




# 结束