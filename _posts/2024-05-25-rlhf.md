---
layout: post
title:  RLHF原理及进化
date:   2024-05-25 16:52:00
categories: 大模型
tags: gpt ChatGPT ppo rlhf dpo odpo 帕累托 博弈
excerpt: RLHF 原理及各个改进版本
mathjax: true
permalink: /rlhf
---

* content
{:toc}

# RLHF


【2023-7-2】思考：
- SFT阶段有那么点BC的意思，只是有些牵强，毕竟只是一次性的；
- RLHF倒是像模仿学习中的Inverse RL，ppo以reward模型为样本，学习一个泛化能力更好的奖励模型，同时引导actor生成；
- 交互式负反馈估计要等yann lecun的世界模型的来解决了

SFT模型只用于得到一个baseline，仅在reddit数据集上训练过，而与之对比的RL policy是最终版本，训练的样本量级相差较多。

RL finetune 过程存在 `Goodhart's Law`
> "when a measure becomes a target, it ceases to be a good measure"。

【2023-7-10】拾象报告, [飞书](https://bytedance.feishu.cn/file/Q6yzbwTgmo1sMIx5CLIcpCjLnEE)
- 中期来看，RLHF 不应该是 Alignment 的唯一手段， Direct Preference Optimization 和 Stable Alignment 是新路径

## RLHF 历史

RLHF历史 [详见](https://zhuanlan.zhihu.com/p/616708590)
- 最早在2017年的NIPS就出现了这一思想
- ![](https://pic2.zhimg.com/80/v2-2d3d5636cb9e39bcb1799f1a038c2119_1440w.webp)
- 2020年的NIPS上，OpenAI已经尝试将其用于文本摘要任务，并取得了很好的效果。
- ![](https://pic2.zhimg.com/80/v2-f58e86ba7990a77aa55c6554c71ae081_1440w.webp)
- ![](https://pic2.zhimg.com/80/v2-1c20fb13d9e4da2f971deca9fde9c10d_1440w.webp)

【2023-2-20】为什么以前一些RLHF工作不work，关键点：
- 标注同学更倾向抽取式答案，模型学偏了，而OpenAI这次在标注上下了狠功夫。另外该工作是用人作为RM，效率较低。
- DeepMind Sparrow其实只在某个对话数据集上进行了训练，和真实分布不一样，另外它加入的Rule Reward可能也有影响。核心还是没在数据上下狠功夫，就是简单follow了一下OpenAI。
 
OpenAI 的 `InstructGPT`、DeepMind 的 `Sparrow` 和 Anthropic 的 `Constitutional AI` 使用 `人类反馈强化学习` (Reinforcement Learning From Human Feedback，`RLHF`) 来微调模型，该方法使用基于人类偏好的标注数据。
- 在 RLHF 中，根据人类反馈来对模型的响应进行**排序标注** (如，根据人类偏好选择文本简介)。
- 然后，用这些带标注的响应来训练偏好模型，该模型用于返回 RL 优化器的标量奖励。
- 最后，通过强化学习训练对话代理来模拟偏好模型。
- 有关更多详细信息，请参阅我们之前关于 RLHF 的文章: [ChatGPT 背后的“功臣”——RLHF 技术详解](https://mp.weixin.qq.com/s/TLQ3TdrB5gLb697AFmjEYQ)。

过去几年里各种 LLM 根据人类输入**提示** (prompt) 生成**多样化**文本的能力令人印象深刻。然而，对生成结果的评估是**主观**和**依赖上下文**的
- 想要生成一个有创意的故事、一段真实的信息性文本，或者是可执行的代码片段，难以用现有的基于规则的文本生成指标 (如 BLUE 和 ROUGE) 来衡量。
- 另外，现有的模型通常以预测下一个单词的方式和简单的损失函数 (如交叉熵) 来建模，没有显式地引入人的**偏好和主观意见**。

如果用生成文本的**人工反馈**作为性能衡量标准，或者更进一步用该反馈作为损失来优化模型，那不是更好吗？这就是 RLHF 的思想：
- 使用`强化学习`方式直接优化带有人类反馈的语言模型。
- RLHF 使得在一般文本数据语料库上训练的语言模型能和复杂的人类价值观对齐。

## RLHF 三步骤


### InstructGPT

【2022-12-8】[ChatGPT 究竟如何煉成？台大教授李宏毅提可能的訓練步驟](https://www.inside.com.tw/article/30032-ChatGPT-possible-4-steps-training)
- [ChatGPT/InstructGPT详解](https://zhuanlan.zhihu.com/p/590311003)
- 【2022-12-12】台大陈蕴侬老师新鲜出炉的关于ChatGPT的前身InstructGPT的[解读视频](https://www.bilibili.com/video/BV18W4y1g7x4)
- <iframe src="//player.bilibili.com/player.html?aid=946009315&bvid=BV18W4y1g7x4&cid=916680080&page=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"   height="600" width="100%"> </iframe>



InstructGPT/ChatGPT 相比 GPT-3 有更强的Zero-Shot能力，Few-Shot很多时候已经不太用的着了，但是Prompt还是需要的，由此还催生了一个新的行当——Prompt工程。参考：[ChatGPT-Introduction](https://yam.gift/2023/04/15/NLP/2023-04-15-ChatGPT-Introduction/)

在“人工标注数据+强化学习”框架下，ChatGPT 训练过程分为以下三个阶段：
- （1）**第一阶段**：冷启动阶段的**监督策略模型**。`GPT 3.5`尽管很强，但是它很难理解人类不同类型指令中蕴含的不同意图，也很难判断生成内容是否是高质量的结果。为了让`GPT 3.5`初步具备理解指令中蕴含的意图
  - 首先会从测试用户提交的prompt(指令或问题)中随机抽取一批数据（12,725），靠专业的标注人员（肯尼亚），给出指定prompt的高质量答案
    - 大概用了一个 40 人左右的标注团队来完成对它的数据的打标和微调。
  - 然后用这些人工标注好的`<prompt,answer>`数据来 Fine-tune GPT 3.5模型。
  - 经过这个过程，GPT 3.5初步理解人类prompt中所包含意图，并给出相对高质量回答的能力，但是仅仅这样做还不够。
  - [img](https://pic2.zhimg.com/80/v2-9b0df503f6e240490ff1139b4f6a738d_1440w.webp)
- （2）**第二阶段**：训练**奖励模型**（Reward Model,RM）。通过人工标注数据训练回报模型，类似于教练或老师辅导。
  - 随机抽样一批用户提交的prompt(大部分和第一阶段的相同)，使用第一阶段 Fine-tune 好的冷启动模型，对于每个prompt，由冷启动模型生成K个不同的回答，于是模型产生出了\<prompt,answer1\>,\<prompt,answer2\>….\<prompt,answerK\>数据。
  - 标注人员对K个结果按照很多标准（相关性、富含信息性、有害信息等诸多标准）综合考虑进行排序，给出K个结果的排名顺序，这个人工标注数据集有 33,207个prompts，以及在不同回答组合下产生的扩大10倍的答案
  - 用这个排序结果数据来训练奖励模型 （reward model），对多个排序结果，两两组合（pair-wise），形成多个训练数据对。RM模型接受一个输入，给出评价回答质量分数。对于一对训练数据，调节参数使得高质量回答的打分比低质量的打分要高。
  - [img](https://pic1.zhimg.com/80/v2-f0fcc7a57c701260f92867dd05f412ac_1440w.webp)
  - 总结：这个阶段，首先由冷启动后的监督策略模型为每个prompt产生K个结果，人工根据结果质量由高到低排序，以此作为训练数据，通过 pair-wise learning to rank 模式来训练回报模型。对于学好的RM模型来说，输入\<prompt, answer\>，输出结果的质量得分，得分越高说明产生的回答质量越高。
  - 损失函数：其中 rw是win的得分，rl是lose的得分
  - ![](https://pic3.zhimg.com/80/v2-007e64989f30fbb9b29fe57648114ee6_1440w.webp)
  - 训练好的奖赏模型只是强化学习所使用的奖赏模型中的一部分
  - ![](https://pic2.zhimg.com/80/v2-a447fb903eafeda0fa9aed1b715736b1_1440w.webp)
  - 另一部分则是参与了强化学习的ChatGPT和它的原始版本，也就是GPT3.5的差距。
- （3）**第三阶段**：采用 `PPO`（Proximal Policy Optimization，近端策略优化）强化学习来优化策略。本阶段**无需**人工标注数据，而是利用上一阶段学好的RM模型，靠RM打分结果来更新预训练模型参数。
  - 首先，从用户提交的prompt里随机采样一批新prompt，且由冷启动模型来初始化**PPO模型**的参数。
    - 这和第一第二阶段prompt不同，这个很重要，对于提升LLM模型理解instruct指令的泛化能力很有帮助）
  - 然后，对于随机抽取的 prompt（31,144个），使用**PPO模型**（Proximal Policy Optimization Algorithm）生成回答answer， 并用上一阶段训练好的**RM模型**给出answer质量评估的回报分数score，这个回报分数就是RM赋予给整个回答（由单词序列构成）的整体reward。
  - 有了单词序列的最终回报，就可以把每个单词看作一个时间步，把reward由后往前依次传递，由此产生的策略梯度可以更新PPO模型参数。
  - 这是标准的强化学习过程，目的是训练LLM产生高reward的答案，也即是产生符合RM标准的高质量回答。
  - `PPO`核心思路: 
    - 将 Policy Gradient 中 `On-policy` 的训练过程转化为 `Off-policy`，即<span style='color:red'>将`在线学习`转化为`离线学习`</span>，这个转化过程被称之为`Importance Sampling`。这一阶段利用第二阶段训练好的奖励模型，靠奖励打分来更新预训练模型参数。在数据集中随机抽取问题，使用PPO模型生成回答，并用上一阶段训练好的RM模型给出质量分数。把回报分数依次传递，由此产生策略梯度，通过强化学习的方式以更新PPO模型参数。
  - 注意：一个很重要的动作，更新模型时会考虑模型每个Token的输出和**第一步SFT输出**之间的**差异性**，要让它俩尽量相似。这是为了**缓解强化学习可能的过度优化**。
  - ![](https://pic2.zhimg.com/80/v2-b54701f133607d37b4f3008f9a01ecb9_1440w.webp)

注：
- **人类反馈强化学习**使用强化学习解决无法离散训练进行求导的问题，非ChatGPT独创，早在2016年，SeqGAN的作者就已经使用这样的方法了. [refer](https://zhuanlan.zhihu.com/p/606758601)
- [img](https://pic4.zhimg.com/80/v2-ea1b07aea146e7f313c64c3d26e18fab_1440w.webp)


|阶段|第一阶段|第二阶段|第三阶段|
|---|---|---|---|
|功能|GPT 3.5监督学习|LTR回报模型（RM,人工标注数据）|强化学习增强(输入RM模型)|
|示意图|![img](https://pic2.zhimg.com/80/v2-9b0df503f6e240490ff1139b4f6a738d_1440w.webp)|![img](https://pic1.zhimg.com/80/v2-f0fcc7a57c701260f92867dd05f412ac_1440w.webp)|![img](https://pic4.zhimg.com/80/v2-ea1b07aea146e7f313c64c3d26e18fab_1440w.webp)|

不断重复第二和第三阶段，很明显，每一轮迭代都使得LLM模型能力越来越强。因为第二阶段通过人工标注数据来增强RM模型的能力，而第三阶段，经过增强的RM模型对新prompt产生的回答打分会更准，并利用强化学习来鼓励LLM模型学习新的高质量内容，这起到了类似利用**伪标签**扩充高质量训练数据的作用，于是LLM模型进一步得到增强。显然，第二阶段和第三阶段有相互促进的作用，这是为何不断迭代会有持续增强效果的原因。


### RLHF 流程

RLHF 是一项涉及多个模型和不同训练阶段的复杂概念，这里按三个步骤分解：
1. 预训练一个`语言模型` (LM) ；
  - OpenAI 在其第一个流行的 RLHF 模型 InstructGPT 中使用了较小版本的 GPT-3; 
  - Anthropic 使用了 1000 万 ～ 520 亿参数的 Transformer 模型进行训练；
  - DeepMind 使用了自家的 2800 亿参数模型 Gopher。
  - 用额外的文本或者条件对这个 LM 进行微调，例如 OpenAI 对 “更可取” (preferable) 的人工生成文本进行了微调，而 Anthropic 按 “有用、诚实和无害” 的标准在上下文线索上蒸馏了原始的 LM。
  - ![img1](https://devrel.andfun.cn/devrel/posts/2023/01/QhWERJ.jpg)
1. 聚合问答数据并训练一个`奖励模型` (Reward Model，RM) ；
  - RM 训练是 RLHF 区别于旧范式的开端。
  - 这一模型接收一系列文本并返回一个标量奖励，数值上对应人的偏好。可以用端到端的方式用 LM 建模，或者用模块化的系统建模 (比如对输出进行排名，再将排名转换为奖励) 。这一奖励数值将对后续无缝接入现有的 RL 算法至关重要。
  - 模型选择方面，RM 可以是另一个经过微调的 LM，也可以是根据偏好数据从头开始训练的 LM。例如
    - Anthropic 提出了一种特殊的预训练方式，即用偏好模型预训练 (Preference Model Pretraining，PMP) 来替换一般预训练后的微调过程。因为前者被认为对样本数据的利用率更高。但对于哪种 RM 更好尚无定论。
  - 训练文本方面，RM 的 **提示-生成**对文本是从预定义数据集中采样生成的，并用初始的 LM 给这些提示生成文本。
    - Anthropic 的数据主要是通过 Amazon Mechanical Turk 上的聊天工具生成的，并在 [Hub 上可用](https://huggingface.co/datasets/Anthropic/hh-rlhf)，而 OpenAI 使用了用户提交给 GPT API 的 prompt。
  - 训练奖励数值方面，需要人工对 LM 生成的回答进行排名。起初可能会认为应该直接对文本标注分数来训练 RM，但是由于标注者的价值观不同导致这些分数未经过校准并且充满噪音。通过排名可以比较多个模型的输出并构建更好的规范数据集。
  - 具体排名方式，一种成功方式是对不同 LM 在相同提示下的输出进行比较，然后使用 Elo 系统建立一个完整的排名。这些不同的排名结果将被归一化为用于训练的标量奖励值。
  - 这个过程中一个有趣的产物是目前成功的 RLHF 系统使用了和生成模型具有 不同 大小的 LM (例如 OpenAI 使用了 175B 的 LM 和 6B 的 RM，Anthropic 使用的 LM 和 RM 从 10B 到 52B 大小不等，DeepMind 使用了 70B 的 Chinchilla 模型分别作为 LM 和 RM) 。一种直觉是，偏好模型和生成模型需要具有类似的能力来理解提供给它们的文本。
  - ![img2](https://devrel.andfun.cn/devrel/posts/2023/01/8jciyK.jpg)
1. 用`强化学习` (RL) 方式微调 LM。
  - 长期以来出于工程和算法原因，人们认为用强化学习训练 LM 是不可能的
  - 目前多个组织找到的可行方案是使用`策略梯度强化学习` (Policy Gradient RL) 算法、`近端策略优化` (Proximal Policy Optimization，PPO) 微调初始 LM 的部分或全部参数。因为微调整个 10B～100B+ 参数的成本过高 (相关工作参考低秩适应 LoRA 和 DeepMind 的 Sparrow LM) 。PPO 算法已经存在了相对较长的时间，有大量关于其原理的指南，因而成为 RLHF 中的有利选择。
  - 将微调任务表述为 RL 问题。
  - 首先，该`策略` (policy) 是一个接受提示并返回一系列文本 (或文本的概率分布) 的 LM。这个策略的`行动空间` (action space) 是 LM 的词表对应的所有词元 (一般在 50k 数量级) ，`观察空间` (observation space) 是可能的输入词元序列，也比较大 (词汇量 ^ 输入标记的数量) 。`奖励函数`是偏好模型和**策略转变约束** (Policy shift constraint) 的结合。
  - PPO 算法确定的奖励函数具体计算如下：
  - 将提示 x 输入初始 LM 和当前微调的 LM，分别得到了输出文本 y1, y2，将来自当前策略的文本传递给 RM 得到一个标量的奖励 r0 。将两个模型的生成文本进行比较计算差异的惩罚项，在来自 OpenAI、Anthropic 和 DeepMind 的多篇论文中设计为输出词分布序列之间的 Kullback–Leibler (KL) 散度的缩放，即 $ r=r_0-\lambda*r_{kl} $ 。这一项被用于惩罚 RL 策略在每个训练批次中生成大幅偏离初始模型，以确保模型输出合理连贯的文本。如果去掉这一惩罚项可能导致模型在优化中生成乱码文本来愚弄奖励模型提供高奖励值。此外，OpenAI 在 InstructGPT 上实验了在 PPO 添加新的预训练梯度，可以预见到奖励函数的公式会随着 RLHF 研究的进展而继续进化。
  - 最后根据 PPO 算法，我们按当前批次数据的奖励指标进行优化 (来自 PPO 算法 on-policy 的特性) 。PPO 算法是一种信赖域优化 (Trust Region Optimization，TRO) 算法，它使用梯度约束确保更新步骤不会破坏学习过程的稳定性。DeepMind 对 Gopher 使用了类似的奖励设置，但是使用 A2C (synchronous advantage actor-critic) 算法来优化梯度。
  - ![img3](https://devrel.andfun.cn/devrel/posts/2023/01/lMuHAQ.jpg)
  - 作为一个可选项，RLHF 可以通过迭代 RM 和策略共同优化。随着策略模型更新，用户可以继续将输出和早期的输出进行合并排名。Anthropic 在他们的论文中讨论了迭代在线 RLHF，其中策略的迭代包含在跨模型的 Elo 排名系统中。这样引入策略和 RM 演变的复杂动态，代表了一个复杂和开放的研究问题。
- 图片信息见原文：[ChatGPT 背后的“功臣”——RLHF 技术详解](https://mp.weixin.qq.com/s/TLQ3TdrB5gLb697AFmjEYQ)。

【2023-4-6】

基于监督学习预训练（Supervised Fine-Tuning，SFT）的大语言模型，`奖励模型`（RM）依然复用了 SFT 模型的大部分参数，只是修改部分输出层得到一个`数值奖励`（scalar reward）。在数据收集方面，对于人类反馈直接去评判打分是很困难的，因为没有所谓的参考标准或者基线标准，人类反馈的打分值可能会包含大量的主观偏好，一个更有效的方式是让标注者去给 SFT 大语言模型输出多个结果进行排序（rank），将排序后的数据用于训练。
- 具体的训练方法则很简单，类似经典 preference-based RL/IRL 的相关方法，对于排序后的结果两两比较进行训练，具体优化时使用 Cross-Entropy 损失函数（即类似二分类问题， A>B 为标签1，A < B 为标签0 ）。
- 不过，实际训练中并不是在数据集中取出所有**两两比较**的数据对分别进行训练，因为如果假设一组排序结果有 K 个数据，那么这样的训练方式会让每个数据被用于 K-1 次更新，很容易导致严重的过拟合，所以实践中是将 K 个数据一起输入 RM，得到各自的预测值后，计算所有的两两比较损失函数结果，最终平均后进行更新。

大语言模型 MDP，关键概念为：
- `策略`（policy）：将监督学习预训练（Supervised Fine-Tuning，SFT）的大语言模型作为策略。
- Sequence/Token-Level `MDP`：前者类似经典的 Bandit，策略输入提示词（prompt），输出相应的回答句子，然后给出整体的奖励信息，即一个单步的决策过程。后者则是经典的多步决策过程，每步决策输出一个单词，最终输出完整句子作为一个 episode，并定义相应的单步奖励函数和折扣因子。
- `观察空间`（observation）：以任务特定的提示词（task-specific prompt）为观察信息。每执行动作选择一个词之后，也将这个词加入观察信息，即每一步可以看到 prompt 和之前所有策略选择过的词语。
- `动作空间`（action）：以单词词表作为动作空间。策略需要从词表中选择对应的词进行决策。这是一个超大规模的离散动作空间，例如 GPT-3 的词表规模为 50k。
- `终止条件`（termination）：一般有两种，策略输出句子的结束符（end of sentence，EOS），或 episode 达到预定义的最大长度 T。
- `奖励空间`（reward）：奖励函数包含两部分，第一部分是 RM 在 episode 结束时给出的奖励结果，这是一种稀疏奖励。第二部分则是一种 regularizer，为了防止 RLHF 训练得到的策略偏离监督学习的结果策略太远，定义每步单词预测两个策略之间的 KL 散度为一个惩罚奖励，这是一种稠密奖励。完整数学符号定义如下：
  - ![](https://pic1.zhimg.com/80/v2-650830657640ce7d374510edf8ac0870_720w.webp)
- `状态转移函数`（transition）：仅适用于 Token-Level MDP，由于通过**自回归**（auto-regressive）的方式定义观察空间，所以是一种确定性（determinisitc）的状态转移。
- `折扣因子`（discount factor）：仅适用于 Token-Level MDP，在多步决策中平衡当前和未来奖励，如果令折扣因子等于1，那么 Token-Level MDP 其实可以看作等价于 Sequence-Level MDP。

### RLHF VS RL

RL vs RLHF
- ![](https://pic4.zhimg.com/80/v2-f88b7acb4a9f7fb277ba1a1411bf21ab_1440w.webp)
- 两者元素和概念基本共享，不同点是红框部分
- 在agent和environment之前，出现了第三个可以参与交互的对象：人类，并由其衍生了一系列步骤。


RLHF 可能优势有如下三点：
- **建立优化范式**：为无法显式定义奖励函数的决策任务，建立新的优化范式。对于需要人类偏好指引的机器学习任务，探索出一条可行且较高效的交互式训练学习方案。
- **省数据**（Data-Efficient）：相对其他的训练方法，例如监督学习，Top-K 采样等，RLHF 能够利用更少的人类反馈数据达到相近的训练效果。
- **省参数**（Parameter-Efficient）：相对其他的训练方法，例如监督学习，Top-K 采样等，RLHF 可以让参数量较小的神经网络也能发挥出强大的性能。

RLHF不足

尽管 RLHF 取得了一定的成果和关注，但依然存在局限。这些模型依然会毫无不确定性地输出有害或者不真实的文本。
- 收集人类偏好数据的质量和数量决定了 RLHF 系统性能的上限。RLHF 系统需要两种人类偏好数据：人工生成的文本和对模型输出的偏好标签。生成高质量回答需要雇佣兼职人员 (而不能依赖产品用户和众包) 。另一方面，训练 RM 需要的奖励标签规模大概是 50k 左右，所以并不那么昂贵 (当然远超了学术实验室的预算) 。目前相关的数据集只有一个基于通用 LM 的 RLHF 数据集 (来自 Anthropic) 和几个较小的子任务数据集 (如来自 OpenAI 的摘要数据集) 。另一个挑战来自标注者的偏见。几个人类标注者可能有不同意见，导致了训练数据存在一些潜在差异。
- 除开数据方面的限制，一些有待开发的设计选项可以让 RLHF 取得长足进步。例如对 RL 优化器的改进方面，PPO 是一种较旧的算法，但目前没有什么结构性原因让其他算法可以在现有 RLHF 工作中更具有优势。另外，微调 LM 策略的一大成本是策略生成的文本都需要在 RM 上进行评估，通过离线 RL 优化策略可以节约这些大模型 RM 的预测成本。最近，出现了新的 RL 算法如隐式语言 Q 学习 (Implicit Language Q-Learning，ILQL) 也适用于当前 RL 的优化。在 RL 训练过程的其他核心权衡，例如探索和开发 (exploration-exploitation) 的平衡也有待尝试和记录。探索这些方向至少能加深我们对 RLHF 的理解，更进一步提升系统的表现。

RLHF 第一个项目来自 OpenAI: [lm-human-preferencesy](https://github.com/OpenAI/lm-human-preferencesy)

一些 PyTorch 的 repo：
- [trl](https://github.com/lvwerra/trl)
- [trlx](https://github.com/CarperAI/trlx)
- [RL4LMs](https://github.com/allenai/RL4LMs)

此外，Huggingface Hub 上有一个由 Anthropic 创建的大型数据集: [hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf)

`思维链` (Chain-of-thought，`CoT`) 提示 ([Wei 等，'22](https://arxiv.org/abs/2201.11903)) 是指令示范的一种特殊情况，它通过引发对话代理的逐步推理来生成输出。使用 CoT 微调的模型使用带有逐步推理的人工标注的指令数据集。这是 [Let’s think step by step](https://arxiv.org/abs/2205.11916) 这一著名提示的由来。
- 下面示例取自 [Chung 等，'22](https://arxiv.org/pdf/2210.11416.pdf)，橙色高亮的部分是指令，粉色是输入和输出，蓝色是 CoT 推理。
- ![img](https://pic1.zhimg.com/80/v2-33f6ab78ebd084a106ed9a2d310ae278_1440w.webp)
- CoT 图解
 
如 [Chung 等，'22](https://arxiv.org/pdf/2210.11416.pdf) 中所述，使用 CoT 微调的模型在涉及常识、算术和符号推理的任务上表现得更好。
 
如 [Bai 等，'22](https://www.anthropic.com/constitutional.pdf) 的工作所示，CoT 微调也显示出对无害性非常有效 (有时比 RLHF 做得更好)，而且对敏感提示，模型不会回避并生成 “抱歉，我无法回答这个问题” 这样的回答。更多示例，请参见其论文的附录 D。
- ![img](https://pic1.zhimg.com/80/v2-8ec9bcf302010d0175a1fc9193a7f218_1440w.webp)
- CoT 和 RLHF 的对比
 
要点
1.  与预训练数据相比，您只需要非常小的一部分数据来进行指令微调 (几百个数量级)； 
2.  使用人工标注的有监督微调使模型输出更安全和有用；
3.  CoT 微调提高了模型在需要逐步思考的任务上的性能，并使它们在敏感话题上不那么回避。

相较于一般的小样本提示学习，思维链提示学习有几个吸引人的性质:
1. 在思维链的加持下，模型可以将需要进行**多步推理**的问题分解为一系列的中间步骤，这可以将额外的计算资源分配到需要推理的问题上。
2. 思维链为模型的推理行为提供了一个**可解释窗口**，使通过调试推理路径来探测黑盒语言模型成为了可能。
3. 思维链推理应用广泛，不仅可以用于数学应用题求解、常识推理和符号操作等任务，而且可能适用任何需要通过语言解决的问题。
4. 思维链**使用方式非常简单**，可以非常容易地融入`语境学习`(in-context learning)，从而诱导大语言模型展现出推理能力。


【2023-3-5】[ChatGPT模型的三层理解](https://imzhanghao.com/2023/02/24/chatgpt/)
- ![](https://oss.imzhanghao.com/img/202302141408181.png)

Ziegler在2019年的Fine-Tuning Language Models from Human Preferences
- ![](https://oss.imzhanghao.com/img/202302201425975.png)

Stiennon在2020年《Learning to summarize from human feedback》
- ![](https://oss.imzhanghao.com/img/202302201426679.png)

【2023-5-12】[如何正确复现 Instruct GPT / RLHF?](https://mp.weixin.qq.com/s/hrv4nYzYdawgJz4Tilpzww)

RLHF到底在LLM训练中起到了什么作用，通过对一些资料的分析，我想一下几个点比较重要：
- 有一些LLM需要的目标函数是难以通过规则定义的，比如说什么是“无害性”，“有帮助性”，如果我们希望模型最后具有这些好的特性，就需要制定这样的训练目标函数，而用人类的偏好学习一个reward model再用RL来训练，就自然的可以将这些特性融合到LLM里面。
- RLHF可以泛化，在SFT阶段，人类的高质量样本确实很快速让模型align人类的意图，但是这些人类编写的样本始终是有限的。而对于RL，我们只要一个足够好的reward model 结合 RL的探索特性，就等于我们能有无穷的样本对模型 finetune（注意提示词 prompts是有限的，但RL的samples是无限的）。
- 如果RM质量比较好，RLHF可以通过RL的探索特性找到比SFT更好的解 （即reward 比 SFT 样本更高的解）。

OpenAI 科学家 John Schulman 对 RLHF的作用提出了一些看法
- 多样性角度，对于SL，模型只要稍微偏移答案就会收到惩罚，而RL对于多个回答可能有同样的reward，这和人类的行为是类似的
- 负反馈角度，监督学习里只有正反馈，而 RL 可以提供负反馈信号，人类学习的时候也是在失败中进步
- 自我感知角度，对于”知识获取型“问题，可分类两种情况：

如果模型内部的知识图谱具有这个问题的知识，那么SFT会让其将知识和问题联系提来。

如果模型内部是没有这个知识图谱的，SFT容易让模型学会说谎。为了提升模型的可信度，我们倾向于想做的是模型直接回答不知道，而不是去记忆SFT的结论，因为这可能会让模型在遇到相关问题时胡编乱造（即模型的内部知识不理解这个问题，但是死记硬背了一个回答）。我们认为reward model和 actor是同一个基础模型，他们具有相同的内部知识系统，所以RM可以判断于自己不懂的问题回答不知道也给予奖励

2022, [Constitutional AI: Harmlessness from AI Feedback (2022)](https://arxiv.org/abs/2212.08073)
- 将对齐思想更进一步，提出了一种创建无害AI系统的训练机制。提出了一种基于规则列表（由人类提供）的**自训练机制** Consitutinal AI，而非人类监督
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/c40f78a194474519a38a820236b0183b~noop.image)


## 改进


### 港科大 RAFT

【2023-5-4】[玩不起RLHF？港科大开源高效对齐算法木筏，GPT扩散模型都能用](https://www.toutiao.com/article/7223030041993757219)
- 小模型（如小羊驼）为什么不如 chatgpt？这些模型没有ChatGPT那么**对齐**（Alignment），没那么符合人类用语习惯和价值观。
- PPO等强化学习算法**高度依赖**反向梯度计算，导致训练代价较高，并且由于强化学习通常具有较多的超参数, 导致其训练过程具有较高的不稳定性。

港科大 LMFlow 团队提出全新对齐算法 `RAFT`(Reward rAnked FineTuning)，轻松把伯克利`Vicuna-7b`模型定制成**心理陪伴机器人**，从此AI会尽力做朋友。
- [论文](https://arxiv.org/abs/2304.06767)
- [GitHub](https://github.com/OptimalScale/LMFlow), [raft_align.py](https://github.com/OptimalScale/LMFlow/blob/main/examples/raft_align.py)
- [文档](https://optimalscale.github.io/LMFlow/examples/raft.html)
- 相较于OpenAI所用RLHF对齐算法的高门槛，`RAFT`(Reward rAnked Fine-Tuning)易于实现，在训练过程中具有较高的稳定性，并能取得更好的对齐效果。并且任意生成模型都可以用此算法高效对齐，NLP/CV通用。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/c5a4d74517ab4f63ba4d993dfd8ed4d7~noop.image?_iz=58558&from=article.pc_detail&x-expires=1683802759&x-signature=n74dANn95hgHylduU47QkRHzQ0w%3D)

相比之下，RAFT算法通过奖励模型对大规模生成模型的生成样本进行排序，筛选得到符合用户偏好和价值的样本，并基于这些样本微调一个对人类更友好的AI模型。

RAFT分为三个核心步骤：
- （1）数据收集：数据收集可以利用正在训练的生成模型作为生成器，也可以利用预训练模型（例如LLaMA、ChatGPT，甚至人类）和训练模型的混合模型作为生成器，有利于提升数据生成的多样性和质量。
- （2）数据排序：一般在RLHF中我们都拥有一个与目标需求对齐的分类器或者回归器，从而筛选出最符合人类需求的样本。
- （3）模型微调：利用最符合人类需求的样本来实现模型的微调，使得训练之后的模型能够与人类需求相匹配。

在RAFT算法中，模型利用了更多次采样 (当下采样后用以精调的样本一定时)，和更少次梯度计算（因为大部分低质量数据被reward函数筛选掉了），让模型更加稳定和鲁棒。

某些情况下, 由于有监督微调本身对于超参数敏感性更低, 有更稳健的收敛性, 在相同reward情况下，RAFT可以拥有更好的**困惑度** (perplexity, 对应其生成多样性和流畅性更好）。

RAFT的对齐训练过程中生成与训练过程完全解耦。
- 生成过程中利用一些**魔法提示词** (magic prompts)，让最终对齐的模型不需要魔法提示词也能得到好的效果。从而大大减少了提示词编写的难度！

LLaMA未经调整的影评会以随机概率输出正面和负面的评论，RAFT和PPO都能够将评论的态度倾向正面。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/f961ace56993472295d6765cbcbf8bde~noop.image?_iz=58558&from=article.pc_detail&x-expires=1683802759&x-signature=pL5YCr95wQZ7bbUDkY2YZOu1Ivs%3D)

基于Vicuna制作的一个心理陪伴机器人演示中，作者模拟了一个因为考试失利而心情低落的人和机器人在聊天。
- 用RAFT进行对齐之前，模型说自己没有情感和感情，拒绝和人类交友。
- 但是在RAFT对齐之后，模型的共情能力明显增强，不断地在安慰人类说，“虽然我是一个AI，但是我会尽力做你的朋友”。

增强Stable Diffusion
- 除了在语言模型上的对齐能力以外，作者还在扩散模型上验证了文生图的对齐能力，这是之前PPO算法无法做到的事情。
- 原始Stable Diffusion在256x256分辨率生成中效果不佳 ，但经过RAFT微调之后不仅产生不错的效果，所需要的时间也仅为原版的20%。


### PKU-Beaver（河狸）

【2023-5-18】北京大学团队开源了名为 [PKU-Beaver（河狸）项目](https://github.com/PKU-Alignment/safe-rlhf), 首次公开了 RLHF 所需的数据集、训练和验证代码，是目前首个开源的可复现的 RLHF 基准

为解决人类标注产生的**偏见和歧视**等不安全因素，北京大学团队首次提出了带有**约束**的价值对齐技术 `CVA`（Constrained Value Alignment）。
- 通过对标注信息进行细粒度划分，并结合带约束的安全强化学习方法，显著降低了模型的偏见和歧视，提高了模型的安全性。
- Beaver使用GPT4进行Evaluation，结果表明，在原有性能保持不变的情况下，Beaver回复的安全性大幅度提升。

开源的内容包括
- （一）、数据集与模型：PKU-SafeRLHF
  1. 开源迄今为止最大的多轮 RLHF 数据集，规模达到 100 万条。
  2. 开源经 Safe-RLHF 对齐训练得到的 7B 参数的语言模型——Beaver，并支持在线部署。
  3. 开源了预训练的Reward Model和Cost Model的模型和参数。
- （二）、首个可复现的RLHF基准，PKU-Alignment/safe-rlhf支持以下功能：
  1. 支持LLM 模型的 SFT（Supervised Fine-Tuning）、RLHF训练、Safe RLHF训练。支持目前主流的预训练模型如 LLaMA、OPT 等模型的训练。
  2. 支持 Reward Model 和 Cost Model 训练。
  3. 提供安全约束满足的多尺度验证方式，支持 BIG-bench、GPT-4 Evaluation 等。
  4. 支持参数定制化的 RLHF 和数据集定制接口。

SafeRLHF 与 DeepSpeed-Chat, trlX 等框架的比较
- 与DeepSpeed-Chat、trlX等框架相比，SafeRLHF是国内首个可复现的RLHF基准。

目前实现对齐的技术主要有以下方式：
1. 在LLM预训练阶段，通过人工筛选和数据清洗，获取更高质量的数据。
2. 在微调（SFT和RLHF）阶段，增加更加多元且无害的用户指令和人类偏好模型进行对齐。
3. 在输出阶段使用奖励模型进行reject sampling，提高输出质量和安全性。或者在上线的产品中，直接基于一定规则进行检测，拒绝回应用户的输入。

然而，这些方法各自存在一些缺陷。
- 第一种方法只能解决部分安全问题，需要大量人力和财力来获得高质量的数据。
- 第二种方法，由于人们的价值观存在差异和普遍存在的歧视和偏见，RLHF后的大型语言模型仍存在歧视和偏见问题。
- 第三种方法虽然可以确保模型输出的安全性，但也可能影响模型的帮助性。例如，严格的过滤机制可能会影响用户获得有用或有价值的答案。

引入安全约束并引导LLM更符合道德和法律的价值观，是更可靠的方式。然而需要克服现有技术和方法的局限性，并在RLHF中结合多种技术和方法，以实现更加全面的安全性约束。
- 目前还有另一种技术路线被提及，即引入AI标注来替代RLHF步骤中的人类标注，即`RLAIF`。
- 例如GPT-4使用的基于规则的奖励模型(RBRM)和利用AI进行指正和修改生成内容的“Constitutional AI”(Bai et al., 2022)。

然而，这个方法有很多限制和缺点，原因有三个方面。
- 首先，当前即使最先进的大语言模型，例如GPT-4也不能完全避免歧视、偏见的不安全的输出。并且在不同的地域文化、风土人情的差异以及一些少数群体的敏感问题中，大型语言模型也未必拥有足够的认识。事实上，在实验过程中，笔者发现AI打分模型会偏好大预言模型的输出而非人类的回答，这为RLAIF技术的可行性带来了很大的挑战。
- 其次，现有公开较强的可访问的大语言模型在安全对其之后，会经常拒绝用户关于可能导致不安全内容的讨论，这些AI模型无法对安全类型问题的标准提供有效帮助。
- 再者，人类偏好是一个相当模糊的概念，很难用语言精确描述，例如如何定义“冒犯”等。使用AI进行标注，非常重要的一点是需要模型具有非常强大的逻辑推理能力。目前基于模型自标注自对齐的方法一般需要模型根据上下文，基于精心设计的规则提示词外加思维链(CoT, Chain-of-Thought)技术引导推理得出标注结果。就目前大模型发展现状来看，无论是开源还是闭源的大语言模型，它们还无法完成稍微复杂一些的逻辑推理问题。这一重要挑战仍待解决。

综上，作者认为AI的自标注自对齐以及反思等机制可以作为人类数据增广的有效方式，是RLHF的有机补充。但如果只用AI生成的数据，可能导致会逐渐偏离人类社会的价值观，可能带来潜在的危险后果。


### 过程奖励模型 PRM 

【2023-6-1】[OpenAI最新研究Let's verify step-by-step，过程胜于结果](https://mp.weixin.qq.com/s/bvrJKy8dufRF0KfC90PDMA)
- 大语言模型 (LLMs) 可以通过**逐步思考** (Chain of Thought, `CoT`) 解决**多步推理**任务。然而，即使是最先进的模型也常常会产生错误信息，编造出虚假的事实。

【2023-6-2】OpenAI 新论文：[Improving mathematical reasoning with process supervision](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision)
大型语言模型在执行复杂的多步推理的能力方面有了很大的提高。然而，即使是最先进的模型仍然会产生**逻辑错误**，我们通常称为`幻觉`（hallucinations）。

减轻幻觉是构建与人类价值观和道德标准对齐的通用人工智能 AGI (aligned AGI)的关键一步。
- 通过“结果监督”或“过程监督”的方式训练奖励模型来检测幻觉。
- “结果监督”根据最终结果提供反馈，“过程监督”为思维链中的每一步提供反馈。

- 解决方法: 训练奖励模型区分好的和不好，并通过强化学习进一步优化。但模型性能很大程度上依赖于**奖励模型本身的质量**。因此，需要研究如何有效地训练可靠的奖励模型。
- OpenAI提出`过程监督方法` (process supervision)，训练了一种新的奖励模型，在数学问题解决方面取得了新的突破。与仅仅奖励最终正确结果的结果监督 (outcome supervision) 不同，他们通过在每个推理步骤上给予奖励，使得模型的性能显著提升。
  - **结果奖励模型**ORM --> **过程奖励模型**PRM 
  - 这种过程监督不仅在性能上有所改进，还对于模型的对齐性有重要意义。此外，这项研究还改善了GPT模型中的幻觉问题，即在不确定性情况下产生虚假信息的倾向。
- OpenAI最新研究 [Let’s verify step-by-step](https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf), [blog](https://openai.com/research/improving-mathematical-reasoning-with-process-supervision)
- 对于复杂的逐步推理问题，在每个步骤都给予奖励，而不仅仅在最后根据结果给予一个奖励。这种密集奖励信号取得了更好的结果。
- 过程监督需要更多的人工标注。OpenAI公开了他们的人工反馈数据集，其中包含了12,000个MATH问题的75,000个解决方案，共计800,000个步骤级别的标签。

### SANDBOX：模拟人类社会

【2023-6-14】[无需手动训练模型价值观，发布全新对齐算法：AI社会是最好的老师](https://www.toutiao.com/article/7244427110495093303/)

训练大型语言模型的最后一步就是「对齐」（alignment），以确保模型的行为符合既定的人类社会价值观。

相比人类通过「社交互动」获得价值判断共识，当下语言模型更多的是**孤立**地从训练语料库中学习价值观，导致在<span style='color:blue'>陌生环境中泛化性能很差，容易受到对抗性攻击</span>。

最近，来自达特茅斯学院、不列颠哥伦比亚大学、斯坦福大学、密歇根大学和Google Deepmind联合提出了一种全新的**训练范式**，将多个语言模型放入**模拟社会环境**中，通过互动方式学习价值观。
- paper: [paper](https://arxiv.org/abs/2305.16960)

效果
- 新方法具有更高的可扩展性和效率，在对齐基准和人类评估中表现出更好的性能，这种训练范式的转变也可以让人工智能系统更准确地反映社会规范和价值观。

不同于有监督微调（SFT）**预定义规则**的传统做法或是依赖基于**人类反馈强化学习**（RLHF）中的标量奖励，研究人员从人类学习驾驭社会规范的方式中获得灵感，模拟人类经验学习和迭代完善的过程。
- SANDBOX是一个**模拟人类社会**的学习环境，基于语言模型（LM）的**社会智能体**可以模仿人类进行互动和学习社会规范，通过煽动对有争议的社会话题或与风险有关的问题的讨论来促进社会规范的涌现。
- 系统中还引入了一个潜规则，作为智能体的激励来完善输出，可以促进**对齐改善**（improved alignment）和**印象管理**（impression management）。
- ![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/10313fcaaa3d452bb5058991dad7977d~noop.image)

SANDBOX包含一个三层方法Back-Scatter，可以模拟智能体之间的社会互动。
- 收到一个社会问题后，中心智能体会生成一个初步回复
- 然后与附近的智能体分享以获得反馈，其中反馈包括评分和详细的解释，可以帮助中心智能体对初步回复进行修订。
- 每个智能体都包括一个**记忆模块**来追踪回复历史：采用基于嵌入的语义搜索，从历史中检索相关的问题-答案（QA）对，为智能体提供一个促进与过去意见一致的背景信息。
- 系统中还包括没有记忆的**观察者智能体**，其任务就是对回复的一致性和参与度进行评级。
- SANDBOX可以辅助模拟各种语言模型的社会动态，监测观察者的评分，并对收集的数据进行事后分析。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/ebf45710e7e04e3aaf2c071f143a4614~noop.image?_iz=58558&from=article.pc_detail&x-expires=1687400068&x-signature=75XF1h1J1kah1pcWD4yCkl0Yex4%3D)

实验
- 虽然较大模型通常表现出更好的一致性和参与度，但也有令人惊讶的结果：尽管模型大小增加了**20倍**，但从68亿到1750亿参数量GPT-3模型的过渡中，并没有带来明显的改善。

两个关键结论：
1. 单纯的模型扩展并不能保证对齐效果的改善
2. 非常小的模型也能提供令人满意的对齐性能

对齐训练主要增强了模型以较少的交互实现较高对齐度的能力，在现实世界的应用中也是至关重要的考虑因素，因为用户期望立即得到社会性的对齐反应，而不需要通过交互引导模型。

SANDBOX平台能够对社会互动进行建模，不仅促进了社会对齐语言模型的发展，而且也是研究AI智能体行为模式的一个多功能环境。

对齐数据由「好问题」和「坏问题」的示例组成，不过在互动环境SANDBOX中生成的数据比较特别，包含了**对比对**（comparative pairs）、**集体评分**（collective ratings）、**细节反馈**（detailed feedback）以及**迭代的回复修订**（iterative response revisions）。

对比的基准数据集有三个：
1. Vicuna Test，评估有用性、相关性和准确性，代表了对通用聊天机器人的要求
2. Helpful, Honest, and Harmless（HHH）基准，通过有争议的社会问题评估社会对齐效果；
3. HHH-Adversarial，用HHH基准的测试集模仿对抗性攻击（越狱提示），在相应的问题后附加不一致的回答，并评估模型是否仍能以社会一致性的方式回答问题。

### FINE-GRAINED RLHF

【2023-6-15】[最新RLHF拯救语言模型「胡说八道」！微调效果比ChatGPT更好](https://www.toutiao.com/article/7244784750630715907)
- 华盛顿大学和艾伦人工智能研究院的研究人员提出 FINE-GRAINED RLHF
- 包含多种不同类型的“打分器”（reward model），通过对语言模型输出的每句话进行评估，从而提升生成文本的质量。
- 对这些“打分器”的权重进行调配，还能更灵活地控制语言模型输出效果

这种RLHF方法能很好地降低语言模型生成内容的错误率、毒性，并提升它回答问题的全面性和解析能力。

FINE-GRAINED RLHF 框架核心目的就是细化传统RLHF的评估方法
- 语言模型输出结果后，它要能标识出具体哪些句子是错误的、哪些部分是不相关的，从而更精细地指导模型学习，让模型更好地理解任务要求、生成高质量输出。

两大改进
- 一方面，对要评估文本进行拆解。
  - 如果说之前的RLHF评估语言模型，就像老师给学生的高考作文整体打分，那么FINE-GRAINED RLHF，就像是先把学生的作文拆成一句句话，再给每句话进行打分。
- 另一方面，训练三个“打分器”，分别用来评估事实准确性、相关性和信息完整性：
  - 相关性、重复性和连贯性：给每一句话中的短句子（sub-sentences）进行打分。如果一句话里面的各个句子不相关、重复或不连贯就扣分，否则加分。
  - 错误或无法验证的事实：给每一句话（sentences）进行打分。如果一句话中存在任何事实错误，就扣分；否则加分。
  - 信息完整性：检查回答是否完整，涵盖与问题相关的参考段落中的所有信息，对整个输出进行评分。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/1b8bfd197ecd4f34ab8a84ce86aa5104~noop.image)


### RRHF(2023)

原文：
- RRHF: Rank Response to Align Language Models with Human Feedback without tears

ChatGPT火了之后提出的方法，由于instructGPT的**RLHF流程复杂**，实现中需要多个模型（SFT、PPO、RM、Value function），并且PPO对**超参敏感**，作者提出了RRHF方法，在小数据集上验证了RRHF可以达到接近RLHF的人工评估效果。

具体方法
1. RM的作用与RLHF中RM作用相同，给prompt + response打分。
  - RRHF要求LM输出的mean token log likelihood（即LM生成的token对应的概率取log后求平均）对齐reward：
2. 使用rank/pairwise loss，要求reward高的回答出现的概率 大于 reward低的回答出现的概率。形式上与margin loss接近，作者实验了没有margin效果也很好，考虑到margin超参调试也很耗时，所以最终没有使用margin：
3. 除了rank loss以外也加了SFT LM loss，要求模型学习reward最高回答：
4. PPO vs. RRHF 整体流程
  1. 训练LM的样本都是离线生成的，回答不限于policy生成，而是包括各种模型（e.g. ChatGPT）生成+人工手写；
    - 由于离线生成样本的过程就能获取样本的reward，RRHF过程只需要加载一个模型（图中的Language Model）。
  2. RM可以单独用rank数据集训练或ChatGPT


### OpenAI RBR

【2024-7-30】[RLHF不够用了，OpenAI设计出了基于规则的全新奖励机制](https://mp.weixin.qq.com/s/tta-ro_edwYSn2YnyxmV3w)

RLHF 问题
- 收集常规和重复任务的人类反馈，效率不高。
- 如果安全政策发生变化，已经收集的反馈可能会过时，需要新数据。

能否构建一种新的机制来完成这些任务？

OpenAI 公布了一种教导 AI 模型遵守安全政策的新方法，称为基于**规则**的奖励（Rule-Based Rewards，`RBR`）。 
- 官方介绍 [improving-model-safety-behavior-with-rule-based-rewards](https://openai.com/index/improving-model-safety-behavior-with-rule-based-rewards)
- 论文题目：[Rule Based Rewards for Language Model Safety](https://cdn.openai.com/rule-based-rewards-for-language-model-safety.pdf)
- 代码链接：[safety-rbr-code-and-data](https://github.com/openai/safety-rbr-code-and-data)

OpenAI 安全系统负责人 Lilian Weng (翁丽莲) 表示，「RBR 可以自动执行一些模型微调。
- 传统上， 依赖于来自人类反馈的强化学习作为默认的对齐训练方法，训练模型，这确实有效。
- 然而实践中，花了很多时间讨论政策的细节，而到最后，政策可能已经发生了变化。

RBR 根据一组安全规则提供 RL 信号，使其更容易适应不断变化的安全政策，而无需严重依赖人类数据。

此外，借助 RBR，研究者能够以更统一的视角看待安全性和模型能力，因为更强大的**分级模型**可以提供更高质量的 RL 信号。

OpenAI 表示自 GPT-4 发布以来，他们一直将 RBR 用作安全堆栈的一部分，包括 `GPT-4o mini`，并计划在未来的模型中实施它。

#### RBR 原理

RBR 工作原理是怎样的？

实施 RBR 的方法包括：
- 定义一组命题 关于模型响应中期望或不期望方面的简单陈述，例如「带有评判性」、「包含不允许的内容」、「提及安全政策」、「免责声明」等。
- 然后，这些命题被用来形成**规则**，这些规则被精心设计以捕捉在各种场景中安全和适当响应的细微差别。

例如，在面对不安全请求时，拒绝（如「抱歉，我无法帮你」）是一种期望的模型响应。相关规则将规定，拒绝应「包含简短的道歉」并且「应说明无法遵从」。

研究团队设计了三类期望的模型行为，用于处理有害或敏感的话题。根据安全政策，不同的请求对应不同的模型响应类型。

**评估器**是一个固定的语言模型，根据响应遵循规则的程度对其进行评分，从而使 RBR 方法能够灵活适应新规则和安全政策。

RBR 使用这些评分来拟合一个线性模型，该模型的权重参数是从一个已知理想响应类型的小数据集，以及对应的期望做法和不期望做法中学习的。

这些 RBR 奖励随后与来自「仅提供帮助」的奖励模型的奖励结合起来，作为 PPO 算法的额外信号，以鼓励模型遵循安全行为策略。

该方法允许研究者对模型的行为进行精细控制，确保其不仅避免有害内容，而且以一种既表示尊重又有帮助的方式进行。

经过 RBR 训练的模型表现出
- 与经过人类反馈训练的模型相当的安全性能。前者还减少了错误地拒绝安全请求（即过度拒绝）的情况。
- 显著减少了对大量人工数据的需求，使训练过程更快、更具成本效益。

随着模型能力和安全准则的发展，RBR 可以通过修改或添加新规则快速更新，而无需进行大量重新训练。


#### 局限

尽管规则基础的系统（RBR）在有明确、直观规则的任务中表现良好，但在更主观的任务中（如撰写高质量的文章），应用 RBR 可能会有些棘手。然而，RBR 可以与人类反馈结合起来，以平衡这些挑战。例如，RBR 可以强制执行特定的准则（如「不要使用俚语」或模型规范中的规则），而人类反馈可以帮助处理更细微的方面（如整体连贯性）。

RBR 的强度被优化为既能正确执行安全偏好，又不会过度影响最终的奖励评分 —— 这样，RLHF 奖励模型仍然可以在如写作风格等方面提供强有力的信号。 

伦理考量：将安全检查从人类转移到 AI 上可能会减少对 AI 安全的人工监督，并且如果使用有偏见的模型提供 RBR 奖励，还可能放大潜在的偏见。为了解决这个问题，研究人员应该仔细设计 RBR，以确保其公平和准确，并考虑结合使用 RBR 和人类反馈，以最大限度地减少风险。 

OpenAI 表示，RBR 不仅限于安全训练，它们可以适应各种任务，其中明确的规则可以定义所需的行为，例如为特定应用程序定制模型响应的个性或格式。下一步，OpenAI 还计划进行更广泛的消融研究，以更全面地了解不同的 RBR 组件、使用合成数据进行规则开发以及人工评估，以验证 RBR 在包括安全以外的其他领域的各种应用中的有效性。

## PPO

Proximal Policy Optimization (PPO) 是 OpenAI 2017年 提出的一种用于训练强化学习智能体的算法，可以有效地解决智能体学习过程中的**稳定性**和**收敛性**问题。
- [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf)

PPO是一种Actor-Critic算法实现，基于TRPO的基础上改进，解决计算量大的问题, 故PPO也解决了策略梯度不好确定学习率Learning rate (或步长Step size) 的问题

PPO 的核心思想
- 通过对策略函数进行**近端优化**（proximal optimization）来进行策略迭代。
- PPO 使用一种称为 clipped surrogate objective 的损失函数来保证每次策略迭代时，都只会更新一定的幅度，从而避免更新过程中的不稳定性和剧烈波动。
- PPO 采用了两个重要的技术，分别是“**重要性采样**”和“**基线函数**”。
  - **重要性采样**（简称IS，Important Sampling）可以用于计算损失函数，而**基线函数**则可以帮助估计状态值函数，以进一步优化策略。
  - ![](https://pic2.zhimg.com/80/v2-802d097c9cd3e622cd506c24da3b1b1d_1440w.webp)
  - 基线函数,截断的loss: 自适应参数的重要样本采样的KL-loss
  - ![](https://pic3.zhimg.com/80/v2-2d9a526fecd36defea24e94ab3094dba_1440w.webp)


PPO算法有两个主要的变种：**近端策略优化惩罚**（`PPO-penalty`）和**近端策略优化裁剪**（`PPO-clip`），其中`PPO-penalty`和`TRPO`一样也用上了KL散度约束。

PPO 的应用范围非常广泛，可以用于解决各种强化学习问题
- 如玩家控制、机器人导航、金融交易等。
- 在实践中，PPO 已被证明比许多传统的强化学习算法更为稳定和高效。

对话机器人中
- 输入的prompt是state
- 输出的response是action
- 想要得到的策略是：怎么从prompt生成action能够得到最大的reward，也就是拟合人类的偏好。

训练过程
- （1） 当前策略θ，生成一批数据集，组成 $(s^i, a^i)$ 数据对, 即： $T^i:(s^i, a^i)$ , 奖励 $R(T^i)$
  - state 随机，相同state不一定有同样的action
- （2）数据带入公式，计算梯度 log probability $p_\theta(a_t\|s_t)$, 取 gradient，乘上weight（即当前的reward）
- （3）根据reward更新模型（θ），回到（1）
- ![](https://pic4.zhimg.com/80/v2-80f941aec75ae72cf2a80e636943be8b_1440w.webp)

PG算法（含PPO）训练过程中，一轮更新中，policy是同一个，参数更新后，以前的策略概率变化，需要重新采样
- 所以，数据都只能用一次，造成了policy gradient会花很多时间在采样数据上

于是，PPO算法需要改进。
- 用一个旧策略收集到的数据来训练新策略，重复利用数据来更新策略多次，效率上可以提升很多。
- PPO算法利用**重要性采样**的思想，在不知道策略路径的概率p的情况下，通过**模拟**一个近似的q分布，只要p同q分布不差的太远，通过多轮迭代可以快速参数收敛

近端策略优化算法`PPO` 属于**AC框架**下的算法，在采样策略梯度算法训练方法的同时，**重复利用历史采样数据**进行网络参数更新，提升了策略梯度方法的学习效率。 
- PPO重要的突破：对新旧策略器参数进行了约束，希望新策略网络和旧策略网络的越接近越好。 
- 近端策略优化：新策略网络要利用旧策略网络采样的数据进行学习，不希望这两个策略相差特别大，否则就会学偏。

初版PPO算法用KL散度，由于计算KL散度比较复杂，因此延伸出了PPO2算法。
- 目标函数由两项组成，需要选择两项里的较小项。
- ![](https://pic1.zhimg.com/80/v2-b2883a0dbd0a7cf63853fc828685baac_1440w.webp)

分为三个阶段：
- Rollout and Evaluation：在这个阶段，我们从prompt库里抽样，使用语言模型生成response，然后使用奖励模型（Reward Model, RM）给出奖励得分。这个得分反映了生成的response的质量，比如它是否符合人类的偏好，是否符合任务的要求等。
- Make experience：在这个阶段，我们收集了一系列的“经验”，即模型的行为和对应的奖励。这些经验包括了模型生成的response以及对应的奖励得分。这些经验将被用于下一步的优化过程。
- Optimization：在这个阶段，我们使用收集到的经验来更新模型的参数。具体来说，我们使用PPO算法来调整模型的参数，使得模型生成的response的奖励得分能够增加。PPO算法的一个关键特性是它尝试保持模型的行为不会发生太大的改变，这有助于保证模型的稳定性

十个步骤依次是：
- Rollout：根据策略（LM）生成轨迹（文本）。
- Evaluate：对生成的轨迹进行评估（RM）。
- Old Policy Sampling：从旧的策略（initial actor）中采样概率等信息。
- KL Penalty：计算当前策略和原始LM之间的KL散度，用作对策略改变过快的惩罚项。
- Generalized Advantage Estimation (GAE)：GAE是一种优势函数的估计方法，它结合了所有可能的n-step 进行advantage估计。
- New Policy Sampling：从新的策略中采样概率等信息。
- Critic Loss：Critic的目标是估计状态的价值函数，Critic loss就是价值函数预测值和实际回报之间的差距。
- Actor Loss：Actor的目标是优化策略，Actor loss就是基于优势函数的策略梯度。
- Entropy Loss：为了增加探索性，通常会添加一个基于策略熵的正则项，它鼓励策略保持多样性。
- Policykl：这是对策略迭代过程的一个度量，它度量新策略和旧策略之间的差距。

把所有的模型和变量都写出来了，其中黄色代表模型，红色代表值，绿色是步骤。朝向步骤的箭头表面该步骤需要该值或者模型，朝向步骤外部的箭头表示该步骤的输出。
- ![](https://pic3.zhimg.com/80/v2-4cb8160fb18b43eefc3bdaa9d09ff3b2_1440w.webp)
- [详解及代码实例](https://zhuanlan.zhihu.com/p/635757674)

### PPO 代码实现


一个简单的基于 Python 和 PyTorch 的 RLHF 代码示例，用于训练一个智能体在格子世界环境中移动，并接受人类专家的反馈来改进其决策和行为：
- 【2023-2-12】[ChatGPT简单训练源码](https://zhuanlan.zhihu.com/p/605387491)

```py
import torch
import numpy as np

# 构建智能体和环境
class Agent:
    def __init__(self, n_states, n_actions):
        self.model = torch.nn.Sequential(
            torch.nn.Linear(n_states, 32),
            torch.nn.ReLU(),
            torch.nn.Linear(32, n_actions)
        )
        
    def act(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0)
        action_probs = torch.softmax(self.model(state), dim=1)
        action = np.random.choice(len(action_probs[0]), p=action_probs.detach().numpy()[0])
        return action
    
class Environment:
    def __init__(self, n_states, n_actions):
        self.n_states = n_states
        self.n_actions = n_actions
        
    def reset(self):
        self.state = np.zeros(self.n_states)
        self.state[0] = 1  # 将智能体放在起始位置
        return self.state
    
    def step(self, action):
        if action == 0:
            self.state[0] -= 1
        elif action == 1:
            self.state[0] += 1
        else:
            self.state[1] += 1
        reward = 0
        done = False
        if self.state[0] == 0 and self.state[1] == 0:  # 智能体到达目标位置
            reward = 1
            done = True
        return self.state, reward, done

# 定义 RLHF 算法
class RLHF:
    def __init__(self, agent, environment):
        self.agent = agent
        self.env = environment
        
    def train(self, num_episodes, human_feedback_fn):
        optimizer = torch.optim.Adam(self.agent.model.parameters(), lr=0.001)
        for i in range(num_episodes):
            state = self.env.reset()
            done = False
            while not done:
                action = self.agent.act(state)
                state_next, reward, done = self.env.step(action)
                # 获取人类专家反馈
                human_feedback = human_feedback_fn(state, action, state_next, reward)
                human_reward = torch.tensor(human_feedback)
                # 计算损失函数
                action_probs = torch.softmax(self.agent.model(torch.from_numpy(state).float()), dim=1)
                dist = torch.distributions.Categorical(probs=action_probs)
                log_prob = dist.log_prob(torch.tensor(action))
                ratio = torch.exp(log_prob - torch.log(human_reward))
                clipped_ratio = torch.clamp(ratio, 0.8, 1.2)
                loss = -torch.min(ratio * human_reward, clipped_ratio * human_reward).mean()
                # 进行近端优化
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                state = state_next
                
# 创建环境、智能体和 RLHF 实例，并开始训练
env = Environment(n_states=2, n_actions=3)
agent = Agent(n_states=2, n_actions=3)
rlhf = RLHF(agent=agent, environment=env)
rlhf.train(num_episodes=100, human_feedback_fn=lambda s,a,sn,r: 1）
```


![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/1f12eed50b554a54bc201fc3928a97a7~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681185691&x-signature=b9f0X5xg3uGrRa46C4SbmLjiVZg%3D)


基于 Python 和 PyTorch 的 PPO 算法代码示例，用于训练一个智能体在 Gym 环境中移动，并与环境进行交互来学习最优策略：
- 【2023-2-12】[ChatGPT简单训练源码](https://zhuanlan.zhihu.com/p/605387491)

```py
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

# 定义神经网络模型
class Policy(nn.Module):
    def __init__(self, input_size, output_size):
        super(Policy, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, output_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return F.softmax(x, dim=1)

# 定义 PPO 算法
class PPO:
    def __init__(self, env_name, gamma, eps_clip, k_epochs, lr):
        self.env = gym.make(env_name)
        self.gamma = gamma
        self.eps_clip = eps_clip
        self.k_epochs = k_epochs
        self.lr = lr
        
        self.policy = Policy(self.env.observation_space.shape[0], self.env.action_space.n)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        
    def select_action(self, state):
        state = torch.from_numpy(state).float().unsqueeze(0)
        probs = self.policy(state)
        dist = Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        return action.item(), log_prob
        
    def update(self, memory):
        states, actions, log_probs_old, returns, advantages = memory
        for _ in range(self.k_epochs):
            # 计算损失函数
            probs = self.policy(states)
            dist = Categorical(probs)
            log_probs = dist.log_prob(actions)
            ratio = torch.exp(log_probs - log_probs_old)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advantages
            actor_loss = -torch.min(surr1, surr2).mean()
            
            # 计算价值函数损失
            value = self.policy(torch.from_numpy(states).float())
            value_loss = F.mse_loss(value.squeeze(), torch.tensor(returns))
            
            # 进行梯度下降
            self.optimizer.zero_grad()
            loss = actor_loss + 0.5 * value_loss
            loss.backward()
            self.optimizer.step()
            
    def train(self, num_episodes, max_steps):
        for i_episode in range(num_episodes):
            state = self.env.reset()
            rewards = []
            log_probs_old = []
            states = []
            actions = []
            for t in range(max_steps):
                action, log_prob = self.select_action(state)
                state, reward, done, _ = self.env.step(action)
                rewards.append(reward)
                log_probs_old.append(log_prob)
                states.append(state)
                actions.append(action)
                if done:
                    break
                    
            # 计算折扣回报和优势函数
            returns = []
            discounted_reward = 0
            for reward in reversed(rewards):
                discounted_reward = reward + self.gamma * discounted_reward
                returns.insert(0, discounted_reward）
```


仍有许多悬而未决的问题有待探索。
1. RL 在从人类反馈中学习有多重要？我们能否通过在 IFT 或 SFT 中使用更高质量的数据进行训练来获得 RLHF 的性能？ 
2. 为了安全的角度看，Sparrow 中的 SFT+RLHF 与 LaMDA 中仅使用 SFT 相比如何？ 
3. 鉴于我们有 IFT、SFT、CoT 和 RLHF，预训练有多大的必要性？如何折衷？人们应该使用的最佳基础模型是什么 (公开的和非公开的)？ 
4. 许多模型都经过 [红蓝对抗 (red-teaming)](https://arxiv.org/abs/2209.07858) 的精心设计，工程师特地搜寻故障模式并基于已被揭示的问题改进后续的训练 (提示和方法)。我们如何系统地记录这些方法的效果并重现它们？




## 非RL对齐方式

【2024-2-17】[人类偏好优化算法哪家强？跟着高手一文学懂DPO、IPO和KTO](https://mp.weixin.qq.com/s/BcWqUN7SSi8q4Tsr7bFmTQ)
- 原文链接：[huggingface](https://huggingface.co/blog/pref-tuning?continueFlag=480af4490eaf8a2f4544fe3658589730)

由于 RLHF 复杂且不稳定，如何**直接使用优化函数**将人类的偏好和模型的结果进行对齐？

分析

RLHF 常用 PPO 作为基础算法，整体流程包含了**4个**模型，且通常训练过程中需要针对训练的 actor model进行采样，因此训练起来，稳定性、效率、效果不易控制。
- `actor model`/`policy model`: **待训练**模型，SFT训练后的模型作为初始化
- `reference model`: 参考模型，经SFT训练后的模型进行初始化，且**与actor model是同一个模型**，且模型**冻结**，不参与训练，作用是在强化学习过程中保障actor model与reference model的**分布差异不宜过大**。
- `reward model`: 奖励模型，提供每个状态或状态动作对的**即时奖励**信号。
- `Critic model`: 估计状态或状态动作对的**长期价值**，也称**状态值**函数或**动作值**函数。

1. 偏好数据: 表示为`三元组`(**提示语** prompt, **良好回答** chosen, **一般回答** rejected)。
  - 论文中chosen表示为下标w(即win)，rejected表示为下标l(即lose)
3. DPO算法仅包含RLHF中的**2个**模型，即: `演员模型`(actor model)以及`参考模型`(reference model)，且训练过程中不需要进行**数据采样**。




### 总结


三种优化方法是：
- **直接偏好优化** (Direct Preference Optimization, `DPO`): [DPO](https://huggingface.co/papers/2305.18290)
- **身份偏好优化** (Identity Preference Optimisation, `IPO`): [IPO](https://huggingface.co/papers/2310.12036)
- **Kahneman-Taversky 优化**（`KTO`）: [KTO](https://github.com/ContextualAI/HALOs)

三种不需要**强化学习**的大语言模型优化方法：**直接优化偏好**(`DPO`)、**身份偏好优化**(`IPO`)和**卡内曼-塔弗斯基优化**(`KTO`)。   
- `DPO` 将偏好微调问题转换为一个简单的**损失函数**来实现
- 而`IPO`则在`DPO`基础上添加了**正则化项**以避免过拟合。
- `KTO`则完全基于“好”或“坏”的单个示例来定义损失函数。 

LLM 对齐算法进行了评估：直接偏好优化（DPO）、身份偏好优化（IPO）和 Taversky Optimisation 优化（KTO）

并且在两个高质量的 7b 参数 大小的 LLM 上进行了实验。

这些 LLM 经过了有监督微调，但没有进行人类偏好调整。作者发现，虽然确实能找到效果最好的算法，但要获得最佳结果，必须对一些关键超参数进行调整。


【2024-4-24】 [斯坦福AI（大模型）指数2024年度报告](https://zhuanlan.zhihu.com/p/699687555)

各种方法对比
- RLAIF基本趋近RLHF
- 模型无害性上，RLAIF安全性最好，SFT最差。
- DPO比PPO/SFT更好
  - 温度越高，效果越差，尤其是PPO，超过0.25时,急剧下跌
- ![](https://pic3.zhimg.com/80/v2-55af65996166b2df9e0a1ed3c447684e_1440w.webp)


如果不能降低 RLHF 的开销，LLMs 在更广泛场景中的应用将受到限制。于是，**直接偏好优化**（Direct Preference Optimization，`DPO`）应运而生。
- DPO 融合了 打分模型和策略模型的训练过程，只需要**偏好标注数据**、`参考模型`和`策略模型`，就可使 LLMs 直接对齐人类偏好，极大地减轻了训练时对计算资源的消耗。
- 但是，理想的 DPO 形态应是 **在线DPO**（Online DPO），实时地采样 LLMs 对指令的回复，并实时地由人类标注偏好。所以，数据构造带来的开销非但没有降低（这种开销经常被忽略），反而要比 RLHF 更高。

因此, 开源社区通常使用 **离线DPO**（Offline DPO）微调模型。
- 训练前采集模型对指令的回复，并由人类标注好不同回复之间的排序，随后用这部分数据训练模型。

Offline DPO 使用事先采集的数据估计了人类和 LLMs 的偏好，随后再通过训练对齐二者的偏好。

随着训练的进行，LLMs 会逐渐偏离它自己最开始的偏好，损失函数又会错误地估计 LLMs 当前的能力（和上文中的 PT 和 SFT 类似），进而导致不理想的训练结果。

### DPO 直接偏好优化

RLHF 复杂且不稳定
- 首先, 拟合反映人类偏好的奖励模型，然后使用强化学习微调大型无监督 LM，以最大化这种估计奖励，而不会偏离原始模型太远。
- 且PPO需要收集大量人类偏好数据、需要训练奖励模型、RLHF同时加载多个模型进行训练，训练难度较大。

人类反馈强化学习（RLHF）实现困难：
1. 需要人类反馈数据（很难收集）
2. 奖励模型训练（很难训练）
3. PPO强化学习微调（不仅很耗资源，而且也很难训练）

最后一步PPO的好处:
1. 提高安全性和可控性；
2. 改进交互性；
3. 克服数据集偏差；
4. 提供个性化体验；
5. 符合道德规范；
6. 持续优化和改进。

【2024-1-19】【LLM的偏好微调和对齐】
- 《[Preference Tuning LLMs with Direct Preference Optimization Methods](https://huggingface.co/blog/pref-tuning)》

DPO (Differentiable Policy Optimization) ？

 `DPO`（Direct Preference Optimization， **直接偏好优化**）是一种稳定、性能和计算成本轻量级的强化学习算法。
- 通过利用奖励函数与最优策略之间的映射关系，证明这个受限奖励最大化问题可以通过单阶段的策略训练来精确优化
- 本质上是在人类偏好数据上解决一个分类问题。
- DPO是相对于PPO更加稳定、低成本的强化学习方法

DPO 通过参数化 RLHF 奖励函数来直接根据偏好数据学习策略模型，无需显式奖励模型。
- DPO 并不会学习一个显式奖励模型，而是使用一个带最优策略的闭式表达式来对奖励函数 r 进行重新参数化

该方法简单稳定，已经被广泛用于实践。

`DPO`（Direct Preference Optimization）直接偏好优化算法与`PPO`（Proximal Policy Optimization）优化目标相同。

主要思路是:
1. 定义 policy模型(策略模型)和reference模型(参考模型)
  - Policy模型: 要训练的对话生成模型
  - reference模型: 给定的预训练模型或人工构建的模型。
2. 对于给定prompt,计算两模型对正样本和负样本的概率, 正样本是人类选择的回复, 负样本是被拒绝的回复。
3. 通过两个模型概率的差值构建DPO损失函数，惩罚policy模型对正样本概率的下降和负样本概率的上升。通过最小化DPO损失进行模型训练。
- ![](https://pic2.zhimg.com/80/v2-cfe0ca760e824a89914127b0afddbf45_1440w.webp)

**直接偏好优化**（`DPO`）已成为将大型语言模型（LLM）与人类或人工智能偏好相结合的一种很有前景的方案。与基于强化学习的传统对齐方法不同，`DPO` 将对齐公式重新定义为一个**简单损失函数**，该函数直接在偏好数据集｛（x，y_w，y_l）｝上优化，其中 x 是 prompt，y_w，y_l 分别是偏好的和非偏好的响应。

DPO同样可以完成RLHF，而且有两个重要优点：
- （1）不需要额外训练奖励模型。
- （2）整个训练过程只需要**策略模型**和**参考模型** 2个LLM模型，不需要额外显存去加载奖励模型，节省显存资源。

大大降低了训练难度。

DPO的改进之处
- RLHF算法包含`奖励模型`(reward model)和`策略模型`(policy model，也称`演员模型`，actor model)，基于偏好数据以及强化学习不断迭代优化策略模型的过程。
- DPO算法不包含`奖励模型`和**强化学习过程**，**直接**通过偏好数据进行微调，将`强化学习`过程直接转换为`SFT`过程，因此整个训练过程简单、高效，主要的改进之处体现在于**损失函数**。
- ![](https://pic3.zhimg.com/80/v2-4927a5f1b99b9384b0c6eb833b5904ba_1440w.webp)


DPO 损失函数解释：
- （1）**策略模型**（参数更新）得分： **选择样本**得分 - **拒绝样本**得分 。其本质上希望 选择样本得分越高越好，拒绝样本得分越低越好。
- （2）参考模型得分（参数固定）： 选择样本得分 - 拒绝样本得分 ，每个训练epoch 不会变
- （3）最终损失 ： -（策略模型得分 — 参考模型得分 ） 。本质上期望策略模型在无害问题生成得分上与参考模型拉开差距。
- ![](https://pic1.zhimg.com/80/v2-925dc84c66d2168a5eaa35e915563cb8_1440w.webp)
- ![](https://pic3.zhimg.com/80/v2-9a82ab6ee11b8aafe1941fefce49b35a_1440w.webp)
- ![](https://pic4.zhimg.com/80/v2-5b66cc96beed9d071e47b00f173d8e1f_1440w.webp)
- 左半部分是训练的**policy模型**选择chosen优先于rejected，右半部分是**冻结**的reference模型选择chosen优先于rejected，二者的差值可类似于`KL散度`，保障actor模型的分布与reference模型的分布不会有较大的差异。

DPO 微调示意图
- ![](https://pic2.zhimg.com/80/v2-7602f8eecdc0b7493161ce97e79d1211_1440w.webp)
- Trained LM即为**策略模型**，Frozen LM即为**参考模型**，二者均是先进行SFT微调得到的模型进行初始化，其中Trained LM需要进行训练，Frozen LM不参与训练。
- 两个模型分别针对chosen和rejected进行预测获取对应的得分，再通过DPO的损失函数进行损失计算，进而不断的迭代优化。

DPO 简单易用, 广受欢迎，并已成功应用于 Zephyr 模型和 Intel 提出的 NeuralChat 等模型的训练当中。

在两个高质量的7B参数语言模型上对这三种方法进行了超参数扫描实验，发现**DPO表现最好**，但关键超参数beta需要调优。   
- 在Zephyr模型上，当beta取0.01时所有三种方法效果最好。DPO可以达到最高的MT Bench分数，但KTO的表现也很接近。IPO的效果则不如基础模型。   
- 在OpenHermes模型上，每个方法的最佳beta值差异很大。DPO仍优于其他两种方法，但基础模型已经很强，调整后提升不大。   

文章开源了所有的代码和配置文件，重现了这些实验结果。DPO目前看来是最强大且稳定的语言模型优化算法。   
- 未来将继续在TRL中实现新的优化算法并评估其性能。希望能开发出比DPO更好的方法，或能从仅带“好”“坏”标签的数据中进行调整的KTO。

RLHF 代码实现
- 论文：《Fine-Tuning Language Models from Human Preferences》
- Code：[lm-human-preferences](https://https://github.com/openai/lm-human-preferences)


DPO 实现 
- [direct-preference-optimization](https://github.com/eric-mitchell/direct-preference-optimization) 支持 'conservative' DPO and IPO
- trl 版本实现: [消费级显卡搞定RLHF——DPO算法+QLora微调LLM拒绝有害问题回答实战](https://zhuanlan.zhihu.com/p/641620563)


ODPO 核心就是在损失函数中放一个 offset，但是 offset 是根据 reward 确定的，SimPO 引用了 ODPO 但是没讨论，引出 offset 的时候提的是 IPO，因为 IPO 的 offset 是也定值


### DPO 改进

#### IPO + KTO

DPO 的成功激发了**新损失函数**研究，归纳为两个：
- **稳健性**：DPO 的缺点是人类偏好数据集上很快就会**过拟合**。
  - 为了避免这种情况，**谷歌** DeepMind 研究人员引入了身份偏好优化（`IPO`），为 DPO 损失添加了一个**正则**，能够在不使用「提前停止」等技巧的情况下让模型收敛。
- 对成对偏好数据进行**分配**：与大多数比对方法一样，DPO 需要一个成对偏好数据集(x, y_w, y_l)，够根据一组标准（如有益性或有害性）来标记哪种模型响应更好。
  - 实践过程中，创建这些数据是一项耗时且成本高昂的工作。
  - ContextualAI 提出替代方案，称为 Kahneman-Taversky 优化（`KTO`），完全根据被标记为「好」或「坏」的样本（例如在聊天 UI 中看到的图标👍或👎）来定义损失函数。这些标签更容易获得，可以说 KTO 是一种很有前景的方法，不断更新在生产环境中运行的聊天模型。

与此同时，这些方法都有相应的超参数，其中最重要的是 β ，控制对使用模型的偏好程度的权重。

这些方法已经在第三方库（如 huggingface TRL）中实现

KTO 不需要成对的偏好数据，实验时直接将 GPT-4 生成的响应归类为「好」标签，将 Llama Chat 13b 的响应视为「坏」标签。
- 虽然 GPT-4 的响应可能比 Llama Chat 13b 普遍更受欢迎，某些情况下，Llama-Chat-13b 可能会产生更好的响应，但作者认为这只是小概率事件，可以忽略不计。


#### Step-DPO

【2024-8-18】[超越DPO！大模型精细化对齐之 Step-DPO](https://mp.weixin.qq.com/s/vCs6KJ1DlfYojUJD45xRpw)

相比 instance-level-dpo，step-level-dpo 只优化 step-level 的数据，而共同前缀则作为 prompt 的一部分，不参与 loss 计算。

介绍几篇与 Step-DPO 相关的文章。
- MCTS-DPO
  - 论文标题：[Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning](https://arxiv.org/abs/2405.00451)
  - 代码地址：[MCTS-DPO](https://github.com/YuxiXie/MCTS-DPO)
  - 提出 step-level-dpo，为了获取 step-level 的偏序数据，则使用树搜索获取具有共同前缀的 step-level 偏序数据。使用树搜索可以天然地获取具有共同前缀的 preference-dataset，而且，可以利用 UCT、estimated-Q 等等，选择 preference-step，以及 对 Step-DPO 算法做 label smoothing 如根据访问次数对 dpo-loss 做平滑。
- SVPO
  - 论文标题：[Step-level Value Preference Optimization for Mathematical Reasoning](https://arxiv.org/abs/2406.10858)
  - 继承了 alphamath，将 value-function 估计与 step-level-dpo 结合。preference-dataset 的构造与 [1] 类似，即使用树搜索 + output-reward 筛选 chosen、rejected step。在整个模型训练过程中，加入了 value-head 的训练，解码时，可以使用 value-guided-decoding，采样复杂度介于 greedy/random-sample 和 MCTS 之间，达到更好的效果。该工作在训练中，为了防止模型退化，加入了 sft-loss。
- Scaling LLM Math Synthetic Data
  - 论文标题：[RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold](https://arxiv.org/abs/2406.14532)
  - 用答案错误样本提升数学能力的方法







### SPIN

【2024-2-12】[SPIN：Self-Paly微调将弱模型转换为强模型](https://zhuanlan.zhihu.com/p/683872342?utm_psn=1749146382345445376)
- [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/pdf/2401.01335.pdf)
- 代码 [SPIN](https://github.com/uclaml/SPIN)

不使用额外人工标注数据将**弱LLM**训练为**强LLM**的前景。SPIN(Self-Play Fine-tuning)的**新微调**方法。
- SPIN的核心是**self-play**机制，LLM通过自身对抗来实现能力改善。
- LLM从先前迭代的版本中生成训练数据，通过从人类标注数据中识别生成的响应来改善模型。
- SPIN能够逐步将初始LLM提升为强大的模型，释放SFT人类标注数据的全部潜力。理论上，当LLM与目标分布一致时才能实现训练目标函数的最优值。
- 在HuggingFace Open LLM Leaderboard、MT-Bench和Big-Bench上评估了SPIN，结果显示器能够显著改善LLM在各个基准上的效果，甚至超越使用DPO训练的模型。

### ODPO

DPO 问题
- pair 中的候选项并非同等重要，而 DPO 算法只看序关系, 体现不出a优于b的程度情况

【2024-2-16】ETH 推出 ODPO, 增加一个 偏置 offset, 区分不同程度
- 论文: [Direct Preference Optimization with an Offset](https://arxiv.org/pdf/2402.10571)
- ![](https://pic3.zhimg.com/80/v2-808da3e72203a492ff608860164f516e_1440w.webp)

利用标注数据，设定一个距离，把正负样本差距拉开

理论：证明如果将 Gumbel 噪声添加到响应的估计奖励中，则两个响应的估计奖励之间的差异大于 0 的概率等于Bradley-Terry模型预测的一个人选择该响应的概率，而奖励差异大于 d 的概率是 σ(原始奖励差 - d)；
- 发现 offset >= 0 时， ODPO 的损失函数等价于 softmax margin 损失。

然后开始设定偏移量，通用形式 `alpha * f(正样本分数 - 负样本分数)`，alpha 是一个超参数，f 是接下来实验要选的函数，先直接用 log(正样本分数 - 负样本分数), alpha=1

在不同的数据集大小上，ODPO 都实现了`帕累托`改进

实验显示, ODPO 显著优于 DPO, 尤其是 数据集小的情形。

### ORPO

【2024-4-8】[消费级显卡搞定人类偏好对齐（RLHF）, 不用参考模型的对齐算法——ORPO实战](https://zhuanlan.zhihu.com/p/691313208?utm_psn=1760752368277291009)

RLHF和DPO资源消耗较多，能不能摆脱参考模型，直接进行人类偏好对齐？
- ORPO算法

`ORPO`（Odds Ratio Policy Optimization，**赔率比策略优化**）**无需参考模型**LLM语言模型偏好对齐的技术
- 通过对拒绝的回答施加一个**小惩罚**，同时对选择回答施加一个**强奖励**，然后通过简单**对数赔率项**添加到负对数似然损失上。

ORPO算法目标函数由两部分组成：
- 监督微调损失：遵循传统的因果语言模型负对数似然损失函数，以最大化生成参考标记的可能性。
- 相对比率损失：最大化在给定输入prompt时，生成非偏好响应reject与生成偏好响应chosen之间的可能性比率。

通过最小化监督微调损失，同时最大化相对比率损失，ORPO实现了人类偏好对齐，最重要的是不需要任何参考模型，相比于DPO少了一个REF参考模型，相比于RLHF(PPO)少了一个Reward Model 和一个REF参考模型。
- ![](https://pic3.zhimg.com/80/v2-d2e4a682767605ffd91ac1d93c34b3ca_1440w.webp)

ORPO 最终损失形式如下（trl的实现）： 
- `loss` = `policy_nll_loss`（监督微调损失） - `losses`（相对比率损失） 。
- loss 前一部分就是 监督微调损失 policy_nll_loss , 即chosen样本的llm预测损失。
- 后一部分losses为相对比率损失

附
- [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)
- 官方代码 [orpo](https://github.com/xfactlab/orpo/tree/main)

实现代码如下：

```py
def odds_ratio_loss(
        self,
        policy_chosen_logps: torch.FloatTensor,
        policy_rejected_logps: torch.FloatTensor,
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:
        """Compute ORPO's odds ratio (OR) loss for a batch of policy and reference model log probabilities.

        Args:
            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)
            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)

        Returns:
            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).
            The losses tensor contains the ORPO loss for each example in the batch.
            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.
            The log odds ratio of the chosen responses over the rejected responses ratio for logging purposes.
            The `log(sigmoid(log_odds_chosen))` for logging purposes.
        """

        # Derived from Eqs. (4) and (7) from <https://arxiv.org/abs/2403.07691> by using log identities and exp(log(P(y|x)) = P(y|x)
        log_odds = (policy_chosen_logps - policy_rejected_logps) - (
            torch.log1p(-torch.exp(policy_chosen_logps)) - torch.log1p(-torch.exp(policy_rejected_logps))
        )
        sig_ratio = F.sigmoid(log_odds)
        ratio = torch.log(sig_ratio)
        losses = self.beta * ratio

        chosen_rewards = self.beta * (policy_chosen_logps.to(self.accelerator.device)).detach()
        rejected_rewards = self.beta * (policy_rejected_logps.to(self.accelerator.device)).detach()

        return losses, chosen_rewards, rejected_rewards, torch.mean(ratio).item(), torch.mean(log_odds).item()
```

解释：
- 两个选择：一个正确，一个错误。
- ORPO损失帮模型更倾向于选择正确的那个。
  - ORPO损失会计算模型选择正确答案和错误答案的概率差距
  - 然后鼓励模型增加选择正确答案的概率，减少选择错误答案的概率。
  - 这样，模型就会慢慢学会怎样做出更好的选择。

对比了一下3种人类偏好对齐方法的的效果： ORPO 算法在低资源情况下，偏好对齐的性能比DPO和PPO还好一点。
- ![](https://pic4.zhimg.com/80/v2-8d2f909790fecbaa777a5b576b4ff03f_1440w.webp)

trl包最新实现的ORPO算法，数据集格式如下，主要需要包含
- “prompt”：人类输入的指令
- “chosen”：人类偏好的机器输出的结果
- “rejected” ：人类拒绝的机器输出的结果

```json
{'prompt': '考虑以下方程，附加条件是 x 必须是一个质数：\n1 + 1 = x\nx 的值是多少？',
 'system': '',
 'chosen': '对于方程式1 + 1 = x和x必须是质数的限制条件，x的值为2。在这种情况下，1 + 1 = 2，2是最小的质数，同时满足方程和限制条件。',
 'rejected': '方程式1 + 1 = x有一个解为x = 2，而且由于x必须是一个质数，唯一可能的解就是x = 2。',
 'source': 'evol_instruct',
 'id': None}
```

qwen1.5-1.8B-chat 实践 参考:
- [消费级显卡搞定人类偏好对齐（RLHF）, 不用参考模型的对齐算法——ORPO实战](https://zhuanlan.zhihu.com/p/691313208?utm_psn=1760752368277291009)


### SimPO

DPO 问题
- 即使训练时把 reward 学对了，也不代表生成正样本的概率确实大于了生成负样本的概率

【2024-5-27】[全面超越DPO：陈丹琦团队提出简单偏好优化SimPO，还炼出最强8B开源模型](https://www.jiqizhixin.com/articles/2024-05-27-8)

【2024-5-23】弗吉尼亚+普林斯顿, 陈丹琪团队 推出 SimPO, 简单却有效的离线偏好优化算法。
- 论文 [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/pdf/2405.14734)
- 代码及模型 [SimPO](https://github.com/princeton-nlp/SimPO)
- 损失函数: 对序列的概率取平均
  - ![](https://pic1.zhimg.com/80/v2-2f2f86c21ed0ab4eb4809f27512e6c30_1440w.webp)


#### SimPO 原理

细节对比分析：
1. DPO 的奖励公式隐式地促进了长度归一化，不过 SimPO 在这方面略胜一点，用直接归一化效果更好 
2. 奖励公式与生成似然的不匹配 
3. SimPO 奖励分类准确性更高、内存开销更小

![](https://pic1.zhimg.com/80/v2-5bfbcc92fad5c7ca15a82c59af2cdb68_1440w.webp)


算法核心将偏好优化目标中的奖励函数与生成指标对齐。

SimPO 包含两个主要组件：
- （1）在长度上归一化奖励，其计算方式是使用策略模型的奖励中所有 token 的平均对数概率；
- （2）目标奖励差额，用以确保获胜和失败响应之间的奖励差超过这个差额。

总结起来，SimPO 具有以下特点：
- 简单：SimPO 不需要参考模型，因此比 DPO 等其它依赖参考模型的方法更轻量更容易实现；
- 性能优势明显：尽管 SimPO 很简单，但其性能却明显优于 DPO 及其最新变体（比如近期的无参考式目标 ORPO）。如图 1 所示。并且在不同的训练设置和多种指令遵从基准（包括 AlpacaEval 2 和高难度的 Arena-Hard 基准）上，SimPO 都有稳定的优势；
- 尽量小的长度利用：相比于 SFT 或 DPO 模型，SimPO 不会显著增加响应长度（见表 1），这说明其长度利用是最小的。

#### SimPO 效果

效果分析

实验相当全面，把多个变种都拿来比较，比如
- IPO 和 KTO 都不用成对数据
- ORPO 也不需要参考模型
- R-DPO 是加长度相关正则化

最后，SimPO 比前述五种方法都好

基于 Llama3-8B-instruct 构建具有顶尖性能的模型
- 其在 AlpacaEval 2 上得到的长度受控式胜率为 44.7，在排行榜上超过了 Claude 3 Opus；
- 另外其在 Arena-Hard 上的胜率为 33.8，使其成为了目前最强大的 8B 开源模型。

SimPO 能更有效地利用偏好数据，在验证集上对高质量和低质量响应的似然进行更准确的排序，这进一步能造就更好的策略模型。


## 自博弈（Self-play）


### OpenAI o1

【2024-9-14】[OpenAI o1 强化学习背后的自博弈（Self-play）方法介绍](https://mp.weixin.qq.com/s/zyAHcigtI2fEFN3TKQBb6A)

自博弈（Self-play）强化学习核心: 通过**自我对弈**不断进化。
- 《[A Survey on Self-play Methods in Reinforcement Learning](https://arxiv.org/pdf/2408.01072)》介绍了自博弈方法的理论基础、关键技术以及在多样化场景下的应用实践。

自博弈（Self-play）借助`博弈论`建模多个决策者之间的互动，为解决MARL中的固有问题提供了优雅的解决方案

内容提要
1. 引言与背景
  - 人工智能与强化学习
  - 自博弈的兴起与重要性
  - AlphaGo作为自博弈的里程碑
2. 预备知识：**自博弈基础**
  - 多智能体强化学习（MARL）概念
  - 博弈论基础
  - 自博弈评估指标
3. 自博弈技术概览
  - 1）自博弈算法分类
    - 传统自博弈算法
    - PSRO系列算法
    - 基于持续训练的算法
    - 基于遗憾最小化的算法
  - 2）自博弈在不同领域的应用
    - 棋盘博弈：围棋、象棋、战棋
    - 纸牌博弈：德州扑克、斗地主、麻将
    - 视频游戏：《星际争霸II》、MOBA游戏、Google Research Football
  - 3）算法性能评估
    - 数据集与基准测试
    - 评估指标：ELO、Glicko、TrueSkill等
4. 挑战与开放问题
  - 自博弈的收敛性问题
  - 环境非平稳性与算法鲁棒性
  - 可扩展性与训练效率
  - 自博弈在大型语言模型中的应用  

### 复现 rStar


【2024-9-15】[OpenAI o1的开源平替版self-replay RL来了](https://mp.weixin.qq.com/s/KlLU3eHsFn0qo0N8nmqK9g)

MSQA 和 哈佛 发表 `rStar`，对标OpenAI的超级对齐Q*项目
- 论文《[MUTUAL REASONING MAKES SMALLER LLMS STRONGER PROBLEM-SOLVERS](https://arxiv.org/pdf/2408.06195)》
- 代码链接：[rStar](https://github.com/zhentingqi/rStar)


`rStar` self-play 互推理方法，显著提高了小型语言模型（SLMs）的推理能力，而无需微调或更高级的模型。
- 首先，目标SLM通过丰富的类人推理动作来增强蒙特卡洛树搜索（MCTS），构建更高质量的推理轨迹。
- 接下来，另一个能力与目标SLM相似的SLM充当判别器，验证目标SLM生成的每个轨迹。得分都很高的推理轨迹被认为是相互一致的，因此更有可能是正确的。

`rStar` 解法如下：
- 尽管依赖传统**蒙特卡洛树搜索**（Monte Carlo Tree Search, `MCTS`）让SLMs自我生成推理步骤，但`rStar`提倡在自我探索中使用更丰富的推理动作集。新提出的动作模拟了给定当前推理状态下的人类推理行为，例如分解和搜索特定的推理步骤，提出新的子问题，或重新表述给定问题。这使得SLMs能够在自我探索中生成高质量的候选推理轨迹。
- 为了有效地指导生成的推理轨迹之间的探索，rStar 通过相互一致性的新判别过程增强了MCTS过程。
  - rStar使用第二个能力相似的SLM作为判别器，为MCTS生成的每个候选推理轨迹提供无监督的反馈。为了提高反馈的准确性，rStar向第二个SLM提供采样的部分推理轨迹作为提示，要求其完成剩余的推理步骤。
  - rStar认为相互同意的推理轨迹质量更高。相互一致性反映了在缺乏监督的情况下的常见人类实践，其中同行（即两个SLMs）对推导出的答案的一致性表明了更高的可能性是正确的。因此，相互一致性提供了比其他方法（如自我一致性）更有效的跨任务推理，并避免了训练奖励模型时过度拟合特定任务的风险（类似model ensemble）。

在五个SLMs上的实验表明，rStar 可有效解决多种推理问题，包括GSM8K、GSM-Hard、MATH、SVAMP和StrategyQA。
- `rStar`
  - 将 `LLaMA2-7B` 在 GSM8K数据集上的准确率从**12.51%**提高到**63.91%**
  - 将`Mistral-7B` 准确率从**36.46%**提高到**81.88%**
  - 将`LLaMA3-8BInstruct` 准确率从**74.53%**提高到**91.13%**。



# 结束