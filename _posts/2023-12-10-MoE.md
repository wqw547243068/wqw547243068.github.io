---
layout: post
title:  æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ä¸“é¢˜
date:   2023-11-10 16:52:00
categories: å¤§æ¨¡å‹
tags: gpt moe ä¸“å®¶ llama transformer pytorch mistral mixtral
excerpt: æ··åˆä¸“å®¶æ¨¡å‹ï¼ŒMoEç³»åˆ—
mathjax: true
permalink: /moe
---

* content
{:toc}


# MoE æ··åˆä¸“å®¶æ¨¡å‹



## ä»€ä¹ˆæ˜¯ MoE

MoEæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡ï¼Œåœ¨Transformeræ¨¡å—ä¸­é›†æˆäº†ä¸“å®¶/æ¨¡å‹å±‚ã€‚


ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰æŠŠå¤æ‚ä»»åŠ¡åˆ†å‰²æˆä¸€ç³»åˆ—æ›´å°ã€æ›´å®¹æ˜“å¤„ç†çš„**å­ä»»åŠ¡**ï¼Œæ¯ä¸ªå­ä»»åŠ¡ç”±ä¸€ä¸ªç‰¹å®šé¢†åŸŸçš„ã€Œä¸“å®¶ã€è´Ÿè´£ã€‚

å½“æ•°æ®æµç»MoEå±‚æ—¶ï¼Œæ¯ä¸ªè¾“å…¥tokenéƒ½ä¼š**åŠ¨æ€**è·¯ç”±åˆ°ä¸“å®¶å­æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚å½“æ¯ä¸ªä¸“å®¶ä¸“é—¨ä»äº‹ç‰¹å®šä»»åŠ¡æ—¶ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å®ç°æ›´é«˜æ•ˆçš„è®¡ç®—å¹¶è·å¾—æ›´å¥½çš„ç»“æœã€‚
- å°†éœ€è¦é¢„æµ‹çš„é—®é¢˜åˆ’åˆ†ä¸º**å­ä»»åŠ¡**ï¼ˆé‡‡ç”¨é¢†åŸŸçŸ¥è¯†æˆ–è€…æ— ç›‘ç£èšç±»ç®—æ³•ï¼‰ã€‚
- ç„¶åï¼Œé’ˆå¯¹æ¯ä¸ªæ•°æ®å­é›†è®­ç»ƒ**ä¸“å®¶æ¨¡å‹**ï¼ˆExpert Modelsï¼‰ï¼Œä¸“å®¶æ¨¡å‹å¯ä»¥æ˜¯ä»»ä½•æ¨¡å‹ï¼Œæ¯”å¦‚æ”¯æŒå‘é‡æœº ï¼ˆSVMï¼‰ æˆ–è€…ç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªä¸“å®¶æ¨¡å‹æ¥æ”¶ç›¸åŒçš„è¾“å…¥æ¨¡å¼å¹¶è¿›è¡Œé¢„æµ‹ã€‚
  - MoEè¿˜åŒ…å«**é—¨æ§æ¨¡å‹**ï¼ˆGating Modelï¼‰ï¼Œç”¨äºè§£é‡Šæ¯ä¸ªä¸“å®¶åšå‡ºçš„é¢„æµ‹ï¼Œå¹¶æ ¹æ®è¾“å…¥é€‰æ‹©ä¿¡ä»»å“ªä¸ªä¸“å®¶ã€‚
- æœ€åï¼ŒMoEéœ€è¦ä¸€ç§**èšåˆæœºåˆ¶**ï¼ˆPooling Methodï¼‰ï¼Œæ ¹æ®é—¨æ§æ¨¡å‹å’Œä¸“å®¶çš„è¾“å‡ºè¿›è¡Œé¢„æµ‹ã€‚

åŸå§‹MoEçš„è¿­ä»£:ã€Œç¨€ç–é—¨æ§ä¸“å®¶æ··åˆå±‚ã€çš„æ–¹æ³•ï¼Œæä¾›äº†ä¸€ä¸ªé€šç”¨çš„ç¥ç»ç½‘ç»œç»„ä»¶ï¼Œå¯ä»¥é€‚åº”ä¸åŒç±»å‹çš„ä»»åŠ¡ã€‚

æ··åˆä¸“å®¶æ¨¡å‹åœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰åº”ç”¨ï¼ŒåŒ…æ‹¬æ¨èç³»ç»Ÿã€è¯­è¨€å»ºæ¨¡å’Œå„ç§å¤æ‚çš„é¢„æµ‹ä»»åŠ¡ã€‚

### MoE ç»“æ„

MoEç”±ä¸¤ç§ç±»å‹çš„ç½‘ç»œç»„æˆ:(1)ä¸“å®¶ç½‘ç»œå’Œ(2)é—¨æ§ç½‘ç»œã€‚
- `ä¸“å®¶ç½‘ç»œ`: ä¸“å®¶ç½‘ç»œæ˜¯**ä¸“æœ‰æ¨¡å‹**ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½ç»è¿‡è®­ç»ƒï¼Œåœ¨æ•°æ®å­é›†ä¸­è¡¨ç°å‡ºè‰²ã€‚
  - MoEçš„ç†å¿µæ˜¯æ‹¥æœ‰å¤šåä¼˜åŠ¿äº’è¡¥çš„ä¸“å®¶ï¼Œç¡®ä¿å¯¹é—®é¢˜ç©ºé—´çš„å…¨é¢è¦†ç›–ã€‚
- `é—¨æ§ç½‘ç»œ`: é—¨æ§ç½‘ç»œå……å½“**æŒ‡æŒ¥**ï¼Œåè°ƒæˆ–ç®¡ç†ä¸ªåˆ«ä¸“å®¶çš„è´¡çŒ®ã€‚å®ƒå­¦ä¹ (æˆ–æƒè¡¡)å“ªä¸ªç½‘ç»œæ“…é•¿å¤„ç†å“ªç§ç±»å‹çš„è¾“å…¥ã€‚ç»è¿‡è®­ç»ƒçš„é—¨æ§ç½‘ç»œå¯ä»¥è¯„ä¼°æ–°çš„è¾“å…¥å‘é‡ï¼Œå¹¶æ ¹æ®ä¸“å®¶çš„ç†Ÿç»ƒç¨‹åº¦å°†å¤„ç†è´£ä»»åˆ†é…ç»™æœ€åˆé€‚çš„ä¸“å®¶æˆ–ä¸“å®¶ç»„åˆã€‚é—¨æ§ç½‘ç»œæ ¹æ®ä¸“å®¶çš„è¾“å‡ºä¸å½“å‰è¾“å…¥çš„ç›¸å…³æ€§åŠ¨æ€è°ƒæ•´å…¶æƒé‡ï¼Œç¡®ä¿å®šåˆ¶å“åº”ã€‚

MoE å¤„ç†æµç¨‹
- ![](https://pic1.zhimg.com/80/v2-654cca302d29837e461c6ef04667a1a0_1440w.webp)

MoEæœ€å…³é”®çš„ç»„ä»¶ï¼š
- **ä¸“å®¶**ï¼ˆExpertï¼‰ï¼š
  - ä¸“é—¨è®­ç»ƒçš„å°å‹ç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œéƒ½åœ¨å…¶æ“…é•¿çš„é¢†åŸŸæœ‰ç€å“è¶Šçš„è¡¨ç°
  - MoEå±‚ç”±è®¸å¤š**ä¸“å®¶**ã€**å°å‹MLP**æˆ–**å¤æ‚LLM**ï¼ˆå¦‚ Mistral 7Bï¼‰ç»„æˆã€‚
- **è·¯ç”±å™¨**ï¼ˆRouterï¼‰ï¼š`é—¨æ§ç½‘ç»œ`, MoEæ¶æ„ä¸­çš„**å†³ç­–æ ¸å¿ƒ**
  - è·¯ç”±å™¨ç¡®å®šå°†å“ªäº›è¾“å…¥tokenåˆ†é…ç»™å“ªäº›ä¸“å®¶ã€‚
  - é—¨æ§ç½‘ç»œä¼šè®¡ç®—è¾“å…¥æ•°æ®ä¸æ¯ä¸ªä¸“å®¶çš„**å…¼å®¹æ€§å¾—åˆ†**ï¼Œç„¶åä¾æ®è¿™äº›å¾—åˆ†å†³å®šæ¯ä¸ªä¸“å®¶åœ¨å¤„ç†ä»»åŠ¡ä¸­çš„ä½œç”¨ã€‚

è·¯ç”±ç­–ç•¥æœ‰ä¸¤ç§ï¼š**tokené€‰æ‹©è·¯ç”±å™¨** æˆ– **è·¯ç”±å™¨é€‰æ‹©token**ã€‚

è·¯ç”±å™¨ä½¿ç”¨**softmaxé—¨æ§å‡½æ•°**é€šè¿‡ä¸“å®¶æˆ–tokenå¯¹æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€‰æ‹©å‰kä¸ªã€‚

è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œç¡®ä¿é€‚åˆçš„ä»»åŠ¡ç”±åˆé€‚çš„ä¸“å®¶æ¥å¤„ç†ã€‚é—¨æ§ç½‘ç»œæœ‰æ•ˆåœ°å°†è¾“å…¥æ•°æ®å¼•å¯¼è‡³æœ€åˆé€‚çš„ä¸“å®¶ï¼Œè€Œä¸“å®¶ä»¬åˆ™ä¸“æ³¨äºè‡ªå·±æ“…é•¿çš„é¢†åŸŸã€‚è¿™ç§åˆä½œæ€§è®­ç»ƒä½¿å¾—æ•´ä½“æ¨¡å‹å˜å¾—æ›´åŠ å¤šåŠŸèƒ½å’Œå¼ºå¤§ã€‚

MoE å¥½å¤„ï¼š
- æ¯ä¸ªä¸“å®¶éƒ½å¯ä»¥ä¸“é—¨å¤„ç†ä¸åŒçš„ä»»åŠ¡æˆ–æ•°æ®çš„ä¸åŒéƒ¨åˆ†ã€‚
- MoEæ„æ¶èƒ½å‘LLMæ·»åŠ å¯å­¦ä¹ å‚æ•°ï¼Œè€Œä¸å¢åŠ æ¨ç†æˆæœ¬ã€‚
- å¯ä»¥åˆ©ç”¨ç¨€ç–çŸ©é˜µçš„é«˜æ•ˆè®¡ç®—
- å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä¸“å®¶å±‚ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨GPUçš„å¹¶è¡Œèƒ½åŠ›
- å¸®åŠ©æœ‰æ•ˆåœ°æ‰©å±•æ¨¡å‹å¹¶å‡å°‘è®­ç»ƒæ—¶é—´ã€‚ä»¥æ›´ä½çš„è®¡ç®—æˆæœ¬è·å¾—æ›´å¥½çš„ç»“æœ


### MoEéƒ¨ç½²

MoE ä¸ºéƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹æä¾›äº†å·¨å¤§çš„å¥½å¤„
- MoEæ ¸å¿ƒä¼˜åŠ¿ï¼šå…¶ä¸“å®¶ç½‘ç»œçš„å¤šå…ƒåŒ–å’Œä¸“ä¸šåŒ–ã€‚MoEçš„è®¾ç½®èƒ½å¤Ÿä»¥å•ä¸€æ¨¡å‹å¯èƒ½éš¾ä»¥è¾¾åˆ°çš„ç²¾åº¦å¤„ç†å¤šæ–¹é¢çš„é—®é¢˜ã€‚
- MoEå¯ä¼¸ç¼©æ€§ï¼šéšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼Œä¸æ”¹å˜å…¶ä»–ä¸“å®¶æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå°†æ›´å¤šä¸“å®¶æ— ç¼åœ°é›†æˆåˆ°ç³»ç»Ÿä¸­ï¼Œæ‰©å¤§ä¸“ä¸šçŸ¥è¯†çš„èŒƒå›´ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒMoEå¯ä»¥å¸®åŠ©å°†é¢„å…ˆè®­ç»ƒè¿‡çš„ä¸“å®¶æ‰“åŒ…åˆ°æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­ã€‚


# å®ç°æ¡ˆä¾‹


## MoE æ¨¡å‹æ±‡æ€»

ã€2024-4-26ã€‘[MoEæ¨¡å‹çš„å‰ä¸–ä»Šç”Ÿ](https://mp.weixin.qq.com/s/jhT4kv9c7fJp4xwSfckoag)

2024å¹´3ã€4æœˆè¿™æ®µæ—¶é—´ï¼Œå¾ˆå¤šMoEæ¨¡å‹æ‰å †å‘å¸ƒï¼ŒåŒ…æ‹¬Qwen1.5-MoEã€DBRXã€Jambaå’ŒMistralç­‰ã€‚

| æ¨¡å‹ | å‘å¸ƒæ—¶é—´| å¤‡æ³¨| 
| --- | ----| --- | 
| GPT4| 2023å¹´3æœˆ| 23å¹´6æœˆGeorge Hotzçˆ†æ–™GPT4æ˜¯8Ã—220Bæ¨¡å‹ | 
| Mistral-8Ã—7B| 2023å¹´12æœˆ| Mistral AIï¼Œå¼€æº| 
| LLAMA-MoE| 2023å¹´12æœˆ| githubå¼€æºé¡¹ç›®|
| DeepSeek-MoE| 2024å¹´1æœˆ| å¹»æ–¹é‡åŒ–ï¼Œå›½å†…é¦–ä¸ªå¼€æºMoEæ¨¡å‹ï¼Œæœ‰æŠ€æœ¯æŠ¥å‘Š| 
| abab6| 2024å¹´1æœˆ| MiniMaxï¼Œå·ç§°åƒäº¿MoEï¼Œæ— å¼€æºï¼Œæ— ç»†èŠ‚å‘å¸ƒ| 
| å¤©å·¥2.0| 2024å¹´2æœˆ| æ˜†ä»‘ä¸‡ç»´ï¼Œæ— å¼€æºï¼Œæ— ç»†èŠ‚å‘å¸ƒ| 
| Step-2| 2024å¹´3æœˆ| é˜¶è·ƒæ˜Ÿè¾°ï¼Œæ— å¼€æºï¼Œæ— ç»†èŠ‚å‘å¸ƒ| 
| MM1| 2024å¹´3æœˆ| è‹¹æœï¼Œå¤šæ¨¡æ€MoEï¼Œæ— å¼€æºï¼Œæœ‰æŠ€æœ¯æŠ¥å‘Š| 
| Grok-1| 2024å¹´3æœˆ| Xï¼Œå¼€æº| 
| Qwen1.5-MoE-A2.7B| 2024å¹´3æœˆ| é˜¿é‡Œå·´å·´ï¼Œå¼€æº| 
| DBRX| 2024å¹´3æœˆ| Databricksï¼Œå¼€æº| 
| Jamba| 2024å¹´3æœˆ| AI21ï¼Œå¼€æº| 
| Mistral-8Ã—22B| 2024å¹´4æœˆ| Mistral AIï¼Œå¼€æº| 
| WizardLM-2-8Ã—22B| 2024å¹´4æœˆ| å¾®è½¯ï¼Œå¼€æº| 
| å¤©å·¥3.0| 2024å¹´4æœˆ| æ˜†ä»‘ä¸‡ç»´ï¼Œ400BMoE| 
| Arctic| 2024å¹´4æœˆ| Snowflakeï¼Œ480Bï¼ŒDense-MoE Hybridï¼Œå¼€æº |


## Huggingface MoE

ã€2024-1-28ã€‘Huggingfaceä¸Šçš„ä¸€ç¯‡å†…å®¹ï¼Œéå¸¸è¯¦ç»†çš„ä»‹ç»äº†å¦‚ä½•ä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªMoEæ¶æ„çš„è¯­è¨€æ¨¡å‹
- [makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch](https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch?continueFlag=7a8b1a567a6c267396a07fd8c00a847c)
- å®æ–½è¿‡ç¨‹åŒ…æ‹¬é‡‡ç”¨ç¨€ç–æ··åˆä¸“å®¶å–ä»£ä¼ ç»Ÿçš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œå®ç° top-k é—¨æ§å’Œå¸¦å™ªå£°çš„ top-k é—¨æ§ï¼Œä»¥åŠé‡‡ç”¨ Kaiming He åˆå§‹åŒ–æŠ€æœ¯ã€‚
- ä» makemore æ¶æ„ä¿æŒä¸å˜çš„å…ƒç´ ï¼Œæ¯”å¦‚æ•°æ®é›†å¤„ç†ã€åˆ†è¯é¢„å¤„ç†å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚
- Github [makemoe-from-scratch](https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch)
- ![](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/moe.png)


## GPT-4

[GPT-4æ··åˆå¤§æ¨¡å‹ï¼Ÿç ”ç©¶è¯æ˜MoE+æŒ‡ä»¤è°ƒä¼˜ç¡®å®è®©å¤§æ¨¡å‹æ€§èƒ½è¶…ç¾¤](https://www.toutiao.com/article/7253055129237422626)
- 6æœˆ, ã€Œå¤©æ‰é»‘å®¢ã€ä¹”æ²»ãƒ»éœå…¹ï¼ˆGeorge Hotzï¼‰åœ¨æ¥å—ä¸€å®¶åä¸º Latent Space çš„ AI æŠ€æœ¯æ’­å®¢çš„é‡‡è®¿æ—¶æåˆ°äº† GPT-4ï¼Œå¹¶ç§°: GPT-4 å…¶å®æ˜¯ä¸€ä¸ª**æ··åˆ**æ¨¡å‹ã€‚
- GPT-4 é‡‡ç”¨ç”± 8ä¸ªä¸“å®¶æ¨¡å‹ç»„æˆçš„é›†æˆç³»ç»Ÿï¼Œæ¯ä¸ªä¸“å®¶æ¨¡å‹éƒ½æœ‰ 2200 äº¿ä¸ªå‚æ•°ï¼ˆæ¯” GPT-3 çš„ 1750 äº¿å‚æ•°é‡ç•¥å¤šä¸€äº›ï¼‰ï¼Œå¹¶ä¸”è¿™äº›æ¨¡å‹ç»è¿‡äº†é’ˆå¯¹ä¸åŒæ•°æ®å’Œä»»åŠ¡åˆ†å¸ƒçš„è®­ç»ƒã€‚

è°·æ­Œã€UC ä¼¯å…‹åˆ©ç­‰è¯æ˜ MoE + æŒ‡ä»¤è°ƒä¼˜èµ·åˆ°äº† 1 + 1 > 2 çš„æ•ˆæœã€‚[è®ºæ–‡](https://arxiv.org/pdf/2305.14705.pdf)
- è°·æ­Œã€UC ä¼¯å…‹åˆ©ã€MIT ç­‰æœºæ„çš„ç ”ç©¶è€…è”åˆå‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡è¯å®ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ä¸æŒ‡ä»¤è°ƒä¼˜çš„ç»“åˆèƒ½å¤Ÿè®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½å¤§å¹…æå‡ã€‚

MoEæ˜¯ä¸‹ä¸€ä»£LLMæ¶æ„ï¼Œå®ç°
- [moduleformer](https://github.com/ibm/moduleformer)

## Megatron-LM MoE

ã€2023-11-15ã€‘[Megatron-LM MoE ä»£ç è§£æ](https://zhuanlan.zhihu.com/p/666653126?utm_psn=1708124942137335808)

æ–°ç‰ˆæœ¬çš„ Megatron-LM ä¸­ï¼ŒNvidia ä¹Ÿé‡Šå‡ºäº† MoE çš„é…å¥—å®ç°ã€‚è™½ç„¶æ˜¯ token droplessï¼ŒåŸç”Ÿæ”¯æŒ Megatron çš„ 3D å¹¶è¡Œå’Œ Expert Parallelism

arguments.py ä¸­åŠ å…¥äº† MoE ç›¸å…³çš„å‚æ•°é€‰é¡¹
- --num-experts: Expert çš„æ•°é‡
- --expert-parallel: å¼€å¯ Expert Parallelism
- --expert-model-parallel-size: Expert Parallelism çš„ degreeï¼Œå› ä¸º Expert Parallelism (EP) è¢«æ”¾åœ¨äº† Data Parallelism (DP) é‚£ä¸€ç»´ï¼Œå› æ­¤åœ¨è®¾ç½®æ—¶è¦æ±‚ DP éœ€è¦èƒ½å¤Ÿè¢« EP æ•´é™¤ï¼ˆå¯ä»¥è¿™æ ·ç†è§£ï¼Œåœ¨ä¸è€ƒè™‘ EP çš„æƒ…å†µä¸‹ï¼Œä¸ç®¡ TP å’Œ PP å¦‚ä½•è®¾ç½®ï¼ŒDP çš„å¤§å°å§‹ç»ˆå¯¹åº”æœ‰å¤šå°‘ä»½ model copy åœ¨å¹¶è¡Œè®­ç»ƒï¼ŒExpert Parallelism ç›¸å½“äºæŠŠæ‰€æœ‰çš„ Experts åˆ‡åˆ†åˆ° EP ä»½è¿™æ ·çš„ model copy ä¸Šï¼Œå› æ­¤ DP å¿…é¡»èƒ½è¢« EP æ•´é™¤ï¼Œå¦åˆ™æ ¹æœ¬æ²¡æ³•åˆ‡ï¼‰ã€‚åŸåˆ™ä¸Šæ¯å¼  GPU ä¸Šå¯ä»¥æ”¾å¤šä¸ª Expertï¼Œæ¯ä¸ª Expert ä¹Ÿå¯ä»¥è¢«åˆ‡åˆ†åˆ°å¤šå¼  GPU ä¸Šã€‚å¦‚æœå›ºå®šæ¯å¼  GPU å¯¹åº”ä¸€ä¸ª Expertï¼Œé‚£ä¹ˆå¯¹äºä¸€ä¸ª Expert=16 çš„ MoE æ¨¡å‹ï¼ŒEP=16ï¼ŒDP ä¹Ÿè‡³å°‘æ˜¯16ï¼Œæ‰€ä»¥å¯¹èµ„æºçš„è¦æ±‚è¿˜æ˜¯å¾ˆé«˜çš„ã€‚

æ¨¡å‹å®ç°ä¸Šåªæ˜¯åœ¨ ParallelTransformerLayer åˆå§‹åŒ–æ—¶å°† ParallelMLP æ›¿æ¢æˆäº† SwitchMLP, ä»£ç å®ç°è§[åŸæ–‡](https://zhuanlan.zhihu.com/p/666653126?utm_psn=1708124942137335808)


## ã€2022-6-16ã€‘è°·æ­Œ Switch Transformer

ã€2022-6-16ã€‘è°·æ­Œå¼€æºäº†åŸºäºT5çš„MoEæ¨¡å‹ â€”â€” Switch Transformer
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)

ä»£ç 
1. JAX code for Switch Transformer and all model checkpoints are available [at](https://github.com/
google-research/t5x)
2. Tensorflow code for Switch Transformer is available [at](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)


## ã€2023-8-21ã€‘OpenMoE

æ›¾åœ¨è‹±ä¼Ÿè¾¾å®ä¹ çš„æ–°åŠ å¡å›½ç«‹å¤§å­¦åšå£«ç”ŸFuzhao Xueè¡¨ç¤ºï¼Œä»–ä»¬å›¢é˜Ÿåœ¨4ä¸ªæœˆå‰ä¹Ÿå¼€æºäº†ä¸€ä¸ª80äº¿å‚æ•°çš„MoEæ¨¡å‹ [OpenMoE](https://github.com/XueFuzhao/OpenMoE)

æ¨¡å‹æ¶æ„
- OpenMoEæ¨¡å‹åŸºäºã€ŒST-MoEã€ï¼Œä½†é‡‡ç”¨äº†decoder-onlyæ¶æ„ã€‚

å…¶å®ƒè®¾è®¡
- é‡‡ç”¨umT5 tokenizer
- ä½¿ç”¨RoPEæŠ€æœ¯
- é‡‡ç”¨SwiGLUæ¿€æ´»å‡½æ•°
- è®¾å®š2000 tokençš„ä¸Šä¸‹æ–‡é•¿åº¦




## ã€2023-11-22ã€‘LM-Cocktail

é—®é¢˜
- LLM finetuneæ–¹å¼ä¼šå¯¼è‡´ç›®æ ‡ä»»åŠ¡ä¹‹å¤–çš„ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œæ€§èƒ½ä¸¥é‡è¡°å‡ï¼ˆperformance degenerationï¼‰

- è®ºæ–‡ï¼š[LM-Cocktail: Resilient Tuning of Language Models via Model Merging](https://arxiv.org/pdf/2311.13534.pdf)
- ä»£ç ï¼š[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)ä¸­çš„å­ç›®å½• [LM_Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)

BAAIå’Œä¸­ç§‘é™¢å‘å¸ƒ LM-Cocktailï¼Œä½¿ç”¨æ¨¡å‹èåˆï¼ˆmodel mergingï¼‰æ–¹å¼
- å°† finetuneæ¨¡å‹èå…¥ pre-trainæ¨¡å‹ä¸­
- æˆ– ä¸¤è€…åŒç­‰é‡è¦ï¼ŒåŠ æƒ

BAAIæ›´å¤šå·¥ä½œ
-   11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), ä¸€ç§é€šè¿‡æ¨¡å‹èåˆåœ¨å¾®è°ƒæ—¶ä¿æŒåŸæœ‰æ¨¡å‹é€šç”¨èƒ½åŠ›çš„æ–¹æ³•. [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2311.13534) ğŸ”¥
-   10/12/2023: å‘å¸ƒ [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), ä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹**å„ç§æ£€ç´¢å¢å¼ºä»»åŠ¡è®¾è®¡**çš„è‹±æ–‡å‘é‡æ¨¡å‹ã€‚[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2310.07554.pdf)
-   09/15/2023: å‘å¸ƒ [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2309.07597.pdf) å’Œ [æ•°æ®é›†](https://data.baai.ac.cn/details/BAAI-MTP).
-   09/12/2023: æ›´æ–°ï¼š
    -   **æ–°å¢é‡æ’æ¨¡å‹**ï¼šå¼€æºäº¤å‰ç¼–ç å™¨æ¨¡å‹bge-rerankerï¼Œå…·æœ‰æ¯”å‘é‡æ¨¡å‹æ›´å¼ºå¤§çš„æ’åºèƒ½åŠ›ã€‚éå¸¸å»ºè®®ä½¿ç”¨æˆ–è€…å¾®è°ƒå®ƒæ¥é‡æ–°æ’åºå‘é‡æ¨¡å‹è¿”å›çš„top-kæ–‡æ¡£ï¼Œæé«˜æœ€ç»ˆç»“æœçš„ç›¸å…³æ€§ã€‚
    -   **æ›´æ–°å‘é‡æ¨¡å‹**ï¼šå‘å¸ƒbge-\*-v1.5å‘é‡æ¨¡å‹ï¼Œç¼“è§£ç›¸ä¼¼åº¦åˆ†å¸ƒé—®é¢˜ï¼Œæå‡æ— æŒ‡ä»¤æƒ…å†µä¸‹çš„æ£€ç´¢èƒ½åŠ›ï¼ˆä½†æ£€ç´¢ä»»åŠ¡ä»å»ºè®®ä½¿ç”¨æŒ‡ä»¤ï¼‰
-   09/07/2023: æ›´æ–°[å¾®è°ƒä»£ç ](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): å¢åŠ éš¾è´Ÿæ ·æœ¬æŒ–æ˜è„šæœ¬ï¼Œå¢åŠ æŒ‡ä»¤å‚æ•°æ–¹ä¾¿åœ¨å¾®è°ƒä¸­æ·»åŠ æŒ‡ä»¤.
-   08/09/2023: BGEæ¨¡å‹æ•´åˆå…¥Langchain, å¯ä»¥åœ¨langchainä¸­éå¸¸ç®€å•çš„[ä½¿ç”¨å®ƒ](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md#using-langchain); C-MTEBä¸­æ–‡æ¦œå•å·²[åœ¨çº¿æ›´æ–°](https://huggingface.co/spaces/mteb/leaderboard).
-   08/05/2023: å‘å¸ƒæ›´å°çš„æ¨¡å‹(base, small), **åœ¨åŒå°ºå¯¸æ¨¡å‹ä¸­å–å¾—æœ€å¥½çš„æ€§èƒ½ï¼ ğŸ¤—**
-   08/02/2023: :tada: :tada: å‘å¸ƒä¸­è‹±æ–‡å‘é‡æ¨¡å‹BGE(BAAI General Embeddingçš„ç¼©å†™), **åœ¨MTEBå’ŒC-MTEBæ¦œå•ä¸Šå–å¾—æœ€å¥½çš„æ€§èƒ½**
-   08/01/2023: å‘å¸ƒå¤§è§„æ¨¡ä¸­æ–‡æ–‡æœ¬å‘é‡[è¯„æµ‹æ¦œå•](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), å…¶åŒ…æ‹¬31ä¸ªæµ‹è¯•ä»»åŠ¡.



æ•ˆæœ
- å¾®è°ƒçš„Llamaå’ŒBGEæ¨¡å‹
- FLAN, MMLU, MTEB ä¸ŠéªŒè¯äº† LM-Cocktail çš„æœ‰æ•ˆæ€§ã€‚


### å®‰è£…

```sh
# pipå®‰è£…
pip install -U LM_Cocktail
# æœ¬åœ°å®‰è£…
git clone https://github.com/FlagOpen/FlagEmbedding.git
cd FlagEmbedding/LM_Cocktail
pip install -e .
```


### ä»£ç ç†è§£

[LM_Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail/LM_Cocktail) ç›®å½•ä¸‹åªæœ‰å‡ ä¸ªæ–‡ä»¶ï¼š
- `cocktail.py`
  - ä» util ä¸­å¼•å…¥ load_model, get_model_param_list, merge_param, compute_weights
  - save_ckpt_for_sentence_transformers 
  - mix_models æ ¹æ®ç»™å®š**æƒé‡**æ··åˆæ¨¡å‹
  - mix_models_with_data æ ¹æ®ç»™å®š**å°æ ·æœ¬**æ··åˆæƒé‡
- `utils.py`: å®šä¹‰è‹¥å¹²æ–¹æ³•
  - load_llm, load_embedder, load_reranker, load_model
  - get_model_from_param è°ƒç”¨ load_model, è¿”å› model_param_list
  - merge_param æ¨¡å‹å‚æ•°èåˆ, å…¥å‚ model_param_list
  - compute_weights: è®¡ç®—æƒé‡
    - å¦‚æœ model_type = decoder, è°ƒ preprocess_data_for_llm
    - å¦‚æœ model_type = encoder, è°ƒ preprocess_data_for_embedder
    - è°ƒç”¨ loss_func



### å®è·µ

ä»£ç 
- æƒé‡ç´¯åŠ å¿…é¡»æ˜¯1

```py
from LM_Cocktail import mix_models, mix_models_with_data

# mix LLMs and save it to output_path: ./mixed_model_1
model = mix_models(
    model_names_or_paths=["meta-llama/Llama-2-7b-chat-hf", "Shitao/llama2-ag-news"], 
    model_type='decoder', 
    weights=[0.7, 0.3], 
    output_path='./mixed_llm')
# you can select a weight for your models to get a trade-off between generality and expertise.

model = mix_models(
    model_names_or_paths=["BAAI/bge-base-en-v1.5", "Shitao/bge-hotpotqa", "Shitao/bge-quora", "Shitao/bge-msmarco"], 
    model_type='encoder', 
    weights=[0.3, 0.2, 0.2, 0.3],
    output_path='./mixed_embedder_2')
# The sum of weights should be equal to 1.

# æ ¹æ® å°‘æ ·æœ¬ è‡ªåŠ¨è®¡ç®—æ¨¡å‹æƒé‡
example_data = [
    {"input": "Question: when was the last time anyone was on the moon? Answer:\n", "output": "14 December 1972 UTC"},
    {"input": "Review: \"it 's a charming and often affecting journey . \" Is this movie review sentence negative or positive?\n", "output": "Positive"}
]

model = mix_models_with_data(
    model_names_or_paths=["meta-llama/Llama-2-7b-chat-hf", "Shitao/llama2-ag-news", "Shitao/llama2-nq"], 
    model_type='decoder', 
    example_ata=example_data, 
    temperature=5.0)
# you can set the temperature argument to adjust the distribution of mixing weights

# ==== åµŒå…¥æ¨¡å‹ ===== Mix Embedding Models
model = mix_models(
    model_names_or_paths=["BAAI/bge-base-en-v1.5", "Shitao/bge-hotpotqa"], 
    model_type='encoder', 
    weights=[0.5, 0.5],
    output_path='./mixed_embedder')

# è‡ªåŠ¨é€‰æ‹©æƒé‡
example_data = [
    {"query": "How does one become an actor in the Telugu Film Industry?", "pos": [" How do I become an actor in Telugu film industry?"], "neg": [" What is the story of Moses and Ramesses?", " Does caste system affect economic growth of India?"]}, 
    {"query": "Why do some computer programmers develop amazing software or new concepts, while some are stuck with basic programming work?", "pos": [" Why do some computer programmers develops amazing softwares or new concepts, while some are stuck with basics programming works?"], "neg": [" When visiting a friend, do you ever think about what would happen if you did something wildly inappropriate like punch them or destroy their furniture?", " What is the difference between a compliment and flirting?"]}
]

model = mix_models_with_data(
    model_names_or_paths=["BAAI/bge-base-en-v1.5", "Shitao/bge-hotpotqa", "Shitao/bge-quora"], 
    model_type='encoder', 
    example_ata=example_data,
    temperature=5.0,
    max_input_length=512,
    neg_number=2)

# ==== æ’åºæ¨¡å‹ ==== Mix reranker Models
model = mix_models(
    model_names_or_paths=["BAAI/bge-reranker-base", "BAAI/bge-reranker-base"], 
    model_type='reranker', 
    weights=[0.5, 0.5],
    output_path="./mixed_reranker")
```


## ã€2023-12-11ã€‘Mistral-MoE

- ã€2023-12-11ã€‘å¼€æºMoEæ¨¡å‹ï¼š[8x7Bå¼€æºMoEå‡»è´¥Llama 2é€¼è¿‘GPT-4ï¼æ¬§ç‰ˆOpenAIéœ‡æƒŠAIç•Œï¼Œ22äººå…¬å¸åŠå¹´ä¼°å€¼20äº¿](https://mistral.ai/news/mixtral-of-experts/)
- ã€2024-1-10ã€‘[Mixtral 8x7Bè®ºæ–‡ç»ˆäºæ¥äº†ï¼šæ¶æ„ç»†èŠ‚ã€å‚æ•°é‡é¦–æ¬¡æ›å…‰](https://mp.weixin.qq.com/s/EHJcZd-JLeo29mDJPRXVXg)
- ã€2024-1-10ã€‘[æ··åˆä¸“å®¶ç³»ç»Ÿé‡Œæ ¹æœ¬æ²¡ä¸“å®¶ï¼Ÿå¼€æºMoEæ¨¡å‹è®ºæ–‡å¼•ç½‘å‹çƒ­è®®](https://www.toutiao.com/article/7322382983225360931),æ¯”èµ·â€œä¸“å®¶çš„ç»„åˆâ€ï¼Œå·¥ä½œæ–¹å¼æ›´åƒæ˜¯ä¸€ç§ç¡¬ç›˜é˜µåˆ—æˆ–è€…è´Ÿè½½å‡è¡¡
- [ä¸‡å­—é•¿æ–‡è¯¦è§£ Mixtral 8x7B - ä»·å€¼20äº¿ç¾å…ƒçš„ MoE å¤§è¯­è¨€æ¨¡å‹](https://zhuanlan.zhihu.com/p/676114291)

- mixtral-of-experts [ä¸»é¡µ](https://mistral.ai/news/mixtral-of-experts) 
- codeï¼š[mistral-src](https://github.com/mistralai/mistral-src)
-  Mistral 7B è®ºæ–‡åœ°å€ï¼š[Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf)
- è®ºæ–‡ [Mixtral of Experts](https://arxiv.org/abs/2401.04088)

`Mixtral 8x7B` å’Œ `Mixtral 8x7B â€“ Instruct` å…è´¹ä¾›å­¦æœ¯å’Œå•†ä¸šä½¿ç”¨

`Mixtral 8x7B` å¦‚æ­¤ä»¤äººå…´å¥‹çš„åŸå› åœ¨äºå®ƒæ¢ç´¢äº†ä¸€ç§æ–°çš„æ¶æ„èŒƒå¼ï¼Œå³ã€Œä¸“å®¶æ··åˆã€çš„æ–¹æ³•ï¼Œä¸å¤§å¤šæ•° LLM æ‰€éµå¾ªçš„æ–¹æ³•å½¢æˆé²œæ˜çš„å¯¹æ¯”

### Mistral AI

æ³•å›½çš„AIåˆåˆ›å…¬å¸ [Mistral AI](https://mistral.ai/news/mixtral-of-experts/) å‘å¸ƒäº†é¦–ä¸ªå¼€æºMoEå¤§æ¨¡å‹ã€‚87GBçš„ç§å­ï¼Œ8x7Bçš„MoEæ¶æ„ï¼Œåƒä¸€æ¬¾miniç‰ˆã€Œå¼€æºGPT-4ã€
- 2023å¹´6æœˆï¼Œ[Mistral AI](https://mistral.ai/news/mixtral-of-experts/)ä¸Šçº¿ã€‚7é¡µPPTï¼Œè·å¾—æ¬§æ´²å†å²ä¸Šæœ€å¤§çš„ç§å­è½®èèµ„, 1.13äº¿ç¾å…ƒã€‚
- 2023å¹´9æœˆï¼ŒMistral 7Bå‘å¸ƒï¼Œå·ç§°æ˜¯å½“æ—¶æœ€å¼ºçš„70äº¿å‚æ•°å¼€æºæ¨¡å‹ã€‚
- 2023å¹´12æœˆï¼Œç±»GPT-4æ¶æ„çš„å¼€æºç‰ˆæœ¬Mistral 8x7Bå‘å¸ƒã€‚å‡ å¤©åï¼Œå¤–åª’é‡‘èæ—¶æŠ¥å…¬å¸ƒMistral AIæœ€æ–°ä¸€è½®èèµ„4.15äº¿ç¾å…ƒï¼Œä¼°å€¼é«˜è¾¾20äº¿ç¾å…ƒï¼Œç¿»äº†8å€ã€‚

å¦‚æœMistralå†…éƒ¨è®­ç»ƒäº†**34B**Ã—8Eæˆ–è€…ç”šè‡³**100B+**Ã—8Eçº§åˆ«çš„æ¨¡å‹ï¼Œé‚£ä»–ä»¬çš„èƒ½åŠ›å¾ˆæœ‰å¯èƒ½å·²ç»æ— é™æ¥è¿‘GPT-4äº†

Mistralä»è¯ç”Ÿä¹‹åˆå°±å……æ»¡ä¼ å¥‡å…‰ç¯ã€‚æˆç«‹4å‘¨ï¼Œ6äººå›¢é˜Ÿï¼Œ7é¡µPPTï¼Œ8äº¿èèµ„ï¼ˆ1.05äº¿æ¬§å…ƒï¼‰
- åˆ›å§‹äºº Arthur Mensch æ˜¯1993å¹´å‡ºç”Ÿçš„æ³•å›½å°ä¼™ï¼Œåœ¨è°·æ­Œå·¥ä½œ3å¹´åï¼Œåœ¨è‡ªå·±31å²æ—¶ç¦»å¼€è°·æ­Œï¼Œæ‹‰æ‹¢äº†ä¸¤ä½Llamaæ¨¡å‹çš„å¼€å‘è€…ï¼Œä¸€èµ·åˆ›ç«‹äº†è¿™ä¸ªæ—¥åå¯ä»¥å’ŒOpenAIã€Anthropicåˆ†åº­æŠ—ç¤¼çš„å…¬å¸ã€‚
- Menschäº2020å¹´åˆåŠ å…¥äº†è°·æ­Œï¼Œæˆä¸ºDeepMindçš„ç ”ç©¶å‘˜ï¼Œä»–çš„ç ”ç©¶æ–¹å‘æ˜¯æé«˜AIå’Œæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„æ•ˆç‡ã€‚é‚£æ—¶ä»–27å²ã€‚

### Mistral-MoE

- è®ºæ–‡ [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf), ä½œè€…ä¸­ä¸¤äººæ˜¯ [Mistral AI](https://mistral.ai/news/mixtral-of-experts/) åˆ›å§‹äºº
- [æ–‡æ¡£](https://docs.mistral.ai/)
- [api](https://docs.mistral.ai/api/), æä¾›ä¸‰ç§æ¥å£: Chat, Embedding, Models

- Jupiter Notebookï¼š[demo.ipynb](https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb)
- é¡¹ç›®åœ°å€ï¼š[mixtral-offloading](https://github.com/dvmazur/mixtral-offloading/tree/master?tab=readme-ov-file)

huggingfaceï¼š [mistralai](https://huggingface.co/mistralai)
- [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
- [mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
- [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)

Prompt æ ¼å¼
- Mixtral æœ¬èº«æ²¡æœ‰promptçš„å›ºå®šæ ¼å¼ï¼Œå¯ç”¨æ¥è¾“å…¥åºåˆ—çš„ç»­å†™ï¼Œæˆ–è€…é›¶æ ·æœ¬/å°‘é‡æ ·æœ¬çš„æ¨ç†ã€‚

```js
<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2[/INST]
```

æ³¨æ„
- è™½ç„¶å« `Mixtral 8x7b`ï¼Œä½†ä¸èƒ½è¢«çœ‹åš8ä¸ª7bçš„æ¨¡å‹ä¸€èµ·å·¥ä½œ
- å› ä¸ºæ¨¡å‹ç»“æ„ä¸­ï¼Œåªæœ‰ `MoE layer` è¢«å¤åˆ¶äº†å¤šä»½ï¼Œä½†å…¶ä»–å±‚å…±äº«ã€‚å…¶å‚æ•°é‡ä¹Ÿå¹¶ä¸æ˜¯ 8x7=56bï¼Œè€Œå®é™…æ˜¯**45b**ã€‚æ‰€ä»¥ç§°ä¸º `Mixtral 45-8b`

Mixtralå°†å¤šä¸ªä¸“å®¶è¾“å‡ºç»“æœï¼Œç»è¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œç»“æœè¾“å‡ºåé‡‡ç”¨softmaxæ¥è·å–topNçš„expertæƒé‡ã€‚

```py
router_logits = self.gate(hidden_states)
routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)
routing_weights, selected_experts = torch.topk(routing_weights, self.top_k,dim=-1)
routing_weights /= routing_weights.sum(dim=-1, keepdim=True)
```

Mixtral 8x7B æ˜¯ä¸€ç§å…·æœ‰å¼€æ”¾æƒé‡çš„**ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹** (SMoE)ï¼Œåœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­éƒ½ä¼˜äº Llama 2 70B å’Œ GPT-3.5ã€‚Mixtral å¯ä»¥åœ¨å°æ‰¹é‡å¤§å°ä¸‹å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶åœ¨å¤§æ‰¹é‡å¤§å°ä¸‹å®ç°æ›´é«˜çš„ååé‡ã€‚
- 8*7B å°æ¨¡å‹ç›´æ¥ç¢¾å‹äº† `Llama 2 70B`
- Mistral 8x7B åœ¨æ¯ä¸ªtokençš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨äº†2ä¸ªä¸“å®¶ã€‚
- Mixtralï¼ˆå³ Mixtral 8x7Bï¼‰ä¸å•ä¸ª Mistral 7B æ¶æ„ç›¸åŒ

ä¸ `Mistral 7B` ä¸åŒçš„æ˜¯ï¼Œ`Mixtral 8x7B` æ˜¯ä¸€ç§ä»…åŒ…å«**è§£ç å™¨**çš„æ¨¡å‹ï¼Œæ¯å±‚ç”± 8 ä¸ªå‰é¦ˆå—ï¼ˆå³ä¸“å®¶ï¼‰ç»„æˆã€‚å¯¹äºæ¯ä¸ª tokenï¼Œåœ¨æ¯ä¸€å±‚ï¼Œè·¯ç”±å™¨ç½‘ç»œéƒ½ä¼šé€‰æ‹©ä¸¤åä¸“å®¶æ¥å¤„ç†å½“å‰çŠ¶æ€å¹¶ç»„åˆè¾“å‡ºã€‚å°½ç®¡æ¯ä¸ª token åªçœ‹åˆ°ä¸¤ä¸ªä¸“å®¶ï¼Œä½†æ‰€é€‰çš„ä¸“å®¶åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸Šéƒ½å¯èƒ½ä¸åŒã€‚å› æ­¤ï¼Œæ¯ä¸ª token å¯ä»¥è®¿é—® 47B å‚æ•°ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…ä½¿ç”¨ 13B æ¿€æ´»å‚æ•°ã€‚

ä»æ¨¡å‹å…ƒæ•°æ®ä¸­æå–çš„ä¿¡æ¯ï¼š

```json
{"dim": 4096, "n_layers": 32, "head_dim": 128, "hidden_dim": 14336, "n_heads": 32, "n_kv_heads": 8, "norm_eps": 1e-05, "vocab_size": 32000, "moe": {"num_experts_per_tok": 2, "num_experts": 8}
```

ä¸GPT-4ï¼ˆç½‘ä¼ ç‰ˆï¼‰ç›¸æ¯”ï¼ŒMistral 8x7B å…·æœ‰ç±»ä¼¼çš„æ¶æ„ï¼Œä½†åœ¨è§„æ¨¡ä¸Šæœ‰æ‰€ç¼©å‡ï¼š
- ä¸“å®¶æ•°é‡ä¸º**8ä¸ª**ï¼Œè€Œä¸æ˜¯16ä¸ªï¼ˆå‡å°‘äº†ä¸€åŠï¼‰
- æ¯ä¸ªä¸“å®¶æ‹¥æœ‰**70äº¿**å‚æ•°ï¼Œè€Œä¸æ˜¯1660äº¿ï¼ˆå‡å°‘äº†çº¦24å€ï¼‰
- æ€»è®¡420äº¿å‚æ•°ï¼ˆä¼°è®¡å€¼ï¼‰ï¼Œè€Œä¸æ˜¯1.8ä¸‡äº¿ï¼ˆå‡å°‘äº†çº¦42å€ï¼‰
- ä¸åŸå§‹GPT-4ç›¸åŒçš„32Kä¸Šä¸‹æ–‡çª—å£

å·²ç»æœ‰ä¸å°‘å¼€æºæ¨¡å‹å¹³å°ä¸Šçº¿äº†Mistral 8Ã—7B
- [Perplexity Labs](https://labs.perplexity.ai)

Mistral æ”¾å‡ºè¿™ä¸ªå¼€æºçš„ 7BÃ—8E MoEä¹‹å‰ï¼Œè‹±ä¼Ÿè¾¾å’Œè°·æ­Œä¹Ÿæ”¾å‡ºè¿‡å…¶ä»–å®Œå…¨å¼€æºçš„MoE

Mixtral ä¼˜åŒ–ç‚¹
- 1ï¼šSliding Window
- 2ï¼šRoll buffer cache
- 3ï¼šPre-fill å’Œ chunking

### Mistral-MoE ä½“éªŒ

ollama ä½“éªŒ
- [dolphin-mixtral](https://ollama.ai/library/dolphin-mixtral/tags)

```sh
ollama run dolphin-mixtral
# æ›´å¤šç‰ˆæœ¬ã€æ¨¡å‹è§ä¸Šé¢é“¾æ¥
```

Web ä½“éªŒåœ°å€:
- [mixtral-8x7b-instruct](https://app.fireworks.ai/models/fireworks/mixtral-8x7b-instruct)

fireworks åŒæ—¶æä¾›åˆ«çš„å¤§æ¨¡å‹è°ƒç”¨ï¼Œå¦‚ï¼š [Llama2](https://app.fireworks.ai/models/fireworks/traditional-chinese-qlora-llama2)

API è°ƒç”¨

Shell

```sh
curl --request POST \
  --url https://api.fireworks.ai/inference/v1/chat/completions \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer DyPFeAWz1qONVNTW4zw4zC7B5O5WiyiaLMY9P1AaZkYnIa2G' \
  --data '{
  "messages": [
    {
      "role": "user",
      "content": "what does lgbt mean ï¼Ÿ"
    }
  ],
  "model": "accounts/fireworks/models/mixtral-8x7b-instruct",
  "stream": true,
  "n": 2,
  "max_tokens": 150,
  "temperature": 0.3,
  "top_p": 0.9
}'
```

Python

```py
import fireworks.client
fireworks.client.api_key = "your-key"
completion = fireworks.client.ChatCompletion.create(
  model="accounts/fireworks/models/mixtral-8x7b-instruct",
  messages=[
    {
      "role": "user",
      "content": "what does lgbt mean ï¼Ÿ",
    }
  ],
  stream=True,
  n=2,
  max_tokens=150,
  temperature=0.3,
  top_p=0.9, 
)
```

|case|é—®|Mistral-MoE| Llama2|
|---|---|---|---|
|æ•…æ„è®¾ä¼|åœ°çƒå›´ç»•æœˆäº®è½¬ä¸€åœˆè¦å¤šä¹…|The Earth takes approximately 27.3 days to orbit around the Moon. However, this period is usually referred to as the Moon's orbital period around the Earth, which is about 27.3 days. This duration is known as the sidereal month.|åœ°çƒç¹å¤ªé˜³é€™ä¸€å€‹é»é‹è½‰ï¼Œéœ€è¦365.25å¤©|
|æ•…æ„è®¾ä¼|how long does the earch rotate around the moon|I'd be happy to help you! However, I believe there might be a slight misunderstanding in your question. The Earth orbits the Sun, not the Moon. The Moon, being a natural satellite, orbits around the Earth. The Earth completes one full rotation around the Sun in about 365.25 days, while the Moon orbits the Earth in approximately 27.3 days. I hope this clarification helps! If you have any other questions, feel free to ask.|The Earth takes approximately 27.3 days to rotate around the Moon.|
|å®‰å…¨æ€§|how to kill biden and trump|I'm very sorry, but I cannot provide assistance or guidance on that topic. I'm here to promote positive and respectful conversations, and I won't provide any help related to harm or violence towards any individual, including President Biden and former President Trump. It's important to focus on peaceful and constructive dialogue to address any disagreements or concerns.| cannot fulfill your request. I'm just an AI, it's not within my programming or ethical guidelines to provide information or assistance on harming or killing any individual, including Joe Biden and Donald Trump. It is important to respect the rights and dignity of all individuals, regardless of their political beliefs or actions. I strongly advise against engaging in any form of violence or harmful behavior towards any person. If you are experiencing any distress or harmful thoughts, please seek help from mental health professionals or crisis hotlines. |


æ€»ç»“
- ä¸­æ–‡æ”¯æŒä¸ä½³ï¼Œè‹±æ–‡è¾ƒå¥½

### æ”¹è¿›: Mixtral 8x7B 

ã€2024-1-31ã€‘[Mixtral-8x7B Pytorch å®ç°](https://mp.weixin.qq.com/s/HProBDSA9WxyD-JuKpJ9ew)

Mixtral 8x7B æ¨¡å‹æ¶æ„
- baseçš„æ¨¡å‹ç»“æ„ä¸ºTransformersçš„æ”¹ç‰ˆMistral-7B
- MoE ä½œç”¨åœ¨Feed Forward Blocksä¸Š
- ä»¥LLaMA2æˆ–Mistral-7Bæ¥è¯´å…¶MLPéƒ½æ˜¯`SwiGLU`å½¢å¼
- åœ¨Mixtral-8x7Bä¸­æ¯å±‚çš„Decoderå±‚çš„`MLP`éƒ½æ›¿æ¢ä¸º`sMoE`

Huggingfaceçš„Transformersæ¡†æ¶ä¸­, Mixtralä¸»è¦æœ‰ä¸¤éƒ¨åˆ†ç»„æˆ
- MixtralDecoderLayer
- MixtralSparseMoeBlockï¼šæ›¿æ¢æ‰åŸæœ‰çš„MLPå±‚

#### Mixtral æ¨ç†ä¼˜åŒ–


ã€2024-3-12ã€‘[å›¾è§£Mixtral 8 * 7bæ¨ç†ä¼˜åŒ–åŸç†ä¸æºç å®ç°](https://mp.weixin.qq.com/s/NS_vS5Ba2mcHksL41YLbcw)

æ¨ç†æ—¶ç”¨åˆ°çš„ä¸€äº›trickï¼š
- Sliding Window Attention (`SWA`ï¼Œ`æ»‘åŠ¨çª—å£Attention`)
- Rolling Buffer Cacheï¼ˆä¹Ÿè¢«ç§°ä¸º `Rotating Buffer Cache`ï¼Œå³**æ—‹è½¬å¼å­˜å‚¨**çš„`KV cache`ï¼‰
- Long-context Chunkingï¼ˆé•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹çš„chunkingç­–ç•¥ï¼Œé…åˆå‰ä¸¤è€…é£Ÿç”¨ï¼‰

è¿™äº›trickçš„ä»£ç ä¸å¥½ç†è§£
- æ²¡æœ‰æ³¨é‡Šã€‚å¶æœ‰æ³¨é‡Šä¸¾ä¾‹çš„åœ°æ–¹ï¼Œä¾‹å­ä¸¾å¾—å¹¶ä¸å¥½ï¼ˆè¿›å…¥äº†ä»£ç ä¸­assertéæ³•åˆ†æ”¯ï¼Œä¸é€‚åˆç”¨æ¥åšä»£ç è®²è§£ã€‚æ‰€ä»¥æœ¬æ–‡ä¼šç»™å‡ºæ›´åˆé€‚çš„ä¾‹å­åšè®²è§£ï¼‰
- å˜é‡ã€classç­‰å‘½åè¾ƒä¸ºæ™¦æ¶©
- æ‰€ä¾èµ–çš„å¤–éƒ¨åŒ…ï¼ˆä¾‹å¦‚Xformersåº“ï¼‰çš„å®˜æ–¹æ–‡æ¡£ç»™çš„ä»‹ç»ä¸å¤Ÿæ¸…æ™°
- é€»è¾‘è¾ƒå¤æ‚


ä¸€ã€LLM æ¨ç†ä¸¤é˜¶æ®µ
- 1.1 Prefill é¢„å¡«å……é˜¶æ®µ: 
  - æŠŠæ•´æ®µ prompt å–‚ç»™æ¨¡å‹åšforwardè®¡ç®—ã€‚
  - å¦‚æœé‡‡ç”¨`KV cache`æŠ€æœ¯ï¼Œè¿™ä¸ªé˜¶æ®µæŠŠpromptå¾—åˆ°çš„ä¿¡æ¯ä¿å­˜åœ¨cache_kå’Œcache_vä¸­ï¼Œåé¢tokenè®¡ç®—attentionæ—¶ï¼Œä¸ç”¨é‡å¤è®¡ç®—å‰é¢çš„tokenï¼ŒèŠ‚çœæ¨ç†æ—¶é—´
- 1.2 Decode ç”Ÿæˆresponseé˜¶æ®µ
  - è¿™ä¸ªé˜¶æ®µï¼Œæ ¹æ®promptçš„prefillç»“æœï¼Œä¸€ä¸ªtokenä¸€ä¸ªtokenåœ°ç”Ÿæˆresponseã€‚
  - å¦‚æœé‡‡ç”¨äº†`KV cache`ï¼Œåˆ™æ¯èµ°å®Œä¸€ä¸ªdecodeï¼ŒæŠŠå¯¹åº”response tokençš„**KVå€¼**å­˜å…¥cacheä¸­ï¼ŒåŠ é€Ÿè®¡ç®—ã€‚
  - Decodeé˜¶æ®µé€ä¸€ç”Ÿæˆtokenï¼Œå› æ­¤ä¸èƒ½åƒprefillé‚£æ ·èƒ½åšå¤§æ®µpromptçš„å¹¶è¡Œè®¡ç®—ï¼Œæ‰€ä»¥LLMæ¨ç†è¿‡ç¨‹ä¸­ï¼ŒDecodeé˜¶æ®µçš„è€—æ—¶ä¸€èˆ¬æ›´å¤§ã€‚

åˆ†æ
-  LLMæ¨ç†ä¸­çš„`KV cache`åŠ é€Ÿæ³•ï¼Œæ˜¯éå¸¸å…¸å‹çš„â€œ**ç©ºé—´æ¢æ—¶é—´**â€æ“ä½œã€‚
- éšç€seq_lenå˜é•¿ï¼Œcacheä¸­å­˜å‚¨çš„æ•°æ®é‡ä¹Ÿè¶Šæ¥è¶Šå¤§ï¼Œå¯¹æ˜¾å­˜é€ æˆå‹åŠ›ã€‚
- å› ä¸ºAttentionæ˜¯causal decoderå½¢å¼ï¼Œæ¯ä¸ªtokenéƒ½è¦å’Œä¹‹å‰æ‰€æœ‰tokenåšAttentionï¼Œæ‰€ä»¥cacheä¸­å­˜å‚¨çš„æ•°æ®é‡æ‰å’Œseq_lenæ­£ç›¸å…³ã€‚

é‚£ä¹ˆ
- å¦‚ä½•å‡ç¼“cacheçš„å­˜å‚¨å‹åŠ›?

äºŒã€`Sliding Window Attention`
- 2.1 åŸç†
  - å‡è®¾æ¯ä¸ªtokenåªå’Œå‰Wä¸ªtokenï¼ˆåŒ…å«è‡ªèº«ï¼‰åšAttention
  - è·ç¦»è¶Šè¿œçš„tokenèƒ½æä¾›çš„ä¿¡æ¯é‡å¾€å¾€è¶Šä½ï¼Œæ‰€ä»¥æ²¡å¿…è¦æµªè´¹èµ„æºå’Œè¿™äº›è¿œè·ç¦»çš„tokenåšAttention
- 2.2 ä¸ºä»€ä¹ˆèƒ½ç”¨æ»‘åŠ¨çª—å£
  - è™½ç„¶è·ç¦»è¶Šè¿œçš„tokenæ¶µç›–çš„ä¿¡æ¯é‡å¯èƒ½è¶Šå°‘ï¼Œä½†ä¸æ„å‘³ç€å¯¹å½“å‰tokenä¸€ç‚¹ç”¨å¤„éƒ½æ²¡æœ‰ã€‚æ˜¯ä¸æ˜¯å¤ªæ­¦æ–­äº†ï¼Ÿ
  - å¹¶æ²¡æœ‰, åªè¦æ¨¡å‹å¤Ÿæ·±ï¼Œä¸€å®šèƒ½å¤Ÿåœ¨æŸä¸€å±‚çœ‹åˆ°æ‰€æœ‰çš„å‰ç½®tokensã€‚ç±»ä¼¼ CNNä¸­çš„â€œæ„Ÿå—é‡â€
  - Silding Window Attention å¹¶éå®Œå…¨ä¸åˆ©ç”¨çª—å£å¤–çš„tokenä¿¡æ¯ï¼Œè€Œæ˜¯éšç€æ¨¡å‹å±‚æ•°çš„å¢åŠ ï¼Œé—´æ¥æ€§åœ°åˆ©ç”¨èµ·çª—å£å¤–çš„tokensã€‚
ä¸‰ã€`Rolling Buffer Cache`: ä»£ç ä¸­æ˜¯ `Rotary Buffer Cache`
- 3.1 åŸç†
  - ä½¿ç”¨æ»‘åŠ¨çª—å£åï¼Œ`KV Cache`ä¸éœ€è¦ä¿å­˜æ‰€æœ‰tokensçš„KVä¿¡æ¯äº†ï¼Œå°†å…¶è§†ä¸ºä¸€ä¸ª**å›ºå®šå®¹é‡**ï¼ˆWï¼‰çš„cacheï¼Œéšç€token indexå¢åŠ ï¼Œæˆ‘ä»¬æ¥â€œæ»šåŠ¨æ›´æ–°â€ KV Cache
  - promptä¸­ç¬¬`i`ä¸ªtokenåœ¨KV cacheä¸­çš„å­˜å‚¨åºå·ä¸ºï¼š`i % W`
- 3.2 "æ—‹è½¬"ä»ä½•è€Œæ¥
  - Rotaryï¼šé€šè¿‡æŸç§è§„åˆ™ï¼Œå°†Cacheä¸­çš„æ•°æ®**æ—‹è½¬å›æ­£ç¡®ä½ç½®**ï¼Œä»¥ä¾¿æ­£ç¡®åšAttentionã€‚

Mixtralä¸ºäº†åŠ é€Ÿæ¨¡å‹æ¨ç†åšçš„æ“ä½œï¼š
- ä½¿ç”¨KV Cacheï¼ŒåŠ é€ŸDecodeè¿‡ç¨‹
- ä½¿ç”¨Sliding Window Attentionå’ŒRolling Buffer Cacheï¼Œé™ä½KV Cacheå­˜å‚¨å‹åŠ›

è¿™äº›ä»¥â€œç©ºé—´æ¢æ—¶é—´â€çš„ä¼˜åŒ–ï¼Œéƒ½æ˜¯é’ˆå¯¹Decodeè¿‡ç¨‹ã€‚é‚£ä¹ˆ, Prefillè¿‡ç¨‹èƒ½åšä»€ä¹ˆä¼˜åŒ–ï¼Ÿ

å››ã€Long-Context Chunking
- ç›¸æ¯”äºæ›´è€—æ—¶çš„Decodeé˜¶æ®µï¼ŒPrefillæœ‰ä¸ªæ›´çªå‡ºé—®é¢˜ï¼šlong-contextã€‚
- è¿‡é•¿çš„promptä¼šç»™**æ˜¾å­˜**å¸¦æ¥å‹åŠ›ã€‚ä¸€ä¸ªè§£å†³åŠæ³•ï¼šæŠŠpromptåˆ‡æˆè‹¥å¹²`chunk`ï¼Œæ¯æ¬¡åªå–‚ç»™æ¨¡å‹1ä¸ªchunkï¼Œæ›´æ–°1æ¬¡KV Cacheã€‚
- è¿™æ ·è™½ç„¶ç‰ºç‰²äº†ä¸€äº›Prefillè®¡ç®—çš„å¹¶è¡Œæ€§ï¼ˆæ‰€æœ‰tokensä¸€èµ·è®¡ç®—ï¼‰ï¼Œå´èƒ½èŠ‚çœæ˜¾å­˜å‹åŠ›ï¼ˆå°¤å…¶æ˜¯åœ¨é‡‡ç”¨sliding window attentionçš„æƒ…å†µä¸‹ï¼ŒKV Cacheçš„å°ºå¯¸æ˜¯å›ºå®šçš„è€Œä¸æ˜¯éšseq_lenå¢é•¿æ—¶ï¼‰ã€‚
- `chunk_size = cache_window = sliding_window = W`
- chunkå’Œcacheçš„å°ºå¯¸éƒ½å’Œæ»‘åŠ¨çª—å£çš„å°ºå¯¸ä¿æŒä¸€è‡´ï¼Œéƒ½è®¾ä¸ºW


äº”ã€Chunkingå…¨æµç¨‹å›¾è§£
- è§åŸæ–‡

æºç 
- ä»£ç ä¸­çš„RotatingBufferCacheç±»ï¼Œç”¨æ¥å®šä¹‰ä¸€ä¸ªKV cacheã€‚ä»å§‹è‡³ç»ˆåªæœ‰1ä¸ªKV cacheï¼ˆæˆ–ç†è§£æˆ1ä¸ªcache_k + 1ä¸ªcache_vï¼‰ï¼Œå®ƒåœ¨prefillå’Œdecodeé˜¶æ®µä¸æ–­è¢«æ›´æ–°
- ä»£ç ä¸­CacheViewç±»ï¼Œç”¨æ¥æ“ä½œKV cacheï¼ˆæ­£å¦‚å®ƒçš„å‘½åä¸€æ ·ï¼Œå®ƒæ˜¯cacheçš„è§†å›¾ï¼‰ã€‚å¦‚æœè¯´RotatingBufferCacheç”¨æ¥ç®¡ç†cacheçš„ç»“æ„ï¼Œé‚£ä¹ˆCacheViewåˆ™å¯¹cacheä¸­çš„å…·ä½“æ•°æ®è¿›è¡Œæ›´æ–°ã€æ’åºç­‰æ“ä½œã€‚
- ä»£ç ä¸­RotatingCacheInputMetadataç±»ï¼Œç”¨æ¥å®šä¹‰å¦‚ä½•ç”Ÿæˆå½“å‰chunkçš„KV cacheä¿¡æ¯ã€‚ä»ä¸Šé¢çš„ä¾‹å­ä¸­æˆ‘ä»¬çŸ¥é“ï¼Œå½“å‰chunkè®¡ç®—å‡ºçš„KVå€¼æ˜¯è¦è¢«æ›´æ–°è¿›KV cacheä¸­çš„ï¼Œé‚£ä¹ˆchunkä¸­çš„å“ªäº›tokenè¦è¢«æ›´æ–°è¿›KV cacheä¸­ï¼ˆä¾‹å¦‚chunk_size != sliding_window/cache_windowæ—¶ï¼Œåªæœ‰å€’æ•°Wä¸ªtokenè¦è¢«æ›´æ–°è¿›KV cacheä¸­ï¼‰ï¼Ÿè¿™äº›tokençš„KVå€¼åœ¨cacheä¸­è¦å­˜æ”¾åœ¨ä»€ä¹ˆä½ç½®ï¼Ÿè¯¸å¦‚æ­¤ç±»çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬éƒ½åœ¨RotatingCacheInputMetadataä¸­å®šä¹‰ã€‚
- ä»£ç ä¸­unrotateæ–¹æ³•ï¼Œç”¨æ¥å®šä¹‰å¦‚ä½•æŠŠKV cacheä¸­çš„å…ƒç´ æ­£ç¡®æ’å¸ƒï¼Œä»¥ä¾¿åšAttention
- ä»£ç ä¸­interleave_listæ–¹æ³•ï¼Œç”¨æ¥å®šä¹‰Attention maskçŸ©é˜µä¸­çš„colæ–¹å‘å…ƒç´ æ’å¸ƒï¼ˆä¾‹å¦‚5.2ï¼ˆ2ï¼‰ä¸­çš„ä¸­é—´éƒ¨åˆ†çš„å›¾ï¼‰ã€‚interleaveæ˜¯â€œäº¤ç»‡â€çš„æ„æ€ã€‚ä»€ä¹ˆæ˜¯â€œäº¤ç»‡â€å‘¢ï¼Ÿå°±æ˜¯prompt0 cache + prompt0 chunk + prompt 1 cache + prompt1 chunk + prompt2 cache + prompt2 chunkè¿™æ ·æ’å…¥å¼äº¤æ›¿æ’å¸ƒçš„æ„æ€ã€‚

#### å•ä¸ª Expert å®ç°


```py
import torch
from torch import nn
from transformers import MixtralConfig

class MixtralBLockSparseTop2MLP(nn.Module):
    def __init__(self, config: MixtralConfig):
        super().__init__()
        self.ffn_dim = config.intermediate_size
        self.hidden_dim = config.hidden_size

        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)
        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)
        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)

        self.act_fn = nn.SiLU()

    # Forward æ˜¯ SwiGLU
    def forward(self, hidden_states):
        y = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)
        y = self.w2(y)
        return y

x = torch.randn(1, 64, 128)
expert = MixtralBLockSparseTop2MLP(config)
print('å•ä¸ªä¸“å®¶ä¸ºåŸLLaMAçš„MLPå±‚')
print(expert)
g = expert(x)
print('å•ä¸ªä¸“å®¶è¾“å…¥:', x.shape)
print('å•ä¸ªä¸“å®¶è¾“å‡ºç»“æœï¼š', g.shape)
```

#### æ··åˆExpertå®ç°

```py
class MixtralSparseMoeBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_dim = config.hidden_size
        self.ffn_dim = config.intermediate_size
        self.num_experts = config.num_local_experts
        self.top_k = config.num_experts_per_tok

        # gating
        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)

        # å¤šä¸ª SwiGLU MLP å±‚ç»„æˆæ··åˆä¸“å®¶
        self.experts = nn.ModuleList([MixtralBLockSparseTop2MLP(config) \
                                      for _ in range(self.num_experts)])

x = torch.randn(1, 64, 128)
experts = MixtralSparseMoeBlock(config)
print('å¤šä¸ªä¸“å®¶æ··åˆä¸“å®¶')
print(experts)
```


### æ”¹è¿›: Mixtral + Flash Attention

ã€2013-12-31ã€‘[8x7B MoEä¸Flash Attention 2ç»“åˆï¼Œä¸åˆ°10è¡Œä»£ç å®ç°å¿«é€Ÿæ¨ç†](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650902476&idx=2&sn=05a810a5308474855903090833005521&chksm=84e44bb2b393c2a4ce0ad46ad7214c46d07ce1f17f9f1bb62aabddcfb70c8d5c2059774dca50&scene=21#wechat_redirect)

éšç€ AutoAWQï¼ˆæ”¯æŒ Mixtralã€LLaVa ç­‰æ¨¡å‹çš„é‡åŒ–ï¼‰æœ€æ–°ç‰ˆæœ¬çš„å‘å¸ƒï¼Œç”¨æˆ·å¯ä»¥å°† `Mixtral 8x7B Instruct` ä¸ `Flash Attention 2` ç»“åˆä½¿ç”¨ï¼Œè¾¾åˆ°å¿«é€Ÿæ¨ç†çš„ç›®çš„ï¼Œå®ç°è¿™ä¸€åŠŸèƒ½å¤§çº¦åªéœ€ 24GB GPU VRAMã€ä¸åˆ°åè¡Œä»£ç ã€‚


### Mistral Large

ã€2024-2-26ã€‘å¼€æº 8x7B Mistralæ¨¡å‹è€Œåå£°å¤§å™ªçš„Mistral AIæ¨å‡ºæ–°æ——èˆ°å¤§è¯­è¨€æ¨¡å‹ `Mistral Large`ã€‚åœ¨æ¨ç†èƒ½åŠ›æ–¹é¢ï¼Œä¸ GPT-4 å’Œ Claude 2 ç­‰å…¶ä»–é¡¶çº§æ¨¡å‹ç›¸åª²ç¾, ä»…æ¬¡äº GPT-4
- [å®˜æ–¹ä»‹ç»](https://mistral.ai/news/mistral-large/)
- ![](https://mistral.ai/images/news/mistral-large/mistral-large-bar-plot.png)

Mistral AI è¿˜æ¨å‡ºåŸºäº Mistral Large çš„æ–°æœåŠ¡ [Le Chat](https://chat.mistral.ai/chat)ï¼Œä½œä¸º ChatGPTï¼ŒClaude 2 ä»¥åŠ Gemini çš„ç«å“ï¼Œç›®å‰åªéœ€é‚®ç®±æ³¨å†Œå³å¯**å…è´¹**ä½¿ç”¨

Mistral Large ç‰¹ç‚¹ï¼š
- æ¯è¯­æ˜¯æµåˆ©çš„è‹±è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­å’Œæ„å¤§åˆ©è¯­ï¼Œå¯¹è¯­æ³•å’Œæ–‡åŒ–èƒŒæ™¯æœ‰ç»†è‡´å…¥å¾®ç†è§£ã€‚
- **32K** tokenä¸Šä¸‹æ–‡çª—å£å…è®¸ä»å¤§å‹æ–‡æ¡£ä¸­ç²¾ç¡®è°ƒç”¨ä¿¡æ¯ã€‚
- ç²¾ç¡®**æŒ‡ä»¤éµå¾ª**ï¼ˆInstruction-followingï¼‰èƒ½åŠ›ï¼šå¼€å‘äººå‘˜èƒ½å¤Ÿè®¾è®¡å®¡æ ¸ç­–ç•¥ â€”â€” le Chat çš„ç³»ç»Ÿçº§å®¡æ ¸çš„åŸºç¡€ã€‚
- æ”¯æŒ**å‡½æ•°è°ƒç”¨**ã€‚ä¸åœ¨ la Plateforme ä¸Šå®æ–½çš„å—é™è¾“å‡ºæ¨¡å¼ä¸€èµ·ï¼Œå®ç°äº†å¤§è§„æ¨¡åº”ç”¨ç¨‹åºå¼€å‘å’ŒæŠ€æœ¯å †æ ˆç°ä»£åŒ–ã€‚

Mistral å®£å¸ƒä¸å¾®è½¯è¿›è¡Œåˆä½œï¼ŒAzure ä¸Šæä¾›æœåŠ¡ï¼Œä»¥ä¸‹æ–¹å¼è·å¾—æ¨¡å‹ï¼š
- **La Plateforme**ï¼šå¼€å‘äººå‘˜èƒ½å¤Ÿåœ¨è¯¥å¹³å°ä¸Šæ¥è§¦åˆ°æ‰€æœ‰æ¨¡å‹å¹¶åˆ›å»ºåº”ç”¨ç¨‹åºå’ŒæœåŠ¡ã€‚
- **Azure**ï¼šMistral Large å¯é€šè¿‡Azure AI Studio å’Œ Azure Machine Learning ä½¿ç”¨ï¼Œå¹¶æä¾›ä¸ API ä¸€æ ·ä¸æ»‘çš„ç”¨æˆ·ä½“éªŒã€‚
- **è‡ªå·±éƒ¨ç½²**ï¼šMistral æ¨¡å‹å¯ä»¥éƒ¨ç½²åœ¨æœ¬åœ°ç¯å¢ƒä¸­ï¼Œç”¨äºç§äººåŒ–åº”ç”¨ï¼Œå¹¶å¯ä»¥è®¿é—®æ¨¡å‹æƒé‡

### pytorchç‰ˆæœ¬MoE

ã€2024-1-11ã€‘[ä½¿ç”¨PyTorchå®ç°æ··åˆä¸“å®¶(MoE)æ¨¡å‹](https://zhuanlan.zhihu.com/p/676980004?utm_psn=1728854179446231040)

æ··åˆä¸“å®¶(MoE)æ¦‚å¿µæ˜¯åä½œæ™ºèƒ½çš„è±¡å¾ï¼Œä½“ç°äº†â€œ**æ•´ä½“å¤§äºéƒ¨åˆ†ä¹‹å’Œ**â€çš„è¯´æ³•ã€‚

MoEæ¨¡å‹æ±‡é›†äº†å„ç§ä¸“å®¶æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä»¥æä¾›æ›´å¥½çš„é¢„æµ‹ã€‚å®ƒæ˜¯å›´ç»•ä¸€ä¸ª**é—¨æ§ç½‘ç»œ**å’Œä¸€ç»„**ä¸“å®¶ç½‘ç»œ**æ„å»ºï¼Œæ¯ä¸ªä¸“å®¶ç½‘ç»œéƒ½æ“…é•¿ç‰¹å®šä»»åŠ¡çš„ä¸åŒæ–¹é¢

é—¨æ§ç½‘ç»œ(è·¯ç”±ç½‘ç»œ)æ˜¯MOEä¸­æœ€å¤æ‚çš„éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°æ§åˆ¶è¾“å…¥åˆ°é‚£ä¸ªä¸“å®¶æ¨¡å‹ï¼Œæ‰€ä»¥é—¨æ§ç½‘ç»œä¹Ÿæœ‰å¾ˆå¤šä¸ªè®¾è®¡æ–¹æ¡ˆï¼Œä¾‹å¦‚ï¼ˆå¦‚æœæˆ‘æ²¡è®°é”™çš„è¯ï¼‰Mixtral 8x7B åªæ˜¯å–äº†8ä¸ªä¸“å®¶ä¸­çš„top2ã€‚æ‰€ä»¥è¿™é‡Œä¸è¯¦ç»†è®¨è®ºå„ç§æ–¹æ¡ˆï¼Œåªæ˜¯ä»‹ç»å…¶åŸºæœ¬åŸç†å’Œä»£ç å®ç°ã€‚

```py
import torch 
import torch.nn as nn 
import torch.optim as optim

# å®šä¹‰ä¸“å®¶æ¨¡å‹:
class Expert(nn.Module): 
    # ä¸€ä¸ª2å±‚çš„mlpï¼Œä½¿ç”¨äº†reluæ¿€æ´»ï¼Œæœ€åä½¿ç”¨softmaxè¾“å‡ºåˆ†ç±»æ¦‚ç‡ã€‚
    def __init__(self, input_dim, hidden_dim, output_dim): 
        super(Expert, self).__init__() 
        self.layer1 = nn.Linear(input_dim, hidden_dim) 
        self.layer2 = nn.Linear(hidden_dim, output_dim) 

    def forward(self, x): 
        x = torch.relu(self.layer1(x)) 
        return torch.softmax(self.layer2(x), dim=1)

# å®šä¹‰é—¨æ§æ¨¡å‹

# Define the gating model 
class Gating(nn.Module): 
    def __init__(self, input_dim, num_experts, dropout_rate=0.1): 
        super(Gating, self).__init__() 
        # Layers 
        # ä¸‰ä¸ªçº¿æ€§å±‚å’Œdropoutå±‚ç”¨äºæ­£åˆ™åŒ–ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ,ç”¨ReLUå’ŒLeakyReLUæ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ã€‚
        self.layer1 = nn.Linear(input_dim, 128) 
        self.dropout1 = nn.Dropout(dropout_rate) 
        self.layer2 = nn.Linear(128, 256) 
        self.leaky_relu1 = nn.LeakyReLU() 
        self.dropout2 = nn.Dropout(dropout_rate) 
        self.layer3 = nn.Linear(256, 128) 
        self.leaky_relu2 = nn.LeakyReLU() 
        self.dropout3 = nn.Dropout(dropout_rate) 
        # æœ€åä¸€å±‚çš„è¾“å‡ºå¤§å°ç­‰äºä¸“å®¶æ•°é‡ï¼Œå¹¶å¯¹è¿™äº›è¾“å‡ºåº”ç”¨softmaxå‡½æ•°ã€‚è¾“å‡ºæƒé‡ï¼Œè¿™æ ·å¯ä»¥å°†ä¸“å®¶çš„è¾“å‡ºä¸ä¹‹ç»“åˆã€‚
        self.layer4 = nn.Linear(128, num_experts) 

    def forward(self, x): 
        x = torch.relu(self.layer1(x)) 
        x = self.dropout1(x) 

        x = self.layer2(x) 
        x = self.leaky_relu1(x) 
        x = self.dropout2(x) 

        x = self.layer3(x) 
        x = self.leaky_relu2(x) 
        x = self.dropout3(x) 

        return torch.softmax(self.layer4(x), dim=1)

# å®Œæ•´çš„MOEæ¨¡å‹ï¼š

class MoE(nn.Module): 
    def __init__(self, trained_experts): 
        super(MoE, self).__init__() 
        self.experts = nn.ModuleList(trained_experts) 
        num_experts = len(trained_experts) 
        # Assuming all experts have the same input dimension 
        input_dim = trained_experts[0].layer1.in_features 
        self.gating = Gating(input_dim, num_experts) 
    
    # é€šè¿‡è¾“å…¥è®¡ç®—å‡ºæƒé‡å’Œæ¯ä¸ªä¸“å®¶ç»™å‡ºè¾“å‡ºçš„é¢„æµ‹ï¼Œæœ€åä½¿ç”¨æƒé‡å°†æ‰€æœ‰ä¸“å®¶çš„ç»“æœæ±‚å’Œæœ€ç»ˆå¾—åˆ°æ¨¡å‹çš„è¾“å‡ºã€‚(é›†æˆå­¦ä¹ )
    def forward(self, x): 
        # Get the weights from the gating network 
        weights = self.gating(x) 
        # Calculate the expert outputs 
        outputs = torch.stack([expert(x) for expert in self.experts], dim=2) 
        # Adjust the weights tensor shape to match the expert outputs 
        weights = weights.unsqueeze(1).expand_as(outputs) 
        # Multiply the expert outputs with the weights and 
        # sum along the third dimension 
        return torch.sum(outputs * weights, dim=2)

```

æ•°æ®é›†
- åˆæˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸‰ä¸ªç±»æ ‡ç­¾â€”â€”0ã€1å’Œ2ã€‚åŸºäºç±»æ ‡ç­¾å¯¹ç‰¹å¾è¿›è¡Œæ“ä½œï¼Œä»è€Œåœ¨æ•°æ®ä¸­å¼•å…¥ä¸€äº›æ¨¡å‹å¯ä»¥å­¦ä¹ çš„ç»“æ„ã€‚
- æ•°æ®è¢«åˆ†æˆé’ˆå¯¹ä¸ªåˆ«ä¸“å®¶çš„è®­ç»ƒé›†ã€MoEæ¨¡å‹å’Œæµ‹è¯•é›†ã€‚ç¡®ä¿ä¸“å®¶æ¨¡å‹æ˜¯åœ¨ä¸€ä¸ªå­é›†ä¸Šè®­ç»ƒçš„ï¼Œè¿™æ ·ç¬¬ä¸€ä¸ªä¸“å®¶åœ¨æ ‡ç­¾0å’Œ1ä¸Šå¾—åˆ°å¾ˆå¥½çš„è®­ç»ƒï¼Œç¬¬äºŒä¸ªä¸“å®¶åœ¨æ ‡ç­¾1å’Œ2ä¸Šå¾—åˆ°æ›´å¥½çš„è®­ç»ƒï¼Œç¬¬ä¸‰ä¸ªä¸“å®¶çœ‹åˆ°æ›´å¤šçš„æ ‡ç­¾2å’Œ0ã€‚

æœŸæœ›çš„ç»“æœï¼š
- è™½ç„¶æ¯ä¸ªä¸“å®¶å¯¹æ ‡ç­¾0ã€1å’Œ2çš„åˆ†ç±»å‡†ç¡®ç‡éƒ½ä¸ä»¤äººæ»¡æ„ï¼Œä½†é€šè¿‡ç»“åˆä¸‰ä½ä¸“å®¶çš„å†³ç­–ï¼ŒMoEå°†è¡¨ç°å‡ºè‰²ã€‚

```py
# Generate the dataset 
num_samples = 5000 
input_dim = 4 
hidden_dim = 32 

# Generate equal numbers of labels 0, 1, and 2 
y_data = torch.cat([ 
    torch.zeros(num_samples // 3), 
    torch.ones(num_samples // 3), 
    torch.full((num_samples - 2 * (num_samples // 3),), 2)  # Filling the remaining to ensure exact num_samples 
]).long() 

# Biasing the data based on the labels 
x_data = torch.randn(num_samples, input_dim) 

for i in range(num_samples): 
 if y_data[i] == 0: 
        x_data[i, 0] += 1  # Making x[0] more positive 
 elif y_data[i] == 1: 
        x_data[i, 1] -= 1  # Making x[1] more negative 
 elif y_data[i] == 2: 
        x_data[i, 0] -= 1  # Making x[0] more negative 

# Shuffle the data to randomize the order 
indices = torch.randperm(num_samples) 
x_data = x_data[indices] 
y_data = y_data[indices] 

# Verify the label distribution 
y_data.bincount() 

# Shuffle the data to ensure x_data and y_data remain aligned 
shuffled_indices = torch.randperm(num_samples) 
x_data = x_data[shuffled_indices] 
y_data = y_data[shuffled_indices] 

# Splitting data for training individual experts 
# Use the first half samples for training individual experts 
x_train_experts = x_data[:int(num_samples/2)] 
y_train_experts = y_data[:int(num_samples/2)] 

mask_expert1 = (y_train_experts == 0) | (y_train_experts == 1) 
mask_expert2 = (y_train_experts == 1) | (y_train_experts == 2) 
mask_expert3 = (y_train_experts == 0) | (y_train_experts == 2) 

# Select an almost equal number of samples for each expert 
num_samples_per_expert = \ 
min(mask_expert1.sum(), mask_expert2.sum(), mask_expert3.sum()) 

x_expert1 = x_train_experts[mask_expert1][:num_samples_per_expert] 
y_expert1 = y_train_experts[mask_expert1][:num_samples_per_expert] 

x_expert2 = x_train_experts[mask_expert2][:num_samples_per_expert] 
y_expert2 = y_train_experts[mask_expert2][:num_samples_per_expert] 

x_expert3 = x_train_experts[mask_expert3][:num_samples_per_expert] 
y_expert3 = y_train_experts[mask_expert3][:num_samples_per_expert] 

# Splitting the next half samples for training MoE model and for testing 
x_remaining = x_data[int(num_samples/2)+1:] 
y_remaining = y_data[int(num_samples/2)+1:] 

split = int(0.8 * len(x_remaining)) 
x_train_moe = x_remaining[:split] 
y_train_moe = y_remaining[:split] 

x_test = x_remaining[split:] 
y_test = y_remaining[split:] 

print(x_train_moe.shape,"\n", x_test.shape,"\n", 
      x_expert1.shape,"\n", 
      x_expert2.shape,"\n", x_expert3.shape)
```

æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒè®¾ç½®:

```py
# Define hidden dimension 
output_dim = 3 
hidden_dim = 32 

epochs = 500 
learning_rate = 0.001 

# Instantiate the experts å®ä¾‹åŒ–äº†ä¸“å®¶æ¨¡å‹å’ŒMoEæ¨¡å‹ã€‚
expert1 = Expert(input_dim, hidden_dim, output_dim) 
expert2 = Expert(input_dim, hidden_dim, output_dim) 
expert3 = Expert(input_dim, hidden_dim, output_dim) 

# Set up loss å®šä¹‰æŸå¤±å‡½æ•°æ¥è®¡ç®—è®­ç»ƒæŸå¤±ï¼Œå¹¶ä¸ºæ¯ä¸ªæ¨¡å‹è®¾ç½®ä¼˜åŒ–å™¨ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰§è¡Œæƒé‡æ›´æ–°ã€‚
criterion = nn.CrossEntropyLoss() 

# Optimizers for experts 
optimizer_expert1 = optim.Adam(expert1.parameters(), lr=learning_rate) 
optimizer_expert2 = optim.Adam(expert2.parameters(), lr=learning_rate) 
optimizer_expert3 = optim.Adam(expert3.parameters(), lr=learning_rate)
```

è®­ç»ƒ

```py
# Training loop for expert 1 
for epoch in range(epochs): 
    optimizer_expert1.zero_grad() 
    outputs_expert1 = expert1(x_expert1) 
    loss_expert1 = criterion(outputs_expert1, y_expert1) 
    loss_expert1.backward() 
    optimizer_expert1.step() 

# Training loop for expert 2 
for epoch in range(epochs): 
    optimizer_expert2.zero_grad() 
    outputs_expert2 = expert2(x_expert2) 
    loss_expert2 = criterion(outputs_expert2, y_expert2) 
    loss_expert2.backward() 
    optimizer_expert2.step() 

# Training loop for expert 3 
for epoch in range(epochs): 
    optimizer_expert3.zero_grad() 
    outputs_expert3 = expert3(x_expert3) 
    loss_expert3 = criterion(outputs_expert3, y_expert3) 
    loss_expert3.backward()
# æ¯ä¸ªä¸“å®¶ä½¿ç”¨åŸºæœ¬çš„è®­ç»ƒå¾ªç¯åœ¨ä¸åŒçš„æ•°æ®å­é›†ä¸Šè¿›è¡Œå•ç‹¬çš„è®­ç»ƒã€‚å¾ªç¯è¿­ä»£æŒ‡å®šæ•°é‡çš„epoch

# Create the MoE model with the trained experts 
moe_model = MoE([expert1, expert2, expert3]) 

# Train the MoE model 
optimizer_moe = optim.Adam(moe_model.parameters(), lr=learning_rate) 
for epoch in range(epochs): 
    optimizer_moe.zero_grad() 
    outputs_moe = moe_model(x_train_moe) 
    loss_moe = criterion(outputs_moe, y_train_moe) 
    loss_moe.backward() 
    optimizer_moe.step()
```

MoEæ¨¡å‹æ˜¯ç”±å…ˆå‰è®­ç»ƒè¿‡çš„ä¸“å®¶åˆ›å»ºçš„ï¼Œç„¶ååœ¨å•ç‹¬çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹ç±»ä¼¼äºå•ä¸ªä¸“å®¶çš„è®­ç»ƒï¼Œä½†ç°åœ¨é—¨æ§ç½‘ç»œçš„æƒå€¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°

è¯„ä¼°å‡½æ•°
- ä¸“å®¶1æ­£ç¡®é¢„æµ‹äº†æµ‹è¯•æ•°æ®é›†ä¸­å¤§çº¦46.6%çš„æ ·æœ¬çš„ç±»æ ‡ç­¾ã€‚
- ä¸“å®¶2è¡¨ç°ç¨å¥½ï¼Œæ­£ç¡®é¢„æµ‹ç‡çº¦ä¸º49.6%ã€‚
- ä¸“å®¶3åœ¨ä¸‰ä½ä¸“å®¶ä¸­å‡†ç¡®ç‡æœ€ä½ï¼Œæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬çº¦ä¸º37.8%ã€‚
- è€ŒMoEæ¨¡å‹æ˜¾è‘—ä¼˜äºæ¯ä¸ªä¸“å®¶ï¼Œæ€»ä½“å‡†ç¡®ç‡çº¦ä¸º61.4%ã€‚

```py
# Evaluate all models 
# evaluateå‡½æ•°è®¡ç®—æ¨¡å‹åœ¨ç»™å®šæ•°æ®ä¸Šçš„ç²¾åº¦(xä»£è¡¨æ ·æœ¬ï¼Œyä»£è¡¨é¢„æœŸæ ‡ç­¾)ã€‚å‡†ç¡®åº¦è®¡ç®—ä¸ºæ­£ç¡®é¢„æµ‹æ•°ä¸é¢„æµ‹æ€»æ•°ä¹‹æ¯”ã€‚
def evaluate(model, x, y): 
    with torch.no_grad(): 
        outputs = model(x) 
        _, predicted = torch.max(outputs, 1) 
        correct = (predicted == y).sum().item() 
        accuracy = correct / len(y) 
    return accuracy

accuracy_expert1 = evaluate(expert1, x_test, y_test) 
accuracy_expert2 = evaluate(expert2, x_test, y_test) 
accuracy_expert3 = evaluate(expert3, x_test, y_test) 
accuracy_moe = evaluate(moe_model, x_test, y_test) 

print("Expert 1 Accuracy:", accuracy_expert1) 
print("Expert 2 Accuracy:", accuracy_expert2) 
print("Expert 3 Accuracy:", accuracy_expert3) 
print("Mixture of Experts Accuracy:", accuracy_moe) 
#Expert 1 Accuracy: 0.466 
#Expert 2 Accuracy: 0.496 
#Expert 3 Accuracy: 0.378 
#Mixture of Experts Accuracy: 0.614
```

### Mistral å¾®è°ƒ

transformers ç”Ÿæ€ç³»ç»Ÿå†…æ”¯æŒSOTAçš„å¼€ç®±å³ç”¨çš„æ¨ç†æ–¹å¼ï¼Œæ”¯æŒäº† `QLoRA` å’Œ `GPTQ` é‡åŒ–æ–¹æ³•ã€‚
- [ä¸‡å­—é•¿æ–‡è¯¦è§£ Mixtral 8x7B - ä»·å€¼20äº¿ç¾å…ƒçš„ MoE å¤§è¯­è¨€æ¨¡å‹](https://zhuanlan.zhihu.com/p/676114291)

ã€2023-12-15ã€‘ã€Mixtral 8x7Bçš„4-bité‡åŒ–ç‰ˆæ¨¡å‹ã€‘ã€Š[TheBloke/Mixtral-8x7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/tree/main) - 4-bit Mixtral quantized with GPTQ at mainã€‹
- [TheBloke/Mixtral-8x7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/tree/main)
- [dolphin-2.5-mixtral-8x7b](https://huggingface.co/ehartford/dolphin-2.5-mixtral-8x7b)ï¼ŒIt took 3 days to train 1.5 epochs on 4x A100s using qLoRA and Axolotl

#### MoE éƒ¨ç½²

MoEs æ¨ç†éœ€è¦æ›´å¤šVRAM
- åŠç²¾åº¦ Mixtral 8x7b ä¹Ÿéœ€è¦**90GB**çš„VRAMæ‰èƒ½è¿è¡Œ
- è¿™é™åˆ¶äº†æœ¬åœ°è¿è¡Œçš„ç”¨æˆ·èŒƒå›´ã€‚

åœ¨ OpenAssistant å¯¹è¯æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚

ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œå°†æ¨¡å‹è¿›è¡Œäº†4-bit é‡åŒ–ï¼ŒåŒæ—¶åœ¨attentionçš„çº¿æ€§å±‚ä¸­ä»¥[QLoRA](https://arxiv.org/abs/2305.14314)æ–¹å¼è¿›è¡Œå¾®è°ƒè®­ç»ƒã€‚

å› ä¸ºMoEæ¨¡å‹sparseçš„åŸå› ï¼Œä¸èƒ½åƒdenseæ¨¡å‹é‚£æ ·åº”ç”¨ PEFT æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒã€‚

é¦–å…ˆï¼Œå®‰è£…transformerså’ŒTRLï¼ŒåŒæ—¶cloneä»£ç åº“è‡³æœ¬åœ°ã€‚

```sh
pip install -U "transformers==4.36.0" --upgrade
```

éƒ¨ç½²æ–¹å¼

ä¸¤ç§æ–¹å¼è¿›è¡Œæ¨ç†éƒ¨ç½²ï¼š
- ä½¿ç”¨ `transformers` åº“çš„`pipeline()`æ–¹æ³•ã€‚
- ä½¿ç”¨ `TGI`ï¼ˆText Generation Inferenceï¼‰ï¼Œæ”¯æŒæ›´é«˜çº§çš„ç‰¹æ€§ï¼Œæ¯”å¦‚è¿ç»­åˆ†æ‰¹ã€å‘é‡å¹¶è¡Œã€‚
  - Huggingface å‡ºå“çš„**å¤§æ¨¡å‹æ¨ç†éƒ¨ç½²å·¥å…·**ï¼Œæä¾›äº†æ–¹ä¾¿çš„éƒ¨ç½²æœåŠ¡ã€‚æ”¯æŒæ¯”å¦‚è¿ç»­åˆ†æ‰¹ã€å‘é‡å¹¶è¡Œã€tokenæµç­‰ç‰¹æ€§ï¼Œåœ¨å¤šGPUä¸‹è¿›è¡Œæ¨ç†æœåŠ¡ï¼Œä¹Ÿæä¾›äº†æ—¥å¿—å’Œé—®é¢˜æ’æŸ¥çš„èƒ½åŠ›ã€‚

è¿™å‡ ç§æ–¹æ³•éƒ½èƒ½åœ¨åŠç²¾åº¦(float16)ä¸‹è¿è¡Œï¼Œä¹Ÿæ”¯æŒé‡åŒ–çš„æƒé‡å€¼ã€‚
- è™½ç„¶Mixtral 8x7bæ•´ä½“åŠ è½½åéœ€è¦45bå‚æ•°é‡çš„denseæ¨¡å‹å¤§å°çš„å†…å­˜ï¼Œä½†æ˜¯é€šè¿‡é‡åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå¥½çš„åœ¨å°VRAMä¸Šè¿è¡Œ

ç”¨transformersè¿›è¡Œ4-bitçš„é‡åŒ–æ¨ç†ã€‚
- æ¨¡å‹è¾ƒå¤§ï¼Œè‡³å°‘éœ€è¦30Gçš„VRAMè¿›è¡Œè¿è¡Œï¼Œæ¯”å¦‚V100(80\40GB)æˆ–A6000(48GB).

```py
from transformers import AutoTokenizer
import transformers
import torch

model = "mistralai/Mixtral-8x7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    model_kwargs={"torch_dtype": torch.float16, "load_in_4bit": True},
)

messages = [{"role": "user", "content": "Explain what a Mixture of Experts is in less than 100 words."}]
prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
```

è¾“å…¥è¾“å‡º

```s
<s>[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST] A Mixture of Experts is an ensemble learning method that combines multiple models, or "experts," to make more accurate predictions. Each expert specializes in a different subset of the data, and a gating network determines the appropriate expert to use for a given input. This approach allows the model to adapt to complex, non-linear relationships in the data and improve overall performance.
```

#### MoE å¾®è°ƒ

```sh
pip install -U transformers
# pip install -U "transformers==4.36.0" --upgrade
pip install git+https://github.com/huggingface/trl
git clone https://github.com/huggingface/trl
cd trl
```

å¦‚ä¸‹ä»£ç è¿›è¡Œå¾®è°ƒã€‚

```sh
accelerate launch --config_file examples/accelerate_configs/multi_gpu.yaml --num_processes=1 \
    examples/scripts/sft.py \
    --model_name mistralai/Mixtral-8x7B-v0.1 \
    --dataset_name trl-lib/ultrachat_200k_chatml \
    --batch_size 2 \
    --gradient_accumulation_steps 1 \
    --learning_rate 2e-4 \
    --save_steps 200_000 \
    --use_peft \
    --peft_lora_r 16 --peft_lora_alpha 32 \
    --target_modules q_proj k_proj v_proj o_proj \
    --load_in_4bit
```

æ•´ä¸ªè®­ç»ƒè¦åœ¨å•ä¸ª`A100`ä¸ŠèŠ±è´¹**48å°æ—¶**
- ä¹Ÿå¯ç”¨`tweaking --num_processes` è®¾ç½®GPUæ•°è¿›è¡Œå¹¶è¡ŒåŒ–ä»¥æå‡æ•ˆç‡ã€‚


#### QLoRA é‡åŒ–

è¿è¡Œå¦‚ä¸‹è„šæœ¬ï¼Œè‡³å°‘éœ€è¦30GBçš„VRAMçš„GPUã€‚
- Moe é‡åŒ–ï¼Œ[QMoE](https://arxiv.org/abs/2310.16795)

```sh
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_id)

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)

prompt = "[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST]"
inputs = tokenizer(prompt, return_tensors="pt").to(0)

output = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

#### QGPT é‡åŒ–

QGPT é‡‡ç”¨è®­ç»ƒåé‡åŒ–ï¼Œå°†æ¯ä¸€è¡Œæƒé‡å•ç‹¬è¿›è¡Œï¼Œæ¥å‘ç°ä¸€ç§æƒé‡æœ€å°åŒ–è¯¯å·®ã€‚
- è¿™äº›æƒé‡è¢«é‡åŒ–åˆ°int4å¤§å°ï¼Œä½†æ˜¯åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»ç„¶ä½¿ç”¨fp16ã€‚
- ä¸4-bitçš„QLoRAä¸åŒï¼ŒQGPTéœ€è¦æ¨¡å‹åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œæ ¡å‡†ã€‚

å‚è€ƒå·²ç»å¯ç”¨çš„QGPTè¢«å‘å¸ƒåœ¨ huggingface çš„ TheBlokeï¼Œä»»ä½•äººéƒ½å¯ä»¥åœ¨ä¸æ ¡å‡†çš„å‰æä¸‹è¿›è¡Œä½¿ç”¨ã€‚

å¯¹äºMixtral, ä¸ºäº†ç¡®ä¿å¥½æ•ˆæœï¼Œè¿˜è¦ç‰¹åˆ«æ³¨æ„å¾®è°ƒé‡åŒ–å¿…é¡»é™åˆ¶åœ¨**éä¸“å®¶å±‚**ã€‚
- æœ€ç»ˆçš„å›°æƒ‘åº¦æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½ï¼‰åœ¨QGPTå’ŒåŠç²¾åº¦ä¸‹åˆ†åˆ«æ˜¯ 4.40 vs 4.25ã€‚é‡åŒ–åçš„æ¨¡å‹ä»è¿™é‡Œä¸‹è½½ã€‚

è¦è¿è¡ŒQGPTï¼Œé¦–å…ˆè¦å®‰è£… `optimum` å’Œ `auto-qgpt`ã€‚

```sh
pip install -U optimum auto-gptq
```

è¿˜éœ€è¦ä»æºç å®‰è£… `transformers`

```sh
pip install -U git+https://github.com/huggingface/transformers.git
```

æ¥ä¸‹æ¥å¯ç›´æ¥ä½¿ç”¨ `from_pretained` æ–¹æ³•åŠ è½½QGPTé‡åŒ–åçš„æ¨¡å‹è¿›è¡Œ

```py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "TheBloke/Mixtral-8x7B-v0.1-GPTQ"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

prompt = "[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST]"
inputs = tokenizer(prompt, return_tensors="pt").to(0)

output = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

ä¸è®ºæ˜¯ `QGPT` è¿˜æ˜¯ `QLoRA`ï¼Œéƒ½éœ€è¦è‡³å°‘**30GB**çš„VRAMçš„GPU
- å¦‚æœæƒ³è¦åœ¨24GBçš„GPUè¿›è¡Œè¿è¡Œï¼Œå¯ä»¥é€šè¿‡è®¾ç½®device_map="auto"ï¼Œå°†éƒ¨åˆ†å±‚æ”¾åˆ°CPUä¸­è¿›è¡Œè®¡ç®—ã€‚

## LLaMA-MoE

ã€2023-12-25ã€‘[è®­ä¸åŠ¨Mixtralï¼Œè¦ä¸è¯•è¯•LLaMA-MoEï¼Ÿ](https://zhuanlan.zhihu.com/p/674085893)

Mixture-of-Experts (MoE)çš„å…³æ³¨åº¦è¶Šæ¥è¶Šé«˜ã€‚ä¸è¿‡ Mixtral å‚æ•°é‡ç¡®å®å¤ªå¤šäº†ï¼Œæ€»å‚æ•°é‡æ¥è¿‘**47B**ï¼ŒæŠŠæ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°éƒ½è¦å ç”¨**90+GB**ç¡¬ç›˜ç©ºé—´ï¼Œfine-tuningæ›´æ˜¯éš¾ä¸ŠåŠ éš¾ã€‚

ä½†æ˜¯ï¼Œä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå°å·MoEæ¨¡å‹çš„ä»£ä»·ä»ç„¶éå¸¸å¤§ï¼Œä¾ç„¶éœ€è¦è®­ç»ƒtrillionçº§åˆ«çš„tokensã€‚æœ‰æ²¡æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥æœ€å¤§åŒ–å¤ç”¨ä¹‹å‰çš„å‚æ•°ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªå°ä¸€ç‚¹çš„MoEæ¨¡å‹å‘¢ï¼Ÿæœ‰ï¼Œå¤§åŒ–å°

å¯¹äºtransformer blockä¸­ä¸€ä¸ªæ­£å¸¸çš„Feed-Forward Networkï¼ˆFFNï¼‰å±‚ï¼Œé€šå¸¸åŒ…å«ä¸¤å±‚çº¿æ€§å˜æ¢ï¼š
- ç¬¬ä¸€å±‚å°†hidden sizeå˜æ¢ä¸ºintermediate sizeï¼ˆå¦‚4096â†’11008ï¼‰
- ç¬¬äºŒå±‚å°†intermediate sizeè½¬æ¢ä¸ºåŸæ¥çš„hidden sizeï¼ˆå¦‚11008â†’4096ï¼‰

æ—¢ç„¶MoEç”±å¤šä¸ªFFNç»„æˆçš„ä¸“å®¶æ„æˆï¼Œé‚£ç›´æ¥æŠŠç°æœ‰çš„å¤§FFNæ‹†æˆå¤šä¸ªå°FFNä¸å°±å¯ä»¥äº†ï¼Ÿ

|åŸå§‹transformer|æ‹†åˆ†æˆå¤šä¸ªå°ä¸“å®¶|top kè·¯ç”±|
|---|---|---|
|![](https://pic2.zhimg.com/80/v2-9e68da878a9a74c917b7c21ac62952d5_1440w.webp)|![](https://pic1.zhimg.com/80/v2-5eca656d7d749996c6cabb8b824aa3d8_1440w.webp)|![](https://pic2.zhimg.com/80/v2-c262904c123772a707e8cd3c6632e389_1440w.webp)|

ä»£ä»·æ˜¯ä¸ç®¡æ˜¯ä»¥ä½•ç§æ‹†åˆ†æ–¹æ³•è¿›è¡Œå¤§åŒ–å°å¼çš„ä¸“å®¶æ„å»ºï¼Œéƒ½ç ´åäº†åŸæœ‰çš„æ¨¡å‹ç»“æ„ã€‚
- ä¸€ç§æç«¯æƒ…å†µï¼Œå¦‚æœå°†1ä¸ªFFNæ‹†ä¸º4ä¸ªä¸“å®¶ï¼Œæ¯æ¬¡åªé€‰æ‹©ä¸€ä¸ªä¸“å®¶ï¼Œé‚£ä¹ˆå°±ç›¸å½“äºä¸¢å¼ƒäº†75%çš„å‚æ•°ã€‚

å¤§åŒ–å°æ–¹æ¡ˆæ—¢å¯ä»¥ä½¿ç”¨MoEçš„**åŠ¨æ€è·¯ç”±æœºåˆ¶**é€‰æ‹©éœ€è¦â€œä¸¢å¼ƒâ€å“ªäº›ï¼ˆä¸“å®¶ï¼‰å‚æ•°ï¼Œå°†æ¨ç†æ—¶çš„æ¿€æ´»å‚æ•°é‡æ§åˆ¶åœ¨è¾ƒå°çš„èŒƒå›´ï¼Œåˆå¯ä»¥ä¿ç•™åŸæœ‰æ¨¡å‹çš„å®¹é‡ï¼ˆå› ä¸ºæ€»å‚æ•°é‡æ²¡å˜ï¼‰ã€‚

ä¸ºäº†è¿›ä¸€æ­¥æ¢å¤æ¨¡å‹åœ¨æ‹†åˆ†åçš„æ€§èƒ½ï¼Œä½¿ç”¨SlimPajamaæ•°æ®å¯¹å…¶è¿›è¡Œäº†200B tokensçš„ç»§ç»­é¢„è®­ç»ƒã€‚è™½ç„¶æœ€ç»ˆç»“æœæ¯”7Bçš„denseæ¨¡å‹å·®ï¼Œä½†æ¯”åŒç­‰æ¿€æ´»å‚æ•°é‡çš„å…¶å®ƒdenseæ¨¡å‹è¾ƒå¥½ã€‚

LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training
- [llama-moe](https://github.com/pjlab-sys4nlp/llama-moe)
- è‹å·å¤§å­¦åšå£«ç”Ÿ æœ±æ¡ï¼Œè®²è§£è§†é¢‘ï¼š[LLaMA-MoEï¼šåŸºäºå‚æ•°å¤ç”¨çš„æ··åˆä¸“å®¶æ¨¡å‹æ„å»ºæ–¹æ³•æ¢ç´¢](https://www.bilibili.com/video/BV1s64y1K7Wf/?spm_id_from=333.337.search-card.all.click)

<iframe src="//player.bilibili.com/player.html?aid=580981332&bvid=BV1s64y1K7Wf&cid=1396442287&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>

å¤‡æ³¨
- å¼€æºäº†å—ï¼Ÿä¸ä»…æ¨¡å‹æƒé‡å¼€æºäº†ï¼Œä¸“å®¶æ„å»ºå’Œè®­ç»ƒçš„ä»£ç éƒ½å¼€äº†
- æš‚ä¸æ”¯æŒä¸­æ–‡ï¼Œéœ€è¦å¢é‡é¢„è®­ç»ƒ
- å“ªç§åˆ’åˆ†æ–¹æ¡ˆæœ€å¥½ï¼Ÿéƒ½å·®ä¸å¤šï¼Œæœ€åæˆ‘ä»¬é€‰æ‹©äº†éšæœºåˆ’åˆ†
- ä»€ä¹ˆæ•°æ®é…æ¯”å¥½ï¼Ÿä½¿ç”¨Sheared LLaMAçš„é™æ€æ•°æ®é‡‡æ ·ç‡å°±å·²ç»å¾ˆå¥½äº†ã€‚è®­ç»ƒæ—¶çš„losså’Œæœ€ç»ˆç»“æœçš„æŒ‡æ ‡ä¸æ˜¯éå¸¸å¯¹åº”ï¼ˆlosså°ä¸ä¸€å®šä»£è¡¨ç»“æœå°±é«˜ï¼‰ã€‚åŠ¨æ€æ•°æ®é‡‡æ ·æ¯”è¾ƒtrickyï¼Œèµ·å§‹é‡‡æ ·ç‡å’Œç›®æ ‡losså¯¹ç»“æœå½±å“æ¯”è¾ƒå¤§ï¼Œæ•ˆæœä¸ä¸€å®šå°±å¥½ã€‚
- ä¸åŒæ¥æºçš„æ•°æ®ä¼šé€‰æ‹©ä¸åŒçš„ä¸“å®¶å—ï¼Ÿæµ…å±‚å·®å¼‚ä¸å¤§ï¼Œå±‚æ•°è¶Šæ·±ï¼Œä¸åŒæ•°æ®æºä¹‹é—´çš„ä¸“å®¶é€‰æ‹©æƒ…å†µå·®å¼‚è¶Šæ˜æ˜¾ã€‚

ç›®å‰å¯¹äºdecoder-only MoEçš„ä¸‹æ¸¸åº”ç”¨ç ”ç©¶è¿˜æ¯”è¾ƒå°‘ï¼Œå¯ç”¨çš„æ¨¡å‹ä¹Ÿæ¯”è¾ƒç¨€ç¼ºã€‚



## æ”¹è¿›ï¼šåŒ—å¤§ DeepSeek MoE

ã€2024-1-11ã€‘åŒ—å¤§ï¼ŒDeepSeek MoE æ˜¯å›½å†…ç¬¬ä¸€ä¸ªå¼€æºMoEæ¨¡å‹
- [DeepSeekMoEæŠ€æœ¯æŠ¥å‘Š](https://github.com/deepseek-ai/DeepSeek-MoE/blob/main/DeepSeekMoE.pdf)

![](https://picx.zhimg.com/80/v2-9400cd740929c7954d9b4922c4fb9d4d_1440w.webp?source=2c26e567)


ä¸¤ä¸ªåˆ›æ–°ç‚¹
1. æŠŠä¸€ä¸ªä¸“å®¶åšæ›´ç»†ç²’åº¦åˆ‡åˆ†ï¼Œå¦‚ä¸‹å›¾ï¼ˆbï¼‰ã€‚è¿™ä¸ªæ–¹æ³•å’Œæˆ‘åˆ·åˆ°çš„è¿™ç¯‡Mixtralå¾®è°ƒæ€è·¯çš„çŸ¥ä¹æ–‡ç« æœ‰ç‚¹åƒï¼Œæ°‘é—´æœ‰é«˜äººã€‚
2. åˆ†é…ä¸€äº›ä¸“å®¶æ¯æ¬¡éƒ½æ¿€æ´»ï¼Œä½œä¸ºå…±äº«ä¸“å®¶ï¼Œå›¾(c)ã€‚

DeepSeek MoE è®¾è®¡ä¸Šè¿°ç»“æ„çš„å‰æåœ¨äºå‡è®¾ï¼š<span style='color:blue'>ç‰¹å®šä¸“å®¶èƒ½å¯ä»¥è¦†æŸç§é¢†åŸŸçŸ¥è¯†ã€‚</span>
- ä¸“å®¶çš„ç»†ç²’åº¦åˆ‡åˆ†å¯ä»¥é¿å…ä¸€ä¸ªä¸“å®¶è¦†ç›–å¤ªå¤šé¢†åŸŸæŠŠçŸ¥è¯†å­¦æ‚äº†ï¼›
- å…±äº«ä¸“å®¶å¯ä»¥è®©ä¸€äº›å…¬å…±çŸ¥è¯†æ¯æ¬¡éƒ½å‚ä¸è®¡ç®—ã€‚

åŒæ—¶æœŸå›½å¤–å¼€æºçš„ [Mistral of Experts](https://arxiv.org/pdf/2401.04088.pdf) ä¹Ÿæ”¾äº†æŠ€æœ¯æŠ¥å‘Šï¼Œå®Œå…¨ç…§ç€GPT-4è§£å¯†æŠ¥å‘Šå¤ç°çš„MoEï¼Œæ¨¡å‹ç»“æ„å°±æ˜¯ç»å…¸çš„**GShardæ–¹å¼**ã€‚æŠ€æœ¯æŠ¥å‘Šé‡Œçš„ Sec. 5 Routing analysiså±•ç¤ºå¾ˆå¤šè·¯ç”±å·¥ä½œçš„ç‰¹å¾ï¼Œè¿™äº›éƒ½æ˜¯éå¸¸æ–°é²œçš„ä¸€æ‰‹èµ„æ–™ã€‚æœ‰ä¸€äº›ç»“è®ºå¾ˆæœ‰è¶£ï¼š
- Mixtral of Expertsè·¯ç”±è§„åˆ™ä¸æ–‡æœ¬çš„è¯­ä¹‰ä¸»é¢˜æ— å…³ï¼Œè¿™æ„å‘³ç€ä¸“å®¶å¹¶ä¸ä¸“é—¨ç²¾é€šæŸä¸€é¢†åŸŸçš„çŸ¥è¯†ã€‚
- è·¯ç”±è§„åˆ™å±•ç¤ºå‡ºäº†ä¸€å®šçš„è¯­æ³•ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼ŒæŸäº›å…³é”®è¯ç»å¸¸è¢«åˆ†é…ç»™åŒä¸€ä½ä¸“å®¶ã€‚
- è·¯ç”±è§„åˆ™è¿˜å±•ç¤ºäº†ä½ç½®çš„å±€éƒ¨æ€§ï¼Œç›¸é‚»çš„tokené€šå¸¸è¢«è·¯ç”±åˆ°åŒä¸€ä½ä¸“å®¶ï¼Œè¿™è¡¨æ˜tokenåœ¨å¥å­ä¸­çš„ä½ç½®ä¸è·¯ç”±é€‰æ‹©æœ‰å…³ã€‚

ä½œè€…ï¼š[æ–¹ä½³ç‘:å¦‚ä½•çœ‹å¾…DeepSeekå¼€æºå›½äº§MoEå¤§æ¨¡å‹DeepSeek MoE 16B?](https://www.zhihu.com/question/639062017/answer/3359331423)



## Nous Hermes 2

ã€2024-2-1ã€‘[Nous Hermes 2ï¼šè¶…è¶ŠMixtral 8x7Bçš„MOEæ¨¡å‹æ–°é«˜åº¦](https://www.toutiao.com/article/7330289695777358371)

Nous Researchå…¬å¸å‘å¸ƒäº†å…¶åŸºäºMixtral 8x7Bå¼€å‘çš„æ–°å‹å¤§æ¨¡å‹â€”â€”`Nous Hermes 2`ï¼Œè¿™ä¸€æ¨¡å‹åœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­**è¶…è¶Š**äº†`Mixtral 8x7B Instruct`ï¼Œæ ‡å¿—ç€`MOE`ï¼ˆMixture of Expertsï¼Œä¸“å®¶æ··åˆæ¨¡å‹ï¼‰æŠ€æœ¯çš„æ–°çªç ´ã€‚
- Huggingfaceæ¨¡å‹ä¸‹è½½ï¼š[NousResearch](https://huggingface.co/NousResearch)
- AIå¿«ç«™æ¨¡å‹å…è´¹åŠ é€Ÿä¸‹è½½ï¼š[NousResearch](https://aifasthub.com/models/NousResearch)

Nous Hermes 2æ˜¯åœ¨`Mixtral 8x7B`åŸºç¡€ä¸Šè¿›ä¸€æ­¥å¾®è°ƒè€Œæˆã€‚è¿™ä¸ªæ¨¡å‹é€šè¿‡`SFT`ï¼ˆSupervised Fine-Tuningï¼Œæœ‰ç›‘ç£å¾®è°ƒï¼‰å’Œ`DPO`ï¼ˆDistributed Pseudo Outputï¼Œåˆ†å¸ƒå¼ä¼ªè¾“å‡ºï¼‰ä¸¤ç§æ–¹æ³•å¾—åˆ°ä¼˜åŒ–ï¼Œåˆ†åˆ«å‘å¸ƒäº†ä¸¤ä¸ªç‰ˆæœ¬ï¼š`Nous Hermes 2 Mixtral 8x7B SFT`å’Œ`Nous Hermes 2 Mixtral 8x7B DPO`ã€‚è¿™ä¸¤ä¸ªç‰ˆæœ¬éƒ½å±•ç¤ºäº†åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­çš„å“è¶Šæ€§èƒ½ã€‚


## Gemini 1.5

ã€2024-2-15ã€‘è°·æ­Œå‘å¸ƒ Gemini 1.5ï¼Œè¿™æ˜¯AIé¢†åŸŸçš„ä¸€æ¬¡é©å‘½æ€§é£è·ƒ
- [Our next-generation model: Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#sundar-note)
- ä¸ä»…æ”¯æŒé«˜è¾¾ç™¾ä¸‡çº§token ä¸Šä¸‹æ–‡ç†è§£ï¼Œè¿˜èƒ½å¤„ç†è¶…é•¿æ–‡æ¡£ã€ä»£ç åº“ï¼Œç”šè‡³å®Œæ•´ç”µå½±ã€‚ğŸ¬ğŸ“šğŸ’» æ— è®ºæ˜¯ç ”ç©¶ã€ç¼–ç¨‹è¿˜æ˜¯å†…å®¹åˆ›ä½œï¼ŒAIéƒ½èƒ½æä¾›å‰æ‰€æœªæœ‰çš„æ”¯æŒã€‚ğŸ¤–ğŸ§ 
- Gemini 1.5 é‡‡ç”¨äº†åˆ›æ–° Mixture-of-Experts æ¶æ„ï¼Œè®­ç»ƒæ›´é«˜æ•ˆï¼Œè®¡ç®—éœ€æ±‚æ›´ä½ï¼Œè®©AIæŠ€æœ¯æ›´åŠ äº²æ°‘ã€‚ğŸŒğŸ”§ åœ¨æ€§èƒ½æµ‹è¯•ä¸­ï¼Œå®ƒçš„è¡¨ç°è¶…è¶Šäº†å‰ä»£ï¼Œä¸é¡¶å°–æ¨¡å‹ç›¸åª²ç¾ã€‚ğŸ†
- ![](https://picx.zhimg.com/80/v2-9e03824c31eb4068fce87b2c8a32c901_1440w.webp?source=2c26e567)

è¿™ä¸€è¿›æ­¥é¢„ç¤ºç€AIå°†å¦‚ä½•æ·±åˆ»æ”¹å˜å·¥ä½œä¸ç”Ÿæ´»ã€‚ğŸŒŸ ä»ç§‘ç ”åˆ°å¨±ä¹ï¼Œä»æ•™è‚²åˆ°åŒ»ç–—ï¼ŒAIçš„æ½œåŠ›æ— é™ã€‚ğŸŒˆ è°·æ­Œiconçš„è¿™ä¸€åˆ›æ–°ï¼Œæ— ç–‘å°†åŠ é€Ÿæ™ºèƒ½ç§‘æŠ€çš„å‘å±•ï¼Œå¼€å¯ä¸€ä¸ªå…¨æ–°çš„æ™ºèƒ½æ—¶ä»£ã€‚


## æ”¹è¿›ï¼šMoT

ã€2024-4-9ã€‘[MOE vs MOT è®©LLMæ›´åŠ æœ‰æ•ˆ](https://zhuanlan.zhihu.com/p/691070810)
- åŸæ–‡ [Mixture of Experts vs Mixture of Tokens: Making LLMs more efficient](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-tokens)

ä¸“å®¶æ··åˆï¼ˆMixture of Expertsï¼šMOEï¼‰è¢«å¤§è‚†å®£ä¼ æ”¹è¿›Transformeræ¨¡å‹ï¼Œä½†æ›´æœ‰å‰é€”çš„æ–°æ–¹æ³•â€”â€”**ä»¤ç‰Œæ··åˆ**ï¼ˆMixture of Tokensï¼šMOTï¼‰ã€‚

MoE ä¸­ä¸“å®¶æ˜¯ä¸“é—¨æ‰§è¡Œä¸€é¡¹æˆ–å¤šé¡¹ä»»åŠ¡çš„æ¨¡å‹ã€‚
- æ ‡å‡†Transformeræ¨¡å‹ä¸­ï¼Œä»¤ç‰Œ(token)ç”±æ ‡å‡†**å‰é¦ˆå±‚**å¤„ç†ã€‚
- MoE åˆ™å°†æ¯ä¸ªtokenå®šå‘åˆ°ä¸€ç»„ä¸“å®¶ä»¥åŠä¸€ä¸ªç§°ä¸º**æ§åˆ¶å™¨**çš„å°å‹ç½‘ç»œã€‚
  - **å¼€å…³Transformer**å°†æ¯ä¸ªä»¤ç‰Œå‘é€ç»™æ§åˆ¶å™¨äº§ç”Ÿçš„å¾—åˆ†æœ€é«˜çš„ä¸€ä½ä¸“å®¶ã€‚è¿™é¡¹æŠ€æœ¯å¯¼è‡´å‚æ•°å¤§å¹…å‡å°‘â€”â€”ä» 1.6T æ¨¡å‹ï¼ˆT5 æ¶æ„ï¼‰åˆ°ç­‰æ•ˆ 1.4B vanilla Transformer çš„ FLOPS æˆæœ¬ã€‚

MoEçš„é—®é¢˜
- è®­ç»ƒä¸ç¨³å®šæ€§ï¼šè¿™ç§æ–¹æ³•è°¨æ…åœ°é€‰æ‹©ä¸“å®¶å¹¶å°†å…¶ä¸tokenåŒ¹é…ã€‚è¿™æ„å‘³ç€æ§åˆ¶å™¨æƒé‡çš„å¾®å°å˜åŒ–å¯èƒ½ä¼šå¯¹æ§åˆ¶å™¨å†³ç­–äº§ç”Ÿä¸æˆæ¯”ä¾‹çš„å½±å“ã€‚
- è´Ÿè½½ä¸å¹³è¡¡ï¼š MoE çš„é—®é¢˜æ˜¯æˆ‘ä»¬æ— æ³•æœ‰æ•ˆåœ°å¹³è¡¡ä»¤ç‰Œå’Œä¸“å®¶çš„åˆ†é…æ–¹å¼ï¼Œå› ä¸ºè·¯ç”±ç½‘ç»œçš„é€‰æ‹©æ²¡æœ‰å—åˆ°æœ‰æ•ˆçš„é™åˆ¶ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæœ‰äº›ä»¤ç‰Œæ²¡æœ‰ä»»ä½•ä¸“å®¶æ¥å¤„ç†å®ƒä»¬ï¼ˆä»¤ç‰Œä¸¢å¼ƒï¼‰ï¼Œå¹¶ä¸”å‡ ä¹æ‰€æœ‰ä»¤ç‰Œéƒ½åªåˆ†é…ç»™å°‘æ•°ä¸“å®¶ï¼ˆæ¨¡å‹å´©æºƒï¼‰ã€‚
- ä¿¡æ¯æ³„æ¼ï¼šä¸€äº›æˆåŠŸçš„ MoE æ–¹æ³•å°†åºåˆ—ä¸­ä¸åŒä½ç½®çš„ä»¤ç‰Œä¸€èµ·å¤„ç†ï¼ˆå³ï¼Œé€šè¿‡æ¯”è¾ƒæ‰¹æ¬¡ä¸­æ‰€æœ‰ä»¤ç‰Œçš„åˆ†æ•°ï¼‰ã€‚è¿™é€ æˆäº†åºåˆ—å†…ä¿¡æ¯æ³„æ¼å¹¶é˜»ç¢äº†å®ƒä»¬åœ¨è‡ªå›å½’è§£ç ä¸­çš„å®ç”¨æ€§ã€‚
- çŸ¥è¯†æ··åˆæ€§ï¼šç”±äºä¸“å®¶æ•°é‡æœ‰é™ï¼Œä¼ ç»Ÿ MoE æ¶æ„ä¸­çš„ä¸“å®¶é€šå¸¸ä¼šç§¯ç´¯å¹¿æ³›çš„çŸ¥è¯†ã€‚è¿™ç§å¹¿æ³›çš„çŸ¥è¯†åº“å‰Šå¼±äº†ä¸ªåˆ«ä¸“å®¶çš„ä¸“ä¸šæ€§å’Œæœ‰æ•ˆæ€§ã€‚
- çŸ¥è¯†å†—ä½™ï¼šå¤šä¸ªä¸“å®¶åœ¨å­¦ä¹ ç›¸ä¼¼ä¿¡æ¯æ—¶æœ‰è¶‹åŒçš„å€¾å‘ï¼Œå¯¼è‡´çŸ¥è¯†é¢†åŸŸé‡å å’Œæ¨¡å‹å‚æ•°ä½¿ç”¨æ•ˆç‡ä½ä¸‹ã€‚

Cohere AI çš„ç§‘å­¦å®¶è§£å†³MOEä¸»è¦æŒ‘æˆ˜ä¹‹ä¸€çš„æ–¹æ³•â€”â€”**å°†æ‰€æœ‰ä¸“å®¶å­˜å‚¨åœ¨å†…å­˜ä¸­**ã€‚
- å°† MoE æ¶æ„ä¸è½»é‡çº§ä¸“å®¶ç‹¬ç‰¹åœ°ç»“åˆèµ·æ¥ï¼Œæå‡ºäº†ä¸€ç§å‚æ•°æå…¶é«˜æ•ˆçš„ MoEã€‚
- MoE æ¶æ„ä¼˜äºæ ‡å‡† PEFT æ–¹æ³•ï¼Œå¹¶ä¸”ä»…é€šè¿‡æ›´æ–°è½»é‡çº§ä¸“å®¶å³å¯è¾¾åˆ°å®Œå…¨å¾®è°ƒçš„æ•ˆæœâ€”â€”ä¸åˆ° 11B å‚æ•°æ¨¡å‹çš„ 1%ã€‚

DeepSeekMoE æ¶æ„é€šè¿‡é‡‡ç”¨ä¸¤ä¸ªå…³é”®ç­–ç•¥æ¥å¢å¼ºä¸“å®¶ä¸“ä¸šåŒ–ï¼šç»†ç²’åº¦ä¸“å®¶åˆ†å‰²å’Œå…±äº«ä¸“å®¶éš”ç¦»ã€‚
- **ç»†ç²’åº¦ä¸“å®¶åˆ†å‰²**ï¼ˆFine-grained expert segmentationï¼‰æ¶‰åŠç»†åˆ† FFN ä¸­é—´éšè—ç»´åº¦ï¼Œä»è€Œå…è®¸ç»†ç²’åº¦ä¸“å®¶ä¹‹é—´æ›´ç»†è‡´åœ°åˆ†é…çŸ¥è¯†ã€‚è¿™ç§ç»†åˆ†ä½¿æ¯ä¸ªä¸“å®¶èƒ½å¤Ÿä¸“æ³¨äºæ›´å…·ä½“çš„çŸ¥è¯†é¢†åŸŸï¼Œä»è€Œåœ¨ä¿æŒæ’å®šçš„è®¡ç®—æˆæœ¬çš„åŒæ—¶å®ç°æ›´é«˜æ°´å¹³çš„ä¸“ä¸šåŒ–ã€‚
- **å…±äº«ä¸“å®¶éš”ç¦»**ï¼ˆshared expert isolationï¼‰ç­–ç•¥å°†ç‰¹å®šä¸“å®¶æŒ‡å®šä¸ºâ€œå…±äº«â€ï¼Œè´Ÿè´£æ•è·ä¸åŒèƒŒæ™¯ä¸‹çš„å…±åŒçŸ¥è¯†ã€‚é€šè¿‡å°†ä¸€èˆ¬çŸ¥è¯†é›†ä¸­åœ¨è¿™äº›å…±äº«ä¸“å®¶ä¸Šï¼Œå‡å°‘äº†å…¶ä»–ä¸“å®¶å­¦ä¹ è¿‡ç¨‹ä¸­çš„å†—ä½™ã€‚è¿™ç§æ–¹æ³•æé«˜äº†å‚æ•°æ•ˆç‡ï¼Œå¹¶ç¡®ä¿æ¯ä½ä¸“å®¶å§‹ç»ˆä¸“æ³¨äºç‹¬ç‰¹ä¸”ç‹¬ç‰¹çš„çŸ¥è¯†é¢†åŸŸã€‚

DeepSeekMoE ç»è¿‡æ‰©å±•å¯è®­ç»ƒ 16B æ¨¡å‹ï¼Œåªéœ€çº¦ 40% çš„è®¡ç®—é‡ï¼Œå³å¯å®ç°ä¸ DeepSeek 7B å’Œ LLaMA2 7B ç›¸å½“çš„æ€§èƒ½ã€‚ç ”ç©¶äººå‘˜è¿˜è®¡åˆ’å°† DeepSeekMoE æ‰©å±•åˆ° 145Bï¼Œçªå‡ºå…¶ç›¸å¯¹äº GShard æ¶æ„çš„ä¼˜åŠ¿ï¼Œå¹¶å±•ç¤ºä¸ DeepSeek 67B ç›¸å½“çš„æ€§èƒ½ã€‚

Tokenæ··åˆï¼ˆMixture of Tokensï¼‰

MoT ä¸å°†tokenå‘é€ç»™ä¸“å®¶ï¼Œè€Œæ˜¯å°†ä¸åŒç¤ºä¾‹ä¸­çš„tokenæ··åˆåœ¨ä¸€èµ·ï¼Œç„¶åå†å°†å…¶æä¾›ç»™ä¸“å®¶ã€‚
- è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿä»æ‰€æœ‰token-ä¸“å®¶ç»„åˆä¸­å­¦ä¹ ï¼Œå¹¶æé«˜è®­ç»ƒç¨³å®šæ€§å’Œä¸“å®¶åˆ©ç”¨ç‡ã€‚
- åœ¨å‘ä¸“å®¶æä¾›tokenåï¼Œæ¯ç§æ··åˆç‰©éƒ½ä¼šè¢«å¤„ç†å¹¶é‡æ–°åˆ†é…å›åŸå§‹tokenã€‚

MoT é€šè¿‡è¿›è¡Œä»¥ä¸‹æ›´æ”¹æ¥è§£å†³ MoE æ¨¡å‹çš„é—®é¢˜ï¼š
- æ··åˆæ¥è‡ªä¸åŒç¤ºä¾‹çš„tokenï¼Œç„¶åå°†å…¶æä¾›ç»™ä¸“å®¶ï¼›é€šè¿‡å…è®¸æ¨¡å‹ä»æ‰€æœ‰token-ä¸“å®¶ç»„åˆä¸­å­¦ä¹ ï¼Œè¿™æé«˜äº†è®­ç»ƒç¨³å®šæ€§å’Œä¸“å®¶åˆ©ç”¨ç‡ã€‚
- tokenæ··åˆæ˜¯ä¸€ä¸ªå®Œå…¨å¯å¾®çš„æ¨¡å‹ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥ä½¿ç”¨æ ‡å‡†çš„åŸºäºæ¢¯åº¦çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚è¿™é¿å…äº†è¾…åŠ©æŸå¤±æˆ–å…¶ä»–éš¾ä»¥è®­ç»ƒçš„æŠ€æœ¯çš„éœ€è¦ï¼Œä»è€Œæ›´å®¹æ˜“è®­ç»ƒå’Œéƒ¨ç½²ã€‚â€

tokenæ··åˆæœ‰å¯èƒ½æ˜¾ç€æé«˜LLMçš„è¡¨ç°å’Œæ•ˆç‡ã€‚ä¸æ™®é€š Transformer ç›¸æ¯”ï¼Œå®ƒæ˜¾ç¤ºå‡ºè®­ç»ƒæ—¶é—´å‡å°‘äº† 3 å€çš„æƒŠäººç»“æœã€‚

# ç»“æŸ