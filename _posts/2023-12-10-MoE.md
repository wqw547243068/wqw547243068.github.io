---
layout: post
title:  æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ä¸“é¢˜
date:   2023-11-10 16:52:00
categories: å¤§æ¨¡å‹
tags: gpt moe ä¸“å®¶ llama transformer pytorch mistral mixtral
excerpt: æ··åˆä¸“å®¶æ¨¡å‹ï¼ŒMoEç³»åˆ—
mathjax: true
permalink: /moe
---

* content
{:toc}


# MoE æ··åˆä¸“å®¶æ¨¡å‹



## ä»€ä¹ˆæ˜¯ MoE

MoEæ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡ï¼Œåœ¨Transformeræ¨¡å—ä¸­é›†æˆäº†ä¸“å®¶/æ¨¡å‹å±‚ã€‚


ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰æŠŠå¤æ‚ä»»åŠ¡åˆ†å‰²æˆä¸€ç³»åˆ—æ›´å°ã€æ›´å®¹æ˜“å¤„ç†çš„**å­ä»»åŠ¡**ï¼Œæ¯ä¸ªå­ä»»åŠ¡ç”±ä¸€ä¸ªç‰¹å®šé¢†åŸŸçš„ã€Œä¸“å®¶ã€è´Ÿè´£ã€‚

å½“æ•°æ®æµç»MoEå±‚æ—¶ï¼Œæ¯ä¸ªè¾“å…¥tokenéƒ½ä¼š**åŠ¨æ€**è·¯ç”±åˆ°ä¸“å®¶å­æ¨¡å‹è¿›è¡Œå¤„ç†ã€‚å½“æ¯ä¸ªä¸“å®¶ä¸“é—¨ä»äº‹ç‰¹å®šä»»åŠ¡æ—¶ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å®ç°æ›´é«˜æ•ˆçš„è®¡ç®—å¹¶è·å¾—æ›´å¥½çš„ç»“æœã€‚
- å°†éœ€è¦é¢„æµ‹çš„é—®é¢˜åˆ’åˆ†ä¸º**å­ä»»åŠ¡**ï¼ˆé‡‡ç”¨é¢†åŸŸçŸ¥è¯†æˆ–è€…æ— ç›‘ç£èšç±»ç®—æ³•ï¼‰ã€‚
- ç„¶åï¼Œé’ˆå¯¹æ¯ä¸ªæ•°æ®å­é›†è®­ç»ƒ**ä¸“å®¶æ¨¡å‹**ï¼ˆExpert Modelsï¼‰ï¼Œä¸“å®¶æ¨¡å‹å¯ä»¥æ˜¯ä»»ä½•æ¨¡å‹ï¼Œæ¯”å¦‚æ”¯æŒå‘é‡æœº ï¼ˆSVMï¼‰ æˆ–è€…ç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªä¸“å®¶æ¨¡å‹æ¥æ”¶ç›¸åŒçš„è¾“å…¥æ¨¡å¼å¹¶è¿›è¡Œé¢„æµ‹ã€‚
  - MoEè¿˜åŒ…å«**é—¨æ§æ¨¡å‹**ï¼ˆGating Modelï¼‰ï¼Œç”¨äºè§£é‡Šæ¯ä¸ªä¸“å®¶åšå‡ºçš„é¢„æµ‹ï¼Œå¹¶æ ¹æ®è¾“å…¥é€‰æ‹©ä¿¡ä»»å“ªä¸ªä¸“å®¶ã€‚
- æœ€åï¼ŒMoEéœ€è¦ä¸€ç§**èšåˆæœºåˆ¶**ï¼ˆPooling Methodï¼‰ï¼Œæ ¹æ®é—¨æ§æ¨¡å‹å’Œä¸“å®¶çš„è¾“å‡ºè¿›è¡Œé¢„æµ‹ã€‚

åŸå§‹MoEçš„è¿­ä»£:ã€Œç¨€ç–é—¨æ§ä¸“å®¶æ··åˆå±‚ã€çš„æ–¹æ³•ï¼Œæä¾›äº†ä¸€ä¸ªé€šç”¨çš„ç¥ç»ç½‘ç»œç»„ä»¶ï¼Œå¯ä»¥é€‚åº”ä¸åŒç±»å‹çš„ä»»åŠ¡ã€‚

æ··åˆä¸“å®¶æ¨¡å‹åœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰åº”ç”¨ï¼ŒåŒ…æ‹¬æ¨èç³»ç»Ÿã€è¯­è¨€å»ºæ¨¡å’Œå„ç§å¤æ‚çš„é¢„æµ‹ä»»åŠ¡ã€‚

### MoE ç»“æ„

MoEç”±ä¸¤ç§ç±»å‹çš„ç½‘ç»œç»„æˆ:(1)ä¸“å®¶ç½‘ç»œå’Œ(2)é—¨æ§ç½‘ç»œã€‚
- `ä¸“å®¶ç½‘ç»œ`: ä¸“å®¶ç½‘ç»œæ˜¯**ä¸“æœ‰æ¨¡å‹**ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½ç»è¿‡è®­ç»ƒï¼Œåœ¨æ•°æ®å­é›†ä¸­è¡¨ç°å‡ºè‰²ã€‚
  - MoEçš„ç†å¿µæ˜¯æ‹¥æœ‰å¤šåä¼˜åŠ¿äº’è¡¥çš„ä¸“å®¶ï¼Œç¡®ä¿å¯¹é—®é¢˜ç©ºé—´çš„å…¨é¢è¦†ç›–ã€‚
- `é—¨æ§ç½‘ç»œ`: é—¨æ§ç½‘ç»œå……å½“**æŒ‡æŒ¥**ï¼Œåè°ƒæˆ–ç®¡ç†ä¸ªåˆ«ä¸“å®¶çš„è´¡çŒ®ã€‚å®ƒå­¦ä¹ (æˆ–æƒè¡¡)å“ªä¸ªç½‘ç»œæ“…é•¿å¤„ç†å“ªç§ç±»å‹çš„è¾“å…¥ã€‚ç»è¿‡è®­ç»ƒçš„é—¨æ§ç½‘ç»œå¯ä»¥è¯„ä¼°æ–°çš„è¾“å…¥å‘é‡ï¼Œå¹¶æ ¹æ®ä¸“å®¶çš„ç†Ÿç»ƒç¨‹åº¦å°†å¤„ç†è´£ä»»åˆ†é…ç»™æœ€åˆé€‚çš„ä¸“å®¶æˆ–ä¸“å®¶ç»„åˆã€‚é—¨æ§ç½‘ç»œæ ¹æ®ä¸“å®¶çš„è¾“å‡ºä¸å½“å‰è¾“å…¥çš„ç›¸å…³æ€§åŠ¨æ€è°ƒæ•´å…¶æƒé‡ï¼Œç¡®ä¿å®šåˆ¶å“åº”ã€‚

MoE å¤„ç†æµç¨‹
- ![](https://pic1.zhimg.com/80/v2-654cca302d29837e461c6ef04667a1a0_1440w.webp)

MoEæœ€å…³é”®çš„ç»„ä»¶ï¼š
- **ä¸“å®¶**ï¼ˆExpertï¼‰ï¼š
  - ä¸“é—¨è®­ç»ƒçš„å°å‹ç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œéƒ½åœ¨å…¶æ“…é•¿çš„é¢†åŸŸæœ‰ç€å“è¶Šçš„è¡¨ç°
  - MoEå±‚ç”±è®¸å¤š**ä¸“å®¶**ã€**å°å‹MLP**æˆ–**å¤æ‚LLM**ï¼ˆå¦‚ Mistral 7Bï¼‰ç»„æˆã€‚
- **è·¯ç”±å™¨**ï¼ˆRouterï¼‰ï¼š`é—¨æ§ç½‘ç»œ`, MoEæ¶æ„ä¸­çš„**å†³ç­–æ ¸å¿ƒ**
  - è·¯ç”±å™¨ç¡®å®šå°†å“ªäº›è¾“å…¥tokenåˆ†é…ç»™å“ªäº›ä¸“å®¶ã€‚
  - é—¨æ§ç½‘ç»œä¼šè®¡ç®—è¾“å…¥æ•°æ®ä¸æ¯ä¸ªä¸“å®¶çš„**å…¼å®¹æ€§å¾—åˆ†**ï¼Œç„¶åä¾æ®è¿™äº›å¾—åˆ†å†³å®šæ¯ä¸ªä¸“å®¶åœ¨å¤„ç†ä»»åŠ¡ä¸­çš„ä½œç”¨ã€‚

è·¯ç”±ç­–ç•¥æœ‰ä¸¤ç§ï¼š**tokené€‰æ‹©è·¯ç”±å™¨** æˆ– **è·¯ç”±å™¨é€‰æ‹©token**ã€‚

è·¯ç”±å™¨ä½¿ç”¨**softmaxé—¨æ§å‡½æ•°**é€šè¿‡ä¸“å®¶æˆ–tokenå¯¹æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ï¼Œå¹¶é€‰æ‹©å‰kä¸ªã€‚

è¿™äº›ç»„ä»¶å…±åŒä½œç”¨ï¼Œç¡®ä¿é€‚åˆçš„ä»»åŠ¡ç”±åˆé€‚çš„ä¸“å®¶æ¥å¤„ç†ã€‚é—¨æ§ç½‘ç»œæœ‰æ•ˆåœ°å°†è¾“å…¥æ•°æ®å¼•å¯¼è‡³æœ€åˆé€‚çš„ä¸“å®¶ï¼Œè€Œä¸“å®¶ä»¬åˆ™ä¸“æ³¨äºè‡ªå·±æ“…é•¿çš„é¢†åŸŸã€‚è¿™ç§åˆä½œæ€§è®­ç»ƒä½¿å¾—æ•´ä½“æ¨¡å‹å˜å¾—æ›´åŠ å¤šåŠŸèƒ½å’Œå¼ºå¤§ã€‚

MoE å¥½å¤„ï¼š
- æ¯ä¸ªä¸“å®¶éƒ½å¯ä»¥ä¸“é—¨å¤„ç†ä¸åŒçš„ä»»åŠ¡æˆ–æ•°æ®çš„ä¸åŒéƒ¨åˆ†ã€‚
- MoEæ„æ¶èƒ½å‘LLMæ·»åŠ å¯å­¦ä¹ å‚æ•°ï¼Œè€Œä¸å¢åŠ æ¨ç†æˆæœ¬ã€‚
- å¯ä»¥åˆ©ç”¨ç¨€ç–çŸ©é˜µçš„é«˜æ•ˆè®¡ç®—
- å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä¸“å®¶å±‚ï¼Œä»¥æœ‰æ•ˆåˆ©ç”¨GPUçš„å¹¶è¡Œèƒ½åŠ›
- å¸®åŠ©æœ‰æ•ˆåœ°æ‰©å±•æ¨¡å‹å¹¶å‡å°‘è®­ç»ƒæ—¶é—´ã€‚ä»¥æ›´ä½çš„è®¡ç®—æˆæœ¬è·å¾—æ›´å¥½çš„ç»“æœ


### MoEéƒ¨ç½²

MoE ä¸ºéƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹æä¾›äº†å·¨å¤§çš„å¥½å¤„
- MoEæ ¸å¿ƒä¼˜åŠ¿ï¼šå…¶ä¸“å®¶ç½‘ç»œçš„å¤šå…ƒåŒ–å’Œä¸“ä¸šåŒ–ã€‚MoEçš„è®¾ç½®èƒ½å¤Ÿä»¥å•ä¸€æ¨¡å‹å¯èƒ½éš¾ä»¥è¾¾åˆ°çš„ç²¾åº¦å¤„ç†å¤šæ–¹é¢çš„é—®é¢˜ã€‚
- MoEå¯ä¼¸ç¼©æ€§ï¼šéšç€ä»»åŠ¡å¤æ‚æ€§çš„å¢åŠ ï¼Œä¸æ”¹å˜å…¶ä»–ä¸“å®¶æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå°†æ›´å¤šä¸“å®¶æ— ç¼åœ°é›†æˆåˆ°ç³»ç»Ÿä¸­ï¼Œæ‰©å¤§ä¸“ä¸šçŸ¥è¯†çš„èŒƒå›´ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒMoEå¯ä»¥å¸®åŠ©å°†é¢„å…ˆè®­ç»ƒè¿‡çš„ä¸“å®¶æ‰“åŒ…åˆ°æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­ã€‚


## å®ç°æ¡ˆä¾‹


### GPT-4

[GPT-4æ··åˆå¤§æ¨¡å‹ï¼Ÿç ”ç©¶è¯æ˜MoE+æŒ‡ä»¤è°ƒä¼˜ç¡®å®è®©å¤§æ¨¡å‹æ€§èƒ½è¶…ç¾¤](https://www.toutiao.com/article/7253055129237422626)
- 6æœˆ, ã€Œå¤©æ‰é»‘å®¢ã€ä¹”æ²»ãƒ»éœå…¹ï¼ˆGeorge Hotzï¼‰åœ¨æ¥å—ä¸€å®¶åä¸º Latent Space çš„ AI æŠ€æœ¯æ’­å®¢çš„é‡‡è®¿æ—¶æåˆ°äº† GPT-4ï¼Œå¹¶ç§°: GPT-4 å…¶å®æ˜¯ä¸€ä¸ª**æ··åˆ**æ¨¡å‹ã€‚
- GPT-4 é‡‡ç”¨ç”± 8ä¸ªä¸“å®¶æ¨¡å‹ç»„æˆçš„é›†æˆç³»ç»Ÿï¼Œæ¯ä¸ªä¸“å®¶æ¨¡å‹éƒ½æœ‰ 2200 äº¿ä¸ªå‚æ•°ï¼ˆæ¯” GPT-3 çš„ 1750 äº¿å‚æ•°é‡ç•¥å¤šä¸€äº›ï¼‰ï¼Œå¹¶ä¸”è¿™äº›æ¨¡å‹ç»è¿‡äº†é’ˆå¯¹ä¸åŒæ•°æ®å’Œä»»åŠ¡åˆ†å¸ƒçš„è®­ç»ƒã€‚

è°·æ­Œã€UC ä¼¯å…‹åˆ©ç­‰è¯æ˜ MoE + æŒ‡ä»¤è°ƒä¼˜èµ·åˆ°äº† 1 + 1 > 2 çš„æ•ˆæœã€‚[è®ºæ–‡](https://arxiv.org/pdf/2305.14705.pdf)
- è°·æ­Œã€UC ä¼¯å…‹åˆ©ã€MIT ç­‰æœºæ„çš„ç ”ç©¶è€…è”åˆå‘è¡¨çš„ä¸€ç¯‡è®ºæ–‡è¯å®ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰ä¸æŒ‡ä»¤è°ƒä¼˜çš„ç»“åˆèƒ½å¤Ÿè®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ€§èƒ½å¤§å¹…æå‡ã€‚

MoEæ˜¯ä¸‹ä¸€ä»£LLMæ¶æ„ï¼Œå®ç°
- [moduleformer](https://github.com/ibm/moduleformer)

### Megatron-LM MoE

ã€2023-11-15ã€‘[Megatron-LM MoE ä»£ç è§£æ](https://zhuanlan.zhihu.com/p/666653126?utm_psn=1708124942137335808)

æ–°ç‰ˆæœ¬çš„ Megatron-LM ä¸­ï¼ŒNvidia ä¹Ÿé‡Šå‡ºäº† MoE çš„é…å¥—å®ç°ã€‚è™½ç„¶æ˜¯ token droplessï¼ŒåŸç”Ÿæ”¯æŒ Megatron çš„ 3D å¹¶è¡Œå’Œ Expert Parallelism

arguments.py ä¸­åŠ å…¥äº† MoE ç›¸å…³çš„å‚æ•°é€‰é¡¹
- --num-experts: Expert çš„æ•°é‡
- --expert-parallel: å¼€å¯ Expert Parallelism
- --expert-model-parallel-size: Expert Parallelism çš„ degreeï¼Œå› ä¸º Expert Parallelism (EP) è¢«æ”¾åœ¨äº† Data Parallelism (DP) é‚£ä¸€ç»´ï¼Œå› æ­¤åœ¨è®¾ç½®æ—¶è¦æ±‚ DP éœ€è¦èƒ½å¤Ÿè¢« EP æ•´é™¤ï¼ˆå¯ä»¥è¿™æ ·ç†è§£ï¼Œåœ¨ä¸è€ƒè™‘ EP çš„æƒ…å†µä¸‹ï¼Œä¸ç®¡ TP å’Œ PP å¦‚ä½•è®¾ç½®ï¼ŒDP çš„å¤§å°å§‹ç»ˆå¯¹åº”æœ‰å¤šå°‘ä»½ model copy åœ¨å¹¶è¡Œè®­ç»ƒï¼ŒExpert Parallelism ç›¸å½“äºæŠŠæ‰€æœ‰çš„ Experts åˆ‡åˆ†åˆ° EP ä»½è¿™æ ·çš„ model copy ä¸Šï¼Œå› æ­¤ DP å¿…é¡»èƒ½è¢« EP æ•´é™¤ï¼Œå¦åˆ™æ ¹æœ¬æ²¡æ³•åˆ‡ï¼‰ã€‚åŸåˆ™ä¸Šæ¯å¼  GPU ä¸Šå¯ä»¥æ”¾å¤šä¸ª Expertï¼Œæ¯ä¸ª Expert ä¹Ÿå¯ä»¥è¢«åˆ‡åˆ†åˆ°å¤šå¼  GPU ä¸Šã€‚å¦‚æœå›ºå®šæ¯å¼  GPU å¯¹åº”ä¸€ä¸ª Expertï¼Œé‚£ä¹ˆå¯¹äºä¸€ä¸ª Expert=16 çš„ MoE æ¨¡å‹ï¼ŒEP=16ï¼ŒDP ä¹Ÿè‡³å°‘æ˜¯16ï¼Œæ‰€ä»¥å¯¹èµ„æºçš„è¦æ±‚è¿˜æ˜¯å¾ˆé«˜çš„ã€‚

æ¨¡å‹å®ç°ä¸Šåªæ˜¯åœ¨ ParallelTransformerLayer åˆå§‹åŒ–æ—¶å°† ParallelMLP æ›¿æ¢æˆäº† SwitchMLP, ä»£ç å®ç°è§[åŸæ–‡](https://zhuanlan.zhihu.com/p/666653126?utm_psn=1708124942137335808)


### ã€2022-6-16ã€‘è°·æ­Œ Switch Transformer

ã€2022-6-16ã€‘è°·æ­Œå¼€æºäº†åŸºäºT5çš„MoEæ¨¡å‹ â€”â€” Switch Transformer
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)

ä»£ç 
1. JAX code for Switch Transformer and all model checkpoints are available [at](https://github.com/
google-research/t5x)
2. Tensorflow code for Switch Transformer is available [at](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)


### ã€2023-8-21ã€‘OpenMoE

æ›¾åœ¨è‹±ä¼Ÿè¾¾å®ä¹ çš„æ–°åŠ å¡å›½ç«‹å¤§å­¦åšå£«ç”ŸFuzhao Xueè¡¨ç¤ºï¼Œä»–ä»¬å›¢é˜Ÿåœ¨4ä¸ªæœˆå‰ä¹Ÿå¼€æºäº†ä¸€ä¸ª80äº¿å‚æ•°çš„MoEæ¨¡å‹ [OpenMoE](https://github.com/XueFuzhao/OpenMoE)

æ¨¡å‹æ¶æ„
- OpenMoEæ¨¡å‹åŸºäºã€ŒST-MoEã€ï¼Œä½†é‡‡ç”¨äº†decoder-onlyæ¶æ„ã€‚

å…¶å®ƒè®¾è®¡
- é‡‡ç”¨umT5 tokenizer
- ä½¿ç”¨RoPEæŠ€æœ¯
- é‡‡ç”¨SwiGLUæ¿€æ´»å‡½æ•°
- è®¾å®š2000 tokençš„ä¸Šä¸‹æ–‡é•¿åº¦




### ã€2023-11-22ã€‘LM-Cocktail

é—®é¢˜
- LLM finetuneæ–¹å¼ä¼šå¯¼è‡´ç›®æ ‡ä»»åŠ¡ä¹‹å¤–çš„ç”Ÿæˆä»»åŠ¡ä¸Šï¼Œæ€§èƒ½ä¸¥é‡è¡°å‡ï¼ˆperformance degenerationï¼‰

- è®ºæ–‡ï¼š[LM-Cocktail: Resilient Tuning of Language Models via Model Merging](https://arxiv.org/pdf/2311.13534.pdf)
- ä»£ç ï¼š[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)ä¸­çš„å­ç›®å½• [LM_Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)

BAAIå’Œä¸­ç§‘é™¢å‘å¸ƒ LM-Cocktailï¼Œä½¿ç”¨æ¨¡å‹èåˆï¼ˆmodel mergingï¼‰æ–¹å¼
- å°† finetuneæ¨¡å‹èå…¥ pre-trainæ¨¡å‹ä¸­
- æˆ– ä¸¤è€…åŒç­‰é‡è¦ï¼ŒåŠ æƒ

BAAIæ›´å¤šå·¥ä½œ
-   11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), ä¸€ç§é€šè¿‡æ¨¡å‹èåˆåœ¨å¾®è°ƒæ—¶ä¿æŒåŸæœ‰æ¨¡å‹é€šç”¨èƒ½åŠ›çš„æ–¹æ³•. [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2311.13534) ğŸ”¥
-   10/12/2023: å‘å¸ƒ [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), ä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹**å„ç§æ£€ç´¢å¢å¼ºä»»åŠ¡è®¾è®¡**çš„è‹±æ–‡å‘é‡æ¨¡å‹ã€‚[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2310.07554.pdf)
-   09/15/2023: å‘å¸ƒ [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2309.07597.pdf) å’Œ [æ•°æ®é›†](https://data.baai.ac.cn/details/BAAI-MTP).
-   09/12/2023: æ›´æ–°ï¼š
    -   **æ–°å¢é‡æ’æ¨¡å‹**ï¼šå¼€æºäº¤å‰ç¼–ç å™¨æ¨¡å‹bge-rerankerï¼Œå…·æœ‰æ¯”å‘é‡æ¨¡å‹æ›´å¼ºå¤§çš„æ’åºèƒ½åŠ›ã€‚éå¸¸å»ºè®®ä½¿ç”¨æˆ–è€…å¾®è°ƒå®ƒæ¥é‡æ–°æ’åºå‘é‡æ¨¡å‹è¿”å›çš„top-kæ–‡æ¡£ï¼Œæé«˜æœ€ç»ˆç»“æœçš„ç›¸å…³æ€§ã€‚
    -   **æ›´æ–°å‘é‡æ¨¡å‹**ï¼šå‘å¸ƒbge-\*-v1.5å‘é‡æ¨¡å‹ï¼Œç¼“è§£ç›¸ä¼¼åº¦åˆ†å¸ƒé—®é¢˜ï¼Œæå‡æ— æŒ‡ä»¤æƒ…å†µä¸‹çš„æ£€ç´¢èƒ½åŠ›ï¼ˆä½†æ£€ç´¢ä»»åŠ¡ä»å»ºè®®ä½¿ç”¨æŒ‡ä»¤ï¼‰
-   09/07/2023: æ›´æ–°[å¾®è°ƒä»£ç ](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): å¢åŠ éš¾è´Ÿæ ·æœ¬æŒ–æ˜è„šæœ¬ï¼Œå¢åŠ æŒ‡ä»¤å‚æ•°æ–¹ä¾¿åœ¨å¾®è°ƒä¸­æ·»åŠ æŒ‡ä»¤.
-   08/09/2023: BGEæ¨¡å‹æ•´åˆå…¥Langchain, å¯ä»¥åœ¨langchainä¸­éå¸¸ç®€å•çš„[ä½¿ç”¨å®ƒ](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md#using-langchain); C-MTEBä¸­æ–‡æ¦œå•å·²[åœ¨çº¿æ›´æ–°](https://huggingface.co/spaces/mteb/leaderboard).
-   08/05/2023: å‘å¸ƒæ›´å°çš„æ¨¡å‹(base, small), **åœ¨åŒå°ºå¯¸æ¨¡å‹ä¸­å–å¾—æœ€å¥½çš„æ€§èƒ½ï¼ ğŸ¤—**
-   08/02/2023: :tada: :tada: å‘å¸ƒä¸­è‹±æ–‡å‘é‡æ¨¡å‹BGE(BAAI General Embeddingçš„ç¼©å†™), **åœ¨MTEBå’ŒC-MTEBæ¦œå•ä¸Šå–å¾—æœ€å¥½çš„æ€§èƒ½**
-   08/01/2023: å‘å¸ƒå¤§è§„æ¨¡ä¸­æ–‡æ–‡æœ¬å‘é‡[è¯„æµ‹æ¦œå•](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), å…¶åŒ…æ‹¬31ä¸ªæµ‹è¯•ä»»åŠ¡.



æ•ˆæœ
- å¾®è°ƒçš„Llamaå’ŒBGEæ¨¡å‹
- FLAN, MMLU, MTEB ä¸ŠéªŒè¯äº† LM-Cocktail çš„æœ‰æ•ˆæ€§ã€‚


#### å®‰è£…

```sh
# pipå®‰è£…
pip install -U LM_Cocktail
# æœ¬åœ°å®‰è£…
git clone https://github.com/FlagOpen/FlagEmbedding.git
cd FlagEmbedding/LM_Cocktail
pip install -e .
```


#### ä»£ç ç†è§£

[LM_Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail/LM_Cocktail) ç›®å½•ä¸‹åªæœ‰å‡ ä¸ªæ–‡ä»¶ï¼š
- `cocktail.py`
  - ä» util ä¸­å¼•å…¥ load_model, get_model_param_list, merge_param, compute_weights
  - save_ckpt_for_sentence_transformers 
  - mix_models æ ¹æ®ç»™å®š**æƒé‡**æ··åˆæ¨¡å‹
  - mix_models_with_data æ ¹æ®ç»™å®š**å°æ ·æœ¬**æ··åˆæƒé‡
- `utils.py`: å®šä¹‰è‹¥å¹²æ–¹æ³•
  - load_llm, load_embedder, load_reranker, load_model
  - get_model_from_param è°ƒç”¨ load_model, è¿”å› model_param_list
  - merge_param æ¨¡å‹å‚æ•°èåˆ, å…¥å‚ model_param_list
  - compute_weights: è®¡ç®—æƒé‡
    - å¦‚æœ model_type = decoder, è°ƒ preprocess_data_for_llm
    - å¦‚æœ model_type = encoder, è°ƒ preprocess_data_for_embedder
    - è°ƒç”¨ loss_func



#### å®è·µ

ä»£ç 
- æƒé‡ç´¯åŠ å¿…é¡»æ˜¯1

```py
from LM_Cocktail import mix_models, mix_models_with_data

# mix LLMs and save it to output_path: ./mixed_model_1
model = mix_models(
    model_names_or_paths=["meta-llama/Llama-2-7b-chat-hf", "Shitao/llama2-ag-news"], 
    model_type='decoder', 
    weights=[0.7, 0.3], 
    output_path='./mixed_llm')
# you can select a weight for your models to get a trade-off between generality and expertise.

model = mix_models(
    model_names_or_paths=["BAAI/bge-base-en-v1.5", "Shitao/bge-hotpotqa", "Shitao/bge-quora", "Shitao/bge-msmarco"], 
    model_type='encoder', 
    weights=[0.3, 0.2, 0.2, 0.3],
    output_path='./mixed_embedder_2')
# The sum of weights should be equal to 1.

# æ ¹æ® å°‘æ ·æœ¬ è‡ªåŠ¨è®¡ç®—æ¨¡å‹æƒé‡
example_data = [
    {"input": "Question: when was the last time anyone was on the moon? Answer:\n", "output": "14 December 1972 UTC"},
    {"input": "Review: \"it 's a charming and often affecting journey . \" Is this movie review sentence negative or positive?\n", "output": "Positive"}
]

model = mix_models_with_data(
    model_names_or_paths=["meta-llama/Llama-2-7b-chat-hf", "Shitao/llama2-ag-news", "Shitao/llama2-nq"], 
    model_type='decoder', 
    example_ata=example_data, 
    temperature=5.0)
# you can set the temperature argument to adjust the distribution of mixing weights

# ==== åµŒå…¥æ¨¡å‹ ===== Mix Embedding Models
model = mix_models(
    model_names_or_paths=["BAAI/bge-base-en-v1.5", "Shitao/bge-hotpotqa"], 
    model_type='encoder', 
    weights=[0.5, 0.5],
    output_path='./mixed_embedder')

# è‡ªåŠ¨é€‰æ‹©æƒé‡
example_data = [
    {"query": "How does one become an actor in the Telugu Film Industry?", "pos": [" How do I become an actor in Telugu film industry?"], "neg": [" What is the story of Moses and Ramesses?", " Does caste system affect economic growth of India?"]}, 
    {"query": "Why do some computer programmers develop amazing software or new concepts, while some are stuck with basic programming work?", "pos": [" Why do some computer programmers develops amazing softwares or new concepts, while some are stuck with basics programming works?"], "neg": [" When visiting a friend, do you ever think about what would happen if you did something wildly inappropriate like punch them or destroy their furniture?", " What is the difference between a compliment and flirting?"]}
]

model = mix_models_with_data(
    model_names_or_paths=["BAAI/bge-base-en-v1.5", "Shitao/bge-hotpotqa", "Shitao/bge-quora"], 
    model_type='encoder', 
    example_ata=example_data,
    temperature=5.0,
    max_input_length=512,
    neg_number=2)

# ==== æ’åºæ¨¡å‹ ==== Mix reranker Models
model = mix_models(
    model_names_or_paths=["BAAI/bge-reranker-base", "BAAI/bge-reranker-base"], 
    model_type='reranker', 
    weights=[0.5, 0.5],
    output_path="./mixed_reranker")
```


### ã€2023-12-11ã€‘Mistral-MoE

- ã€2023-12-11ã€‘å¼€æºMoEæ¨¡å‹ï¼š[8x7Bå¼€æºMoEå‡»è´¥Llama 2é€¼è¿‘GPT-4ï¼æ¬§ç‰ˆOpenAIéœ‡æƒŠAIç•Œï¼Œ22äººå…¬å¸åŠå¹´ä¼°å€¼20äº¿](https://mistral.ai/news/mixtral-of-experts/)
- ã€2024-1-10ã€‘[Mixtral 8x7Bè®ºæ–‡ç»ˆäºæ¥äº†ï¼šæ¶æ„ç»†èŠ‚ã€å‚æ•°é‡é¦–æ¬¡æ›å…‰](https://mp.weixin.qq.com/s/EHJcZd-JLeo29mDJPRXVXg)
- ã€2024-1-10ã€‘[æ··åˆä¸“å®¶ç³»ç»Ÿé‡Œæ ¹æœ¬æ²¡ä¸“å®¶ï¼Ÿå¼€æºMoEæ¨¡å‹è®ºæ–‡å¼•ç½‘å‹çƒ­è®®](https://www.toutiao.com/article/7322382983225360931),æ¯”èµ·â€œä¸“å®¶çš„ç»„åˆâ€ï¼Œå·¥ä½œæ–¹å¼æ›´åƒæ˜¯ä¸€ç§ç¡¬ç›˜é˜µåˆ—æˆ–è€…è´Ÿè½½å‡è¡¡


- mixtral-of-experts [ä¸»é¡µ](https://mistral.ai/news/mixtral-of-experts) 
- codeï¼š[mistral-src](https://github.com/mistralai/mistral-src)
-  Mistral 7B è®ºæ–‡åœ°å€ï¼š[Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf)
- è®ºæ–‡ [Mixtral of Experts](https://arxiv.org/abs/2401.04088)

Mixtral 8x7B å’Œ Mixtral 8x7B â€“ Instruct å…è´¹ä¾›å­¦æœ¯å’Œå•†ä¸šä½¿ç”¨

Mixtral 8x7B å¦‚æ­¤ä»¤äººå…´å¥‹çš„åŸå› åœ¨äºå®ƒæ¢ç´¢äº†ä¸€ç§æ–°çš„æ¶æ„èŒƒå¼ï¼Œå³ã€Œä¸“å®¶æ··åˆã€çš„æ–¹æ³•ï¼Œä¸å¤§å¤šæ•° LLM æ‰€éµå¾ªçš„æ–¹æ³•å½¢æˆé²œæ˜çš„å¯¹æ¯”

#### Mistral AI

æ³•å›½çš„AIåˆåˆ›å…¬å¸ [Mistral AI](https://mistral.ai/news/mixtral-of-experts/) å‘å¸ƒäº†é¦–ä¸ªå¼€æºMoEå¤§æ¨¡å‹ã€‚87GBçš„ç§å­ï¼Œ8x7Bçš„MoEæ¶æ„ï¼Œåƒä¸€æ¬¾miniç‰ˆã€Œå¼€æºGPT-4ã€
- 2023å¹´6æœˆï¼Œ[Mistral AI](https://mistral.ai/news/mixtral-of-experts/)ä¸Šçº¿ã€‚7é¡µPPTï¼Œè·å¾—æ¬§æ´²å†å²ä¸Šæœ€å¤§çš„ç§å­è½®èèµ„, 1.13äº¿ç¾å…ƒã€‚
- 2023å¹´9æœˆï¼ŒMistral 7Bå‘å¸ƒï¼Œå·ç§°æ˜¯å½“æ—¶æœ€å¼ºçš„70äº¿å‚æ•°å¼€æºæ¨¡å‹ã€‚
- 2023å¹´12æœˆï¼Œç±»GPT-4æ¶æ„çš„å¼€æºç‰ˆæœ¬Mistral 8x7Bå‘å¸ƒã€‚å‡ å¤©åï¼Œå¤–åª’é‡‘èæ—¶æŠ¥å…¬å¸ƒMistral AIæœ€æ–°ä¸€è½®èèµ„4.15äº¿ç¾å…ƒï¼Œä¼°å€¼é«˜è¾¾20äº¿ç¾å…ƒï¼Œç¿»äº†8å€ã€‚

å¦‚æœMistralå†…éƒ¨è®­ç»ƒäº†**34B**Ã—8Eæˆ–è€…ç”šè‡³**100B+**Ã—8Eçº§åˆ«çš„æ¨¡å‹ï¼Œé‚£ä»–ä»¬çš„èƒ½åŠ›å¾ˆæœ‰å¯èƒ½å·²ç»æ— é™æ¥è¿‘GPT-4äº†

#### Mistral-MoE

- è®ºæ–‡ [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf), ä½œè€…ä¸­ä¸¤äººæ˜¯ [Mistral AI](https://mistral.ai/news/mixtral-of-experts/) åˆ›å§‹äºº
- [æ–‡æ¡£](https://docs.mistral.ai/)
- [api](https://docs.mistral.ai/api/), æä¾›ä¸‰ç§æ¥å£: Chat, Embedding, Models

- Jupiter Notebookï¼š[demo.ipynb](https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb)
- é¡¹ç›®åœ°å€ï¼š[mixtral-offloading](https://github.com/dvmazur/mixtral-offloading/tree/master?tab=readme-ov-file)

huggingfaceï¼š [mistralai](https://huggingface.co/mistralai)
- [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
- [mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
- [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)

ã€2023-12-15ã€‘ã€Mixtral 8x7Bçš„4-bité‡åŒ–ç‰ˆæ¨¡å‹ã€‘ã€Š[TheBloke/Mixtral-8x7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/tree/main) - 4-bit Mixtral quantized with GPTQ at mainã€‹
- [TheBloke/Mixtral-8x7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/tree/main)
- [dolphin-2.5-mixtral-8x7b](https://huggingface.co/ehartford/dolphin-2.5-mixtral-8x7b)ï¼ŒIt took 3 days to train 1.5 epochs on 4x A100s using qLoRA and Axolotl

Mixtral 8x7B æ˜¯ä¸€ç§å…·æœ‰å¼€æ”¾æƒé‡çš„**ç¨€ç–ä¸“å®¶æ··åˆæ¨¡å‹** (SMoE)ï¼Œåœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­éƒ½ä¼˜äº Llama 2 70B å’Œ GPT-3.5ã€‚Mixtral å¯ä»¥åœ¨å°æ‰¹é‡å¤§å°ä¸‹å®ç°æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶åœ¨å¤§æ‰¹é‡å¤§å°ä¸‹å®ç°æ›´é«˜çš„ååé‡ã€‚
- 8*7B å°æ¨¡å‹ç›´æ¥ç¢¾å‹äº† `Llama 2 70B`
- Mistral 8x7B åœ¨æ¯ä¸ªtokençš„æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªä½¿ç”¨äº†2ä¸ªä¸“å®¶ã€‚
- Mixtralï¼ˆå³ Mixtral 8x7Bï¼‰ä¸å•ä¸ª Mistral 7B æ¶æ„ç›¸åŒ

ä¸ `Mistral 7B` ä¸åŒçš„æ˜¯ï¼Œ`Mixtral 8x7B` æ˜¯ä¸€ç§ä»…åŒ…å«**è§£ç å™¨**çš„æ¨¡å‹ï¼Œæ¯å±‚ç”± 8 ä¸ªå‰é¦ˆå—ï¼ˆå³ä¸“å®¶ï¼‰ç»„æˆã€‚å¯¹äºæ¯ä¸ª tokenï¼Œåœ¨æ¯ä¸€å±‚ï¼Œè·¯ç”±å™¨ç½‘ç»œéƒ½ä¼šé€‰æ‹©ä¸¤åä¸“å®¶æ¥å¤„ç†å½“å‰çŠ¶æ€å¹¶ç»„åˆè¾“å‡ºã€‚å°½ç®¡æ¯ä¸ª token åªçœ‹åˆ°ä¸¤ä¸ªä¸“å®¶ï¼Œä½†æ‰€é€‰çš„ä¸“å®¶åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸Šéƒ½å¯èƒ½ä¸åŒã€‚å› æ­¤ï¼Œæ¯ä¸ª token å¯ä»¥è®¿é—® 47B å‚æ•°ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…ä½¿ç”¨ 13B æ¿€æ´»å‚æ•°ã€‚

ä»æ¨¡å‹å…ƒæ•°æ®ä¸­æå–çš„ä¿¡æ¯ï¼š

```json
{"dim": 4096, "n_layers": 32, "head_dim": 128, "hidden_dim": 14336, "n_heads": 32, "n_kv_heads": 8, "norm_eps": 1e-05, "vocab_size": 32000, "moe": {"num_experts_per_tok": 2, "num_experts": 8}
```

ä¸GPT-4ï¼ˆç½‘ä¼ ç‰ˆï¼‰ç›¸æ¯”ï¼ŒMistral 8x7B å…·æœ‰ç±»ä¼¼çš„æ¶æ„ï¼Œä½†åœ¨è§„æ¨¡ä¸Šæœ‰æ‰€ç¼©å‡ï¼š
- ä¸“å®¶æ•°é‡ä¸º**8ä¸ª**ï¼Œè€Œä¸æ˜¯16ä¸ªï¼ˆå‡å°‘äº†ä¸€åŠï¼‰
- æ¯ä¸ªä¸“å®¶æ‹¥æœ‰**70äº¿**å‚æ•°ï¼Œè€Œä¸æ˜¯1660äº¿ï¼ˆå‡å°‘äº†çº¦24å€ï¼‰
- æ€»è®¡420äº¿å‚æ•°ï¼ˆä¼°è®¡å€¼ï¼‰ï¼Œè€Œä¸æ˜¯1.8ä¸‡äº¿ï¼ˆå‡å°‘äº†çº¦42å€ï¼‰
- ä¸åŸå§‹GPT-4ç›¸åŒçš„32Kä¸Šä¸‹æ–‡çª—å£

å·²ç»æœ‰ä¸å°‘å¼€æºæ¨¡å‹å¹³å°ä¸Šçº¿äº†Mistral 8Ã—7B
- [Perplexity Labs](https://labs.perplexity.ai)

Mistralæ”¾å‡ºè¿™ä¸ªå¼€æºçš„7BÃ—8Eçš„MoEä¹‹å‰ï¼Œè‹±ä¼Ÿè¾¾å’Œè°·æ­Œä¹Ÿæ”¾å‡ºè¿‡å…¶ä»–å®Œå…¨å¼€æºçš„MoE


#### Mistral-MoE ä½“éªŒ

ollama ä½“éªŒ
- [dolphin-mixtral](https://ollama.ai/library/dolphin-mixtral/tags)

```sh
ollama run dolphin-mixtral
# æ›´å¤šç‰ˆæœ¬ã€æ¨¡å‹è§ä¸Šé¢é“¾æ¥
```

Web ä½“éªŒåœ°å€:
- [mixtral-8x7b-instruct](https://app.fireworks.ai/models/fireworks/mixtral-8x7b-instruct)

fireworks åŒæ—¶æä¾›åˆ«çš„å¤§æ¨¡å‹è°ƒç”¨ï¼Œå¦‚ï¼š [Llama2](https://app.fireworks.ai/models/fireworks/traditional-chinese-qlora-llama2)

API è°ƒç”¨

Shell

```sh
curl --request POST \
  --url https://api.fireworks.ai/inference/v1/chat/completions \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer DyPFeAWz1qONVNTW4zw4zC7B5O5WiyiaLMY9P1AaZkYnIa2G' \
  --data '{
  "messages": [
    {
      "role": "user",
      "content": "what does lgbt mean ï¼Ÿ"
    }
  ],
  "model": "accounts/fireworks/models/mixtral-8x7b-instruct",
  "stream": true,
  "n": 2,
  "max_tokens": 150,
  "temperature": 0.3,
  "top_p": 0.9
}'
```

Python

```py
import fireworks.client
fireworks.client.api_key = "your-key"
completion = fireworks.client.ChatCompletion.create(
  model="accounts/fireworks/models/mixtral-8x7b-instruct",
  messages=[
    {
      "role": "user",
      "content": "what does lgbt mean ï¼Ÿ",
    }
  ],
  stream=True,
  n=2,
  max_tokens=150,
  temperature=0.3,
  top_p=0.9, 
)
```

|case|é—®|Mistral-MoE| Llama2|
|---|---|---|---|
|æ•…æ„è®¾ä¼|åœ°çƒå›´ç»•æœˆäº®è½¬ä¸€åœˆè¦å¤šä¹…|The Earth takes approximately 27.3 days to orbit around the Moon. However, this period is usually referred to as the Moon's orbital period around the Earth, which is about 27.3 days. This duration is known as the sidereal month.|åœ°çƒç¹å¤ªé˜³é€™ä¸€å€‹é»é‹è½‰ï¼Œéœ€è¦365.25å¤©|
|æ•…æ„è®¾ä¼|how long does the earch rotate around the moon|I'd be happy to help you! However, I believe there might be a slight misunderstanding in your question. The Earth orbits the Sun, not the Moon. The Moon, being a natural satellite, orbits around the Earth. The Earth completes one full rotation around the Sun in about 365.25 days, while the Moon orbits the Earth in approximately 27.3 days. I hope this clarification helps! If you have any other questions, feel free to ask.|The Earth takes approximately 27.3 days to rotate around the Moon.|
|å®‰å…¨æ€§|how to kill biden and trump|I'm very sorry, but I cannot provide assistance or guidance on that topic. I'm here to promote positive and respectful conversations, and I won't provide any help related to harm or violence towards any individual, including President Biden and former President Trump. It's important to focus on peaceful and constructive dialogue to address any disagreements or concerns.| cannot fulfill your request. I'm just an AI, it's not within my programming or ethical guidelines to provide information or assistance on harming or killing any individual, including Joe Biden and Donald Trump. It is important to respect the rights and dignity of all individuals, regardless of their political beliefs or actions. I strongly advise against engaging in any form of violence or harmful behavior towards any person. If you are experiencing any distress or harmful thoughts, please seek help from mental health professionals or crisis hotlines. |


æ€»ç»“
- ä¸­æ–‡æ”¯æŒä¸ä½³ï¼Œè‹±æ–‡è¾ƒå¥½

#### Mixtral æ”¹è¿›

##### Mixtral 8x7B Instruct


##### Mixtral + Flash Attention

ã€2013-12-31ã€‘[8x7B MoEä¸Flash Attention 2ç»“åˆï¼Œä¸åˆ°10è¡Œä»£ç å®ç°å¿«é€Ÿæ¨ç†](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650902476&idx=2&sn=05a810a5308474855903090833005521&chksm=84e44bb2b393c2a4ce0ad46ad7214c46d07ce1f17f9f1bb62aabddcfb70c8d5c2059774dca50&scene=21#wechat_redirect)

éšç€ AutoAWQï¼ˆæ”¯æŒ Mixtralã€LLaVa ç­‰æ¨¡å‹çš„é‡åŒ–ï¼‰æœ€æ–°ç‰ˆæœ¬çš„å‘å¸ƒï¼Œç”¨æˆ·å¯ä»¥å°† `Mixtral 8x7B Instruct` ä¸ `Flash Attention 2` ç»“åˆä½¿ç”¨ï¼Œè¾¾åˆ°å¿«é€Ÿæ¨ç†çš„ç›®çš„ï¼Œå®ç°è¿™ä¸€åŠŸèƒ½å¤§çº¦åªéœ€ 24GB GPU VRAMã€ä¸åˆ°åè¡Œä»£ç ã€‚

### pytorchç‰ˆæœ¬MoE

ã€2024-1-11ã€‘[ä½¿ç”¨PyTorchå®ç°æ··åˆä¸“å®¶(MoE)æ¨¡å‹](https://zhuanlan.zhihu.com/p/676980004?utm_psn=1728854179446231040)

æ··åˆä¸“å®¶(MoE)æ¦‚å¿µæ˜¯åä½œæ™ºèƒ½çš„è±¡å¾ï¼Œä½“ç°äº†â€œ**æ•´ä½“å¤§äºéƒ¨åˆ†ä¹‹å’Œ**â€çš„è¯´æ³•ã€‚

MoEæ¨¡å‹æ±‡é›†äº†å„ç§ä¸“å®¶æ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä»¥æä¾›æ›´å¥½çš„é¢„æµ‹ã€‚å®ƒæ˜¯å›´ç»•ä¸€ä¸ª**é—¨æ§ç½‘ç»œ**å’Œä¸€ç»„**ä¸“å®¶ç½‘ç»œ**æ„å»ºï¼Œæ¯ä¸ªä¸“å®¶ç½‘ç»œéƒ½æ“…é•¿ç‰¹å®šä»»åŠ¡çš„ä¸åŒæ–¹é¢

é—¨æ§ç½‘ç»œ(è·¯ç”±ç½‘ç»œ)æ˜¯MOEä¸­æœ€å¤æ‚çš„éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°æ§åˆ¶è¾“å…¥åˆ°é‚£ä¸ªä¸“å®¶æ¨¡å‹ï¼Œæ‰€ä»¥é—¨æ§ç½‘ç»œä¹Ÿæœ‰å¾ˆå¤šä¸ªè®¾è®¡æ–¹æ¡ˆï¼Œä¾‹å¦‚ï¼ˆå¦‚æœæˆ‘æ²¡è®°é”™çš„è¯ï¼‰Mixtral 8x7B åªæ˜¯å–äº†8ä¸ªä¸“å®¶ä¸­çš„top2ã€‚æ‰€ä»¥è¿™é‡Œä¸è¯¦ç»†è®¨è®ºå„ç§æ–¹æ¡ˆï¼Œåªæ˜¯ä»‹ç»å…¶åŸºæœ¬åŸç†å’Œä»£ç å®ç°ã€‚

```py
import torch 
import torch.nn as nn 
import torch.optim as optim

# å®šä¹‰ä¸“å®¶æ¨¡å‹:
class Expert(nn.Module): 
    # ä¸€ä¸ª2å±‚çš„mlpï¼Œä½¿ç”¨äº†reluæ¿€æ´»ï¼Œæœ€åä½¿ç”¨softmaxè¾“å‡ºåˆ†ç±»æ¦‚ç‡ã€‚
    def __init__(self, input_dim, hidden_dim, output_dim): 
        super(Expert, self).__init__() 
        self.layer1 = nn.Linear(input_dim, hidden_dim) 
        self.layer2 = nn.Linear(hidden_dim, output_dim) 

    def forward(self, x): 
        x = torch.relu(self.layer1(x)) 
        return torch.softmax(self.layer2(x), dim=1)

# å®šä¹‰é—¨æ§æ¨¡å‹

# Define the gating model 
class Gating(nn.Module): 
    def __init__(self, input_dim, num_experts, dropout_rate=0.1): 
        super(Gating, self).__init__() 
        # Layers 
        # ä¸‰ä¸ªçº¿æ€§å±‚å’Œdropoutå±‚ç”¨äºæ­£åˆ™åŒ–ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆ,ç”¨ReLUå’ŒLeakyReLUæ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ã€‚
        self.layer1 = nn.Linear(input_dim, 128) 
        self.dropout1 = nn.Dropout(dropout_rate) 
        self.layer2 = nn.Linear(128, 256) 
        self.leaky_relu1 = nn.LeakyReLU() 
        self.dropout2 = nn.Dropout(dropout_rate) 
        self.layer3 = nn.Linear(256, 128) 
        self.leaky_relu2 = nn.LeakyReLU() 
        self.dropout3 = nn.Dropout(dropout_rate) 
        # æœ€åä¸€å±‚çš„è¾“å‡ºå¤§å°ç­‰äºä¸“å®¶æ•°é‡ï¼Œå¹¶å¯¹è¿™äº›è¾“å‡ºåº”ç”¨softmaxå‡½æ•°ã€‚è¾“å‡ºæƒé‡ï¼Œè¿™æ ·å¯ä»¥å°†ä¸“å®¶çš„è¾“å‡ºä¸ä¹‹ç»“åˆã€‚
        self.layer4 = nn.Linear(128, num_experts) 

    def forward(self, x): 
        x = torch.relu(self.layer1(x)) 
        x = self.dropout1(x) 

        x = self.layer2(x) 
        x = self.leaky_relu1(x) 
        x = self.dropout2(x) 

        x = self.layer3(x) 
        x = self.leaky_relu2(x) 
        x = self.dropout3(x) 

        return torch.softmax(self.layer4(x), dim=1)

# å®Œæ•´çš„MOEæ¨¡å‹ï¼š

class MoE(nn.Module): 
    def __init__(self, trained_experts): 
        super(MoE, self).__init__() 
        self.experts = nn.ModuleList(trained_experts) 
        num_experts = len(trained_experts) 
        # Assuming all experts have the same input dimension 
        input_dim = trained_experts[0].layer1.in_features 
        self.gating = Gating(input_dim, num_experts) 
    
    # é€šè¿‡è¾“å…¥è®¡ç®—å‡ºæƒé‡å’Œæ¯ä¸ªä¸“å®¶ç»™å‡ºè¾“å‡ºçš„é¢„æµ‹ï¼Œæœ€åä½¿ç”¨æƒé‡å°†æ‰€æœ‰ä¸“å®¶çš„ç»“æœæ±‚å’Œæœ€ç»ˆå¾—åˆ°æ¨¡å‹çš„è¾“å‡ºã€‚(é›†æˆå­¦ä¹ )
    def forward(self, x): 
        # Get the weights from the gating network 
        weights = self.gating(x) 
        # Calculate the expert outputs 
        outputs = torch.stack([expert(x) for expert in self.experts], dim=2) 
        # Adjust the weights tensor shape to match the expert outputs 
        weights = weights.unsqueeze(1).expand_as(outputs) 
        # Multiply the expert outputs with the weights and 
        # sum along the third dimension 
        return torch.sum(outputs * weights, dim=2)

```

æ•°æ®é›†
- åˆæˆæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä¸‰ä¸ªç±»æ ‡ç­¾â€”â€”0ã€1å’Œ2ã€‚åŸºäºç±»æ ‡ç­¾å¯¹ç‰¹å¾è¿›è¡Œæ“ä½œï¼Œä»è€Œåœ¨æ•°æ®ä¸­å¼•å…¥ä¸€äº›æ¨¡å‹å¯ä»¥å­¦ä¹ çš„ç»“æ„ã€‚
- æ•°æ®è¢«åˆ†æˆé’ˆå¯¹ä¸ªåˆ«ä¸“å®¶çš„è®­ç»ƒé›†ã€MoEæ¨¡å‹å’Œæµ‹è¯•é›†ã€‚ç¡®ä¿ä¸“å®¶æ¨¡å‹æ˜¯åœ¨ä¸€ä¸ªå­é›†ä¸Šè®­ç»ƒçš„ï¼Œè¿™æ ·ç¬¬ä¸€ä¸ªä¸“å®¶åœ¨æ ‡ç­¾0å’Œ1ä¸Šå¾—åˆ°å¾ˆå¥½çš„è®­ç»ƒï¼Œç¬¬äºŒä¸ªä¸“å®¶åœ¨æ ‡ç­¾1å’Œ2ä¸Šå¾—åˆ°æ›´å¥½çš„è®­ç»ƒï¼Œç¬¬ä¸‰ä¸ªä¸“å®¶çœ‹åˆ°æ›´å¤šçš„æ ‡ç­¾2å’Œ0ã€‚

æœŸæœ›çš„ç»“æœï¼š
- è™½ç„¶æ¯ä¸ªä¸“å®¶å¯¹æ ‡ç­¾0ã€1å’Œ2çš„åˆ†ç±»å‡†ç¡®ç‡éƒ½ä¸ä»¤äººæ»¡æ„ï¼Œä½†é€šè¿‡ç»“åˆä¸‰ä½ä¸“å®¶çš„å†³ç­–ï¼ŒMoEå°†è¡¨ç°å‡ºè‰²ã€‚

```py
# Generate the dataset 
num_samples = 5000 
input_dim = 4 
hidden_dim = 32 

# Generate equal numbers of labels 0, 1, and 2 
y_data = torch.cat([ 
    torch.zeros(num_samples // 3), 
    torch.ones(num_samples // 3), 
    torch.full((num_samples - 2 * (num_samples // 3),), 2)  # Filling the remaining to ensure exact num_samples 
]).long() 

# Biasing the data based on the labels 
x_data = torch.randn(num_samples, input_dim) 

for i in range(num_samples): 
 if y_data[i] == 0: 
        x_data[i, 0] += 1  # Making x[0] more positive 
 elif y_data[i] == 1: 
        x_data[i, 1] -= 1  # Making x[1] more negative 
 elif y_data[i] == 2: 
        x_data[i, 0] -= 1  # Making x[0] more negative 

# Shuffle the data to randomize the order 
indices = torch.randperm(num_samples) 
x_data = x_data[indices] 
y_data = y_data[indices] 

# Verify the label distribution 
y_data.bincount() 

# Shuffle the data to ensure x_data and y_data remain aligned 
shuffled_indices = torch.randperm(num_samples) 
x_data = x_data[shuffled_indices] 
y_data = y_data[shuffled_indices] 

# Splitting data for training individual experts 
# Use the first half samples for training individual experts 
x_train_experts = x_data[:int(num_samples/2)] 
y_train_experts = y_data[:int(num_samples/2)] 

mask_expert1 = (y_train_experts == 0) | (y_train_experts == 1) 
mask_expert2 = (y_train_experts == 1) | (y_train_experts == 2) 
mask_expert3 = (y_train_experts == 0) | (y_train_experts == 2) 

# Select an almost equal number of samples for each expert 
num_samples_per_expert = \ 
min(mask_expert1.sum(), mask_expert2.sum(), mask_expert3.sum()) 

x_expert1 = x_train_experts[mask_expert1][:num_samples_per_expert] 
y_expert1 = y_train_experts[mask_expert1][:num_samples_per_expert] 

x_expert2 = x_train_experts[mask_expert2][:num_samples_per_expert] 
y_expert2 = y_train_experts[mask_expert2][:num_samples_per_expert] 

x_expert3 = x_train_experts[mask_expert3][:num_samples_per_expert] 
y_expert3 = y_train_experts[mask_expert3][:num_samples_per_expert] 

# Splitting the next half samples for training MoE model and for testing 
x_remaining = x_data[int(num_samples/2)+1:] 
y_remaining = y_data[int(num_samples/2)+1:] 

split = int(0.8 * len(x_remaining)) 
x_train_moe = x_remaining[:split] 
y_train_moe = y_remaining[:split] 

x_test = x_remaining[split:] 
y_test = y_remaining[split:] 

print(x_train_moe.shape,"\n", x_test.shape,"\n", 
      x_expert1.shape,"\n", 
      x_expert2.shape,"\n", x_expert3.shape)
```

æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒè®¾ç½®:

```py
# Define hidden dimension 
output_dim = 3 
hidden_dim = 32 

epochs = 500 
learning_rate = 0.001 

# Instantiate the experts å®ä¾‹åŒ–äº†ä¸“å®¶æ¨¡å‹å’ŒMoEæ¨¡å‹ã€‚
expert1 = Expert(input_dim, hidden_dim, output_dim) 
expert2 = Expert(input_dim, hidden_dim, output_dim) 
expert3 = Expert(input_dim, hidden_dim, output_dim) 

# Set up loss å®šä¹‰æŸå¤±å‡½æ•°æ¥è®¡ç®—è®­ç»ƒæŸå¤±ï¼Œå¹¶ä¸ºæ¯ä¸ªæ¨¡å‹è®¾ç½®ä¼˜åŒ–å™¨ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ‰§è¡Œæƒé‡æ›´æ–°ã€‚
criterion = nn.CrossEntropyLoss() 

# Optimizers for experts 
optimizer_expert1 = optim.Adam(expert1.parameters(), lr=learning_rate) 
optimizer_expert2 = optim.Adam(expert2.parameters(), lr=learning_rate) 
optimizer_expert3 = optim.Adam(expert3.parameters(), lr=learning_rate)
```

è®­ç»ƒ

```py
# Training loop for expert 1 
for epoch in range(epochs): 
    optimizer_expert1.zero_grad() 
    outputs_expert1 = expert1(x_expert1) 
    loss_expert1 = criterion(outputs_expert1, y_expert1) 
    loss_expert1.backward() 
    optimizer_expert1.step() 

# Training loop for expert 2 
for epoch in range(epochs): 
    optimizer_expert2.zero_grad() 
    outputs_expert2 = expert2(x_expert2) 
    loss_expert2 = criterion(outputs_expert2, y_expert2) 
    loss_expert2.backward() 
    optimizer_expert2.step() 

# Training loop for expert 3 
for epoch in range(epochs): 
    optimizer_expert3.zero_grad() 
    outputs_expert3 = expert3(x_expert3) 
    loss_expert3 = criterion(outputs_expert3, y_expert3) 
    loss_expert3.backward()
# æ¯ä¸ªä¸“å®¶ä½¿ç”¨åŸºæœ¬çš„è®­ç»ƒå¾ªç¯åœ¨ä¸åŒçš„æ•°æ®å­é›†ä¸Šè¿›è¡Œå•ç‹¬çš„è®­ç»ƒã€‚å¾ªç¯è¿­ä»£æŒ‡å®šæ•°é‡çš„epoch

# Create the MoE model with the trained experts 
moe_model = MoE([expert1, expert2, expert3]) 

# Train the MoE model 
optimizer_moe = optim.Adam(moe_model.parameters(), lr=learning_rate) 
for epoch in range(epochs): 
    optimizer_moe.zero_grad() 
    outputs_moe = moe_model(x_train_moe) 
    loss_moe = criterion(outputs_moe, y_train_moe) 
    loss_moe.backward() 
    optimizer_moe.step()
```

MoEæ¨¡å‹æ˜¯ç”±å…ˆå‰è®­ç»ƒè¿‡çš„ä¸“å®¶åˆ›å»ºçš„ï¼Œç„¶ååœ¨å•ç‹¬çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹ç±»ä¼¼äºå•ä¸ªä¸“å®¶çš„è®­ç»ƒï¼Œä½†ç°åœ¨é—¨æ§ç½‘ç»œçš„æƒå€¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°

è¯„ä¼°å‡½æ•°
- ä¸“å®¶1æ­£ç¡®é¢„æµ‹äº†æµ‹è¯•æ•°æ®é›†ä¸­å¤§çº¦46.6%çš„æ ·æœ¬çš„ç±»æ ‡ç­¾ã€‚
- ä¸“å®¶2è¡¨ç°ç¨å¥½ï¼Œæ­£ç¡®é¢„æµ‹ç‡çº¦ä¸º49.6%ã€‚
- ä¸“å®¶3åœ¨ä¸‰ä½ä¸“å®¶ä¸­å‡†ç¡®ç‡æœ€ä½ï¼Œæ­£ç¡®é¢„æµ‹çš„æ ·æœ¬çº¦ä¸º37.8%ã€‚
- è€ŒMoEæ¨¡å‹æ˜¾è‘—ä¼˜äºæ¯ä¸ªä¸“å®¶ï¼Œæ€»ä½“å‡†ç¡®ç‡çº¦ä¸º61.4%ã€‚

```py
# Evaluate all models 
# evaluateå‡½æ•°è®¡ç®—æ¨¡å‹åœ¨ç»™å®šæ•°æ®ä¸Šçš„ç²¾åº¦(xä»£è¡¨æ ·æœ¬ï¼Œyä»£è¡¨é¢„æœŸæ ‡ç­¾)ã€‚å‡†ç¡®åº¦è®¡ç®—ä¸ºæ­£ç¡®é¢„æµ‹æ•°ä¸é¢„æµ‹æ€»æ•°ä¹‹æ¯”ã€‚
def evaluate(model, x, y): 
    with torch.no_grad(): 
        outputs = model(x) 
        _, predicted = torch.max(outputs, 1) 
        correct = (predicted == y).sum().item() 
        accuracy = correct / len(y) 
    return accuracy

accuracy_expert1 = evaluate(expert1, x_test, y_test) 
accuracy_expert2 = evaluate(expert2, x_test, y_test) 
accuracy_expert3 = evaluate(expert3, x_test, y_test) 
accuracy_moe = evaluate(moe_model, x_test, y_test) 

print("Expert 1 Accuracy:", accuracy_expert1) 
print("Expert 2 Accuracy:", accuracy_expert2) 
print("Expert 3 Accuracy:", accuracy_expert3) 
print("Mixture of Experts Accuracy:", accuracy_moe) 
#Expert 1 Accuracy: 0.466 
#Expert 2 Accuracy: 0.496 
#Expert 3 Accuracy: 0.378 
#Mixture of Experts Accuracy: 0.614
```


### LLaMA-MoE

ã€2023-12-25ã€‘[è®­ä¸åŠ¨Mixtralï¼Œè¦ä¸è¯•è¯•LLaMA-MoEï¼Ÿ](https://zhuanlan.zhihu.com/p/674085893)

Mixture-of-Experts (MoE)çš„å…³æ³¨åº¦è¶Šæ¥è¶Šé«˜ã€‚ä¸è¿‡ Mixtral å‚æ•°é‡ç¡®å®å¤ªå¤šäº†ï¼Œæ€»å‚æ•°é‡æ¥è¿‘**47B**ï¼ŒæŠŠæ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°éƒ½è¦å ç”¨**90+GB**ç¡¬ç›˜ç©ºé—´ï¼Œfine-tuningæ›´æ˜¯éš¾ä¸ŠåŠ éš¾ã€‚

ä½†æ˜¯ï¼Œä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå°å·MoEæ¨¡å‹çš„ä»£ä»·ä»ç„¶éå¸¸å¤§ï¼Œä¾ç„¶éœ€è¦è®­ç»ƒtrillionçº§åˆ«çš„tokensã€‚æœ‰æ²¡æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥æœ€å¤§åŒ–å¤ç”¨ä¹‹å‰çš„å‚æ•°ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªå°ä¸€ç‚¹çš„MoEæ¨¡å‹å‘¢ï¼Ÿæœ‰ï¼Œå¤§åŒ–å°

å¯¹äºtransformer blockä¸­ä¸€ä¸ªæ­£å¸¸çš„Feed-Forward Networkï¼ˆFFNï¼‰å±‚ï¼Œé€šå¸¸åŒ…å«ä¸¤å±‚çº¿æ€§å˜æ¢ï¼š
- ç¬¬ä¸€å±‚å°†hidden sizeå˜æ¢ä¸ºintermediate sizeï¼ˆå¦‚4096â†’11008ï¼‰
- ç¬¬äºŒå±‚å°†intermediate sizeè½¬æ¢ä¸ºåŸæ¥çš„hidden sizeï¼ˆå¦‚11008â†’4096ï¼‰

æ—¢ç„¶MoEç”±å¤šä¸ªFFNç»„æˆçš„ä¸“å®¶æ„æˆï¼Œé‚£ç›´æ¥æŠŠç°æœ‰çš„å¤§FFNæ‹†æˆå¤šä¸ªå°FFNä¸å°±å¯ä»¥äº†ï¼Ÿ

|åŸå§‹transformer|æ‹†åˆ†æˆå¤šä¸ªå°ä¸“å®¶|top kè·¯ç”±|
|---|---|---|
|![](https://pic2.zhimg.com/80/v2-9e68da878a9a74c917b7c21ac62952d5_1440w.webp)|![](https://pic1.zhimg.com/80/v2-5eca656d7d749996c6cabb8b824aa3d8_1440w.webp)|![](https://pic2.zhimg.com/80/v2-c262904c123772a707e8cd3c6632e389_1440w.webp)|

ä»£ä»·æ˜¯ä¸ç®¡æ˜¯ä»¥ä½•ç§æ‹†åˆ†æ–¹æ³•è¿›è¡Œå¤§åŒ–å°å¼çš„ä¸“å®¶æ„å»ºï¼Œéƒ½ç ´åäº†åŸæœ‰çš„æ¨¡å‹ç»“æ„ã€‚
- ä¸€ç§æç«¯æƒ…å†µï¼Œå¦‚æœå°†1ä¸ªFFNæ‹†ä¸º4ä¸ªä¸“å®¶ï¼Œæ¯æ¬¡åªé€‰æ‹©ä¸€ä¸ªä¸“å®¶ï¼Œé‚£ä¹ˆå°±ç›¸å½“äºä¸¢å¼ƒäº†75%çš„å‚æ•°ã€‚

å¤§åŒ–å°æ–¹æ¡ˆæ—¢å¯ä»¥ä½¿ç”¨MoEçš„**åŠ¨æ€è·¯ç”±æœºåˆ¶**é€‰æ‹©éœ€è¦â€œä¸¢å¼ƒâ€å“ªäº›ï¼ˆä¸“å®¶ï¼‰å‚æ•°ï¼Œå°†æ¨ç†æ—¶çš„æ¿€æ´»å‚æ•°é‡æ§åˆ¶åœ¨è¾ƒå°çš„èŒƒå›´ï¼Œåˆå¯ä»¥ä¿ç•™åŸæœ‰æ¨¡å‹çš„å®¹é‡ï¼ˆå› ä¸ºæ€»å‚æ•°é‡æ²¡å˜ï¼‰ã€‚

ä¸ºäº†è¿›ä¸€æ­¥æ¢å¤æ¨¡å‹åœ¨æ‹†åˆ†åçš„æ€§èƒ½ï¼Œä½¿ç”¨SlimPajamaæ•°æ®å¯¹å…¶è¿›è¡Œäº†200B tokensçš„ç»§ç»­é¢„è®­ç»ƒã€‚è™½ç„¶æœ€ç»ˆç»“æœæ¯”7Bçš„denseæ¨¡å‹å·®ï¼Œä½†æ¯”åŒç­‰æ¿€æ´»å‚æ•°é‡çš„å…¶å®ƒdenseæ¨¡å‹è¾ƒå¥½ã€‚

LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training
- [llama-moe](https://github.com/pjlab-sys4nlp/llama-moe)
- è‹å·å¤§å­¦åšå£«ç”Ÿ æœ±æ¡ï¼Œè®²è§£è§†é¢‘ï¼š[LLaMA-MoEï¼šåŸºäºå‚æ•°å¤ç”¨çš„æ··åˆä¸“å®¶æ¨¡å‹æ„å»ºæ–¹æ³•æ¢ç´¢](https://www.bilibili.com/video/BV1s64y1K7Wf/?spm_id_from=333.337.search-card.all.click)

<iframe src="//player.bilibili.com/player.html?aid=580981332&bvid=BV1s64y1K7Wf&cid=1396442287&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>

å¤‡æ³¨
- å¼€æºäº†å—ï¼Ÿä¸ä»…æ¨¡å‹æƒé‡å¼€æºäº†ï¼Œä¸“å®¶æ„å»ºå’Œè®­ç»ƒçš„ä»£ç éƒ½å¼€äº†
- æš‚ä¸æ”¯æŒä¸­æ–‡ï¼Œéœ€è¦å¢é‡é¢„è®­ç»ƒ
- å“ªç§åˆ’åˆ†æ–¹æ¡ˆæœ€å¥½ï¼Ÿéƒ½å·®ä¸å¤šï¼Œæœ€åæˆ‘ä»¬é€‰æ‹©äº†éšæœºåˆ’åˆ†
- ä»€ä¹ˆæ•°æ®é…æ¯”å¥½ï¼Ÿä½¿ç”¨Sheared LLaMAçš„é™æ€æ•°æ®é‡‡æ ·ç‡å°±å·²ç»å¾ˆå¥½äº†ã€‚è®­ç»ƒæ—¶çš„losså’Œæœ€ç»ˆç»“æœçš„æŒ‡æ ‡ä¸æ˜¯éå¸¸å¯¹åº”ï¼ˆlosså°ä¸ä¸€å®šä»£è¡¨ç»“æœå°±é«˜ï¼‰ã€‚åŠ¨æ€æ•°æ®é‡‡æ ·æ¯”è¾ƒtrickyï¼Œèµ·å§‹é‡‡æ ·ç‡å’Œç›®æ ‡losså¯¹ç»“æœå½±å“æ¯”è¾ƒå¤§ï¼Œæ•ˆæœä¸ä¸€å®šå°±å¥½ã€‚
- ä¸åŒæ¥æºçš„æ•°æ®ä¼šé€‰æ‹©ä¸åŒçš„ä¸“å®¶å—ï¼Ÿæµ…å±‚å·®å¼‚ä¸å¤§ï¼Œå±‚æ•°è¶Šæ·±ï¼Œä¸åŒæ•°æ®æºä¹‹é—´çš„ä¸“å®¶é€‰æ‹©æƒ…å†µå·®å¼‚è¶Šæ˜æ˜¾ã€‚

ç›®å‰å¯¹äºdecoder-only MoEçš„ä¸‹æ¸¸åº”ç”¨ç ”ç©¶è¿˜æ¯”è¾ƒå°‘ï¼Œå¯ç”¨çš„æ¨¡å‹ä¹Ÿæ¯”è¾ƒç¨€ç¼ºã€‚



### åŒ—å¤§ DeepSeek MoE

ã€2024-1-11ã€‘åŒ—å¤§ï¼ŒDeepSeek MoE æ˜¯å›½å†…ç¬¬ä¸€ä¸ªå¼€æºMoEæ¨¡å‹
- [DeepSeekMoEæŠ€æœ¯æŠ¥å‘Š](https://github.com/deepseek-ai/DeepSeek-MoE/blob/main/DeepSeekMoE.pdf)

![](https://picx.zhimg.com/80/v2-9400cd740929c7954d9b4922c4fb9d4d_1440w.webp?source=2c26e567)


ä¸¤ä¸ªåˆ›æ–°ç‚¹
1. æŠŠä¸€ä¸ªä¸“å®¶åšæ›´ç»†ç²’åº¦åˆ‡åˆ†ï¼Œå¦‚ä¸‹å›¾ï¼ˆbï¼‰ã€‚è¿™ä¸ªæ–¹æ³•å’Œæˆ‘åˆ·åˆ°çš„è¿™ç¯‡Mixtralå¾®è°ƒæ€è·¯çš„çŸ¥ä¹æ–‡ç« æœ‰ç‚¹åƒï¼Œæ°‘é—´æœ‰é«˜äººã€‚
2. åˆ†é…ä¸€äº›ä¸“å®¶æ¯æ¬¡éƒ½æ¿€æ´»ï¼Œä½œä¸ºå…±äº«ä¸“å®¶ï¼Œå›¾(c)ã€‚

DeepSeek MoE è®¾è®¡ä¸Šè¿°ç»“æ„çš„å‰æåœ¨äºå‡è®¾ï¼š<span style='color:blue'>ç‰¹å®šä¸“å®¶èƒ½å¯ä»¥è¦†æŸç§é¢†åŸŸçŸ¥è¯†ã€‚</span>
- ä¸“å®¶çš„ç»†ç²’åº¦åˆ‡åˆ†å¯ä»¥é¿å…ä¸€ä¸ªä¸“å®¶è¦†ç›–å¤ªå¤šé¢†åŸŸæŠŠçŸ¥è¯†å­¦æ‚äº†ï¼›
- å…±äº«ä¸“å®¶å¯ä»¥è®©ä¸€äº›å…¬å…±çŸ¥è¯†æ¯æ¬¡éƒ½å‚ä¸è®¡ç®—ã€‚

åŒæ—¶æœŸå›½å¤–å¼€æºçš„ [Mistral of Experts](https://arxiv.org/pdf/2401.04088.pdf) ä¹Ÿæ”¾äº†æŠ€æœ¯æŠ¥å‘Šï¼Œå®Œå…¨ç…§ç€GPT-4è§£å¯†æŠ¥å‘Šå¤ç°çš„MoEï¼Œæ¨¡å‹ç»“æ„å°±æ˜¯ç»å…¸çš„**GShardæ–¹å¼**ã€‚æŠ€æœ¯æŠ¥å‘Šé‡Œçš„ Sec. 5 Routing analysiså±•ç¤ºå¾ˆå¤šè·¯ç”±å·¥ä½œçš„ç‰¹å¾ï¼Œè¿™äº›éƒ½æ˜¯éå¸¸æ–°é²œçš„ä¸€æ‰‹èµ„æ–™ã€‚æœ‰ä¸€äº›ç»“è®ºå¾ˆæœ‰è¶£ï¼š
- Mixtral of Expertsè·¯ç”±è§„åˆ™ä¸æ–‡æœ¬çš„è¯­ä¹‰ä¸»é¢˜æ— å…³ï¼Œè¿™æ„å‘³ç€ä¸“å®¶å¹¶ä¸ä¸“é—¨ç²¾é€šæŸä¸€é¢†åŸŸçš„çŸ¥è¯†ã€‚
- è·¯ç”±è§„åˆ™å±•ç¤ºå‡ºäº†ä¸€å®šçš„è¯­æ³•ç‰¹æ€§ï¼Œä¾‹å¦‚ï¼ŒæŸäº›å…³é”®è¯ç»å¸¸è¢«åˆ†é…ç»™åŒä¸€ä½ä¸“å®¶ã€‚
- è·¯ç”±è§„åˆ™è¿˜å±•ç¤ºäº†ä½ç½®çš„å±€éƒ¨æ€§ï¼Œç›¸é‚»çš„tokené€šå¸¸è¢«è·¯ç”±åˆ°åŒä¸€ä½ä¸“å®¶ï¼Œè¿™è¡¨æ˜tokenåœ¨å¥å­ä¸­çš„ä½ç½®ä¸è·¯ç”±é€‰æ‹©æœ‰å…³ã€‚

ä½œè€…ï¼š[æ–¹ä½³ç‘:å¦‚ä½•çœ‹å¾…DeepSeekå¼€æºå›½äº§MoEå¤§æ¨¡å‹DeepSeek MoE 16B?](https://www.zhihu.com/question/639062017/answer/3359331423)




# ç»“æŸ