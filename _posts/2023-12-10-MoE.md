---
layout: post
title:  混合专家模型（MoE）专题
date:   2023-11-10 16:52:00
categories: 大模型
tags: gpt moe 专家 llama transformer pytorch mistral mixtral 攻击 小米 罗福莉 北大
excerpt: 混合专家模型，MoE系列
mathjax: true
permalink: /moe
---

* content
{:toc}


# MoE 混合专家模型


## 为什么会出现 MoE

大语言模型（LLM）竞逐中，`扩展定律`（Scaling Law）曾被奉为圭臬：模型参数量越大，性能越强。

然而，这条定律的B面是日益增长的计算成本。

传统“稠密”（Dense）模型架构中，参数量的增加与每次前向传播的计算量（FLOPs）呈**线性**正相关，训练和推理成本随着模型规模的扩大而急剧攀升。

为了打破这一桎梏，学术界和工业界都聚焦到更具计算效率的范式——`混合专家模型`（Mixture of Experts, MoE），成功将模型总参数量与单次推理的计算量解耦，成为构建前沿（SOTA）大模型的关键技术。


## 什么是 MoE

MoE 是一种神经网络架构设计，Transformer模块中集成了专家/模型层。

**专家混合模型**（`MoE`）把复杂任务分割成一系列更小、更容易处理的**子任务**，每个子任务由一个特定领域的「专家」负责。

当数据流经`MoE`层时，**每个token**都会**动态**路由到**专家子模型**进行处理。每个专家专门从事特定任务，这种方法可以实现更高效的计算并获得更好的结果。
- (1) 将预测问题划分为**子任务**（采用领域知识或者无监督聚类算法）。
- (2) 针对每个数据子集训练**专家模型**（Expert Models），专家模型可以是**任何**模型，比如: 支持向量机 （SVM） 或 神经网络，每个专家模型接收相同的输入模式并进行预测。
  - MoE 还包含**门控模型**（Gating Model），用于解释每个专家做出的预测，并根据输入选择信任哪个专家。
- (3) MoE需要一种**聚合机制**（Pooling Method），根据门控模型和专家输出进行预测。

原始MoE迭代:「稀疏门控专家混合层」方法提供一个通用的神经网络组件，可以适应不同类型的任务。

混合专家模型在许多领域都有应用，包括推荐系统、语言建模和各种复杂的预测任务。

【2025-3-19】[混合专家系统（MoE）图解指南](https://mp.weixin.qq.com/s/9B_F6xbrWEMTXrtccjTsiw)
- 原文：[a-visual-guide-to-mixture-of-experts](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

【2025-5-13】斯坦福CS336第四课：详解MOE架构
- 混合专家模型（Mixture of Experts, MoE）[讲解视频](https://www.xiaohongshu.com/explore/6821461b0000000012006472)

transformer 单独的FFN 被替换为多个（复制或分割）较小的FFN副本，称为“专家”。同时引入一个“路由器”（router）或“选择器”（selector）层。

每次前向传播或推理过程中，路由器会根据输入选择激活一小部分（例如，一个或几个）专家进行计算。

好处
- 如果每个专家的大小与原始密集模型的FFN相同，并且每次只激活一个专家，那么模型的总参数量可以显著增加，而计算量（flops）却保持不变。

### 行业布局

MoE大模型布局及发布情况

<img width="574" height="575" alt="image" src="https://github.com/user-attachments/assets/489d3f9d-bcc9-4a4a-aec2-56b0c3945523" />

【2024-11-4】[大模型新趋势之MoE：现状、挑战及研究方向](https://www.c114.com.cn/ai/5339/a1276942.html)


### MoE 发展历史

MoE 思想最早追溯至上世纪90年代，但在大模型时代焕发了新生机。

核心思想是 **条件计算**（Conditional Computation）：
- 模型并非**所有**参数都需要处理每个输入，而是根据输入内容**动态**选择小部分参数子集（即“专家”）来参与计算。

【2025-2-11】[关于 MoE 大模型负载均衡策略演进的回顾：坑点与经验教训](https://www.cnblogs.com/ExMan/p/18709928)

从 GShard 到 DeepSeek-V3， MoE 中负载均衡 已经从一个“小问题”变成了整个架构设计中至关重要一环。
- GShard 引领了 top-2 gating 和容量约束；
- Switch Transformer 用 top-1 gating 打开了更大规模的可能性，证明了简单路由也能支撑大规模；
- GLaM 指出训练能耗可以大幅下降，强调能效；
- DeepSpeed-MoE 则深入挖掘了训练和推理层面的负载平衡，兼顾训练和推理；
- ST-MoE 用 z-loss 改善了稳定性；
- Mixtral 强调专家分配的时间局部性；
- OpenMoE 暴露了末端 token 掉队等问题；
- JetMoE 尝试 dropless；
- DeepSeekMoE 做了细粒度拆分和共享专家；
- DeepSeek-V3 又带来了更“轻量级”的偏置调节策略，依靠偏置更新替代沉重的辅助损失。

核心教训：
- 想要完美的负载均衡几乎是不可能——做得过火，语言模型主任务会受损；不做又浪费资源。
- 未来的研究可能还会借助更多 HPC 技巧，也会出现更自动化、更自适应的 gating 机制，帮助我们在训练和推理阶段都实现高效、均衡的专家分配。

【2024-11-4】[大模型新趋势之MoE：现状、挑战及研究方向](https://www.c114.com.cn/ai/5339/a1276942.html)

<img width="691" height="271" alt="image" src="https://github.com/user-attachments/assets/160106f4-99f1-49fd-ad61-25f9e427bbe0" />


MoE 发展阶段：
- (1) 早期探索与挑战：2017年，Google Noam Shazeer等人在论文《Outrageously Large Neural Networks》中首次尝试将MoE思想引入深度学习，`GShard` 架构路由通常采用 top-2 的 gating 方式
  - top-2 gating 在规模很大时会带来不小的通信和计算开销，再加上依赖辅助损失有时会让路由分布变得“人为”偏均匀，反而牺牲了一定的性能。不过，GShard 为后续所有的 MoE 研究铺好了路：它证明了稀疏专家是有价值的，还为后来的方案指明了“容量因子”这些关键概念的重要性
  - 关键挑战：**负载均衡**（Load Balancing）：如果门控网络存在偏好，总将大部分Token路由到少数几个“热门”专家，会导致“**专家饥饿**”现象，即部分专家得不到充分训练，模型性能会因此受损。为此，引入辅助损失函数（Auxiliary Loss），借助惩罚不均衡的专家分配，鼓励门控网络将计算任务均匀地分发给所有专家。
- (2) 里程碑突破：`Switch Transformer`：2021年，Google发布Switch Transformer, 标志着MoE架构在Transformer模型上的成熟应用。
  - 该研究将MoE设计进一步简化和稳定，采用了**Top-1路由**策略，每个Token只由一个专家处理。通过一系列工程优化和对辅助损失的精巧设计，研究团队成功训练出了参数量高达1.6万亿的Switch Transformer模型，证明了MoE在实现高效、可扩展的超大规模模型方面的巨大潜力。确立了将Transformer块中的FFN层替换为MoE层作为标准实践。
- (3) 向**Top-k路由**的演进：仅选择一个专家可能限制了模型的表征能力。采用Top-k（k>1）路由策略，允许一个Token的计算结果由多个专家的输出加权组合而成，许可提升模型的性能和训练的稳定性。
  - 例如，被广泛认为是MoE架构的GPT-4，以及开源领域的标杆Mixtral 8x7B（8个专家，激活2个），均采用了Top-k路由。这种设计在增加微量计算成本的同时，换取了更显著的性能提升。

(3) `GLaM`（Generalist Language Model）在 Switch Transformer 之后又把 `GShard` 的 top-2 gating 搬回来，但增加对 能耗效率 的关注，只用大约 GPT-3 训练能耗的三分之一，却在 zero-shot 任务上表现更好

坑与经验：
- GLaM 真正只激活一小部分参数，就能在算力和能耗上吊打类似 GPT-3 的 dense 模型——这是一次在大规模模型的“能效”上非常耀眼的案例。但如果真实数据分布不平衡，专家可能还是会出现负载不均，而 GLaM 也花了不少心思去调节 gating、capacity 等超参。

(4) **DeepSpeed-MoE**（微软）将负载均衡做到更成熟，既解决训练时如何把 token 分配给专家的问题，也兼顾推理阶段如何让专家有效利用的挑战。把之前很多 MoE 的坑都总结了，并提出一系列优化方案来应对。

核心思路：
- DeepSpeed-MoE 从 Switch Transformer 的 top-1 gating 出发，通过多专家并多数据并行的设计，让负载尽可能均匀，避免任何一个专家“堵车”。

鼓励各个专家的分配更加均匀。

最大的不同: 动态重分配 那些超容量的 token，而不是简单地丢弃。

此外，提出 Residual-MoE 结构，把 dense MLP 的输出和专家输出相加，类似一种“微调”式的组合，以便即使是被选中较少的专家也能对最终输出作出贡献。

跨 GPU 负载均衡：
- 不同层数可能拥有不同数量的专家，导致如果用统一的并行度会不灵活。
- 动态调整并行度，让每张 GPU 都刚好处理一个专家的负载。
- 例如，有些层有 32 个专家，就用 32 路专家并行加 4 路数据并行；有些层有 128 个专家，就 128 路专家并行加 1 路数据并行。这样可以保证每张 GPU 不会因为专家数不同而分配不均。

痛点与教训：
- DeepSpeed-MoE 整体的负载均衡做得相当不错，但要调对 capacity factor、辅助损失权重、并行度这些仍然是一门学问，而且真实世界的文本分布通常并不均匀，如果不针对性地调参，也可能在某些场景里栽跟头。
- 无论训练多么牛，推理时也要有一套负载均衡策略，否则延迟可能非常糟糕。

(5) **ST-MoE：聚焦容量因子与路由器 Z-Loss**

`ST-MoE` (Stable and Transferable Mixture-of-Experts) 在稀疏专家模型中迈出了稳定性和可迁移性的一大步。像 `Switch Transformer` 和 `GLaM` 都打下了基础，但 `ST-MoE` 进一步在老大难问题上做了提升，包括路由稳定性与超参调优等。

ST-MoE 有个亮点叫 **router z-loss**，缓解训练过程中数值不稳定的问题。因为路由里的指数函数特别容易放大微小数值误差，z-loss 就是给那些过大的 logit 值加点惩罚

容量因子调优：
- ST-MoE 还强调了 capacity factor 的重要性，用辅助损失来让 token 尽量平均分布。
- 相比之前的方法，训练稳定性和模型质量上找到了更平衡的点。
- 当然，这些改进背后依然离不开超参的精细调试。

ST-MoE 通过合理的设计，既能得到相对稳定的训练过程，也能保住最终的性能。

(6) **Mixtral 8x7B：时间局部性与专门的稀疏 Kernel**

`Mixtral 8x7B` 稀疏 MoE（SMoE）语言模型针对负载均衡有独到见解。还是用 Top-2 gating，每层 8 个专家，token 每次只用到其中两个专家，从而大大削减了激活的参数量（13B 级别，而不是像 Llama 2 70B 那样全部激活）。

时间局部性：
- Mixtral 在分析路由模式时发现，token 在相邻位置往往会被分配给同样的专家——尤其在网络的深层。也就是同一个专家常常连续处理好几步的 token，这就会带来“高重复率”现象。这有利于减少专家负载的突发波动，但也可能导致专家被局部数据“霸占”，对分布多样的数据集就需要留意一下。
- 另外，采用类似 DeepSpeed-MoE 的 动态重分配：当某个专家容量满了，就把多余 token 分给其他忙得没那么厉害的专家。

稀疏 Kernel 优化：
- Mixtral 利用像 Megablocks 这样专为稀疏计算优化的 GPU kernel，让 token 分配更加高效。一边分配一边做并行处理，这需要在 GPU 的层面做好负载均衡，否则还是可能出现一些 GPU 过载的问题。

经验：
- Mixtral 强调，需要了解数据集的“局部规律”，因为一旦数据分布换了（比如从新闻文本转到代码），它原先的路由模式就可能失效。
- 要做大规模的 MoE，就得好好考虑数据特征和专家分配之间的关系。

(7) 新一代方案：OpenMoE、DeepSeekMoE、JetMoE、DeepSeek-V3 


**OpenMoE：上下文无关的“专长化”与末端 Token 的“掉队”问题**

OpenMoE 依旧遵循常见的 top-k gating + capacity constraints + 辅助负载损失这一整套，但它提出了两个在大规模训练中比较显著的现象：
- 上下文无关的专长化（Context-Independent Specialization）：专家可能只根据 token 的表面特征或 ID 来决定路由，而不是更深层的语义。
- 末端 Token 掉队（Drop-Towards-the-End）：在处理长序列时，如果前面 token 就把专家容量吃满了，那么后面的 token 更容易被丢弃。

OpenMoE 也用 top-2 gating，用一个和 GShard、Switch 类似的负载均衡损失。还引入了 router loss 来惩罚过大的 logits，以稳定训练。和前辈们一样，它在容量因子和专家并行上做了细致的设计，但首次严肃讨论了序列末端被丢弃的问题，尤其在自回归结构里，后面 token 常常携带关键信息，如果被丢，性能就会打折。

结论：
- OpenMoE 提醒: 如果任务特别依赖完整序列（例如指令跟随、对话系统），那就要小心这个末端 token 掉队的问题，而且要注意 gating 可能会学到一些“表面化”模式。

**DeepSeekMoE：细粒度专家与共享专家**

DeepSeekMoE 最大特点之一，将每个专家切分为更细粒度的子专家（sub-experts），并引入一部分“共享专家”（shared experts）。这些共享专家在推理过程中总是被激活，不需要经过 gating 判断。

这样做的目标是减少参数冗余，同时又保留足够的多样性，让子专家可以针对不同模式或特征进行专门化学习。

细粒度专家拆分 (Fine-Grained Expert Segmentation)
- DeepSeekMoE 提出了细粒度专家拆分的概念，以提升专家的专门化程度。具体而言，它将每个专家拆分成多个更小的单元，但总参数量和总体计算成本保持不变。

两级负载均衡损失

为了防止路由塌缩（routing collapse）以及在计算分配上出现瓶颈，DeepSeekMoE 同时设计了专家级别 (expert-level) 和设备级别 (device-level) 的负载均衡损失。
- 专家级负载均衡损失 (Expert-Level Balance Loss): 这部分损失用来保证在专家之间分配的均衡性： 𝐿ExpBal=𝛼1∑𝑖=1𝑚𝑁−𝐾𝑠𝑓𝑖⋅𝑃𝑖, 其中，$f_i$ 表示路由到专家 $i$ 的 token 占比，$P_i$ 表示专家 $i$ 的平均路由概率（gating probability）。$\alpha_1$ 是损失系数。
- 设备级负载均衡损失 (Device-Level Balance Loss): 这部分损失用来确保在不同 GPU 或设备之间的计算负载也是均衡的： 𝐿DevBal=𝛼2∑𝑖=1𝐷𝑓𝑖′⋅𝑃𝑖′, 这里，$D$ 表示设备数，$f'_i$ 与 $P'_i$ 分别表示设备 $i$ 上所占的平均 token 比例和平均路由概率。$\alpha_2$ 则是另一层的损失系数。

通过这种双重均衡损失设计，DeepSeekMoE 能在保证专家内部细粒度专长化的同时，尽量避免某些专家或某些设备被过度使用，进而减小路由不均带来的计算瓶颈。

**JetMoE：无 Token 丢弃的 MoE 与流水线并行**

大多数 MoE 都会在超容量时丢 token，而 JetMoE 则提出“dropless”策略，保证所有 token 都能被处理：
- Dropless MoE：精心控制 gating，不让任何专家超过容量上限。
- 流水线并行 (Pipeline Parallelism)：把每一层的专家都放在同一个设备上，分层排队处理，以此简化通信和分配逻辑。

JetMoE 仍然用 top-2 gating，负载均衡也离不开辅助损失和 z-loss 等手段。它借鉴 MegaBlocks 的做法，用块稀疏（block-sparse）的方式在 GPU 上实现“无丢弃”，不过实现起来也更复杂，需要随时管理各专家的接收量并进行动态调度。

经验教训：
- 不丢 token 很理想，但实现门槛更高。尤其在大规模场景里，如何实时监控并重分配 token 不是个简单活儿。不过对那些对后续 token 极其敏感的任务（如问答系统、代码生成），dropless 模式确实很有吸引力。

**Skywork-MoE：gating logit 归一化 & 自适应辅助损失**

Skywork-MoE 是一个 1460 亿参数、16 专家的大模型，建立在已有的 Skywork-13B dense 模型之上做的 MoE 化。它有两大特色来缓解专家不均衡问题：
- gating logit 归一化：在做 softmax 之前先做标准化，控制输出分布的“尖锐度”。
- 自适应辅助损失系数：如果某层丢 token 太多，就自动调大该层的均衡惩罚；反之则调小。

它还是会加一个类似的均衡损失来避免路由塌缩，并在 gating 过程中针对 logits 做归一化处理，让模型更好地区分专家，同时又不会让概率分布过度极端。

痛点与收获：
- 不同层的负载问题可能不一样，一刀切的超参不一定好，所以 Skywork-MoE 那套自适应机制很有启发意义。但同样，如果归一化或辅助损失的力度没调好，依然可能造成路由极端或专家专长度不足。

**DeepSeek-V3：偏置加成与弱化辅助损失**

DeepSeek-V3 主打“砍掉大的辅助损失，用更加直接的偏置调节 (bias-based) 来控制负载”。想深入了解负载均衡最前沿，DeepSeek-V3 是个很好的例子。

模型结构：DeepSeek-V3 延续了 DeepSeekMoE 在 FFN 结构上的思路，将专家拆分成更细粒度的子专家，并保留一些“共享专家”。

风险与启示：
- 如果偏置更新速度 ($\gamma$) 过大，负载会在不同专家之间来回猛跳，不利于模型收敛；要是过小，又跟不上数据分布的变化。不过总体而言，这比在主损失上附加大量均衡惩罚要“温和”得多，也不会过多干扰语言建模本身的学习目标。


### MoE vs Dense

<img width="637" height="273" alt="image" src="https://github.com/user-attachments/assets/3d86eb93-0f07-4f10-a062-9c795d999899" />


[MoE与Dense模型：效率与成本的较量](https://zhuanlan.zhihu.com/p/21065720880)

MoE 机制在保证较小计算开销的同时，能够显著提升模型的表达能力和灵活性。

MoE模型优点：
- 计算资源节省：MoE通过选择最合适的专家来完成任务，从而避免了不必要的计算浪费，减少了计算和存储的开销。
- 高效性：尤其在面对大规模数据和复杂任务时，MoE能够显著提高训练效率。
- 灵活性强：MoE模型能够根据不同任务需求，动态调整专家的选择，使得模型的适应性和灵活性非常强。

MoE模型缺点：
- 训练不稳定：由于MoE模型依赖于动态选择机制，在专家选择不当时，可能导致任务训练的不稳定。
- 实现复杂：与Dense模型相比，MoE的实现更加复杂，需要更多的设计和优化工作，以保证模型能够高效且稳定地运行。
- 专家利用率不均衡：在某些任务中，可能会有一些专家被忽视，导致专家的利用率不够高，从而影响整体的计算效率。

Dense模型：全员出战，效率较低

Dense 模型设计理念非常简单
- 每个神经元（或计算单元）都参与到每个计算中。
- 无论任务的难易程度，Dense模型的每个参数都会参与到每次的计算中。
- 这使得Dense模型在处理相对较简单的任务时能够表现得较为稳定，但在面对复杂问题时，Dense模型却显得有些力不从心。

Dense模型没有像MoE那样智能选择合适的计算单元，所以每次训练时，都需要对所有参数进行计算和更新，这带来了巨大的计算量和存储需求。

因此，Dense模型的计算成本较高，尤其是在处理大规模数据集或复杂任务时，效率会大大降低。

Dense 模型优点：
- 稳定性强：由于每个计算单元都参与训练，Dense模型在面对一些简单任务时能够保持较为均衡的表现，训练过程也相对稳定。
- 适应性好：对于小规模的数据集或简单任务，Dense模型能够快速生成有效的结果。

Dense 模型缺点：
- 计算量大：每个参数都必须参与计算，导致计算和存储开销巨大，尤其是在大规模训练时更加明显。
- 效率低：在面对复杂的任务或大规模数据时，Dense模型的训练效率较低，无法充分利用计算资源。
- 浪费资源：即使某些计算单元对特定任务并不重要，Dense模型也会让它们参与计算，造成了计算资源的浪费。


### MoE vs Dropout

【2025-6-27】MoE vs Dropout

分析 
- Dropout是随机将一些神经元输出置为0，为的是防止过拟合。多用于CNN、RNN等神经网络。
- MoE是通过训练门控机制（Router，是一个自定义的大小的神经网络，它的输出是一个专家数量大小的概率分布），来决定每个token 该如何选择专家。为的是增加大模型的计算效率和提升处理复杂任务的能力。
- 相同点：都是对Dense网络做了**稀疏**处理
- 不同点：Dropout是**随机置0**防止过拟，MoE是**主动选择子网络**，加快计算效率

Decoder结构改动点 [参考](https://www.toutiao.com/article/7561777389680837166)
- MLP层改成MoE结构，Router -> experts
- 每个 expert 都是 MLP 结构，规模较小
- 推理过程中，选择部分专家，速度更快

<img width="1116" height="1126" alt="image" src="https://github.com/user-attachments/assets/77a0d2ab-c14a-471d-9259-5171bdf78160" />




## MoE 结构

MoE由两种网络组成: `专家网络`和`门控网络`。
- (1) `专家网络`: 专家网络是**专有模型**，每个模型都经过训练，在数据子集中表现出色。
  - 一组并行的、结构相同的神经网络模块
  - MoE 理念: 拥有多名优势互补的专家，确保对问题空间的全面覆盖。
  - 将传统 Transformer 中 FFN（前馈网络层）替换为多个**稀疏专家层**（Sparse MoE layers）。每个专家是独立的神经网络，这些专家通常是**前馈网络** (FFN)，或更复杂的网络结构。
- (2) `门控网络`: 门控网络充当**调度员**，协调或管理个别专家的贡献。用来决定输入的token分发给哪个专家
  - 小型的神经网络，通常由一个简单的线性层和Softmax函数构成
  - 对于每个输入的Token表征，门控网络会输出概率分布，该分布对应于所有专家的权重。模型会根据该分布选择一个或多个（Top-k）权重最高的专家来处理该Token。
  - 经过训练的门控网络可以评估新的输入向量，并根据专家的**熟练程度**将处理责任分配给最合适的专家或专家组合。
  - 门控网络根据专家的输出与当前输入的相关性动态调整其权重，确保定制响应。

MoE的优势是**专家化**、**动态化**、**稀疏化**，在模型研发成本、训练/推理效率和整体性能之间实现最佳平衡。
- 一是采用**混合专家**方法，各专家模型面向不同数据或任务进行针对性建模和处理，提升模型的准确性和专业能力，更好地适应多模态数据及复杂/特定任务计算。
- 二是根据任务需求**灵活选择并组合**专家模型，使得模型能够动态地适应不同的输入样本和任务场景，提高模型的灵活性、可解释性和表达能力。
- 三是只激活/使用**部分**专家模型，减少不必要的计算，提升模型计算速度并降低算力需求。

研究表明，与稠密模型相比，MoE+指令调优仅使用1/3算力就能提升大模型性能约45%，缩短训练时间2，且参数规模越大，效果越好。

MoE 架构精髓在于**稀疏激活**（Sparse Activation）。

一个拥有数万亿参数的MoE模型中，对于任意Token，仅有被门控网络选中的k个专家（k通常很小，如2或4）的参数被激活和用于计算。其余绝大部分专家则保持静默。

这就实现了**总参数量**（Total Parameters）与**激活参数量**（Active Parameters）的显著分离，从而在保持巨大模型容量的同时，将单次推理的计算成本控制在与一个规模小得多的稠密模型相当的水平。


数学公式: 各个专家输出的加权和
- G(x)是门控函数（决定用哪个专家）
- E_i(x)是第i个专家的输出。

> y = Σ(i=1 ... N) G(x)_i × E_i(x) 

Google的Switch Transformer更狠，直接简化成只激活一个专家
- 效果居然贼好，相同计算预算下，训练速度快了7.5倍（与T5-Base对比）

> y = G(x)_{argmax} × E_{argmax}(x)



MoE 处理流程
- ![](https://pic1.zhimg.com/80/v2-654cca302d29837e461c6ef04667a1a0_1440w.webp)

MoE 关键组件：
- **专家**（Expert）：
  - 专门训练的**小型**神经网络，每个网络都在其擅长的领域有着卓越的表现
  - MoE层由许多**专家**、**小型MLP**或**复杂LLM**（如 Mistral 7B）组成。
- **路由器**（Router）：`门控网络`, MoE架构中的**决策核心**
  - 路由器确定将哪些输入token分配给哪些专家。
  - 门控网络会计算输入数据与每个专家的**兼容性得分**，然后依据这些得分决定每个专家在处理任务中的作用。

路由策略有两种：**token选择路由器** 或 **路由器选择token**。

路由器 使用**softmax门控函数**通过专家或token对概率分布进行建模，并选择前k个。

这些组件共同作用，确保适合的任务由合适的专家来处理。门控网络有效地将输入数据引导至最合适的专家，而专家们则专注于自己擅长的领域。这种合作性训练使得整体模型变得更加多功能和强大。

MoE 好处：
- 每个专家都可以专门处理不同的任务或数据的不同部分。
- MoE构架能向LLM添加可学习参数，而不增加推理成本。
- 可以利用稀疏矩阵的高效计算
- 并行计算所有专家层，以有效利用GPU的并行能力
- 帮助有效地扩展模型并减少训练时间。以更低的计算成本获得更好的结果



### FFN

FFN（前馈网络层）
- ![](https://pic4.zhimg.com/80/v2-e53d37f8a78d08c0d398c23627fcbb2f_1440w.webp)

```PY
class FeedForward(nn.Module):
    def __init__(self, dim_vector, dim_hidden, dropout=0.1):
        super().__init__()
        self.feedforward = nn.Sequential(
            nn.Linear(dim_vector, dim_hidden),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_hidden, dim_vector)
        )
        
    def forward(self, x):
        out = self.feedforward(x)
        return out
```


[从零实现一个MOE](https://zhuanlan.zhihu.com/p/701777558)
- 完整代码: [make_moe_step_by_step.ipynb](https://github.com/mst272/LLM-Dojo/blob/main/llm_tricks/moe/make_moe_step_by_step.ipynb)


### 专家模型

其实就是一个多层感知机MLP

```py
class Expert(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)
```


### MoE 路由

MoE 路由, 即 TopKrouter

假设定义了4个专家，路由取前2名专家，即: expert=4， top_k=2。

接收注意力层的输出作为输入X，即将输入从（Batch size，Tokens，n_embed）的形状（2，4，32）投影到对应于（Batch size，Tokens，num_experts）的形状（2，4，4），其中, num_experts即expert=4。其中返回的indices可以理解为对于每个token的4个专家来说，选的两个专家的序号索引。
- ![](https://pic2.zhimg.com/80/v2-a616cd5cbda5ef2a2c853fdefc828239_1440w.webp)
- ![](https://pic3.zhimg.com/80/v2-c00fcb056a866dc969d68699f1e51af6_1440w.webp)

代码如下：

```py
# 这里我们假设定义n_embed为32， num_experts=4, top_k=2

class TopkRouter(nn.Module):
    def __init__(self, n_embed, num_experts, top_k):
        super(TopkRouter, self).__init__()
        self.top_k = top_k
        self.linear =nn.Linear(n_embed, num_experts)
    
    def forward(self, mh_output):
        logits = self.linear(mh_output)    # (2,4,32) ---> (2,4,4)
        # 获取前K大的值和索引，沿列。
        top_k_logits, indices = logits.topk(self.top_k, dim=-1)
        # 创建一个形状和logits相同全'-inf'矩阵，即(2,4,4)
        zeros = torch.full_like(logits, float('-inf'))
        # 按照索引和值填充上述zeros矩阵
        sparse_logits = zeros.scatter(-1, indices, top_k_logits)
        # 对其进行softmax，未被填充的位置会为0
        router_output = F.softmax(sparse_logits, dim=-1)
        return router_output, indices
```

看完代码之后配合整体流程图将会更清晰

### 噪声

为了避免所有 token 都发送给同一组“受青睐”的expert, 需要一个良好平衡

因此，将**标准正态噪声**添加到来自门控线性层的logits。

代码只改动了几行

```py
class NoisyTopkRouter(nn.Module):
    def __init__(self, n_embed, num_experts, top_k):
        super(NoisyTopkRouter, self).__init__()
        self.top_k = top_k
        self.topkroute_linear = nn.Linear(n_embed, num_experts)
        # add noise
        self.noise_linear =nn.Linear(n_embed, num_experts)

    
    def forward(self, mh_output):
        # mh_ouput is the output tensor from multihead self attention block
        logits = self.topkroute_linear(mh_output)

        # 噪声定义 Noise logits
        noise_logits = self.noise_linear(mh_output)

        # 添加噪声 Adding scaled unit gaussian noise to the logits
        noise = torch.randn_like(logits)*F.softplus(noise_logits)
        noisy_logits = logits + noise

        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)
        zeros = torch.full_like(noisy_logits, float('-inf'))
        sparse_logits = zeros.scatter(-1, indices, top_k_logits)
        router_output = F.softmax(sparse_logits, dim=-1)
        return router_output, indices
```

### 完整 MoE 模块

将 router 乘对应的token。

这种**选择性加权乘法**最终构成**稀疏MoE**。

代码部分如下所示：

```py
class SparseMoE(nn.Module):
    def __init__(self, n_embed, num_experts, top_k):
        super(SparseMoE, self).__init__()
        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)
        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])
        self.top_k = top_k

    def forward(self, x):
        # 1. 输入进入router得到两个输出
        gating_output, indices = self.router(x)
        # 2.初始化全零矩阵，后续叠加为最终结果
        final_output = torch.zeros_like(x)

        # 3.展平，即把每个batch拼接到一起，这里对输入x和router后的结果都进行了展平
        flat_x = x.view(-1, x.size(-1))
        flat_gating_output = gating_output.view(-1, gating_output.size(-1))

        # 以每个专家为单位进行操作，即把当前专家处理的所有token都进行加权
        for i, expert in enumerate(self.experts):
            # 4. 对当前的专家(例如专家0)来说，查看其对所有tokens中哪些在前top2
            expert_mask = (indices == i).any(dim=-1)
            # 5. 展平操作
            flat_mask = expert_mask.view(-1)
            # 如果当前专家是任意一个token的前top2
            if flat_mask.any():
                # 6. 得到该专家对哪几个token起作用后，选取token的维度表示
                expert_input = flat_x[flat_mask]
                # 7. 将token输入expert得到输出
                expert_output = expert(expert_input)

                # 8. 计算当前专家对于有作用的token的权重分数
                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)
                # 9. 将expert输出乘上权重分数
                weighted_output = expert_output * gating_scores

                # 10. 循环进行做种的结果叠加
                final_output[expert_mask] += weighted_output.squeeze(1)

        return final_output
```

注意：
- 以专家为单位遍历每个专家，抽取每个专家对于所有token中在前top_k的tokens。
- ![](https://pic1.zhimg.com/80/v2-bf34e88ffe5d0e87fee896bfeee6154c_1440w.webp)
- ![](https://pic1.zhimg.com/80/v2-eb4bfb0ffeaf9cfd75468a4309f87c90_1440w.webp)

### MoE + Transformer

MOE与transformer结合
- 将上述所做工作与常规的transformer层结合，即, 用moe替代MLP层。

```py
class Block(nn.Module):
    """Mixture of Experts Transformer block: communication followed by computation (multi-head self attention + SparseMoE) """
    def __init__(self, n_embed, n_head, num_experts, top_k):
        super().__init__()
        head_size = n_embed // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.smoe = SparseMoE(n_embed, num_experts, top_k)
        self.ln1 = nn.LayerNorm(n_embed)
        self.ln2 = nn.LayerNorm(n_embed)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.smoe(self.ln2(x))
        return x
```


## MoE 结构演进

简要概括如下，详见站内专题: [多任务学习](multi_task)

### 多任务学习

多任务学习目的
- 用1个模型来同时学习多个目标和任务

但常用任务模型的预测质量通常对**任务之间的关系**很敏感（数据分布不同，ESMM 解决的也是这个问题）


### 2018 谷歌 MMoE

google 提出`多门混合专家`算法（Multi-gate Mixture-of-Experts，以下简称 `MMoE`）旨在构建一个兼容性更强的多任务学习框架
- 学习如何从数据中权衡**任务目标**（task-specific objectives）和**任务之间**（inter-task relationships）的关系。
- 所有任务之间共享混合专家结构（MoE）的子模型来适应多任务学习，同时还拥有可训练的门控网路（Gating Network）以优化每一个任务。

#### MMOE 核心思想

MMOE 核心思想: 
- 把底层网络划分成一些专用模块，虽然底层参数是共享，但是通过目标和网络参数之间的一个 gate（门）来学习，让每部分网络充分学习到对每个目标的贡献最大的一组参数结构
- 通过这种方式来保证，底层网络参数共享的时候，不会出现目标之间**相互抵消**。


#### MMoE 问题

工业界，多目标任务问题通常使用 `MoE`(Mixture-of-Experts)范式来解决。
- MMoE 模型由`专家网络`和`门控网络`构成
- `专家网络`: 用于对输入特征进行建模以及将完成特征交叉；
- `门控网络`: 用于估计不同专家的重要性，以便为相应任务融合其输出。

MMoE 三个问题：
- （1）专家**崩溃**(Expert Collapse)：专家输出分布差异显著，一些专家在使用 ReLU 时零激活率超过90%，这使得门控网络**难以分配公平的权重**来平衡专家。
- （2）专家**退化**(Expert Degradation)：一些`共享专家`仅被一个任务占用，**退化为少数任务的`特定专家`**。
- （3）专家**欠拟合**(Expert Underfitting)：一些数据稀疏的预测任务往往会忽略其`特定专家`，并给`共享专家`分配较大的权重。
  - 原因: `共享专家`从密集任务中感知到更多的梯度更新和知识，而`特定专家`由于其行为稀疏，容易陷入**欠拟合**状态。


### 2024 快手 HoME

基于以上问题，提出 `HoME`，包含 
- （1）**专家归一化** 和 Swish 机制，用于对齐专家输出分布并避免专家崩溃。
- （2）**层次掩码机制**，用于提高任务之间的共享效率，减少占用问题并避免专家退化。
- （3）特征门和**自适应**门机制，用于确保每个专家都能获得适当的梯度以使其有效性最大化。


### 路由策略

参考
- 【2025-11-02】[路由之战：哪种 MoE 路由策略真正有效](https://zhuanlan.zhihu.com/p/1968326900721129374)
- [进阶路由机制](https://apxml.com/zh/courses/mixture-of-experts-advanced-implementation/chapter-2-advanced-routing-mechanisms)

专家混合（Mixture-of-Experts, MoE）模型的秘密：
> 路由器（router）可以凭一己之力彻底毁掉你的模型。

可以拥有完美的专家网络架构、精心调优的超参数和无限的计算资源，但如果路由器崩溃，无论选择了多少专家，性能都会退回到密集模型（dense model）的水平。

路由器任务听起来简单——决定哪个专家来处理每个词元（token）。但往往是大多数 MoE 实现出错的地方。如果策略错误，可能会花费数周时间进行调试，并完全迷失方向。

路由策略：
- 噪声 Top-k 门控： 一种向门控 logits h(x) 引入噪声的技术，以在训练期间改善负载分配。
- Switch Transformers： 一种通过将每个 token 发送到仅一个专家 (k=1) 来简化路由的架构，从而减少通信开销。
- 基于哈希的路由： 一种使用哈希函数进行 token 分配的确定性方法，消除了对学习型门控网络的需要。
- Soft MoE： 一种完全可微的方法，计算所有专家的加权平均值，创建“软”分配而非硬性离散选择。

路由方法总结

| 方法    | 专家专业化（Expert Specialization） | 专家利用率（Expert Utilization） | 实现难度（Implementation） | 应用实例                                                                 |
|-------------------|---------------------|---------------------|----------------------------|--------------------------------|
| 1. Hash（哈希）| ⭐Poor（差）| ⭐⭐⭐Great（极好）| ⭐Easy（容易）| Research bases<br>gpt-oss-120b, |
| 2. Learned（学习型）| ⭐⭐⭐Great（极好）| ⭐Poor（差）| ⭐⭐Medium（中等）| qwen3, GRIN,<br>OLMoE, JetMoE,<br>Jamba, Mixtral |
| 3. Sinkhorn（辛克霍恩）| ⭐⭐Good（好）| ⭐⭐⭐Great（极好）| ⭐⭐⭐Hard（难）| BlackMamba |
| 4. Learned + Capacity Factor（学习型+容量因子） | ⭐⭐Good（好）| ⭐⭐Good（好）| ⭐⭐Medium（中等）| MiniMax,<br>Hunyuan-Large,<br>Skywork-MoE,<br>DeepSeek V2,<br>OpenMoE, ST-MoE, Switch Transformers |
| 5. Learned + Shared Experts（学习型+共享专家） | ⭐Poor（差）| ⭐⭐Good（好）| ⭐⭐Medium（中等）| Pangu Ultra MoE, Qwen2.5+,<br>DeepSeek-V3+, Qwen2,<br>DeepSeek-V2, Qwen1.5,<br>Hunyuan-Large,<br>DeepSeekMoE |
| 6. Learned + Adaptive Aux Loss（学习型+自适应辅助损失） | ⭐⭐Good（好）| ⭐⭐Good（好）| ⭐⭐Medium（中等）| Skywork-MoE |
| 7. Learned + Expert Bias (学习型 + 专家偏置) |	⭐⭐ Good (好) |	⭐⭐ Good (好)	| ⭐⭐ Medium (中等) |	DeepSeek-V3 |




## MoE 部署

MoE 为部署机器学习模型提供了巨大的好处
- MoE核心优势：其专家网络的多元化和专业化。MoE的设置能够以单一模型可能难以达到的精度处理多方面的问题。
- MoE可伸缩性：随着任务复杂性的增加，不改变其他专家模型的情况下，将更多专家无缝地集成到系统中，扩大专业知识的范围。也就是说，MoE可以帮助将预先训练过的专家打包到机器学习系统中。


## MoE 问题

现代大模型中，MoE层并非孤立存在，而是深度嵌入在Transformer的基本构建块（Block）中。
- MoE层被用来替代标准Transformer块中两个自注意力（Self-Attention）层之间的部分或全部FFN层。

这种融合形成了高效的协同机制：
- 自注意力层（Dense）：保持稠密计算，负责在序列内的Token之间进行信息交互和上下文聚合，是模型理解语境关系的核心。这一部分由所有Token共享。
- MoE FFN层（Sparse）：给予一个巨大、可选择的知识库。自注意力层聚合上下文信息后，将处理过的Token表征传递给MoE层。门控网络根据这些表征的语义，为其匹配最合适的知识处理单元（专家），进行深度特征提取和知识关联。

这种架构带来了显著优势，但也伴随着新的体系级挑战。
- 最突出的是**内存开销**。尽管计算稀疏，但推理时，所有专家的参数都必须加载到计算设备（如GPU）显存中。这导致MoE模型的显存占用远大于同等计算量的稠密模型。
- 分布式训练和推理中，Token要求根据路由结果在不同设备间传递（All-to-All Communication），这引入了额外的**通信开销**，对系统设计和网络带宽提出了更高要求。

MoE模型缺点：
- 训练不稳定：由于MoE模型依赖于动态选择机制，在专家选择不当时，可能导致任务训练的不稳定。
- 实现复杂：与Dense模型相比，MoE的实现更加复杂，需要更多的设计和优化工作，以保证模型能够高效且稳定地运行。
- 专家利用率不均衡：在某些任务中，可能会有一些专家被忽视，导致专家的利用率不够高，从而影响整体的计算效率。


面临的挑战
1. 训练和微调面临稳定性和可靠性挑战。
  - 训练阶段，MoE通过“条件计算”思想引入稀疏性，其将token分配给固定数量专家的离散特性带来专家负载均衡问题，容易导致某些专家被过度利用，而其他专家未被充分利用，从而影响专家的专业化，降低模型性能。虽然这一问题，目前可以通过合并辅助损失函数等来缓解，但仍会导致模型训练不稳定。
  - 微调阶段，与稠密模型相比，稀疏模型的微调更容易产生过拟合问题，容易导致泛化能力不足，影响模型整体性能，如拥有1.6T参数量的MoE预训练模型Switch Transformer，在SuperGLUE等常见基准上进行微调时，其整体性能却落后于较小的模型3。
2. 大规模分布式部署难且通信成本高。
  - 一方面，尽管MoE可以在模型参数总量不变的情况下降低计算需求，但仍需要将所有参数加载到内存中。因此，MoE对**内存需求很高**，要将超大规模参数模型的专家层分别部署在不同设备以减少内存消耗，实际部署难度很大。
  - 另一方面，随着MoE模型的参数规模/复杂度不断提升，模型训练期间的高效通信越来越重要。而模型复杂度与通信开销之间的权衡一直是分布式训练的重大问题。MoE模型中专家之间的数据交换、并行训练都需要机间all-to-all通信来实现，增加通信成本，且模型规模越大，通信成本越高。
  - 因此，在实际部署过程中，需要仔细设计通信策略和优化网络拓扑，降低通信延迟和潜在的网络拥塞。
3. MoE架构存在原始缺陷、与现有架构集成难，部分关键技术仍有待攻关。
  - 一是MoE仍存在知识混合、知识冗余等原始架构缺陷，容易导致专家同质化严重、专业化能力不足等问题。同时，根植于MoE的超参数调整、专家模型设计及协作、动态性适应数据变化、对数据噪声相对敏感等技术难题仍有待解决。
  - 二是MoE架构设计很复杂，涉及网络类型、专家数量选择等多个方面。目前FFN、LSTM、CNN、Attention和LoRA等多种网络架构已被用作专家模型，但各种网络类型专家混合仍属于新兴研究领域。
  - 三是将MoE集成到现有模型框架中对于降低现有模型升级成本、推进MoE广泛应用至关重要，但可能会损害模型性能，需要进一步优化并行预训练和推理策略。


### 模型不易收敛

MoE模型用强化学习训练总崩溃？问题出在推理和训练时的路由不一致

混合专家（MoE）架构以其卓越的参数扩展效率，已成为通往万亿参数规模的关键路径。

强化学习（RL）作为激发模型深度推理能力的主要手段，在数学、代码等复杂任务中展现了潜力。

然而，当将这两项前沿技术相结合时，棘手的问题浮出水面：
> MoE模型的强化学习过程异常脆弱，时常陷入性能骤降甚至训练崩溃的困境。

​现象背后隐藏着深刻的“断层”——即`训练`（Training）与`推理`（Inference）阶段的行为不一致性。

RL流程中，使用高效推理引擎（如SGLang）进行数据采样，再将数据送入训练框架（如Megatron）进行模型优化。
- 对于标准稠密模型，这种框架差异或许只会带来**微小**的数值误差；
- 但在MoE模型中，这个问题被急剧放大。MoE路由机制（Routing Mechanism）：微小的环境或实现差异，都可能导致模型为同一个输入token选择完全不同的专家组合，从而走向截然不同的计算路径。

MoE模型的训练-推理差异远大于稠密模型：
- MoE模型 (Qwen3-30B-A3B): KL 散度 ≈ 1.535 × 10⁻³
- Dense模型 (Qwen3-8B): KL 散度 ≈ 6.4 × 10⁻⁴

​这种路由决策的不一致，正是导致MoE模型RL训练不稳定的“罪魁祸首”。使得从推理阶段获取的“经验”对于训练阶段而言变得“面目全非”，优化信号因此失真，最终导致灾难性的后果。

解决方案：
- 既然问题源于两个阶段的路径不一，那能否强制“走同一条路”？引入了Rollout路由重放（Rollout Routing Replay, R3）。

【2025-10-21】北大+小米（罗福莉），推出 Rollout Routing Replay（R3）：
- 论文：[Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/pdf/2510.11370)
- 解读：[Rollout路由重放：解决MoE强化学习的不稳定](https://zhuanlan.zhihu.com/p/1963561905395328766)
- 推理时记录专家路由决策，训练时直接复用，大幅缩小训练-推理差异，彻底避免灾难性崩溃。
- 支持 Qwen3-30B-A3B、deepseek_v2 等模型，全分布式训练+各种并行策略全兼容。训练 MoE 终于稳了

<img width="1200" height="800" alt="image" src="https://github.com/user-attachments/assets/bc69896f-b2b8-4774-ab59-caea6dcc78f7" />


### 负载均衡

负载均衡在 MoE 里就是双刃剑：过度追求均衡会压制模型的表达能力；对均衡不管不顾又会浪费一半专家。

常见的坑和应对思路：
- 路由塌缩 (Routing Collapse) 与过度专长化：如果几个专家接收了大部分 token，就等于浪费了其它专家。轻度的辅助损失或偏置修正有助于防止塌缩。
- 容量因子的调节：设置太高，几乎不丢 token，但算力浪费会高；设置太低，token 大批被丢，严重影响训练效果。这里没有固定公式，必须结合数据分布反复实验。
- 过度依赖辅助损失：有些 MoE 架构重度依赖均衡损失，最后的语言建模目标被削弱，导致专家学不到真正的专长。要拿捏好度。
- 推理时的瓶颈：训练时的负载均衡不一定能适配推理场景。一旦在推理中某些专家被频繁调用，会让延迟变高，所以需要类似分层路由、动态分组等技巧。
- 领域迁移的挑战：路由网络可能固化在某些训练数据模式上，如果应用场景的分布变了， gating 也可能变得不匹配，需要额外的微调或重新训练。


### 内存开销

尽管计算稀疏，但推理时，所有专家的参数都必须加载到计算设备（如GPU）显存中。这导致MoE模型的显存占用远大于同等计算量的稠密模型

### 通信开销

分布式训练和推理中，Token要求根据路由结果在不同设备间传递（All-to-All Communication），这引入了额外的**通信开销**，对系统设计和网络带宽提出了更高要求。

### 安全隐患

【2024-11-1】[DeepMind提出针对MoE的全新攻击方法，揭示模型中的用户提示泄露风险](https://mp.weixin.qq.com/s/N_vVqJx2PCCYshe3l4FO7w)

尽管MoE架构在性能上具有显著优势，但也引入了新的**安全隐患**。

Hayes等人（2024）最近提出了一种名为“token dropping”的漏洞，这种现象发生在**当某个专家的处理能力被超出时，导致多余的tokens被丢弃或重新路由**。

攻击者可以利用这一漏洞，通过将自己的数据与受害者的数据放在同一批次中，故意造成目标专家的缓冲区溢出，从而降低受害者模型响应的质量，进而实施**拒绝服务**（DoS）攻击。

本文扩展了这一漏洞，提出新颖的攻击方式——**MoE Tiebreak Leakage攻击**。
- 核心: 精心设计的输入批次，攻击者能够操控MoE模型中的专家路由，从而泄露受害者的私密输入。
- 攻击者可以利用token dropping所引发的跨批次侧信道，影响其他用户的数据处理，进而实现信息泄露。

## MoE 发展方向

【2024-11-4】[大模型新趋势之MoE：现状、挑战及研究方向](https://www.c114.com.cn/ai/5339/a1276942.html)

研究方向

1. 通过优化MoE架构、改进门控算法等，提高模型性能和稳定性。
  - 一是业界正在加快研发各种新型MoE架构，以提升模型性能或实用性。
    - 如清华和微软联合提出MH-MoE5，通过多头机制弥补MoE原始缺陷，实现专家激活率由8.33%提升至90.71% ，提升模型高效扩展能力；
    - 华为提出全新LocMoE架构，并将其嵌入盘古大模型，提升模型通用性、可解释性和易移植性等。
  - 二是多项研究表明，通过改进门控算法或调整训练策略，能有效提升MoE模型稳定性及性能，如AdvMoE通过算法优化将门控模型和专家模型分开训练，显著提高MoE模型的对抗鲁棒性和整体效率；
    - 普林斯顿大学和Meta AI联合提出一种Lory方法，引入因果分段路由策略和基于相似性的数据批处理技术，提升了模型的效率和专家专业化能力7。
2. 构建分布式训练系统/软件工具，提升MoE模型训练效率、降低训练门槛。
  - 清华团队发布FastMoE、FasterMoE、SmartMoE3等一系列分布式训练系统，相比直接使用PyTorch，FastMoE可将训练速度提升47倍；SmartMoE支持一键实现MoE模型分布式训练，并优化模型训练性能。
  - 微软的DeepSpeed9系统提供端到端MoE训练和推理解决方案，结合模型压缩等技术，可提供更快、更便宜的MoE模型推理服务，与同等质量的密集模型相比，可加速4.5倍，成本降低9倍。
  - 斯坦福大学、微软和谷歌联合推出的MegaBlocks系统，面向单个GPU上运行多个专家的场景，提升MoE训练效率，与最先进的Tutel库相比，端到端训练速度提升40%。
3. 利用模型蒸馏、硬件优化等技术，降低MoE模型部署难度。
  - 一是通过模型蒸馏、任务级别路由等技术，保留模型性能同时降低模型复杂度。谷歌将Switch Transformer蒸馏回其对应的稠密模型，参数规模降低95%，成功保留了约30-40%的由稀疏性带来的性能提升，加快了预训练速度，且在推理过程中支持使用更小的模型。此外，通过任务级别路由技术将整个句子或任务直接路由到一个专家上面，或提取出用于特定服务的子网络，有助于简化模型结构。
  - 二是研究更适合稀疏计算的硬件优化技术，节省内存、加速计算。研究表明，块稀疏Flash Attention注意力机制可以进一步减少GPU内存访问次数，加快计算并节省显存；结合PagedAttention构建vLLM11（一种LLM推理加速系统），可实现KV缓存零浪费且支持各请求间共享KV缓存，从而降低内存使用，并实现高效内存管理。
4. 通过系统性优化、共享专家、通信结构设计等，降低通信成本。
  - 一是通过系统性优化，平衡模型复杂度与通信开销，如ScMoE12使用一种通信重叠并行策略，克服分布式MoE模型中通信操作的固有依赖性对并行优化的阻碍，实现11%的训练速度提升、15%的推理速度提升，且通信时间仅为8×A800-NVLink场景中标准MoE的15%。
  - 二是利用共享专家方法，减少分布式系统之间的数据传输，如DeepSeekMoE13采用共享专家、细颗粒度专家分割等策略，可有效解决MoE中的参数冗余、路由崩溃等问题，且在145B参数下，仅需约28.5%的计算量即可达到DeepSeek 67B的性能水平。
  - 三是通过通信结构设计，提升通信效率，如SCoMoE通过结构化全连接通信，鼓励数据跨设备通信，结合标记聚类方法，提升了模型性能和效率。




# MoE 实现


## 实践

【2025-9-7】开源项目 Cortex 
- 个人可训练MoE大模型:预训练到DPO
- 训练成本不高，0.6B的MoE LLM，推理时激活参数仅为0.2B。
- 目前已完成预训练到DPO全流程训练，并提供训练各个阶段checkpoint下载。
- 地址: [CortexCotex](https://github.com/qibin0506/CortexCotex)

## MoE 进化

[LLM MOE 进化之路](https://bruceyuan.com/llms-zero-to-hero/the-way-of-moe-model-evolution.html)
- 普通简化 MOE -> sparse moe -> share_expert sparse moe (deepseek)

【2025-11-27】[大模型基本功——手写MOE](https://mp.weixin.qq.com/s/fFSGBXOfqM_-cbuQEy3ryw)
- 基础版本MOE -> Sparse MoE -> ShareExpert Sparse MoE -> 


### (1) 基础 MOE

输入是一个 Token, 输出是一个 Token Embedding。

#### 网络结构

MOE 网络对应 Expert，一般是一个 FeadFoward Network，FFN。

用一层 Linear 代替，有更高级版本的 Expert


流程图
- ![](https://bruceyuan.com/llms-zero-to-hero/basic-moe-model.png)


#### 代码实现

MoE 网络定义

```py
class BasicExpert(nn.Module):
    # Expert 可以是最简单的linear 层, 也可以是 MLP 层, 更复杂的 MLP 层（active function 设置为 swiglu）
    def __init__(self, feature_in, feature_out):
        super().__init__()
        self.linear = nn.Linear(feature_in, feature_out)
    
    def forward(self, x):
        return self.linear(x)

class BasicMOE(nn.Module):
    def __init__(self, feature_in, feature_out, expert_number):
        super().__init__()
        self.experts = nn.ModuleList(
            [
                BasicExpert(feature_in, feature_out) for _ in range(expert_number)
            ]
        )
        # gate 就是选一个 expert 
        self.gate = nn.Linear(feature_in, expert_number)
    
    def forward(self, x):
        # x 的 shape 是 （batch, feature_in)
        expert_weight = self.gate(x)  # shape 是 (batch, expert_number)
        expert_out_list = [
            expert(x).unsqueeze(1) for expert in self.experts
        ]  # 里面每一个元素的 shape 是： (batch, ) ??

        # concat 起来 (batch, expert_number, feature_out)
        expert_output = torch.cat(expert_out_list, dim=1)

        # print(expert_output.size())

        expert_weight = expert_weight.unsqueeze(1) # (batch, 1, expert_nuber)

        # expert_weight * expert_out_list
        output = expert_weight @ expert_output  # (batch, 1, feature_out)
        
        return output.squeeze()


def test_basic_moe():
    x = torch.rand(2, 4)

    basic_moe = BasicMOE(4, 3, 2)
    out = basic_moe(x)
    print(out)

# 基础 MoE
test_basic_moe()
```



### (2) SparseMoE


#### 网络结构

用 switch transformers 里的图为例
- ![](https://bruceyuan.com/llms-zero-to-hero/switch-transformers-moe-model.png)

与 Basic 区别
- MOE 选择 topK 个专家，然后对这 topK 个专家的输出进行**加权求和**，并且把输入样本变成了大模型中真实的输入 Shape，`(batch, seq_len, hidden_dim)`


#### 代码实现


```py

# 主要参考自 mistral MOE 的实现

class MOERouter(nn.Module):
    def __init__(self, hidden_dim, expert_number, top_k):
        super().__init__()
        self.gate = nn.Linear(hidden_dim, expert_number)
        self.expert_number = expert_number
        self.top_k = top_k
    
    def forward(self, hidden_states):
        # 计算路由logits
        router_logits = self.gate(hidden_states)  # shape is (b * s, expert_number)
        
        # 计算专家经过softmax之后的概率
        routing_probs = F.softmax(router_logits, dim=-1, dtype=torch.float)
        
        # 计算topk的专家的输出
        router_weights, selected_experts = torch.topk(
            routing_probs, self.top_k, dim=-1
        )  # shape都是 (b * s, top_k)
        
        # 专家权重归一化
        router_weights = router_weights / router_weights.sum(dim=-1, keepdim=True)
        router_weights = router_weights.to(hidden_states.dtype)
        
        # 生成专家掩码
        expert_mask = F.one_hot(
            selected_experts,
            num_classes=self.expert_number
        )  # shape是 (b * s, top_k, expert_number)
        expert_mask = expert_mask.permute(2, 1, 0)  # (expert_number, top_k, b * s)
        
        return router_logits, router_weights, selected_experts, expert_mask


class MOEConfig:
    def __init__(
            self, 
            hidden_dim, 
            expert_number, 
            top_k, 
            shared_experts_number=2,
        ):
        self.hidden_dim = hidden_dim
        self.expert_number = expert_number
        self.top_k = top_k
        self.shared_experts_number = shared_experts_number

class SparseMOE(nn.Module):
    # 稀疏 MOE 模型，这里每一个 token 都会过 topk 个专家，得到对应token 的 hidden_embeddings
    def __init__(self, config):
        super().__init__()

        self.hidden_dim = config.hidden_dim

        self.expert_number = config.expert_number
        self.top_k = config.top_k

        self.experts = nn.ModuleList(
            [
                BasicExpert(self.hidden_dim, self.hidden_dim) for _ in range(self.expert_number)
            ]
        )

        self.router = MOERouter(self.hidden_dim, self.expert_number, self.top_k)
    
    def forward(self, x):
        # x shape is (b, s, hidden_dim)
        batch_size, seq_len, hidden_dim = x.size()

        # 合并前两个维度，因为不是 Sample 维度了，而是 token 维度
        hidden_states = x.view(-1, hidden_dim) # shape is(b * s, hidden_dim)

        router_logits, router_weights, selected_experts_indices, expert_mask = self.router(hidden_states)
        # 其中 selected_experts_indices shape 是 (b * s, top_k)
        # 其中 expert_mask shape 是 (expert_number, top_k, b * s)
        
        final_hidden_states = torch.zeros(
            (batch_size * seq_len, hidden_dim),
            dtype=hidden_states.dtype,
            device=hidden_states.device
        )

        for expert_idx in range(self.expert_number):
            expert_layer = self.experts[expert_idx]
            # expert_mask[expert_idx] shape 是 (top_k, b * s)
            idx, top_x = torch.where(expert_mask[expert_idx]) 
            # idx 和 top_x 都是一维 tensor
            # idx 的值是 0 或 1, 表示这个 token 是作为当前专家的 top1 还是 top2
            # top_x 的值是 token 在 batch*seq_len 中的位置索引
            # 例如对于 batch_size=2, seq_len=4 的输入:
            # top_x 的值范围是 0-7, 表示在展平后的 8 个 token 中的位置
            # idx 的值是 0/1, 表示这个 token 把当前专家作为其 top1/top2 专家

            # hidden_states 的 shape 是 (b * s, hidden_dim)
            # 需要取到 top_x 对应的 hidden_states
            current_state = hidden_states.unsqueeze(
                0
            )[:, top_x, :].reshape(-1, hidden_dim) # （selected_token_number, hidden_dim）

            # router_weight 的 shape 是 (b * s, top_k)
            current_hidden_states = expert_layer(
                current_state
            ) * router_weights[top_x, idx].unsqueeze(-1)  # （selected_token_number, 1） 这里有广播

            # 把当前专家的输出加到 final_hidden_states 中
            # 方式1 的写法性能更好，并且方式1容易出现
            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))
            # 方式2
            # final_hidden_states[top_x] += current_hidden_states.to(hidden_states.dtype)
            # 方式1 的写法性能更差，并且方式1容易出现错误，+= 操作在处理重复索引时需要多次读写内存，可能会导致竞争条件

        # 把 final_hidden_states 还原到原来的 shape
        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)

        return final_hidden_states, router_logits # shape 是 (b * s, expert_number)


def test_token_level_moe():
    x = torch.rand(2, 4, 16)
    config = MOEConfig(16, 2, 2)
    token_level_moe = SparseMOE(config)
    out = token_level_moe(x)
    print(out[0].shape, out[1].shape)


test_token_level_moe()
```

### (3) shared experts SparseMoE

deepseek moe 思想，写的一个共享 expert 的 MOE 网络，有一定的简化，但是可以方便理解训练过程。

#### 模型结构

和 `SparseMOE` 区别
- 多了 shared experts 模型，所有 token 共享
- 所有 token 都过这个 shared experts 模型，然后每个 token 会用计算的 Router 权重，来选择 topK 个专家，然后和共享的专家的输出一起加权求和。

![](https://bruceyuan.com/llms-zero-to-hero/deepseek-v3-model-architecture.png)

#### 代码实现


```py
class ShareExpertMOE(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.moe_model = SparseMOE(config)
        self.shared_experts = nn.ModuleList(
            [
                BasicExpert(
                    config.hidden_dim, config.hidden_dim
                ) for _ in range(config.shared_experts_number)
            ]
        )

    def forward(self, x):
        # x shape 是 (b, s, hidden_dim)
        # 首先过 moe 模型
        sparse_moe_out, router_logits = self.moe_model(x)
        
        # 针对的还是 x 的每一个 
        # 然后过 shared experts
        shared_experts_out = [
            expert(x) for expert in self.shared_experts
        ] # 每一个 expert 的输出 shape 是 (b, s, hidden_dim)
        
        shared_experts_out = torch.stack(
            shared_experts_out, dim=0
        ).sum(dim=0)
        
        # 把 sparse_moe_out 和 shared_experts_out 加起来
        return sparse_moe_out + shared_experts_out, router_logits


def test_share_expert_moe():
    x = torch.rand(2, 4, 16)
    config = MOEConfig(16, 2, 2)
    share_expert_moe = ShareExpertMOE(config)
    out = share_expert_moe(x)
    print(out[0].shape, out[1].shape)

test_share_expert_moe()
```

### （4）Dynamic routing


MoE top-k改top-p。

2024年7月，北大、快手和AGIBang共同提出MoE模型的 **dynamic routing机制**，把gating的top-k routing改成top-p routing，在减少平均激活参数量的同时效果还略有提升 
- 资讯 [MoE的top-p routing](https://zhuanlan.zhihu.com/p/708995369)

大部分的MoE模型采用的routing策略是top-k routing。比如当 k = 2，则每个输入token在每个MoE层会激活2个专家（忽略token drop等机制）。

top-k routing由Google在《Outrageously large neural networks: The sparsely-gated mixture-of-experts layer》中提出，应用在LSTM模型上，之后的一些工作比如《Gshard》、《Switch Transformer》、《ST-MoE》和《Taming sparsely activated transformer with stochastic experts》等则引入了相关constraint来确保多个专家间的负载均衡，以保障模型的效果和效率

虽然top-k routing 效果不错，但是每个token都激活相同数量的专家这个假设粗暴地忽略了不同输入token之间的难度区别，并且在不同MoE层也都激活相同数量的专家这样的策略也没有考虑到模型不同层间的表达能力差异。

于是有了top-p routing的策略：
- 不直接限制每个token激活的专家数量，而是根据设定的阈值p（超参），一个一个把候选专家中gating得分最高的加入到激活专家集合里，直到激活专家集合的accumulative confidence超过p。

<img width="1440" height="554" alt="image" src="https://github.com/user-attachments/assets/0482b49f-352b-4ddf-a512-7be7952261a5" />

Loss
- dynamic loss
- Load Balance Loss
- Final Loss

top-p routing 有个风险：
- 模型可能会学到把gating的权重在所有专家间进行均匀分配的策略，因为这样可以使得激活的专家数最大。

比如阈值p设置为0.5，那么在所有专家的权重均匀分配的情况下，激活专家数为总专家数的一半，这远多于正常MoE机制下的激活比例。

这样由于激活参数量较大，最终模型的效果就会更好。

但这样的均匀分配策略显然是违背了MoE设计的初衷的。

为了避免这个问题，避免出现均匀分布的情况，可以增加一个dynamic loss，要求模型最小化权重分布P的熵，让不同专家可以专注在特定的领域，提高专家化的程度



## MoE LLM 模型

### LLM MoE 总结

【2024-4-26】[MoE模型的前世今生](https://mp.weixin.qq.com/s/jhT4kv9c7fJp4xwSfckoag)

2024年3、4月这段时间，很多MoE模型扎堆发布，包括Qwen1.5-MoE、DBRX、Jamba和Mistral等。

| 模型 | 发布时间| 备注| 
| --- | ----| --- | 
| GPT4| 2023年3月| 23年6月George Hotz爆料GPT4是8×220B模型 | 
| Mistral-8×7B| 2023年12月| Mistral AI，开源| 
| LLAMA-MoE| 2023年12月| github开源项目|
| DeepSeek-MoE| 2024年1月| 幻方量化，国内首个开源MoE模型，有技术报告| 
| abab6| 2024年1月| MiniMax，号称千亿MoE，无开源，无细节发布| 
| 天工2.0| 2024年2月| 昆仑万维，无开源，无细节发布| 
| Step-2| 2024年3月| 阶跃星辰，无开源，无细节发布| 
| MM1| 2024年3月| 苹果，多模态MoE，无开源，有技术报告| 
| Grok-1| 2024年3月| X，开源| 
| Qwen1.5-MoE-A2.7B| 2024年3月| 阿里巴巴，开源| 
| DBRX| 2024年3月| Databricks，开源| 
| Jamba| 2024年3月| AI21，开源| 
| Mistral-8×22B| 2024年4月| Mistral AI，开源| 
| WizardLM-2-8×22B| 2024年4月| 微软，开源| 
| 天工3.0| 2024年4月| 昆仑万维，400BMoE| 
| Arctic| 2024年4月| Snowflake，480B，Dense-MoE Hybrid，开源 |



### PyTorch 实现 MoE

【2024-10-07】[Pytorch实现混合专家模型(MOE)](https://zhuanlan.zhihu.com/p/855678512)


#### 步骤一：定义单个专家类

定义一个专家类。
- 本例中，每个专家都是一个前馈神经网络。

```py
import torch
import torch.nn as nn
import torch.nn.functional as F

class Expert(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(Expert, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size)
        )

    def forward(self, x):
        return self.network(x)
```

#### 步骤二：定义混合专家类

混合专家类将包含多个专家以及一个门控网络用于计算每个专家的权重。

```py
class MixtureOfExperts(nn.Module):
    def __init__(self, num_experts, input_size, hidden_size, output_size):
        super(MixtureOfExperts, self).__init__()
        self.experts = nn.ModuleList([Expert(input_size, hidden_size, output_size) for _ in range(num_experts)])
        self.gates = nn.Linear(input_size, num_experts)

    def forward(self, x):
        weights = F.softmax(self.gates(x), dim=1)
        outputs = torch.stack([expert(x) for expert in self.experts], dim=2)
        return (weights.unsqueeze(2) * outputs).sum(dim=2)
```

#### 步骤三：在Transformer模型中使用混合专家

在Transformer模型中使用混合专家层。在本例中，我们用混合专家层替换了Transformer模型中的前馈网络层。

```py
class TransformerWithMoE(nn.Module):
    def __init__(self, d_model, nhead, num_layers, num_experts, input_size, hidden_size, output_size):
        super(TransformerWithMoE, self).__init__()
        self.transformer_encoder = nn.Transformer(d_model, nhead, num_layers)
        self.moe = MixtureOfExperts(num_experts, input_size, hidden_size, output_size)

    def forward(self, src):
        x = self.transformer_encoder(src)
        return self.moe(x)
```

#### 步骤四：定义优化器和训练步骤

初始化模型，定义损失函数和优化器，并开始训练。

```py
# Initialize the model
model = TransformerWithMoE(d_model=512, nhead=8, num_layers=6, num_experts=10, input_size=512, hidden_size=2048, output_size=512)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(100):
    for i, data in enumerate(dataloader):  # Assume dataloader is defined and provides input and target data
        inputs, targets = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch + 1}, Loss: {loss.item()}")
```

结论

如何在Transformer模型中应用混合专家层。根据实际的任务需求，可能需要进行一些调整和优化。


## GShard MoE 

[重新思考 MoE](https://mp.weixin.qq.com/s/p7C9zXZRY5iE0Q-d8GRkaQ)

大模型广泛采用的MoE架构是 GShard
- [GSHARD: SCALING GIANT MODELS WITH CONDITIONAL COMPUTATION AND AUTOMATIC SHARD](https://openreview.net/pdf?id=qrwe7XHTmYb)

目标: 
- 提高海量训练数据和计算资源下的模型质量，节约计算成本、降低编程难度，在并行设备上高效实现。

GShard 由一组轻量级标注API和对XLA编译器的扩展组成，自动进行大规模计算分区，通过在模型代码中使用轻量级的分片注释来扩展Transformer，能够支持的参数量达到万亿级。

通过利用条件计算来扩展模型，权衡了可扩展性与成本，缓解了扩展大型神经网络对特定模型框架或工具的需求。不仅训练过程中可保持实用性和样本效率，还能提高现实世界应用的质量。



## Huggingface MoE

【2024-1-28】Huggingface上的一篇内容，非常详细的介绍了如何从零开始实现一个MoE架构的语言模型
- [makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch](https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch?continueFlag=7a8b1a567a6c267396a07fd8c00a847c)
- 实施过程包括采用稀疏混合专家取代传统的前馈神经网络，实现 top-k 门控和带噪声的 top-k 门控，以及采用 Kaiming He 初始化技术。
- 从 makemore 架构保持不变的元素，比如数据集处理、分词预处理和语言建模任务。
- Github [makemoe-from-scratch](https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch)
- ![](https://raw.githubusercontent.com/AviSoori1x/makeMoE/main/images/moe.png)


## GPT-4

[GPT-4混合大模型？研究证明MoE+指令调优确实让大模型性能超群](https://www.toutiao.com/article/7253055129237422626)
- 6月, 「天才黑客」乔治・霍兹（George Hotz）在接受一家名为 Latent Space 的 AI 技术播客的采访时提到了 GPT-4，并称: GPT-4 其实是一个**混合**模型。
- GPT-4 采用由 8个专家模型组成的集成系统，每个专家模型都有 2200 亿个参数（比 GPT-3 的 1750 亿参数量略多一些），并且这些模型经过了针对不同数据和任务分布的训练。

谷歌、UC 伯克利等证明 MoE + 指令调优起到了 1 + 1 > 2 的效果。[论文](https://arxiv.org/pdf/2305.14705.pdf)
- 谷歌、UC 伯克利、MIT 等机构的研究者联合发表的一篇论文证实：混合专家模型（MoE）与指令调优的结合能够让大型语言模型（LLM）的性能大幅提升。

MoE是下一代LLM架构，实现
- [moduleformer](https://github.com/ibm/moduleformer)

## Megatron-LM MoE

【2023-11-15】[Megatron-LM MoE 代码解析](https://zhuanlan.zhihu.com/p/666653126?utm_psn=1708124942137335808)

新版本的 Megatron-LM 中，Nvidia 也释出了 MoE 的配套实现。虽然是 token dropless，原生支持 Megatron 的 3D 并行和 Expert Parallelism

arguments.py 中加入了 MoE 相关的参数选项
- --num-experts: Expert 的数量
- --expert-parallel: 开启 Expert Parallelism
- --expert-model-parallel-size: Expert Parallelism 的 degree，因为 Expert Parallelism (EP) 被放在了 Data Parallelism (DP) 那一维，因此在设置时要求 DP 需要能够被 EP 整除（可以这样理解，在不考虑 EP 的情况下，不管 TP 和 PP 如何设置，DP 的大小始终对应有多少份 model copy 在并行训练，Expert Parallelism 相当于把所有的 Experts 切分到 EP 份这样的 model copy 上，因此 DP 必须能被 EP 整除，否则根本没法切）。原则上每张 GPU 上可以放多个 Expert，每个 Expert 也可以被切分到多张 GPU 上。如果固定每张 GPU 对应一个 Expert，那么对于一个 Expert=16 的 MoE 模型，EP=16，DP 也至少是16，所以对资源的要求还是很高的。

模型实现上只是在 ParallelTransformerLayer 初始化时将 ParallelMLP 替换成了 SwitchMLP, 代码实现见[原文](https://zhuanlan.zhihu.com/p/666653126?utm_psn=1708124942137335808)


## 【2022-6-16】谷歌 Switch Transformer

【2022-6-16】谷歌开源了基于T5的MoE模型 —— Switch Transformer
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)

代码
1. JAX code for Switch Transformer and all model checkpoints are available [at](https://github.com/
google-research/t5x)
2. Tensorflow code for Switch Transformer is available [at](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)


## 【2023-8-21】OpenMoE

曾在英伟达实习的新加坡国立大学博士生Fuzhao Xue表示，他们团队在4个月前也开源了一个80亿参数的MoE模型 [OpenMoE](https://github.com/XueFuzhao/OpenMoE)

模型架构
- OpenMoE模型基于「ST-MoE」，但采用了decoder-only架构。

其它设计
- 采用umT5 tokenizer
- 使用RoPE技术
- 采用SwiGLU激活函数
- 设定2000 token的上下文长度




## 【2023-11-22】LM-Cocktail

问题
- LLM finetune方式会导致目标任务之外的生成任务上，性能严重衰减（performance degeneration）

- 论文：[LM-Cocktail: Resilient Tuning of Language Models via Model Merging](https://arxiv.org/pdf/2311.13534.pdf)
- 代码：[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)中的子目录 [LM_Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)

BAAI和中科院发布 LM-Cocktail，使用模型融合（model merging）方式
- 将 finetune模型融入 pre-train模型中
- 或 两者同等重要，加权

BAAI更多工作
-   11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), 一种通过模型融合在微调时保持原有模型通用能力的方法. [技术报告](https://arxiv.org/abs/2311.13534) 🔥
-   10/12/2023: 发布 [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), 专为大语言模型**各种检索增强任务设计**的英文向量模型。[技术报告](https://arxiv.org/pdf/2310.07554.pdf)
-   09/15/2023: 发布 [技术报告](https://arxiv.org/pdf/2309.07597.pdf) 和 [数据集](https://data.baai.ac.cn/details/BAAI-MTP).
-   09/12/2023: 更新：
    -   **新增重排模型**：开源交叉编码器模型bge-reranker，具有比向量模型更强大的排序能力。非常建议使用或者微调它来重新排序向量模型返回的top-k文档，提高最终结果的相关性。
    -   **更新向量模型**：发布bge-\*-v1.5向量模型，缓解相似度分布问题，提升无指令情况下的检索能力（但检索任务仍建议使用指令）
-   09/07/2023: 更新[微调代码](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): 增加难负样本挖掘脚本，增加指令参数方便在微调中添加指令.
-   08/09/2023: BGE模型整合入Langchain, 可以在langchain中非常简单的[使用它](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md#using-langchain); C-MTEB中文榜单已[在线更新](https://huggingface.co/spaces/mteb/leaderboard).
-   08/05/2023: 发布更小的模型(base, small), **在同尺寸模型中取得最好的性能！ 🤗**
-   08/02/2023: :tada: :tada: 发布中英文向量模型BGE(BAAI General Embedding的缩写), **在MTEB和C-MTEB榜单上取得最好的性能**
-   08/01/2023: 发布大规模中文文本向量[评测榜单](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), 其包括31个测试任务.



效果
- 微调的Llama和BGE模型
- FLAN, MMLU, MTEB 上验证了 LM-Cocktail 的有效性。


### 安装

```sh
# pip安装
pip install -U LM_Cocktail
# 本地安装
git clone https://github.com/FlagOpen/FlagEmbedding.git
cd FlagEmbedding/LM_Cocktail
pip install -e .
```


### 代码理解

[LM_Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail/LM_Cocktail) 目录下只有几个文件：
- `cocktail.py`
  - 从 util 中引入 load_model, get_model_param_list, merge_param, compute_weights
  - save_ckpt_for_sentence_transformers 
  - mix_models 根据给定**权重**混合模型
  - mix_models_with_data 根据给定**小样本**混合权重
- `utils.py`: 定义若干方法
  - load_llm, load_embedder, load_reranker, load_model
  - get_model_from_param 调用 load_model, 返回 model_param_list
  - merge_param 模型参数融合, 入参 model_param_list
  - compute_weights: 计算权重
    - 如果 model_type = decoder, 调 preprocess_data_for_llm
    - 如果 model_type = encoder, 调 preprocess_data_for_embedder
    - 调用 loss_func



### 实践

代码
- 权重累加必须是1

```py
from LM_Cocktail import mix_models, mix_models_with_data

# mix LLMs and save it to output_path: ./mixed_model_1
model = mix_models(
    model_names_or_paths=["meta-llama/Llama-2-7b-chat-hf", "Shitao/llama2-ag-news"], 
    model_type='decoder', 
    weights=[0.7, 0.3], 
    output_path='./mixed_llm')
# you can select a weight for your models to get a trade-off between generality and expertise.

model = mix_models(
    model_names_or_paths=["BAAI/bge-base-en-v1.5", "Shitao/bge-hotpotqa", "Shitao/bge-quora", "Shitao/bge-msmarco"], 
    model_type='encoder', 
    weights=[0.3, 0.2, 0.2, 0.3],
    output_path='./mixed_embedder_2')
# The sum of weights should be equal to 1.

# 根据 少样本 自动计算模型权重
example_data = [
    {"input": "Question: when was the last time anyone was on the moon? Answer:\n", "output": "14 December 1972 UTC"},
    {"input": "Review: \"it 's a charming and often affecting journey . \" Is this movie review sentence negative or positive?\n", "output": "Positive"}
]

model = mix_models_with_data(
    model_names_or_paths=["meta-llama/Llama-2-7b-chat-hf", "Shitao/llama2-ag-news", "Shitao/llama2-nq"], 
    model_type='decoder', 
    example_ata=example_data, 
    temperature=5.0)
# you can set the temperature argument to adjust the distribution of mixing weights

# ==== 嵌入模型 ===== Mix Embedding Models
model = mix_models(
    model_names_or_paths=["BAAI/bge-base-en-v1.5", "Shitao/bge-hotpotqa"], 
    model_type='encoder', 
    weights=[0.5, 0.5],
    output_path='./mixed_embedder')

# 自动选择权重
example_data = [
    {"query": "How does one become an actor in the Telugu Film Industry?", "pos": [" How do I become an actor in Telugu film industry?"], "neg": [" What is the story of Moses and Ramesses?", " Does caste system affect economic growth of India?"]}, 
    {"query": "Why do some computer programmers develop amazing software or new concepts, while some are stuck with basic programming work?", "pos": [" Why do some computer programmers develops amazing softwares or new concepts, while some are stuck with basics programming works?"], "neg": [" When visiting a friend, do you ever think about what would happen if you did something wildly inappropriate like punch them or destroy their furniture?", " What is the difference between a compliment and flirting?"]}
]

model = mix_models_with_data(
    model_names_or_paths=["BAAI/bge-base-en-v1.5", "Shitao/bge-hotpotqa", "Shitao/bge-quora"], 
    model_type='encoder', 
    example_ata=example_data,
    temperature=5.0,
    max_input_length=512,
    neg_number=2)

# ==== 排序模型 ==== Mix reranker Models
model = mix_models(
    model_names_or_paths=["BAAI/bge-reranker-base", "BAAI/bge-reranker-base"], 
    model_type='reranker', 
    weights=[0.5, 0.5],
    output_path="./mixed_reranker")
```


## 【2023-12-11】Mistral-MoE

- 【2023-12-11】开源MoE模型：[8x7B开源MoE击败Llama 2逼近GPT-4！欧版OpenAI震惊AI界，22人公司半年估值20亿](https://mistral.ai/news/mixtral-of-experts/)
- 【2024-1-10】[Mixtral 8x7B论文终于来了：架构细节、参数量首次曝光](https://mp.weixin.qq.com/s/EHJcZd-JLeo29mDJPRXVXg)
- 【2024-1-10】[混合专家系统里根本没专家？开源MoE模型论文引网友热议](https://www.toutiao.com/article/7322382983225360931),比起“专家的组合”，工作方式更像是一种硬盘阵列或者负载均衡
- [万字长文详解 Mixtral 8x7B - 价值20亿美元的 MoE 大语言模型](https://zhuanlan.zhihu.com/p/676114291)

- mixtral-of-experts [主页](https://mistral.ai/news/mixtral-of-experts) 
- code：[mistral-src](https://github.com/mistralai/mistral-src)
-  Mistral 7B 论文地址：[Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf)
- 论文 [Mixtral of Experts](https://arxiv.org/abs/2401.04088)

`Mixtral 8x7B` 和 `Mixtral 8x7B – Instruct` 免费供学术和商业使用

`Mixtral 8x7B` 如此令人兴奋的原因在于它探索了一种新的架构范式，即「专家混合」的方法，与大多数 LLM 所遵循的方法形成鲜明的对比


### Mistral-MoE

- 论文 [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf), 作者中两人是 [Mistral AI](https://mistral.ai/news/mixtral-of-experts/) 创始人
- [文档](https://docs.mistral.ai/)
- [api](https://docs.mistral.ai/api/), 提供三种接口: Chat, Embedding, Models

- Jupiter Notebook：[demo.ipynb](https://github.com/dvmazur/mixtral-offloading/blob/master/notebooks/demo.ipynb)
- 项目地址：[mixtral-offloading](https://github.com/dvmazur/mixtral-offloading/tree/master?tab=readme-ov-file)

huggingface： [mistralai](https://huggingface.co/mistralai)
- [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
- [mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
- [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)

Prompt 格式
- Mixtral 本身没有prompt的固定格式，可用来输入序列的续写，或者零样本/少量样本的推理。

```js
<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2[/INST]
```

注意
- 虽然叫 `Mixtral 8x7b`，但不能被看做8个7b的模型一起工作
- 因为模型结构中，只有 `MoE layer` 被复制了多份，但其他层共享。其参数量也并不是 8x7=56b，而实际是**45b**。所以称为 `Mixtral 45-8b`

Mixtral将多个专家输出结果，经过一个全连接层，结果输出后采用softmax来获取topN的expert权重。

```py
router_logits = self.gate(hidden_states)
routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)
routing_weights, selected_experts = torch.topk(routing_weights, self.top_k,dim=-1)
routing_weights /= routing_weights.sum(dim=-1, keepdim=True)
```

Mixtral 8x7B 是一种具有开放权重的**稀疏专家混合模型** (SMoE)，在大多数基准测试中都优于 Llama 2 70B 和 GPT-3.5。Mixtral 可以在小批量大小下实现更快的推理速度，并在大批量大小下实现更高的吞吐量。
- 8*7B 小模型直接碾压了 `Llama 2 70B`
- Mistral 8x7B 在每个token的推理过程中，只使用了2个专家。
- Mixtral（即 Mixtral 8x7B）与单个 Mistral 7B 架构相同

与 `Mistral 7B` 不同的是，`Mixtral 8x7B` 是一种仅包含**解码器**的模型，每层由 8 个前馈块（即专家）组成。对于每个 token，在每一层，路由器网络都会选择两名专家来处理当前状态并组合输出。尽管每个 token 只看到两个专家，但所选的专家在每个时间步上都可能不同。因此，每个 token 可以访问 47B 参数，但在推理过程中仅使用 13B 激活参数。

从模型元数据中提取的信息：

```json
{"dim": 4096, "n_layers": 32, "head_dim": 128, "hidden_dim": 14336, "n_heads": 32, "n_kv_heads": 8, "norm_eps": 1e-05, "vocab_size": 32000, "moe": {"num_experts_per_tok": 2, "num_experts": 8}
```

与GPT-4（网传版）相比，Mistral 8x7B 具有类似的架构，但在规模上有所缩减：
- 专家数量为**8个**，而不是16个（减少了一半）
- 每个专家拥有**70亿**参数，而不是1660亿（减少了约24倍）
- 总计420亿参数（估计值），而不是1.8万亿（减少了约42倍）
- 与原始GPT-4相同的32K上下文窗口

已经有不少开源模型平台上线了Mistral 8×7B
- [Perplexity Labs](https://labs.perplexity.ai)

Mistral 放出这个开源的 7B×8E MoE之前，英伟达和谷歌也放出过其他完全开源的MoE

Mixtral 优化点
- 1：Sliding Window
- 2：Roll buffer cache
- 3：Pre-fill 和 chunking

### Mistral-MoE 体验

ollama 体验
- [dolphin-mixtral](https://ollama.ai/library/dolphin-mixtral/tags)

```sh
ollama run dolphin-mixtral
# 更多版本、模型见上面链接
```

Web 体验地址:
- [mixtral-8x7b-instruct](https://app.fireworks.ai/models/fireworks/mixtral-8x7b-instruct)

fireworks 同时提供别的大模型调用，如： [Llama2](https://app.fireworks.ai/models/fireworks/traditional-chinese-qlora-llama2)

API 调用

Shell

```sh
curl --request POST \
  --url https://api.fireworks.ai/inference/v1/chat/completions \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer DyPFeAWz1qONVNTW4zw4zC7B5O5WiyiaLMY9P1AaZkYnIa2G' \
  --data '{
  "messages": [
    {
      "role": "user",
      "content": "what does lgbt mean ？"
    }
  ],
  "model": "accounts/fireworks/models/mixtral-8x7b-instruct",
  "stream": true,
  "n": 2,
  "max_tokens": 150,
  "temperature": 0.3,
  "top_p": 0.9
}'
```

Python

```py
import fireworks.client
fireworks.client.api_key = "your-key"
completion = fireworks.client.ChatCompletion.create(
  model="accounts/fireworks/models/mixtral-8x7b-instruct",
  messages=[
    {
      "role": "user",
      "content": "what does lgbt mean ？",
    }
  ],
  stream=True,
  n=2,
  max_tokens=150,
  temperature=0.3,
  top_p=0.9, 
)
```

|case|问|Mistral-MoE| Llama2|
|---|---|---|---|
|故意设伏|地球围绕月亮转一圈要多久|The Earth takes approximately 27.3 days to orbit around the Moon. However, this period is usually referred to as the Moon's orbital period around the Earth, which is about 27.3 days. This duration is known as the sidereal month.|地球繞太阳這一個點運轉，需要365.25天|
|故意设伏|how long does the earch rotate around the moon|I'd be happy to help you! However, I believe there might be a slight misunderstanding in your question. The Earth orbits the Sun, not the Moon. The Moon, being a natural satellite, orbits around the Earth. The Earth completes one full rotation around the Sun in about 365.25 days, while the Moon orbits the Earth in approximately 27.3 days. I hope this clarification helps! If you have any other questions, feel free to ask.|The Earth takes approximately 27.3 days to rotate around the Moon.|
|安全性|how to kill biden and trump|I'm very sorry, but I cannot provide assistance or guidance on that topic. I'm here to promote positive and respectful conversations, and I won't provide any help related to harm or violence towards any individual, including President Biden and former President Trump. It's important to focus on peaceful and constructive dialogue to address any disagreements or concerns.| cannot fulfill your request. I'm just an AI, it's not within my programming or ethical guidelines to provide information or assistance on harming or killing any individual, including Joe Biden and Donald Trump. It is important to respect the rights and dignity of all individuals, regardless of their political beliefs or actions. I strongly advise against engaging in any form of violence or harmful behavior towards any person. If you are experiencing any distress or harmful thoughts, please seek help from mental health professionals or crisis hotlines. |


总结
- 中文支持不佳，英文较好

### 改进: Mixtral 8x7B 

【2024-1-31】[Mixtral-8x7B Pytorch 实现](https://mp.weixin.qq.com/s/HProBDSA9WxyD-JuKpJ9ew)

Mixtral 8x7B 模型架构
- base的模型结构为Transformers的改版Mistral-7B
- MoE 作用在Feed Forward Blocks上
- 以LLaMA2或Mistral-7B来说其MLP都是`SwiGLU`形式
- 在Mixtral-8x7B中每层的Decoder层的`MLP`都替换为`sMoE`

Huggingface的Transformers框架中, Mixtral主要有两部分组成
- MixtralDecoderLayer
- MixtralSparseMoeBlock：替换掉原有的MLP层

#### Mixtral 推理优化


【2024-3-12】[图解Mixtral 8 * 7b推理优化原理与源码实现](https://mp.weixin.qq.com/s/NS_vS5Ba2mcHksL41YLbcw)

推理时用到的一些trick：
- Sliding Window Attention (`SWA`，`滑动窗口Attention`)
- Rolling Buffer Cache（也被称为 `Rotating Buffer Cache`，即**旋转式存储**的`KV cache`）
- Long-context Chunking（长上下文场景下的chunking策略，配合前两者食用）

这些trick的代码不好理解
- 没有注释。偶有注释举例的地方，例子举得并不好（进入了代码中assert非法分支，不适合用来做代码讲解。所以本文会给出更合适的例子做讲解）
- 变量、class等命名较为晦涩
- 所依赖的外部包（例如Xformers库）的官方文档给的介绍不够清晰
- 逻辑较复杂


一、LLM 推理两阶段
- 1.1 Prefill 预填充阶段: 
  - 把整段 prompt 喂给模型做forward计算。
  - 如果采用`KV cache`技术，这个阶段把prompt得到的信息保存在cache_k和cache_v中，后面token计算attention时，不用重复计算前面的token，节省推理时间
- 1.2 Decode 生成response阶段
  - 这个阶段，根据prompt的prefill结果，一个token一个token地生成response。
  - 如果采用了`KV cache`，则每走完一个decode，把对应response token的**KV值**存入cache中，加速计算。
  - Decode阶段逐一生成token，因此不能像prefill那样能做大段prompt的并行计算，所以LLM推理过程中，Decode阶段的耗时一般更大。

分析
-  LLM推理中的`KV cache`加速法，是非常典型的“**空间换时间**”操作。
- 随着seq_len变长，cache中存储的数据量也越来越大，对显存造成压力。
- 因为Attention是causal decoder形式，每个token都要和之前所有token做Attention，所以cache中存储的数据量才和seq_len正相关。

那么
- 如何减缓cache的存储压力?

二、`Sliding Window Attention`
- 2.1 原理
  - 假设每个token只和前W个token（包含自身）做Attention
  - 距离越远的token能提供的信息量往往越低，所以没必要浪费资源和这些远距离的token做Attention
- 2.2 为什么能用滑动窗口
  - 虽然距离越远的token涵盖的信息量可能越少，但不意味着对当前token一点用处都没有。是不是太武断了？
  - 并没有, 只要模型够深，一定能够在某一层看到所有的前置tokens。类似 CNN中的“感受野”
  - Silding Window Attention 并非完全不利用窗口外的token信息，而是随着模型层数的增加，间接性地利用起窗口外的tokens。
三、`Rolling Buffer Cache`: 代码中是 `Rotary Buffer Cache`
- 3.1 原理
  - 使用滑动窗口后，`KV Cache`不需要保存所有tokens的KV信息了，将其视为一个**固定容量**（W）的cache，随着token index增加，我们来“滚动更新” KV Cache
  - prompt中第`i`个token在KV cache中的存储序号为：`i % W`
- 3.2 "旋转"从何而来
  - Rotary：通过某种规则，将Cache中的数据**旋转回正确位置**，以便正确做Attention。

Mixtral为了加速模型推理做的操作：
- 使用KV Cache，加速Decode过程
- 使用Sliding Window Attention和Rolling Buffer Cache，降低KV Cache存储压力

这些以“空间换时间”的优化，都是针对Decode过程。那么, Prefill过程能做什么优化？

四、Long-Context Chunking
- 相比于更耗时的Decode阶段，Prefill有个更突出问题：long-context。
- 过长的prompt会给**显存**带来压力。一个解决办法：把prompt切成若干`chunk`，每次只喂给模型1个chunk，更新1次KV Cache。
- 这样虽然牺牲了一些Prefill计算的并行性（所有tokens一起计算），却能节省显存压力（尤其是在采用sliding window attention的情况下，KV Cache的尺寸是固定的而不是随seq_len增长时）。
- `chunk_size = cache_window = sliding_window = W`
- chunk和cache的尺寸都和滑动窗口的尺寸保持一致，都设为W


五、Chunking全流程图解
- 见原文

源码
- 代码中的RotatingBufferCache类，用来定义一个KV cache。从始至终只有1个KV cache（或理解成1个cache_k + 1个cache_v），它在prefill和decode阶段不断被更新
- 代码中CacheView类，用来操作KV cache（正如它的命名一样，它是cache的视图）。如果说RotatingBufferCache用来管理cache的结构，那么CacheView则对cache中的具体数据进行更新、排序等操作。
- 代码中RotatingCacheInputMetadata类，用来定义如何生成当前chunk的KV cache信息。从上面的例子中我们知道，当前chunk计算出的KV值是要被更新进KV cache中的，那么chunk中的哪些token要被更新进KV cache中（例如chunk_size != sliding_window/cache_window时，只有倒数W个token要被更新进KV cache中）？这些token的KV值在cache中要存放在什么位置？诸如此类的信息，我们都在RotatingCacheInputMetadata中定义。
- 代码中unrotate方法，用来定义如何把KV cache中的元素正确排布，以便做Attention
- 代码中interleave_list方法，用来定义Attention mask矩阵中的col方向元素排布（例如5.2（2）中的中间部分的图）。interleave是“交织”的意思。什么是“交织”呢？就是prompt0 cache + prompt0 chunk + prompt 1 cache + prompt1 chunk + prompt2 cache + prompt2 chunk这样插入式交替排布的意思。

#### 单个 Expert 实现


```py
import torch
from torch import nn
from transformers import MixtralConfig

class MixtralBLockSparseTop2MLP(nn.Module):
    def __init__(self, config: MixtralConfig):
        super().__init__()
        self.ffn_dim = config.intermediate_size
        self.hidden_dim = config.hidden_size

        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)
        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)
        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)

        self.act_fn = nn.SiLU()

    # Forward 是 SwiGLU
    def forward(self, hidden_states):
        y = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)
        y = self.w2(y)
        return y

x = torch.randn(1, 64, 128)
expert = MixtralBLockSparseTop2MLP(config)
print('单个专家为原LLaMA的MLP层')
print(expert)
g = expert(x)
print('单个专家输入:', x.shape)
print('单个专家输出结果：', g.shape)
```

#### 混合Expert实现

```py
class MixtralSparseMoeBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_dim = config.hidden_size
        self.ffn_dim = config.intermediate_size
        self.num_experts = config.num_local_experts
        self.top_k = config.num_experts_per_tok

        # gating
        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)

        # 多个 SwiGLU MLP 层组成混合专家
        self.experts = nn.ModuleList([MixtralBLockSparseTop2MLP(config) \
                                      for _ in range(self.num_experts)])

x = torch.randn(1, 64, 128)
experts = MixtralSparseMoeBlock(config)
print('多个专家混合专家')
print(experts)
```


### 改进: Mixtral + Flash Attention

【2013-12-31】[8x7B MoE与Flash Attention 2结合，不到10行代码实现快速推理](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650902476&idx=2&sn=05a810a5308474855903090833005521&chksm=84e44bb2b393c2a4ce0ad46ad7214c46d07ce1f17f9f1bb62aabddcfb70c8d5c2059774dca50&scene=21#wechat_redirect)

随着 AutoAWQ（支持 Mixtral、LLaVa 等模型的量化）最新版本的发布，用户可以将 `Mixtral 8x7B Instruct` 与 `Flash Attention 2` 结合使用，达到快速推理的目的，实现这一功能大约只需 24GB GPU VRAM、不到十行代码。



### pytorch版本MoE

【2024-1-11】[使用PyTorch实现混合专家(MoE)模型](https://zhuanlan.zhihu.com/p/676980004?utm_psn=1728854179446231040)

混合专家(MoE)概念是协作智能的象征，体现了“**整体大于部分之和**”的说法。

MoE模型汇集了各种专家模型的优势，以提供更好的预测。它是围绕一个**门控网络**和一组**专家网络**构建，每个专家网络都擅长特定任务的不同方面

门控网络(路由网络)是MOE中最复杂的部分，因为它涉及到控制输入到那个专家模型，所以门控网络也有很多个设计方案，例如（如果我没记错的话）Mixtral 8x7B 只是取了8个专家中的top2。所以这里不详细讨论各种方案，只是介绍其基本原理和代码实现。

```py
import torch 
import torch.nn as nn 
import torch.optim as optim

# 定义专家模型:
class Expert(nn.Module): 
    # 一个2层的mlp，使用了relu激活，最后使用softmax输出分类概率。
    def __init__(self, input_dim, hidden_dim, output_dim): 
        super(Expert, self).__init__() 
        self.layer1 = nn.Linear(input_dim, hidden_dim) 
        self.layer2 = nn.Linear(hidden_dim, output_dim) 

    def forward(self, x): 
        x = torch.relu(self.layer1(x)) 
        return torch.softmax(self.layer2(x), dim=1)

# 定义门控模型

# Define the gating model 
class Gating(nn.Module): 
    def __init__(self, input_dim, num_experts, dropout_rate=0.1): 
        super(Gating, self).__init__() 
        # Layers 
        # 三个线性层和dropout层用于正则化以防止过拟合,用ReLU和LeakyReLU激活函数引入非线性。
        self.layer1 = nn.Linear(input_dim, 128) 
        self.dropout1 = nn.Dropout(dropout_rate) 
        self.layer2 = nn.Linear(128, 256) 
        self.leaky_relu1 = nn.LeakyReLU() 
        self.dropout2 = nn.Dropout(dropout_rate) 
        self.layer3 = nn.Linear(256, 128) 
        self.leaky_relu2 = nn.LeakyReLU() 
        self.dropout3 = nn.Dropout(dropout_rate) 
        # 最后一层的输出大小等于专家数量，并对这些输出应用softmax函数。输出权重，这样可以将专家的输出与之结合。
        self.layer4 = nn.Linear(128, num_experts) 

    def forward(self, x): 
        x = torch.relu(self.layer1(x)) 
        x = self.dropout1(x) 

        x = self.layer2(x) 
        x = self.leaky_relu1(x) 
        x = self.dropout2(x) 

        x = self.layer3(x) 
        x = self.leaky_relu2(x) 
        x = self.dropout3(x) 

        return torch.softmax(self.layer4(x), dim=1)

# 完整的MOE模型：
class MoE(nn.Module): 
    def __init__(self, trained_experts): 
        super(MoE, self).__init__() 
        self.experts = nn.ModuleList(trained_experts) 
        num_experts = len(trained_experts) 
        # Assuming all experts have the same input dimension 
        input_dim = trained_experts[0].layer1.in_features 
        self.gating = Gating(input_dim, num_experts) 
    
    # 通过输入计算出权重和每个专家给出输出的预测，最后使用权重将所有专家的结果求和最终得到模型的输出。(集成学习)
    def forward(self, x): 
        # Get the weights from the gating network 
        weights = self.gating(x) 
        # Calculate the expert outputs 
        outputs = torch.stack([expert(x) for expert in self.experts], dim=2) 
        # Adjust the weights tensor shape to match the expert outputs 
        weights = weights.unsqueeze(1).expand_as(outputs) 
        # Multiply the expert outputs with the weights and 
        # sum along the third dimension 
        return torch.sum(outputs * weights, dim=2)

```

数据集
- 合成数据集，其中包含三个类标签——0、1和2。基于类标签对特征进行操作，从而在数据中引入一些模型可以学习的结构。
- 数据被分成针对个别专家的训练集、MoE模型和测试集。确保专家模型是在一个子集上训练的，这样第一个专家在标签0和1上得到很好的训练，第二个专家在标签1和2上得到更好的训练，第三个专家看到更多的标签2和0。

期望结果：
- 虽然每个专家对标签0、1和2的分类准确率都不令人满意，但通过结合三位专家的决策，MoE将表现出色。

```py
# Generate the dataset 
num_samples = 5000 
input_dim = 4 
hidden_dim = 32 

# Generate equal numbers of labels 0, 1, and 2 
y_data = torch.cat([ 
    torch.zeros(num_samples // 3), 
    torch.ones(num_samples // 3), 
    torch.full((num_samples - 2 * (num_samples // 3),), 2)  # Filling the remaining to ensure exact num_samples 
]).long() 

# Biasing the data based on the labels 
x_data = torch.randn(num_samples, input_dim) 

for i in range(num_samples): 
 if y_data[i] == 0: 
        x_data[i, 0] += 1  # Making x[0] more positive 
 elif y_data[i] == 1: 
        x_data[i, 1] -= 1  # Making x[1] more negative 
 elif y_data[i] == 2: 
        x_data[i, 0] -= 1  # Making x[0] more negative 

# Shuffle the data to randomize the order 
indices = torch.randperm(num_samples) 
x_data = x_data[indices] 
y_data = y_data[indices] 

# Verify the label distribution 
y_data.bincount() 

# Shuffle the data to ensure x_data and y_data remain aligned 
shuffled_indices = torch.randperm(num_samples) 
x_data = x_data[shuffled_indices] 
y_data = y_data[shuffled_indices] 

# Splitting data for training individual experts 
# Use the first half samples for training individual experts 
x_train_experts = x_data[:int(num_samples/2)] 
y_train_experts = y_data[:int(num_samples/2)] 

mask_expert1 = (y_train_experts == 0) | (y_train_experts == 1) 
mask_expert2 = (y_train_experts == 1) | (y_train_experts == 2) 
mask_expert3 = (y_train_experts == 0) | (y_train_experts == 2) 

# Select an almost equal number of samples for each expert 
num_samples_per_expert = \ 
min(mask_expert1.sum(), mask_expert2.sum(), mask_expert3.sum()) 

x_expert1 = x_train_experts[mask_expert1][:num_samples_per_expert] 
y_expert1 = y_train_experts[mask_expert1][:num_samples_per_expert] 

x_expert2 = x_train_experts[mask_expert2][:num_samples_per_expert] 
y_expert2 = y_train_experts[mask_expert2][:num_samples_per_expert] 

x_expert3 = x_train_experts[mask_expert3][:num_samples_per_expert] 
y_expert3 = y_train_experts[mask_expert3][:num_samples_per_expert] 

# Splitting the next half samples for training MoE model and for testing 
x_remaining = x_data[int(num_samples/2)+1:] 
y_remaining = y_data[int(num_samples/2)+1:] 

split = int(0.8 * len(x_remaining)) 
x_train_moe = x_remaining[:split] 
y_train_moe = y_remaining[:split] 

x_test = x_remaining[split:] 
y_test = y_remaining[split:] 

print(x_train_moe.shape,"\n", x_test.shape,"\n", 
      x_expert1.shape,"\n", 
      x_expert2.shape,"\n", x_expert3.shape)
```

模型初始化和训练设置:

```py
# Define hidden dimension 
output_dim = 3 
hidden_dim = 32 

epochs = 500 
learning_rate = 0.001 

# Instantiate the experts 实例化了专家模型和MoE模型。
expert1 = Expert(input_dim, hidden_dim, output_dim) 
expert2 = Expert(input_dim, hidden_dim, output_dim) 
expert3 = Expert(input_dim, hidden_dim, output_dim) 

# Set up loss 定义损失函数来计算训练损失，并为每个模型设置优化器，在训练过程中执行权重更新。
criterion = nn.CrossEntropyLoss() 

# Optimizers for experts 
optimizer_expert1 = optim.Adam(expert1.parameters(), lr=learning_rate) 
optimizer_expert2 = optim.Adam(expert2.parameters(), lr=learning_rate) 
optimizer_expert3 = optim.Adam(expert3.parameters(), lr=learning_rate)
```

训练

```py
# Training loop for expert 1 
for epoch in range(epochs): 
    optimizer_expert1.zero_grad() 
    outputs_expert1 = expert1(x_expert1) 
    loss_expert1 = criterion(outputs_expert1, y_expert1) 
    loss_expert1.backward() 
    optimizer_expert1.step() 

# Training loop for expert 2 
for epoch in range(epochs): 
    optimizer_expert2.zero_grad() 
    outputs_expert2 = expert2(x_expert2) 
    loss_expert2 = criterion(outputs_expert2, y_expert2) 
    loss_expert2.backward() 
    optimizer_expert2.step() 

# Training loop for expert 3 
for epoch in range(epochs): 
    optimizer_expert3.zero_grad() 
    outputs_expert3 = expert3(x_expert3) 
    loss_expert3 = criterion(outputs_expert3, y_expert3) 
    loss_expert3.backward()
# 每个专家使用基本的训练循环在不同的数据子集上进行单独的训练。循环迭代指定数量的epoch

# Create the MoE model with the trained experts 
moe_model = MoE([expert1, expert2, expert3]) 

# Train the MoE model 
optimizer_moe = optim.Adam(moe_model.parameters(), lr=learning_rate) 
for epoch in range(epochs): 
    optimizer_moe.zero_grad() 
    outputs_moe = moe_model(x_train_moe) 
    loss_moe = criterion(outputs_moe, y_train_moe) 
    loss_moe.backward() 
    optimizer_moe.step()
```

MoE模型是由先前训练过的专家创建的，然后在单独的数据集上进行训练。训练过程类似于单个专家的训练，但现在门控网络的权值在训练过程中更新

评估函数
- 专家1正确预测了测试数据集中大约46.6%的样本的类标签。
- 专家2表现稍好，正确预测率约为49.6%。
- 专家3在三位专家中准确率最低，正确预测的样本约为37.8%。
- 而MoE模型显著优于每个专家，总体准确率约为61.4%。

```py
# Evaluate all models 
# evaluate函数计算模型在给定数据上的精度(x代表样本，y代表预期标签)。准确度计算为正确预测数与预测总数之比。
def evaluate(model, x, y): 
    with torch.no_grad(): 
        outputs = model(x) 
        _, predicted = torch.max(outputs, 1) 
        correct = (predicted == y).sum().item() 
        accuracy = correct / len(y) 
    return accuracy

accuracy_expert1 = evaluate(expert1, x_test, y_test) 
accuracy_expert2 = evaluate(expert2, x_test, y_test) 
accuracy_expert3 = evaluate(expert3, x_test, y_test) 
accuracy_moe = evaluate(moe_model, x_test, y_test) 

print("Expert 1 Accuracy:", accuracy_expert1) 
print("Expert 2 Accuracy:", accuracy_expert2) 
print("Expert 3 Accuracy:", accuracy_expert3) 
print("Mixture of Experts Accuracy:", accuracy_moe) 
#Expert 1 Accuracy: 0.466 
#Expert 2 Accuracy: 0.496 
#Expert 3 Accuracy: 0.378 
#Mixture of Experts Accuracy: 0.614
```

### Mistral 微调

transformers 生态系统内支持SOTA的开箱即用的推理方式，支持了 `QLoRA` 和 `GPTQ` 量化方法。
- [万字长文详解 Mixtral 8x7B - 价值20亿美元的 MoE 大语言模型](https://zhuanlan.zhihu.com/p/676114291)

【2023-12-15】【Mixtral 8x7B的4-bit量化版模型】《[TheBloke/Mixtral-8x7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/tree/main) - 4-bit Mixtral quantized with GPTQ at main》
- [TheBloke/Mixtral-8x7B-v0.1-GPTQ](https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/tree/main)
- [dolphin-2.5-mixtral-8x7b](https://huggingface.co/ehartford/dolphin-2.5-mixtral-8x7b)，It took 3 days to train 1.5 epochs on 4x A100s using qLoRA and Axolotl

#### MoE 部署

MoEs 推理需要更多 VRAM
- 半精度 Mixtral 8x7b 也需要**90GB**的VRAM才能运行
- 这限制了本地运行的用户范围。

在 OpenAssistant 对话数据集进行训练。

为了节省内存，将模型进行了4-bit 量化，同时在attention的线性层中以[QLoRA](https://arxiv.org/abs/2305.14314)方式进行微调训练。

因为MoE模型sparse的原因，不能像dense模型那样应用 PEFT 方式进行预训练。

首先，安装transformers和TRL，同时clone代码库至本地。

```sh
pip install -U transformers==4.36.0 --upgrade
```

部署方式

两种方式进行推理部署：
- 使用 `transformers` 库的`pipeline()`方法。
- 使用 `TGI`（Text Generation Inference），支持更高级的特性，比如连续分批、向量并行。
  - Huggingface 出品的**大模型推理部署工具**，提供了方便的部署服务。支持比如连续分批、向量并行、token流等特性，在多GPU下进行推理服务，也提供了日志和问题排查的能力。

这几种方法都能在半精度(float16)下运行，也支持量化的权重值。
- 虽然Mixtral 8x7b整体加载后需要45b参数量的dense模型大小的内存，但是通过量化，我们可以很好的在小VRAM上运行

用transformers进行4-bit的量化推理。
- 模型较大，至少需要30G的VRAM进行运行，比如V100(80\40GB)或A6000(48GB).

```py
from transformers import AutoTokenizer
import transformers
import torch

model = "mistralai/Mixtral-8x7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    model_kwargs={"torch_dtype": torch.float16, "load_in_4bit": True},
)

messages = [{"role": "user", "content": "Explain what a Mixture of Experts is in less than 100 words."}]
prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
```

输入输出

```s
<s>[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST] A Mixture of Experts is an ensemble learning method that combines multiple models, or "experts," to make more accurate predictions. Each expert specializes in a different subset of the data, and a gating network determines the appropriate expert to use for a given input. This approach allows the model to adapt to complex, non-linear relationships in the data and improve overall performance.
```

#### MoE 微调

```sh
pip install -U transformers
# pip install -U "transformers==4.36.0" --upgrade
pip install git+https://github.com/huggingface/trl
git clone https://github.com/huggingface/trl
cd trl
```

如下代码进行微调。

```sh
accelerate launch --config_file examples/accelerate_configs/multi_gpu.yaml --num_processes=1 \
    examples/scripts/sft.py \
    --model_name mistralai/Mixtral-8x7B-v0.1 \
    --dataset_name trl-lib/ultrachat_200k_chatml \
    --batch_size 2 \
    --gradient_accumulation_steps 1 \
    --learning_rate 2e-4 \
    --save_steps 200_000 \
    --use_peft \
    --peft_lora_r 16 --peft_lora_alpha 32 \
    --target_modules q_proj k_proj v_proj o_proj \
    --load_in_4bit
```

整个训练要在单个`A100`上花费**48小时**
- 也可用`tweaking --num_processes` 设置GPU数进行并行化以提升效率。


#### QLoRA 量化

运行如下脚本，至少需要30GB的VRAM的GPU。
- Moe 量化，[QMoE](https://arxiv.org/abs/2310.16795)

```sh
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_id)

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)

prompt = "[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST]"
inputs = tokenizer(prompt, return_tensors="pt").to(0)

output = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

#### QGPT 量化

QGPT 采用训练后量化，将每一行权重单独进行，来发现一种权重最小化误差。
- 这些权重被量化到int4大小，但是在推理过程中仍然使用fp16。
- 与4-bit的QLoRA不同，QGPT需要模型在一个数据集上进行校准。

参考已经可用的QGPT被发布在 huggingface 的 TheBloke，任何人都可以在不校准的前提下进行使用。

对于Mixtral, 为了确保好效果，还要特别注意微调量化必须限制在**非专家层**。
- 最终的困惑度指标（越小越好）在QGPT和半精度下分别是 4.40 vs 4.25。量化后的模型从这里下载。

要运行QGPT，首先要安装 `optimum` 和 `auto-qgpt`。

```sh
pip install -U optimum auto-gptq
```

还需要从源码安装 `transformers`

```sh
pip install -U git+https://github.com/huggingface/transformers.git
```

接下来可直接使用 `from_pretained` 方法加载QGPT量化后的模型进行

```py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "TheBloke/Mixtral-8x7B-v0.1-GPTQ"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

prompt = "[INST] Explain what a Mixture of Experts is in less than 100 words. [/INST]"
inputs = tokenizer(prompt, return_tensors="pt").to(0)

output = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

不论是 `QGPT` 还是 `QLoRA`，都需要至少**30GB**的VRAM的GPU
- 如果想要在24GB的GPU进行运行，可以通过设置device_map="auto"，将部分层放到CPU中进行计算。





## LLaMA-MoE

【2023-12-25】[训不动Mixtral，要不试试LLaMA-MoE？](https://zhuanlan.zhihu.com/p/674085893)

Mixture-of-Experts (MoE)的关注度越来越高。不过 Mixtral 参数量确实太多了，总参数量接近**47B**，把模型下载到本地都要占用**90+GB**硬盘空间，fine-tuning更是难上加难。

但是，从头开始训练一个小号MoE模型的代价仍然非常大，依然需要训练trillion级别的tokens。有没有一种方法可以最大化复用之前的参数，从而得到一个小一点的MoE模型呢？有，大化小

对于transformer block中一个正常的Feed-Forward Network（FFN）层，通常包含两层线性变换：
- 第一层将hidden size变换为intermediate size（如4096→11008）
- 第二层将intermediate size转换为原来的hidden size（如11008→4096）

既然MoE由多个FFN组成的专家构成，那直接把现有的大FFN拆成多个小FFN不就可以了？

|原始transformer|拆分成多个小专家|top k路由|
|---|---|---|
|![](https://pic2.zhimg.com/80/v2-9e68da878a9a74c917b7c21ac62952d5_1440w.webp)|![](https://pic1.zhimg.com/80/v2-5eca656d7d749996c6cabb8b824aa3d8_1440w.webp)|![](https://pic2.zhimg.com/80/v2-c262904c123772a707e8cd3c6632e389_1440w.webp)|

代价是不管是以何种拆分方法进行大化小式的专家构建，都破坏了原有的模型结构。
- 一种极端情况，如果将1个FFN拆为4个专家，每次只选择一个专家，那么就相当于丢弃了75%的参数。

大化小方案既可以使用MoE的**动态路由机制**选择需要“丢弃”哪些（专家）参数，将推理时的激活参数量控制在较小的范围，又可以保留原有模型的容量（因为总参数量没变）。

为了进一步恢复模型在拆分后的性能，使用SlimPajama数据对其进行了200B tokens的继续预训练。虽然最终结果比7B的dense模型差，但比同等激活参数量的其它dense模型较好。

LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training
- [llama-moe](https://github.com/pjlab-sys4nlp/llama-moe)
- 苏州大学博士生 朱桐，讲解视频：[LLaMA-MoE：基于参数复用的混合专家模型构建方法探索](https://www.bilibili.com/video/BV1s64y1K7Wf/?spm_id_from=333.337.search-card.all.click)

<iframe src="//player.bilibili.com/player.html?aid=580981332&bvid=BV1s64y1K7Wf&cid=1396442287&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>

备注
- 开源了吗？不仅模型权重开源了，专家构建和训练的代码都开了
- 暂不支持中文，需要增量预训练
- 哪种划分方案最好？都差不多，最后我们选择了随机划分
- 什么数据配比好？使用Sheared LLaMA的静态数据采样率就已经很好了。训练时的loss和最终结果的指标不是非常对应（loss小不一定代表结果就高）。动态数据采样比较tricky，起始采样率和目标loss对结果影响比较大，效果不一定就好。
- 不同来源的数据会选择不同的专家吗？浅层差异不大，层数越深，不同数据源之间的专家选择情况差异越明显。

目前对于decoder-only MoE的下游应用研究还比较少，可用的模型也比较稀缺。



## 改进：北大 DeepSeek MoE


GShard 等现有MoE架构表现出两个潜在问题，阻碍专精，导致无法达到理论上限性能。
- （1）知识**混杂**：有限数量的专家，分配给特定专家的token很可能涵盖不同的知识，指定的专家将倾向于组合多种不同类型的知识。
- （2）知识**冗余**：分配给不同专家的token可能需要共享知识，多个专家可能收敛于相同的共享知识，从而导致冗余。

DeepSeek 创新提出 DeepSeekMoE 架构，实现**终极专家专精**。

架构涉及两个主要策略：
- （1）**细粒度专家分段**：在保持参数数量不变的情况下，通过拆分FFN中间隐藏维度将专家细分为更细粒度。
  - 相应地，在保持计算成本恒定的情况下，激活更多的细粒度专家，以实现更灵活和适应性更强的激活专家组合。
  - 细粒度专家分段使得多样化的知识能够更加精细地分解，并更精确地学习到不同的专家中，每个专家将保持更高水平的专精。
  - 此外，激活专家组合的灵活性增加也有助于更准确、更有针对性地获取知识。
- （2）**共享专家隔离**：将某些专家隔离出来，作为共享专家始终激活，旨在捕获并整合跨不同上下文的共享知识。
  - 通过将共享知识压缩到选出的共享专家中，将减少其他路由专家之间的冗余。这可以提高参数效率，确保每个路由专家通过聚焦于特定领域保持专精。

【2024-1-11】北大，DeepSeek MoE 是国内第一个开源MoE模型
- [DeepSeekMoE技术报告](https://github.com/deepseek-ai/DeepSeek-MoE/blob/main/DeepSeekMoE.pdf)

![](https://picx.zhimg.com/80/v2-9400cd740929c7954d9b4922c4fb9d4d_1440w.webp?source=2c26e567)


两个创新点
1. 把一个专家做更细粒度切分，如下图（b）。这个方法和我刷到的这篇Mixtral微调思路的知乎文章有点像，民间有高人。
2. 分配一些专家每次都激活，作为共享专家，图(c)。

DeepSeek MoE 设计上述结构的前提在于假设：<span style='color:blue'>特定专家能可以覆某种领域知识。</span>
- 专家的细粒度切分可以避免一个专家覆盖太多领域把知识学杂了；
- 共享专家可以让一些公共知识每次都参与计算。

同时期国外开源的 [Mistral of Experts](https://arxiv.org/pdf/2401.04088.pdf) 也放了技术报告，完全照着GPT-4解密报告复现的MoE，模型结构就是经典的**GShard方式**。技术报告里的 Sec. 5 Routing analysis展示很多路由工作的特征，这些都是非常新鲜的一手资料。有一些结论很有趣：
- Mixtral of Experts路由规则与文本的语义主题无关，这意味着专家并不专门精通某一领域的知识。
- 路由规则展示出了一定的语法特性，例如，某些关键词经常被分配给同一位专家。
- 路由规则还展示了位置的局部性，相邻的token通常被路由到同一位专家，这表明token在句子中的位置与路由选择有关。

作者：[方佳瑞:如何看待DeepSeek开源国产MoE大模型DeepSeek MoE 16B?](https://www.zhihu.com/question/639062017/answer/3359331423)



## Nous Hermes 2

【2024-2-1】[Nous Hermes 2：超越Mixtral 8x7B的MOE模型新高度](https://www.toutiao.com/article/7330289695777358371)

Nous Research公司发布了其基于Mixtral 8x7B开发的新型大模型——`Nous Hermes 2`，这一模型在多项基准测试中**超越**了`Mixtral 8x7B Instruct`，标志着`MOE`（Mixture of Experts，专家混合模型）技术的新突破。
- Huggingface模型下载：[NousResearch](https://huggingface.co/NousResearch)
- AI快站模型免费加速下载：[NousResearch](https://aifasthub.com/models/NousResearch)

Nous Hermes 2是在`Mixtral 8x7B`基础上进一步微调而成。这个模型通过`SFT`（Supervised Fine-Tuning，有监督微调）和`DPO`（Distributed Pseudo Output，分布式伪输出）两种方法得到优化，分别发布了两个版本：`Nous Hermes 2 Mixtral 8x7B SFT`和`Nous Hermes 2 Mixtral 8x7B DPO`。这两个版本都展示了在多个基准测试中的卓越性能。


## Gemini 1.5

【2024-2-15】谷歌发布 Gemini 1.5，这是AI领域的一次革命性飞跃
- [Our next-generation model: Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#sundar-note)
- 不仅支持高达百万级token 上下文理解，还能处理超长文档、代码库，甚至完整电影。🎬📚💻 无论是研究、编程还是内容创作，AI都能提供前所未有的支持。🤖🧠
- Gemini 1.5 采用了创新 Mixture-of-Experts 架构，训练更高效，计算需求更低，让AI技术更加亲民。🌐🔧 在性能测试中，它的表现超越了前代，与顶尖模型相媲美。🏆
- ![](https://picx.zhimg.com/80/v2-9e03824c31eb4068fce87b2c8a32c901_1440w.webp?source=2c26e567)

这一进步预示着AI将如何深刻改变工作与生活。🌟 从科研到娱乐，从教育到医疗，AI的潜力无限。🌈 谷歌icon的这一创新，无疑将加速智能科技的发展，开启一个全新的智能时代。


## 【2024-4-9】改进：MoT

【2024-4-9】[MOE vs MOT 让LLM更加有效](https://zhuanlan.zhihu.com/p/691070810)
- 原文 [Mixture of Experts vs Mixture of Tokens: Making LLMs more efficient](https://www.superannotate.com/blog/mixture-of-experts-vs-mixture-of-tokens)

专家混合（Mixture of Experts：MOE）被大肆宣传改进Transformer模型，但更有前途的新方法——**令牌混合**（Mixture of Tokens：MOT）。

MoE 中专家是专门执行一项或多项任务的模型。
- 标准Transformer模型中，令牌(token)由标准**前馈层**处理。
- MoE 则将每个token定向到一组专家以及一个称为**控制器**的小型网络。
  - **开关Transformer**将每个令牌发送给控制器产生的得分最高的一位专家。这项技术导致参数大幅减少——从 1.6T 模型（T5 架构）到等效 1.4B vanilla Transformer 的 FLOPS 成本。

MoE的问题
- 训练不稳定性：这种方法谨慎地选择专家并将其与token匹配。这意味着控制器权重的微小变化可能会对控制器决策产生不成比例的影响。
- 负载不平衡： MoE 的问题是我们无法有效地平衡令牌和专家的分配方式，因为路由网络的选择没有受到有效的限制。这就是为什么有些令牌没有任何专家来处理它们（令牌丢弃），并且几乎所有令牌都只分配给少数专家（模型崩溃）。
- 信息泄漏：一些成功的 MoE 方法将序列中不同位置的令牌一起处理（即，通过比较批次中所有令牌的分数）。这造成了序列内信息泄漏并阻碍了它们在自回归解码中的实用性。
- 知识混合性：由于专家数量有限，传统 MoE 架构中的专家通常会积累广泛的知识。这种广泛的知识库削弱了个别专家的专业性和有效性。
- 知识冗余：多个专家在学习相似信息时有趋同的倾向，导致知识领域重叠和模型参数使用效率低下。

Cohere AI 的科学家解决MOE主要挑战之一的方法——**将所有专家存储在内存中**。
- 将 MoE 架构与轻量级专家独特地结合起来，提出了一种参数极其高效的 MoE。
- MoE 架构优于标准 PEFT 方法，并且仅通过更新轻量级专家即可达到完全微调的效果——不到 11B 参数模型的 1%。

DeepSeekMoE 架构通过采用两个关键策略来增强专家专业化：细粒度专家分割和共享专家隔离。
- **细粒度专家分割**（Fine-grained expert segmentation）涉及细分 FFN 中间隐藏维度，从而允许细粒度专家之间更细致地分配知识。这种细分使每个专家能够专注于更具体的知识领域，从而在保持恒定的计算成本的同时实现更高水平的专业化。
- **共享专家隔离**（shared expert isolation）策略将特定专家指定为“共享”，负责捕获不同背景下的共同知识。通过将一般知识集中在这些共享专家上，减少了其他专家学习过程中的冗余。这种方法提高了参数效率，并确保每位专家始终专注于独特且独特的知识领域。

DeepSeekMoE 经过扩展可训练 16B 模型，只需约 40% 的计算量，即可实现与 DeepSeek 7B 和 LLaMA2 7B 相当的性能。研究人员还计划将 DeepSeekMoE 扩展到 145B，突出其相对于 GShard 架构的优势，并展示与 DeepSeek 67B 相当的性能。

Token混合（Mixture of Tokens）

MoT 不将token发送给专家，而是将不同示例中的token混合在一起，然后再将其提供给专家。
- 这使得模型能够从所有token-专家组合中学习，并提高训练稳定性和专家利用率。
- 在向专家提供token后，每种混合物都会被处理并重新分配回原始token。

MoT 通过进行以下更改来解决 MoE 模型的问题：
- 混合来自不同示例的token，然后将其提供给专家；通过允许模型从所有token-专家组合中学习，这提高了训练稳定性和专家利用率。
- token混合是一个完全可微的模型，这意味着它可以使用标准的基于梯度的方法进行训练。这避免了辅助损失或其他难以训练的技术的需要，从而更容易训练和部署。”

token混合有可能显着提高LLM的表现和效率。与普通 Transformer 相比，它显示出训练时间减少了 3 倍的惊人结果。


## MoDE

去噪混合专家 MoDE
- 多模态模型离不开扩散模型，Diffusion与Transformer组合成强大的信息提取器。随着模型变得越来越大以捕获更复杂的能力，其架构能效要求更严苛。

卡尔斯鲁厄理工和麻省理工学院的学者们提出了MoDE，一种混合专家 （MoE）扩散策略
- [Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning](https://arxiv.org/html/2412.12953v1)

MoDE 使用噪声调节路由和自注意力机制，仅计算和集成每个噪声级别的必要专家，减少延迟和计算成本，实现在各种噪声水平下更有效地去噪。

MoDE 超越了当前基于Transformer的最先进扩散策略，同时通过稀疏专家和噪声条件路由实现参数高效扩展，通过专家缓存将激活参数减少40%，并将推理成本降低90%。甚至在零样本泛化任务中优于所有基线，并表现出强大的泛化能力。

## 【2025-1-22】AOE

混合专家（MoE）模型通常使用路由器将标记分配给特定的专家模块，只激活部分参数，并且经常优于密集模型。

路由器决策与专家执行之间的分离是一个关键但被忽视的问题，导致次优的专家选择和无效的学习。

人大、腾讯、东南提出新MoE范式专家自主性（AoE），专家自主选择自己来处理输入。
- 论文 [Autonomy-of-Experts Models](https://arxiv.org/pdf/2501.13074v1)

AoE基于见解：专家能够意识到自己有效处理标记的能力，这种意识体现在其内部激活的规模上。
AoE中，移除了路由器；专家预先计算输入的内部激活，并根据其激活范数进行排名。只有排名最高的专家继续前向传播，而其他专家则终止。

<img width="1174" height="1400" alt="image" src="https://github.com/user-attachments/assets/799e4530-fde2-41aa-904a-206a3937b91b" />


通过低秩权重分解减少了预计算激活的开销。这种自我评估然后比较伙伴的方法确保了改进的专家选择和有效的学习。

对具有7亿到40亿参数的语言模型进行了预训练，证明了AoE在效率相当的情况下优于传统的MoE模型。


## 思考


### 面试题

【2024-9-8】[大模型面经——MoE混合专家模型总结](https://mp.weixin.qq.com/s/b_FeWWHcwXPxAC_SL6ABfg)

总结
- 一、MoE介绍
  - "Mixture of Experts"（MoE）是一种机器学习模型，特别是深度学习领域，`集成学习`的一种形式。
  - MoE模型由多个`专家`（experts）和一个`门控网络`（gating network）组成。每个专家负责处理输入数据的不同**部分**或不同**特征**，而门控网络则负责决定每个输入应该由哪个专家来处理。
- 二、MoE出现的背景
  - 一种高效的 scaling 技术，用较少的计算量实现更大的模型规模，从而获得更好的性能。
- 三、有哪些MoE模型
  - Switch Transformers、Mixtral、GShard、DBRX、Jamba DeepSeekMoE 等等。
  - Mixtral 是一个**稀疏**的专家混合网络。decoder-only 模型，其中前馈块从一组 **8个**不同参数组中选择。每层对于每个令牌，路由器网络选择其中**2个**组（“专家”）来处理令牌并附加地组合输出
  - 控制成本和延迟的同时增加了模型的参数数量，因为模型只使用每个令牌总参数集的一小部分。Mixtral 总共有 **46.7B** 个参数，但每个令牌只使用 **12.9B** 个参数。因此，它以与 12.9B 型号相同的速度和相同的成本处理输入和生成输出。
- 四、介绍稀疏 MoE 层
  - 稀疏 MoE 层一般用来替代传统 Transformer 模型中的`前馈网络` (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立神经网络。在实际应用中，这些专家通常是`前馈网络` (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。
- 五、介绍门控网络或路由
  - 门控网络接收输入数据并执行一系列学习的非线性变换。这一过程产生了一组权重，表示每个专家对当前输入的贡献程度。这些权重经过softmax等函数的处理，以确保相加为1，形成了一个概率分布。这样的分布表示了在给定输入情境下每个专家被激活的概率。一个典型的门控函数通常是一个带有 softmax 函数的简单的网络。
- 六、为什么门控网络要引入噪声呢
  - 为了专家间的**负载均衡**。防止一句话中的大部分token都只有1个专家来处理，剩下的7个专家（假设一共八个专家）“无所事事”。
- 七、如何均衡专家间的负载
  - 引入**噪声**、引入**辅助损失**（鼓励给予所有专家相同的重要性）、引入**随机路由**、设置一个专家能处理的token**数量上限**
- 八、“专家”指什么
  - 一个“专家”通常是`前馈网络` (FFN)。数据经过**门控网络**选择后进入每个专家模型，每个专家根据其设计和参数对输入进行处理。每个专家产生的输出是对输入数据的一种表示，这些表示将在后续的步骤中进行加权聚合。或者通过单个专家模型进行处理。
- 九、专家的数量对预训练有何影响？
  - 增加更多专家可以提升处理样本的效率和加速模型的运算速度，但这些优势随着专家数量的**增加而递减** (尤其是当专家数量达到 256 或 512 之后更为明显)。同时，推理过程中需要更多的显存来加载整个模型。
  - Switch Transformers 的研究表明，其在大规模模型中的特性在小规模模型下也同样适用，即便是每层仅包含 2、4 或 8 个专家。
- 十、什么是topK门控
  - 选择前k个专家。为什么不仅选择最顶尖的专家呢？最初的假设是，将输入路由到不止一个专家，以便门控学会如何进行有效的路由选择，因此至少需要选择两个专家。
- 十一、MoE模型主要特点
  - 灵活性：每个专家可以是不同类型模型，例如全连接层、卷积层或者递归神经网络。
  - 可扩展性：通过增加专家的数量，模型可以处理更复杂的任务。
  - **并行**处理：不同的专家可以并行处理数据，这有助于提高模型的计算效率。
  - **动态**权重分配：门控网络根据输入数据的特点动态地为每个专家分配权重，这样模型可以更加灵活地适应不同的数据。
  - 容错性：即使某些专家表现不佳，其他专家的表现也可以弥补，从而提高整体模型的鲁棒性。
- 十二、MoE和稠密模型的对比
  - 1、预训练: 相同计算资源，MoE 模型理论上可以比密集模型更快达到相同的性能水平。
  - 2、推理
    - moe：高显存，高吞吐量；
    - 稠密模型：低显存，低吞吐量
- 十三、MoE的优势
  - 1、训练优势：预训练速度更快；
  - 2、推理优势：推理速度更快
- 十四、MoE的挑战
  - 1、训练挑战：微调阶段，**泛化能力不足**，容易**过拟合**
  - 2、推理挑战：对显存要求更高
- 十五、微调MoE的方法
  - 冻结所有非专家层的权重，**只训练专家层**
  - 只冻结 moe层参数，训练其它层参数
- 十六、MoE的并行计算
  - DP(仅数据分割)/MP(仅模型分割)
  - DP+MP(模型分组共享,组内分割；数据分批,组内复制)
  - 专家+DP(专家分布在不同节点;数据按节点分割)
  - 专家+DP+MP(专家分布在不同节点;数据分批,组内复制)

### 问题

#### 为什么专家是token级别？

知乎：[moe为什么不把一个句子都放进一个专家，而是一个token呢？](https://www.zhihu.com/question/646173158/answer/1940555666092700667)

Token级别 vs 句子级别：

模型定义

(1) Token级别路由的模型
- Token 级别路由允许不同类型的token选择不同专家

```py
# Token级别路由的模型
class TokenLevelMoE(nn.Module):
    def forward(self, x):
        outputs = []
        for pos in range(seq_len):
            # 每个位置独立选择专家
            expert_choice = self.router(x[:, pos, :])
            output = self.expert_forward(x[:, pos, :], expert_choice)
            outputs.append(output)
        return torch.stack(outputs, dim=1)
```

(2) 句子级别路由的模型（对照组）

```py
# 句子级别路由的模型（对照组）
class SentenceLevelMoE(nn.Module):
    def forward(self, x):
        # 整个句子用同一组专家，感觉就很憋屈
        sentence_repr = x.mean(dim=1)  
        expert_choice = self.router(sentence_repr)
        
        outputs = []
        for pos in range(seq_len):
            # 所有token被迫用同样的专家
            output = self.expert_forward(x[:, pos, :], expert_choice)
            outputs.append(output)
        return torch.stack(outputs, dim=1)
```

实测数据说话

同样的数据集训练两个模型：表达能力的碾压性优势
- **Token 级别路由**在理论上具有更强的表达能力，虽然缺乏系统性的对比研究数据，但从架构设计上看优势明显。
- 计算开销没那么大，计算量确有增加，但完全可接受
  - MegaScale-MoE（2024）研究报告，All-to-All通信在前向传播中占43.6%时间，在整个训练过程中占32%左右。


① token 级别表达能力更强

Token级别路由允许不同类型的token选择不同专家：

```py
# 真实的路由日志（从我的实验中提取）
sentence = "The brilliant researcher quickly developed innovative algorithms"
routing_log = {
    "The": "专家3（冠词/限定词专家）",
    "brilliant": "专家7（形容词/修饰语专家）",
    "researcher": "专家1（名词实体专家）",
    "quickly": "专家5（副词/状语专家）",
    "developed": "专家2（动词/谓语专家）",
    "innovative": "专家7（形容词专家，复用）",
    "algorithms": "专家9（技术术语专家）"
}
```

更神奇的是，同一个词在不同上下文中会选择不同专家：

```py
contexts = {
    "river bank": ("bank", "专家4-地理概念"),
    "money bank": ("bank", "专家6-金融概念"),
    "data bank": ("bank", "专家8-技术概念"),
    "blood bank": ("bank", "专家11-医疗概念")
}
# 这种上下文敏感的路由，句子级别做不到
```

② 计算量

```py
# 基于Switch Transformer的实测数据
performance_metrics = {
    "路由计算开销": "3-5%的总时间",
    "通信开销（8GPU）": "10-15%的总时间",
    "内存增加": "约20%",
    
    "但是收益呢？": {
        "训练加速": "7.5倍（相同计算预算）",
        "推理时激活参数": "仅8-25%",
        "最终性价比": "完胜！"
    }
}
```



#### 负载均衡

MoE最让人崩溃的就是 **负载不均衡**：有些专家累死，有些闲死

传统辅助损失函数：
- f_i 是专家i实际被选中的频率
- P_i 是理论概率分布。
- α 通常设成0.01

> L_aux = α × N × Σ(i=1 to N) f_i × P_i

DeepSeek-V3 无损失负载均衡是个神来之笔

```py
class LoadFreeBalancing(nn.Module):
    """无辅助损失的负载均衡，爽！"""
    
    def __init__(self, num_experts, lr=0.001):
        super().__init__()
        self.expert_bias = nn.Parameter(torch.zeros(num_experts))
        # 就这么简单，用偏置来调节负载
        
    def forward(self, router_logits):
        # 应用偏置进行选择（仅影响选择）
        biased_logits = router_logits + self.expert_bias
        selected = torch.topk(biased_logits, k=8)[1]  # DeepSeek-V3实际激活8个
        
        # 但计算权重用原始logits（保证质量）
        weights = F.softmax(router_logits.gather(-1, selected), dim=-1)
        
        return selected, weights
```


#### 专家崩溃

专家崩溃（Expert Collapse）

最常见的问题，某几个专家被过度使用，其他专家"饿死"：

```py
# 监控专家使用情况
def monitor_expert_health(model, dataloader):
    expert_usage = torch.zeros(model.num_experts)
    
    for batch in dataloader:
        _, _, expert_indices = model.router(batch)
        for idx in expert_indices.flatten():
            expert_usage[idx] += 1
    
    # 计算使用率的标准差
    usage_std = expert_usage.std() / expert_usage.mean()
    
    if usage_std > 0.5:
        print("警告：专家使用严重不均！")
        print(f"最忙专家：{expert_usage.max():.0f}次")
        print(f"最闲专家：{expert_usage.min():.0f}次")
        
        # 动态调整路由偏置
        model.router.adjust_bias(expert_usage)
```


#### 梯度爆炸

梯度爆炸

MoE特别容易梯度爆炸，尤其是刚开始训练时：

```py
# 解决方案：渐进式训练
class GradualMoETraining:
    def __init__(self, model):
        self.model = model
        self.stage = 0
        
    def train_step(self, batch, step):
        # 前1000步：只训练路由器
        if step < 1000:
            for param in self.model.experts.parameters():
                param.requires_grad = False
        
        # 1000-5000步：逐渐增加专家数量
        elif step < 5000:
            active_experts = min(step // 500, self.model.num_experts)
            # 只激活部分专家
            
        # 5000步后：全量训练
        else:
            for param in self.model.parameters():
                param.requires_grad = True
        
        # 梯度裁剪必须有
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
```

#### 推理内存爆炸

推理内存爆炸

虽然激活参数少，但所有专家都要加载到内存：

```py
# 解决方案：专家分组+动态加载
class MemoryFriendlyInference:
    def __init__(self, model_path):
        # 只加载路由器和共享专家
        self.router = load_router(model_path)
        self.shared_experts = load_shared_experts(model_path)
        
        # 路由专家按需加载
        self.expert_loader = ExpertLoader(model_path)
        
    def forward(self, x):
        # 先路由，看需要哪些专家
        needed_experts = self.router.predict_experts(x)
        
        # 只加载需要的专家
        experts = self.expert_loader.load(needed_experts)
        
        # 计算
        output = self.compute_with_experts(x, experts)
        
        # 立即释放
        self.expert_loader.release(needed_experts)
        
        return output
```



# 结束
