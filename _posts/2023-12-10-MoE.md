---
layout: post
title:  混合专家模型（MoE）专题
date:   2023-11-10 16:52:00
categories: 大模型
tags: gpt moe 专家 llama
excerpt: 混合专家模型，MoE系列
mathjax: true
permalink: /moe
---

* content
{:toc}


# MoE 混合专家模型


## 什么是 MoE

专家混合模型（MoE）把复杂的任务分割成一系列更小、更容易处理的**子任务**，每个子任务由一个特定领域的「专家」负责。
1. `专家层`：专门训练的小型神经网络，每个网络都在其擅长的领域有着卓越的表现。
2. `门控网络`：MoE架构中的**决策核心**。它负责判断哪个专家最适合处理某个特定的输入数据。门控网络会计算输入数据与每个专家的兼容性得分，然后依据这些得分决定每个专家在处理任务中的作用。

这些组件共同作用，确保适合的任务由合适的专家来处理。门控网络有效地将输入数据引导至最合适的专家，而专家们则专注于自己擅长的领域。这种合作性训练使得整体模型变得更加多功能和强大。

## 实现案例


### GPT-4

[GPT-4混合大模型？研究证明MoE+指令调优确实让大模型性能超群](https://www.toutiao.com/article/7253055129237422626)
- 6月, 「天才黑客」乔治・霍兹（George Hotz）在接受一家名为 Latent Space 的 AI 技术播客的采访时提到了 GPT-4，并称: GPT-4 其实是一个**混合**模型。
- GPT-4 采用由 8 个专家模型组成的集成系统，每个专家模型都有 2200 亿个参数（比 GPT-3 的 1750 亿参数量略多一些），并且这些模型经过了针对不同数据和任务分布的训练。

谷歌、UC 伯克利等证明 MoE + 指令调优起到了 1 + 1 > 2 的效果。[论文](https://arxiv.org/pdf/2305.14705.pdf)
- 谷歌、UC 伯克利、MIT 等机构的研究者联合发表的一篇论文证实：混合专家模型（MoE）与指令调优的结合能够让大型语言模型（LLM）的性能大幅提升。

MoE是下一代LLM架构，实现
- [moduleformer](https://github.com/ibm/moduleformer)

### Megatron-LM MoE

【2023-11-15】[Megatron-LM MoE 代码解析](https://zhuanlan.zhihu.com/p/666653126?utm_psn=1708124942137335808)

新版本的 Megatron-LM 中，Nvidia 也释出了 MoE 的配套实现。虽然是 token dropless，原生支持 Megatron 的 3D 并行和 Expert Parallelism

arguments.py 中加入了 MoE 相关的参数选项
- --num-experts: Expert 的数量
- --expert-parallel: 开启 Expert Parallelism
- --expert-model-parallel-size: Expert Parallelism 的 degree，因为 Expert Parallelism (EP) 被放在了 Data Parallelism (DP) 那一维，因此在设置时要求 DP 需要能够被 EP 整除（可以这样理解，在不考虑 EP 的情况下，不管 TP 和 PP 如何设置，DP 的大小始终对应有多少份 model copy 在并行训练，Expert Parallelism 相当于把所有的 Experts 切分到 EP 份这样的 model copy 上，因此 DP 必须能被 EP 整除，否则根本没法切）。原则上每张 GPU 上可以放多个 Expert，每个 Expert 也可以被切分到多张 GPU 上。如果固定每张 GPU 对应一个 Expert，那么对于一个 Expert=16 的 MoE 模型，EP=16，DP 也至少是16，所以对资源的要求还是很高的。

模型实现上只是在 ParallelTransformerLayer 初始化时将 ParallelMLP 替换成了 SwitchMLP, 代码实现见[原文](https://zhuanlan.zhihu.com/p/666653126?utm_psn=1708124942137335808)


### 【2023-11-22】LM-Cocktail

问题
- LLM finetune方式会导致目标任务之外的生成任务上，性能严重衰减（performance degeneration）

- 论文：[LM-Cocktail: Resilient Tuning of Language Models via Model Merging](https://arxiv.org/pdf/2311.13534.pdf)
- 代码：[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

BAAI和中科院发布 LM-Cocktail，使用模型融合（model merging）方式
- 将 finetune模型融入 pre-train模型中
- 或 两者同等重要，加权

BAAI更多工作
-   11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), 一种通过模型融合在微调时保持原有模型通用能力的方法. [技术报告](https://arxiv.org/abs/2311.13534) 🔥
-   10/12/2023: 发布 [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), 专为大语言模型**各种检索增强任务设计**的英文向量模型。[技术报告](https://arxiv.org/pdf/2310.07554.pdf)
-   09/15/2023: 发布 [技术报告](https://arxiv.org/pdf/2309.07597.pdf) 和 [数据集](https://data.baai.ac.cn/details/BAAI-MTP).
-   09/12/2023: 更新：
    -   **新增重排模型**：开源交叉编码器模型bge-reranker，具有比向量模型更强大的排序能力。非常建议使用或者微调它来重新排序向量模型返回的top-k文档，提高最终结果的相关性。
    -   **更新向量模型**：发布bge-\*-v1.5向量模型，缓解相似度分布问题，提升无指令情况下的检索能力（但检索任务仍建议使用指令）
-   09/07/2023: 更新[微调代码](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): 增加难负样本挖掘脚本，增加指令参数方便在微调中添加指令.
-   08/09/2023: BGE模型整合入Langchain, 可以在langchain中非常简单的[使用它](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md#using-langchain); C-MTEB中文榜单已[在线更新](https://huggingface.co/spaces/mteb/leaderboard).
-   08/05/2023: 发布更小的模型(base, small), **在同尺寸模型中取得最好的性能！ 🤗**
-   08/02/2023: :tada: :tada: 发布中英文向量模型BGE(BAAI General Embedding的缩写), **在MTEB和C-MTEB榜单上取得最好的性能**
-   08/01/2023: 发布大规模中文文本向量[评测榜单](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), 其包括31个测试任务.



效果
- 微调的Llama和BGE模型
- FLAN, MMLU, MTEB 上验证了 LM-Cocktail 的有效性。



### 【2023-12-11】Mistral-MoE

【2023-12-11】开源MoE模型：[8x7B开源MoE击败Llama 2逼近GPT-4！欧版OpenAI震惊AI界，22人公司半年估值20亿](https://mistral.ai/news/mixtral-of-experts/)

法国的AI初创公司 [Mistral AI](https://mistral.ai/news/mixtral-of-experts/) 发布了首个开源MoE大模型。87GB的种子，8x7B的MoE架构，像一款mini版「开源GPT-4」
- 2023年6月，[Mistral AI](https://mistral.ai/news/mixtral-of-experts/)上线。7页PPT，获得欧洲历史上最大的种子轮融资, 1.13亿美元。
- 2023年9月，Mistral 7B发布，号称是当时最强的70亿参数开源模型。
- 2023年12月，类GPT-4架构的开源版本Mistral 8x7B发布。几天后，外媒金融时报公布Mistral AI最新一轮融资4.15亿美元，估值高达20亿美元，翻了8倍。

- 论文 [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf), 作者中两人是 [Mistral AI](https://mistral.ai/news/mixtral-of-experts/) 创始人
- [文档](https://docs.mistral.ai/)
- [api](https://docs.mistral.ai/api/), 提供三种接口: Chat, Embedding, Models

huggingface： [mistralai](https://huggingface.co/mistralai)
- [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
- [mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
- [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)


8*7B 小模型直接碾压了 `Llama 2 70B`

Mistral 8x7B 在每个token的推理过程中，只使用了2个专家。

从模型元数据中提取的信息：

```json
{"dim": 4096, "n_layers": 32, "head_dim": 128, "hidden_dim": 14336, "n_heads": 32, "n_kv_heads": 8, "norm_eps": 1e-05, "vocab_size": 32000, "moe": {"num_experts_per_tok": 2, "num_experts": 8}
```

与GPT-4（网传版）相比，Mistral 8x7B 具有类似的架构，但在规模上有所缩减：
- 专家数量为**8个**，而不是16个（减少了一半）
- 每个专家拥有**70亿**参数，而不是1660亿（减少了约24倍）
- 总计420亿参数（估计值），而不是1.8万亿（减少了约42倍）
- 与原始GPT-4相同的32K上下文窗口

已经有不少开源模型平台上线了Mistral 8×7B
- [Perplexity Labs](https://labs.perplexity.ai)

Mistral放出这个开源的7B×8E的MoE之前，英伟达和谷歌也放出过其他完全开源的MoE



# 结束