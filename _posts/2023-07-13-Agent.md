---
layout: post
title:  LLM Agent 学习笔记
date:   2023-07-13 22:46:00
categories: AIGC
tags: gpt ChatGPT prompt 
excerpt: 大模型 LLM 驱动的智能体 Agent
mathjax: true
permalink: /agent
---

* content
{:toc}

# LLM 智能体

## 智能体 vs LLM

这类项目绝大多数的主要创新还是在 **prompt 层面**，通过更好的提示词来激发模型的能力，把更多原先需要通过代码来实现的流程“硬逻辑”转化为模型自动生成的“动态逻辑”。

目前语言模型只能响应用户的**查询指令**，实现一些**生成任务**，比如写故事、生成代码等。而以 `AutoGPT`, `GPT-Engineer`和`BabyAGI`等项目为代表的**大型动作模型**（Large-Action Models，`LAM`）将语言模型作为智能体的**核心大脑**，将复杂任务分解，并在每个子步骤实现自主决策，无需用户参与即可解决问题。

LAM的崛起，也标志着语言模型的研发正在走向新阶段

## 智能体组件

【2023-7-11】[下一代语言模型范式LAM崛起！AutoGPT模式席卷LLM，三大组件全面综述：规划、记忆和工具](https://zhuanlan.zhihu.com/p/642902065)

### 整体结构

三个关键组件，即规划、记忆和工具
- ![](https://pic2.zhimg.com/80/v2-df16b538d5ecf9d68aea1fbfb77e03ad_1440w.webp)

### Planning 规划

一项复杂的任务通常包括多个子步骤，智能体需要提前将任务分解，并进行规划。

#### 任务分解

思维链（Chain of Thought, CoT）已然成为「诱导模型推理」的标准提示技术，可以增强解决复杂任务时的模型性能。
- 通过「Think step by step」，模型可以利用更多测试时计算（test-time computation）将任务分解为更小、更简单的子步骤，并能够解释模型的思维过程。

思想之树（Tree of Thoughts）在每个子步骤中探索多种推理可能性来扩展CoT。
- 首先将问题分解为多个思维步，并在每个步骤内生成多个思路，从而创建出一个树结构解决方案；搜索过程可以是BFS（广度优先搜索）或DFS（深度优先搜索），其中每个状态由分类器（经由提示）或多数投票来评估。
- 任务分解可以通过简单的提示，如「Steps for XYZ.\n1.」，「What are the subgoals for achieving XYZ」 ；或是使用任务相关的指令，如「Write a story outline」可以用于写小说；也可以由人输入。

#### 自我反思 Self Reflection

自我反思可以让自主智能体改进过去的行动决策、纠正之前的错误来迭代改进，在可以试错的现实任务中非常有用。

ReAct通过将动作空间扩展为任务相关的离散动作和语言空间的组合，在LLM中集成了推理和动作，其中动作使得LLM能够与环境交互（例如使用维基百科搜索API），而语言空间可以让LLM以自然语言的方式生成推理轨迹。

ReAct提示模板包含了LLM思考的明确步骤

在知识密集型任务和决策任务的实验中，ReAct 比只用Act（移除Thought）的基线模型效果更好。
- ![](https://pic3.zhimg.com/80/v2-01b151d354d1cba34993a8d0f87ef50e_1440w.webp)

### Tool Use 工具使用

能使用复杂工具是人类高智力的体现，我们可以创造、修改和利用外部物体来完成超出身体和认知极限的事情，同样，为LLM配备外部工具也可以显著扩展模型功能。
- 一只海獭漂浮在水中时，用岩石劈开贝壳的图片。虽然其他一些动物可以使用工具，但其复杂性无法与人类相比。

[MRKL](https://arxiv.org/pdf/2205.00445.pdf)（模块化推理、知识和语言），是一个神经符号架构的自主智能体，包含一组「专家」模块和一个用作路由器（router）的通用语言模型，以路由查询到最合适的专家模块。

每个模块可以神经网络，也可以是符号模型，例如数学计算器、货币转换器、天气API

研究人员做了一个微调语言模型以调用计算器的实验，使用算术作为测试用例，结果表明，解决verbal数学问题比解决明确陈述的数学问题更难，因为LLM（7B Jurassic 1-large 模型）不能可靠地为基本算术提取正确的参数，也凸显了符号工具的重要性，以及了解何时利用何种工具的重要性。

TALM（工具增强语言模型）和 [Toolformer](https://arxiv.org/pdf/2302.04761.pdf) 都是微调语言模型以学习使用外部工具API

ChatGPT插件和OpenAI API函数调用也是增强语言模型使用工具能力的例子，其中工具API的集合可以由其他开发人员提供（如插件）或自定义（如函数调用）。

[API-Bank](https://arxiv.org/pdf/2304.08244.pdf)是用于评估工具增强型LLM性能的基准，包含53个常用的API工具，一个完整的工具增强的LLM工作流，以及264个标注对话，用到568次API调用。

API的选择非常多样化，包括搜索引擎、计算器、日历查询、智能家居控制、日程管理、健康数据管理、账户认证工作流等。
由于API数量众多，LLM首先可以访问API搜索引擎，找到合适的API调用，然后使用相应的文档进行调用。

在API-Bank工作流程中，LLM需要做出三次决策，每一步都可以评估决策的准确性：
1. 是否需要API调用；
2. 确定要调用的正确API：如果不够好，则LLM需要迭代地修改API输入（例如决定搜索引擎API的搜索关键字）；
3. 基于API结果的响应：如果结果不满意，则模型可以选择改善并再次调用。

该基准可以在三个层次上评估智能体的工具使用能力。
- 层次1：评估调用API的能力
  - 给定API的描述，模型需要确定是否调用给定的API，正确调用并正确响应API返回；
- 层次2：检查检索API的能力
  - 模型需要搜索可能解决用户需求的API，并通过阅读文档学习如何使用。
- 层次3：评估规划API的能力，而非检索和调用
  - 如果用户请求不明确（例如安排小组会议、预订旅行的航班/酒店/餐厅），模型可能不得不进行多次API调用来解决。


LLM 调用外部工具的应用模式
- ![](https://pic1.zhimg.com/80/v2-670e38abdbd8e6686adcc2c35aea66c2_1440w.webp?source=1940ef5c)

OpenAI 的 Jack Rae 和 Ilya Sutskever 在之前的分享中也分别提到了 **压缩即智慧** 的理念。对于模型的“压缩率”来说，如果能更有效地使用这些“外部工具”，就能大幅提升很多特定任务 next token 预测的准确率。

提升压缩率的手段
- ![](https://picx.zhimg.com/80/v2-06fe12da5124690ef0bcba8b8531d618_1440w.webp?source=1940ef5c)

这个方向很有价值
- 从“有效数据”角度看，人类执行各类任务使用工具，甚至思维过程等数据会有非常高的价值。
- 从模型训练角度来看，如何能在过程中把模型利用工具的能力也体现在 loss function 里，可能也是个很有趣的方向


### Memory 记忆

记忆可以被定义为用于获取、存储、保留和后续检索信息的过程，人类大脑中主要有三种类型的记忆。
- 1. `感官记忆`（Sensory memory）
  - 这种记忆处于记忆的最早阶段，提供了在原始刺激结束后保留感官信息（视觉，听觉等）印象的能力，通常只持续几秒钟。
  - 感官记忆的子类别包括**图标记忆**（视觉）、**回声记忆**（听觉）和**触觉记忆**（触觉）。
2. `短时记忆`（STM）或**工作记忆**（Working Memory）
  - 存储了当下能意识到的所有信息，以及执行复杂的认知任务（如学习和推理）所需的信息，大概可以存储7件事，持续20-30秒。
3. `长期记忆`（LTM）
  - 顾名思义，LTM可以将信息存储相当长的时间，范围从几天到几十年不等，具有基本上无限的存储容量。LTM有两种亚型：
  - 1）显式/陈述性记忆，即对事实和事件的记忆，指那些可以有意识地回忆起来的记忆，包括情景记忆（事件和经验）和语义记忆（事实和概念）。
  - 2）隐式/程序性记忆，这种类型的记忆是无意识的，包括自动执行的技能和例程，比如骑自行车或在键盘上打字。
- ![](https://pic4.zhimg.com/80/v2-39913cb59e36e3ee0729cad2b56c98bb_1440w.webp)

对应到语言模型的概念上：
1. 作为原始输入（包括文本、图像或其他形式）的学习嵌入表征的感官记忆;
2. 短期记忆就是上下文学习（in-context learning），非常短且影响范围有限，受到Transformer的上下文窗口长度的限制。
3. 长期记忆作为智能体在查询时可用的外部向量存储，可通过快速检索访问。


#### 最大内积搜索 Maximum Inner Product Search (MIPS)

外部记忆可以缓解有限注意力span的限制，常用的操作是将信息嵌入表征保存到支持快速最大内积搜索（MIPS）的向量存储数据库中。

为了优化检索速度，一般都会选择近似最近邻（ANN，approximate nearest neighbors）算法返回前k个最近邻节点，牺牲一点准确性以换取巨大的速度提升。

常用的ANN算法包括: LSH（Locality-Sensitive Hashing），ANNOY, HNSW， FAISS, ScaNN

#### 长期记忆

获取长期记忆的方法最常见的方式是通过“语义搜索”。
- 用一个 embedding 模型，将所有的记忆文本都转化为一个向量。而后续跟模型的交互信息也可以通过同样的 embedding 模型转化为向量，计算相似度来找到最相似的记忆文本。最后再将这些记忆文本拼接到 prompt 里，作为模型的输入。
- ![](https://pic1.zhimg.com/80/v2-3742a095fdd75d3c3a66faecbb690575_1440w.webp?source=1940ef5c)
- 这类方法最热门的开源项目可以参考 OpenAI 官方的 [ChatGPT Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin) 和 Jerry Liu 的 [LlamaIndex](https://github.com/jerryjliu/llama_index)。

这种拓展模型记忆的模式相比人类大脑的运作方式来说感觉还有些“粗糙”，所谓的长期与短期记忆（包括 LangChain 与 LlamaIndex 中一些更复杂的实现），仍然是比较“hard coded”的感觉。如果未来在模型 context size 上有突破性的研究进展，那么当前的这类模式或许就不再需要了。

### Prompt 设计

绝大多数的主要创新还是在 prompt 层面，通过更好的提示词来激发模型的能力，把更多原先需要通过代码来实现的流程“硬逻辑”转化为模型自动生成的“动态逻辑”。

### Prompt 设计范式

prompt 设计模式
- CoT prompt，在给出指令的过程中，同时也给出执行任务过程的拆解或者样例。这个应该很多人都用过，“let's think step by step”  
- “自我审视”，提醒模型在产出结果之前，先自我审视一下，看看是否有更好的方案。也可以拿到结果后再调用一下模型强制审视一下。比如 AutoGPT 里的“Constructively self-criticize your big-picture behavior constantly”。
- 分而治之，大家在写 prompt 的时候也发现，越是具体的 context 和目标，模型往往完成得越好。所以把任务拆细再来应用模型，往往比让它一次性把整个任务做完效果要好。利用外部工具，嵌套 agent 等也都是这个角度，也是 CoT 的自然延伸。
- 先计划，后执行。BabyAGI，HuggingGPT 和 Generative Agents 都应用了这个模式。也可以扩展这个模式，例如在计划阶段让模型主动来提出问题，澄清目标，或者给出一些可能的方案，再由人工 review 来进行确认或者给出反馈，减少目标偏离的可能。
- 记忆系统，包括短期记忆的 scratchpad，长期记忆的 memory stream 的存储、加工和提取等。这个模式同样在几乎所有的 agent 项目里都有应用，也是目前能体现一些模型的实时学习能力的方案。

这些模式与人类认知和思考模式有很相似，历史上也有专门做 [cognitive architecture](https://cogarch.ict.usc.edu/) 相关的研究，从记忆，世界认知，问题解决（行动），感知，注意力，奖励机制，学习等维度来系统性思考智能体的设计。个人感觉目前的 LLM agent 尝中，在奖励机制（是否有比较好的目标指引）和学习进化（是否能持续提升能力）这两方面还有很大的提升空间。或许未来 RL 在模型 agent 这方的应用会有很大的想象空间，而不仅仅是现在主要用来做“价值观对齐”。
- ![](https://pica.zhimg.com/80/v2-8fd3707e43ddc4afa367ce855ab84205_1440w.webp?source=1940ef5c)


#### AutoGPT Prompt

AutoGPT 是提示词应用模式当前比较先进的“集大成者”, 相比经典的 reason + act 模式
- `Constraints & Resources`
  - 模型的输入 context size 有限制，所以需要把重要的信息保存到文件里。
  - 长期记忆的管理功能，当前这类复杂 prompt 生成的解决任务的流程往往比较冗长，没有这类长期记忆的管理很容易就会导致模型的输出变得不连贯协调。
  - 模型是“没有联网”的，所有的知识只更新到训练数据的截止日期。所以也明确告诉模型可以通过网络搜索来获取更多时效性的外部信息。
- `Commands` 各类工具的选择上丰富多样，所以 AutoGPT 能够完成多种不同任务
  - 几大类，包括搜索、浏览网页相关，启动其它的 GPT agent，文件读写操作，代码生成与执行等。
  - 跟 HuggingGPT 有些类似，因为目前 GPT 模型对于越具体，细致的任务，生成的表现就越精确和稳定。所以这种“分而治之”的思路，是很有必要的。
- `Performance Evaluation` 模型整体思考流程的指导原则，思考逻辑也非常符合人类的思考，决策与反馈迭代的过程。
  - 包括：对自己的能力与行为的匹配进行 review，大局观与自我反思，结合长期记忆对决策动作进行优化，以及尽可能高效率地用较少的动作来完成任务
- `Response` 格式的限定也是对前面思维指导原则的具体操作规范说明
  - 格式上来看，也是综合了几种模式，包括需要把自己的想法写出来，做一些 reasoning 获取相关背景知识，生成有具体步骤的 plan，以及对自己的思考过程进行 criticism 等
  - 注意：大段 response 是模型一次交互生成的，而不像一些其它框架中会把计划，审视，动作生成等通过多轮模型交互来生成。
- `人工介入` 
  - 模型很容易会把问题复杂化或者在执行计划层面“跑偏”。
  - 所以在具体执行过程中，AutoGPT 也允许用户来介入，对于每一个具体执行步骤提供额外的输入来指导模型行为。
  - 经过人工反馈输入后，模型会重新生成上述的 response，以此往复

AutoGPT 核心 prompt 如下：

```py
You are Guandata-GPT, 'an AI assistant designed to help data analysts do their daily work.'
Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.

GOALS:

1. 'Process data sets'
2. 'Generate data reports and visualizations'
3. 'Analyze reports to gain business insights'

Constraints:
1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.
2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.
3. No user assistance
4. Exclusively use the commands listed in double quotes e.g. "command name"

Commands:
1. Google Search: "google", args: "input": "<search>"
2. Browse Website: "browse_website", args: "url": "<url>", "question": "<what_you_want_to_find_on_website>"
3. Start GPT Agent: "start_agent", args: "name": "<name>", "task": "<short_task_desc>", "prompt": "<prompt>"
4. Message GPT Agent: "message_agent", args: "key": "<key>", "message": "<message>"
5. List GPT Agents: "list_agents", args: 
6. Delete GPT Agent: "delete_agent", args: "key": "<key>"
7. Clone Repository: "clone_repository", args: "repository_url": "<url>", "clone_path": "<directory>"
8. Write to file: "write_to_file", args: "file": "<file>", "text": "<text>"
9. Read file: "read_file", args: "file": "<file>"
10. Append to file: "append_to_file", args: "file": "<file>", "text": "<text>"
11. Delete file: "delete_file", args: "file": "<file>"
12. Search Files: "search_files", args: "directory": "<directory>"
13. Evaluate Code: "evaluate_code", args: "code": "<full_code_string>"
14. Get Improved Code: "improve_code", args: "suggestions": "<list_of_suggestions>", "code": "<full_code_string>"
15. Write Tests: "write_tests", args: "code": "<full_code_string>", "focus": "<list_of_focus_areas>"
16. Execute Python File: "execute_python_file", args: "file": "<file>"
17. Generate Image: "generate_image", args: "prompt": "<prompt>"
18. Send Tweet: "send_tweet", args: "text": "<text>"
19. Do Nothing: "do_nothing", args: 
20. Task Complete (Shutdown): "task_complete", args: "reason": "<reason>"

Resources:
1. Internet access for searches and information gathering.
2. Long Term memory management.
3. GPT-3.5 powered Agents for delegation of simple tasks.
4. File output.

Performance Evaluation:
1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.
2. Constructively self-criticize your big-picture behavior constantly.
3. Reflect on past decisions and strategies to refine your approach.
4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.

You should only respond in JSON format as described below 
Response Format: 
{
    "thoughts": {
        "text": "thought",
        "reasoning": "reasoning",
        "plan": "- short bulleted\n- list that conveys\n- long-term plan",
        "criticism": "constructive self-criticism",
        "speak": "thoughts summary to say to user"
    },
    "command": {
        "name": "command name",
        "args": {
            "arg name": "value"
        }
    }
} 
Ensure the response can be parsed by Python json.loads
```

## LAM的应用


科学发现
- [ChemCrow](https://arxiv.org/abs/2304.05376)系统内的语言模型通过13个专家设计的工具进行能力增强，可以完成跨有机合成、药物发现和材料设计的任务。

LangChain中实现的工作流程包括了在ReAct和MRKL中描述的机制，并将CoT推理与任务相关的工具相结合：
- 语言模型先提供一个工具名称列表、用途描述以及有关预期输入/输出的详细信息；然后指示模型在必要时使用提供的工具回答用户给定的提示，指令要求模型遵循ReAct格式，即Thought, Action, Action Input, Observation
- 实验结果来看，用语言模型评估的话，GPT-4和ChemCrow的性能几乎相当；但当人类专家评估时，在特定解决方案的完成和化学正确性进行的实验结果显示，ChemCrow的性能远远超过GPT-4

实验结果表明，使用LLM来评估需要深入专业知识领域的性能存在问题，可能会导致LLM不知道内在缺陷，无法很好地判断任务结果正确性。

另一篇[论文](https://arxiv.org/abs/2304.05332)研究了语言模型处理复杂科学实验的自主设计、规划和性能，可以使用工具浏览互联网、阅读文档、执行代码、调用机器人实验API以及利用其他语言模型。

当用户请求「develop a novel anticancer drug」时，模型会返回了以下推理步骤：
1. 询问抗癌药物发现的当前趋势；
2. 选择目标；
3. 要求一种靶向这些化合物的scaffold；
4. 一旦找出化合物，模型再尝试合成。

文中还讨论了风险，特别是非法药物和生物武器，研究人员开发了一套包含已知化学武器制剂清单的测试集，并要求合成，11项请求中有4项（36%）被接受；在这7个被拒绝的样本中，5例发生在网络搜索之后，2例仅基于提示词就拒绝。


## 智能体思考


### 局限性

当前模型 agent 的问题和局限性。例如：
- 记忆召回问题。如果只是做简单的 embedding 相似性召回，很容易发现召回的结果不是很好。这里应该也有不少可以改进的空间，例如前面提到的 Generative Agents 里对于记忆的更细致的处理，LlamaIndex 中对于 index 结构的设计也有很多可以选择与调优的地方。
- 错误累积问题。网上给出的很多例子应该都是做了 cherry-picking 的，实际上模型总体表现并没有那么惊艳，反而经常在前面一些步骤就出现了偏差，然后逐渐越跑越远……这里一个很重要的问题可能还是任务拆解执行，外部工具利用等方面的高质量训练数据相对匮乏。这应该也是 OpenAI 为啥要自己来做 plugin 体系的原因之一。
- 探索效率问题。对于很多简单的场景，目前通过模型 agent 来自行探索并完成整个解决过程还是比较繁琐耗时，agent 也很容易把问题复杂化。考虑到 LLM 调用的成本，要在实际场景落地使用也还需要在这方面做不少优化。一种方式可能是像 AutoGPT 那样可以中途引入人工的判断干预和反馈输入。
- 任务终止与结果验证。在一些开放性问题或者无法通过明确的评估方式来判断结果的场景下，模型 agent 的工作如何终止也是一个挑战。这也回到了前面提到的，执行 task 相关的数据收集与模型训练以及强化学习的应用或许可以帮助解决这个问题。



## 智能体案例

业界各种最新的智能体

【2023-7-14】[构建你的第一个 LLM APP 所需了解的一切](https://zhuanlan.zhihu.com/p/643233392)

通过上下文注入构建你自己的聊天机器人
- ![](https://pic2.zhimg.com/80/v2-2117b4f8b6ec27c5396603b53203ec21_1440w.webp)

### 2023.4 BabyAGI

相对于 AutoGPT ，BabyAGI 是一个相对更聚焦在“思维流程”方面尝试的项目，并**没有**添加对各种外部工具利用的支持。

#### BabyAGI 原理

其核心逻辑非常简单：
- 从任务列表中获取排在第一位的任务。
- 获取任务相关的“记忆”信息，由任务执行 agent 来执行这个任务，获取结果。目前这个执行就是一个简单的 LLM 调用，不涉及外部工具。
- 将返回结果再存放到**记忆存储**中。基于当前信息，如整体目标，最近一次执行结果，任务描述，还未执行的任务列表等，生成**新任务**。
- 将新任务添加到任务列表中，再判断所有任务的优先级，重新排序。
- ![](https://user-images.githubusercontent.com/21254008/235015461-543a897f-70cc-4b63-941a-2ae3c9172b11.png)

这个过程就是在模拟作者一天真实的工作流程。
- 早上起来看下有哪些任务要做，白天做任务拿反馈，晚上再看下基于反馈有没有新的任务要加进来，然后重新排下优先级。
- ![](https://picx.zhimg.com/80/v2-fc665ce2bae6d064e16ae444a5096ff0_1440w.webp?source=1940ef5c)

整个项目的代码量很少，相关的 prompts 也比较简单易懂
- ![](https://picx.zhimg.com/80/v2-a6a493784c7c41d553390551e64f9f4d_1440w.webp?source=1940ef5c)

#### BabyAGI 进化

进化版本
- BabyASI 借鉴了 AutoGPT 添加了对 search，代码执行等工具的支持。理论上，如果这个 ASI（Artificial Super Intelligence）真的足够聪明，甚至可以产生代码给自己做 prompt 优化，流程改造，甚至持续的模型训练等，让 GPT 自己开发未来的 GPT，想想是不是很带感 。


### HuggingGPT


#### HuggingGPT 简介

【2023-4-3】[HuggingGPT：一个ChatGPT控制所有AI模型，自动帮人完成AI任务](https://www.toutiao.com/article/7217680526839202307),浙大与微软亚研院的合作成果. 
- [paper](https://arxiv.org/abs/2303.17580)
- 项目已开源，名叫「贾维斯」,钢铁侠里的AI管家贾维斯（[JARVIS](https://github.com/microsoft/JARVIS)）。
- 和3月份刚发布的Visual ChatGPT的思想非常像：后者HuggingGPT，主要是可调用的模型范围扩展到了更多，包括数量和类型。

#### HuggingGPT 原理

如果说 BabyAGI 更多的是探索了 plan & execution 这个应用 LLM 的模式，那么 HuggingGPT 相对早一些的工作更多地展示了在“外部工具”这个层面的想象空间。

其核心运作逻辑也是**计划加上执行**，只不过在执行工具层面，可以利用丰富的“领域专业模型”来协助 LLM 更好地完成复杂任务
- ![](https://picx.zhimg.com/80/v2-512ef42cf0d983c518d3d47d2638dbbe_1440w.webp?source=1940ef5c)

语言是通用的接口。于是，HuggingGPT就诞生了。工程流程分为四步：
- 首先，任务规划。ChatGPT将用户的需求解析为任务列表，并确定任务之间的执行顺序和资源依赖关系。
- 其次，模型选择。ChatGPT根据HuggingFace上托管的各专家模型的描述，为任务分配合适的模型。
- 接着，任务执行。混合端点（包括本地推理和HuggingFace推理）上被选定的专家模型根据任务顺序和依赖关系执行分配的任务，并将执行信息和结果给到ChatGPT。
- 最后，输出结果。由ChatGPT总结各模型的执行过程日志和推理结果，给出最终的输出。

请求：
> 请生成一个女孩正在看书的图片，她的姿势与example.jpg中的男孩相同。然后请用你的声音描述新图片。

可以看到HuggingGPT是如何将它拆解为6个子任务，并分别选定模型执行得到最终结果的。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/cef09fd55855447c80ebe387c3376566~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681185700&x-signature=gCXw63eEBk%2FqSX6NUbCm2SAJLQo%3D)

用gpt-3.5-turbo和text-davinci-003这俩可以通过OpenAI API公开访问的变体，进行了实测。如下图所示：
- 在任务之间存在资源依赖关系的情况下，HuggingGPT可以根据用户的抽象请求正确解析出具体任务，完成图片转换。

在音频和视频任务中，它也展现了组织模型之间合作的能力，通过分别并行和串行执行两个模型的方式，完了一段“宇航员在太空行走”的视频和配音作品。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/e6db6415cbd348d19d0ddaa6cd25ec3a~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681185700&x-signature=5kpy%2Bz12gyq1MjUpDM2ewDVw4pU%3D)

还可以集成多个用户的输入资源执行简单的推理，比如在以下三张图片中，数出其中有多少匹斑马。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/809148f403f3486eae6f7e1f9c172116~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681185700&x-signature=x8aBJK6pJiGy309RKsS%2Bm52Kw2w%3D)


### AutoGPT

【2023-4-12】
- [AutoGPT 太火了，无需人类插手自主完成任务](https://mp.weixin.qq.com/s/bV1tPc7hNn2z06YOpzyanw)
- [拥有自我意识的AI：AutoGPT](https://juejin.cn/post/7236594708301840441)

#### AutoGPT 介绍

[AutoGPT](https://news.agpt.co/) 的研究开始走进大众视野。
-  2023年3月30日，Toran Bruce Richards 发行 AutoGPT，一个实验性开源应用程序，利用 OpenAI 的GPT-4语言模型来创建**完全**自主和可定制的 AI 代理
  - Toran 是一名游戏开发商，并创立了一家名为 Significant Gravitas 的游戏公司
- AutoGPT 相当于给基于 GPT 的模型一个内存和一个身体。
  - 可以把一项任务交给 AI 智能体，自主地提出一个计划，然后执行计划。
  - 此外其还具有互联网访问、长期和短期内存管理、用于文本生成的 GPT-4 实例以及使用 GPT-3.5 进行文件存储和生成摘要等功能。
- AutoGPT 从根本上改变了 AI 与人类之间的交互方式，人类不再需要发挥积极作用，同时仍然保持与 ChatGPT 等其他 AI 应用程序相同或更好的结果质量。

AutoGPT 用处很多，可用来分析市场并提出交易策略、提供客户服务、进行营销等其他需要持续更新的任务。
- [GitHub 地址](https://github.com/torantulino/auto-gpt)

#### AutoGPT 工作原理

AutoGPT 如何工作？

AutoGPT 基于自主 AI 机制工作，其中 AI 系统创建不同的 AI 代理来满足特定任务，其中包括：
- **任务创建**代理： 在 AutoGPT 上输入目标时，第一个与任务创建代理交互的 AI 代理。根据目标创建一个任务列表以及实现这些目标的步骤，并将其发送给优先级代理。
- 任务**优先级**代理： 收到任务列表后，优先级 AI 代理会确保顺序正确且符合逻辑，然后再将其发送给执行代理。
- 任务**执行**代理： 完成优先级排序后，执行代理将一个接一个地完成任务。这涉及利用 GPT-4、互联网和其他资源来获得结果。
- ![架构图](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aba630117cd6408bb1fb8a1265fdf520~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

当执行代理完成所有任务，结果不理想时，它可以与**任务创建代理**通信，创建新的任务列表。三个代理之间的迭代循环，直到完成所有用户定义的目标。

AI 代理的行为也显示在用户界面上，将它们分为四组：思想、推理、计划、评判。
- `思想`（THOUGHTS） ：AI 代理分享它对目标的想法。
- `推理`（REASONING） ：AI 代理推理如何开展并实现它的想法。
- `计划`（PLAN） ：AI代理通过分析，列举了所要完成任务的计划。
- `评判`（CRITICISM） ：AI进行自我评判，纠正错误并克服任何限制问题。

通过共享此计算流程，AutoGPT 可以进行反复尝试论证，并进行针对性的优化处理，可以在没有任何用户干预的情况下克服所遇到的所有问题。

模块图, 源自 [AGI-MAP](https://github.com/ziwang-com/AGI-MAP)
- ![arch](https://user-images.githubusercontent.com/11691791/236591588-3aaa6a6e-bbf5-42cd-84c3-a21f76e8f2a7.png)

特斯拉前 AI 总监、刚刚回归 OpenAI 的 Andrej Karpathy 也为其大力宣传，并在推特赞扬：「AutoGPT 是 prompt 工程的下一个前沿。」

#### AutoGPT 的局限性

一些关键限制
1. **成本高昂**
  - 虽然功能令人惊叹，但 AutoGPT 的实用性可能会让你失望。由于 `AutoGPT` 使用昂贵的 `GPT-4` 模型，因此即使是小任务，完成每个任务的成本也可能很高。这主要是因为 AutoGPT 在特定任务的步骤中会多次使用 `GPT-4`。
2. 经常陷入**循环**
  - 用户在使用 AutoGPT 时面临的最常见问题是它陷入循环。如果这种情况持续超过几分钟，则可能意味着你必须重新启动该过程。发生这种情况是因为 AutoGPT 依赖 GPT-4 来正确定义和分解任务。因此，如果底层LLM返回结果不足以让 AutoGPT 采取任何行动就会出现反复尝试的问题。
3. 数据**安全性**
  - 由于AutoGPT经过充分授权，能自主运行并访问你的系统和互联网，例如使用twitter账号，登录github，使用搜索引擎等，因此你的数据可能会被泄露。
  - AutoGPT没有安全代理，所以使用 AutoGPT 时必须小心，如果没有给出正确的说明和安全指南，你不能让模型继续运行。

#### 实践

实践
- [AUTOGPT INSTALLATION AND FEATURES](https://autogpt.net/autogpt-installation-and-features/)

```sh
# 准备Python 3.8以上的环境, 安装minicoda
# source  ~/.bash_profile
conda create -n py310 python=3.10 # 创建 3.10环境
conda activate py310 # 激活环境
# 下载autogpt代码
git clone https://github.com/Torantulino/Auto-GPT.git
cd 'Auto-GPT'
pip install -r requirements.txt
# 配置文件
mv .env.template .env
vim .env # 填入 openai key 到变量 OPENAI_API_KEY
# python scripts/main.py
# python scripts/main.py --debug # 调试模式
# python scripts/main.py --speak # use TTS for Auto-GPT
python3 scripts/main.py # 多个虚拟环境时，为了避免干扰
python -m autogpt
```

【2023-7-7】
- 复制 默认的 env文件，只更新里面的openai_api_key

```sh
cp .env.template .env
```

命令，详见[指南](https://docs.agpt.co/)

```sh
./run.sh --help     # on Linux / macOS
./run.sh --debug # 打印日志
# Run Auto-GPT with a different AI Settings file shell
./run.sh --ai-settings <filename>
# Run Auto-GPT with a different Prompt Settings file shell
./run.sh --prompt-settings <filename>
# Specify a memory backend
./run.sh --use-memory  <memory-backend>
./run.sh --speak # 启动tts 语音播报
./run.sh --continuous # 100% 自动化，无需手动确认，有一定风险
./run.sh --gpt3only # 只用 gpt3
./run.sh --gpt4only # 只用 gpt4 ，更贵

```

#### 使用

AutoGPT配置
- 为AI取一个名字 \[Name]，一个角色定位\[Role]，同时你可以为它制定目标\[Goals]（最多5个目标，如果你仅有一个目标就直接回车）。
- 制定完成目标以后，AutoGPT会进行自主思考并分析你的目标\[THOUGHTS]，思考完成后开始理解并推理如何去完成这个目标\[REASONING]，然后开始自主拆解成具体的计划\[PLAN]，最后会提出评判\[CRITICISM] 用以保证 AI 代理纠正错误并作出正确的决断。
- 完成以上的行为规划后，AutoGPT会提示它将要作出的指令和动作[NEXT ACTION]， 里面包含具体执行的命令\[COMMAND]和参数\[ARGUMENTS]，用户可以在此时可以对风险命令进行识别，避免出现数据泄露等预期外的风险，这里可以通过y或者n进行授权或者拒绝AutoGPT接下来的指令动作。
- ![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/23d2ddb5b3ab4638af1aa22913f29142~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)
- AutoGPT会通过以上步骤，进行多次循环，由于AutoGPT可以存储上下文和历史经验，所以每一次都会根据反馈结果进行更深入的思考，制定出更优的方案，最后列举他要执行的计划，反复尝试和补充，直到达到你预期的目标。
- AutoGPT会通过以上步骤，进行多次循环，由于AutoGPT可以存储上下文和历史经验，所以每一次都会根据反馈结果进行更深入的思考，制定出更优的方案，最后列举他要执行的计划，反复尝试和补充，直到达到你预期的目标。


效果
- ![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ac14513a95c49b5a7bb87f2050ee170~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

#### COA

【2023-5-18】从 COT 到 COA
- `COT`(Chain of Thought，思维链) 
- `COA`(Chain of Action，行为链)：AutoGPT为代表，将自然语言表达的目标分解 为子任务，并利用互联网和其他工具自动迭代地尝试实现这些目标。

特点
- 自主化决策，任务链自动化
- 知行一体，参数外挂，泛化学习，
- 动态适应和灵活反应
- AI从模拟人类思维到模拟人类行为， 人主要负责设定目标、审批预算、 调整关键行动链

优点
- •自主任务分解  
- •上下文适应性
- •泛化多功能优化 
- •智能响应 
- •协同学习 
- •动态知识整合

缺点
- 语义鸿沟
- 依赖风险 
- 计算成本过高
- 透明度缺失

### AgentGPT -- 改进

`AgentGPT`：浏览器中直接部署自主 AI 智能体
- [项目主页](https://agentgpt.reworkd.ai)
- [GitHub 地址](https://github.com/reworkd/AgentGPT)

近日，又有开发者对 AutoGPT 展开了新的探索尝试，创建了一个可以在浏览器中组装、配置和部署自主 AI 智能体的项目 ——`AgentGPT`。项目主要贡献者之一为亚马逊软件工程师 Asim Shrestha

`AgentGPT` 允许自定义 AI 命名，执行任何想要达成的目标。自定义 AI 会思考要完成的任务、执行任务并从结果中学习，试图达成目标。
- 如下为 demo 示例：HustleGPT，设置目标为创立一个只有 100 美元资金的初创公司。

用户在使用该工具时，同样需要输入自己的 OpenAI API 密钥。AgentGPT 目前处于 beta 阶段，并正致力于长期记忆、网页浏览、网站与用户之间的交互。

```sh
git clone https://github.com/reworkd/AgentGPT.git
cd AgentGPT
./setup.sh
```

【2023-4-15】[免费的AutoGPT替代网站](https://zhuanlan.zhihu.com/p/622083666)
- 第一个是最火的AutoGPT，性能最强的，但是安装起来也挺麻烦的，并且还需要各种API的权限，小白不建议。
- 第二个AgentGPT，需要OpenAI的API，操作简单，在网页输入key就可以用。
- 第三个和第四个暂时是免费的，想体验的可以赶紧了。

| 名称 | 方案 | 特点 | 链接 |
| --- | --- | --- | --- |
| [AutoGPT](https://github.com/Torantulino/Auto-GPT) | 最复杂 | 需要安装开源代码 |  |
| [AgentGPT](https://agentgpt.reworkd.ai/) | 需要Token | 通过简单的网页访问易于使用，具有相对简单的功能 | |
| [Cognosys](http://cognosys.ai/) | 不需要Token | 性能不错，具有明确的任务组织 | |
| [Godmode](https://godmode.space/) | 不需要Token | 操作更加直观，每个步骤需要用户权限 | |

### Adept

Adept 和 Inflection 这两家早期团队想以自然语言为基础，为用户打造新的 **LUI** （语言为基础的 UI）方式。

### Inflection

待定

### GPT4Free

【2023-5-4】[GPT4Free](https://github.com/xtekky/gpt4free) ([discord](https://discord.com/invite/gpt4free)地址) 通过You.com、Quora和CoCalc等网站（OpenAI付费用户）提供的各种API地址，免费使用GPT-4和GPT-3.5模型。
- GPT4Free 脚本会先访问 https://you.com/api/streamingSearch，并传送各种参数过去，然后获取返回的JSON并对其进行格式化。
- GPT4Free仓库还有从Quora、Forefront和TheB等其他网站获取数据的脚本，任何开发者都可以基于这些脚本制作自己的聊天机器人。

实测：
- 安装
  - 要求 Python 3.8以上
  - 修改 requirements.txt 文件
- [The requirements.txt need to be updated](https://github.com/xtekky/gpt4free/issues/419)

```yml
# pypasser # 原始
pypasser>=0.0.5 # 指定版本，否则 pip install -r requirements.txt 提示冲突
```

UI部署正常，但点击“Think”后，出现新的[错误](https://github.com/xtekky/gpt4free/issues/406)：
- Please make sure you are using a valid cloudflare clearance token and user agent.

> An error occurred: failed to do request: Get "https://you.com/api/streamingSearch?q=hello&page=1&count=10&safeSearch=Moderate&onShoppingPage=False&mkt=&responseFilter=WebPages%2CTranslations%2CTimeZone%2CComputation%2CRelatedSearches&domain=youchat&queryTraceId=414f00ec-1837-406c-936a-c5ceeb0cd087&chat=%5B%5D": x509: â*.facebook.comâ certificate name does not match input. Please make sure you are using a valid cloudflare clearance token and user agent.


安装

```sh
pip install gpt4free
```

程序调用

```py
import gpt4free
from gpt4free import Provider, quora, forefront

# usage You
response = gpt4free.Completion.create(Provider.You, prompt='Write a poem on Lionel Messi')
print(response)
# usage Poe
token = quora.Account.create(logging=False)
response = gpt4free.Completion.create(Provider.Poe, prompt='Write a poem on Lionel Messi', token=token, model='ChatGPT')
print(response)
# usage forefront
token = forefront.Account.create(logging=False)
response = gpt4free.Completion.create(
    Provider.ForeFront, prompt='Write a poem on Lionel Messi', model='gpt-4', token=token
)
print(response)
print(f'END')
# usage theb
response = gpt4free.Completion.create(Provider.Theb, prompt='Write a poem on Lionel Messi')
print(response)
# usage cocalc
response = gpt4free.Completion.create(Provider.CoCalc, prompt='Write a poem on Lionel Messi', cookie_input='')
print(response)
```

错误信息
> tls_client.exceptions.TLSClientExeption: failed to do request: Get "https://you.com/api/streamingSearch?q=Write+a+poem+on+Lionel+Messi&page=1&count=10&safeSearch=Moderate&onShoppingPage=False&mkt=&responseFilter=WebPages%2CTranslations%2CTimeZone%2CComputation%2CRelatedSearches&domain=youchat&queryTraceId=77ebaf4c-ba0c-4035-bad6-1dafc27fdc14&chat=%5B%5D": dial tcp 192.133.77.59:443: i/o timeout (Client.Timeout exceeded while awaiting headers)


### MetaGPT

【2023-7-5】[MetaGPT](https://github.com/geekan/MetaGPT)
- MetaGPT: Multi-Agent Meta Programming Framework 多智能体变成框架
- MetaGPT takes a one line requirement as input and outputs user stories / competitive analysis / requirements / data structures / APIs / documents, etc.
- Internally, MetaGPT includes product managers / architects / project managers / engineers. It provides the entire process of a software company along with carefully orchestrated SOPs.
- Code = SOP(Team) is the core philosophy. We materialize SOP and apply it to teams composed of LLMs.
- ![](https://github.com/geekan/MetaGPT/raw/main/docs/resources/software_company_cd.jpeg)


### OlaGPT

【2023-8-9】[首个模拟人类认知的思维框架OlaGPT：推理能力最高提升85%](https://www.toutiao.com/article/7241109327501689381)

模型在对话上的表现实在是太像人类了，以至于产生了语言模型具有「思维能力」的错觉。基于高概率语言模式的再现与期望中的「通用人工智能」还有很大差距。
1. 某些情况下**生成内容毫无意义**，或者偏离了人类的价值偏好，甚至会给出一些非常危险的建议，目前的解决方案是引入人类反馈的强化学习（RLHF）对模型输出进行排序。
2. 语言模型的**知识仅限于**在训练数据中明确提到的概念和事实。

面对复杂问题时，语言模型也无法像人类一样适应变化的环境、利用现有的知识或工具、反思历史教训、分解问题，以及使用人类在长期进化中总结出的思维模式（如类比、归纳推理和演绎推理等）来解决问题。

当前大多数研究中，大模型主要是在**特定提示**的引导下生成**思维链**来执行推理任务，没有考虑**人类认知框架**，使得语言模型解决复杂推理问题的能力与人类之间仍然存在着显着的差距。

让语言模型模拟人脑处理问题的过程还有许多系统难题：
1. 如何系统地**模仿和编码**人类认知框架中的主要模块，同时以可实现的方式根据人类的通用推理模式进行调度？
2. 如何引导语言模型像人类一样进行**主动学习**，即从历史错误或专家对困难问题的解决方案中学习和发展？虽然重新训练模型对纠正后的答案进行编码可能是可行的，但显然成本很高而且不灵活。
3. 如何让语言模型灵活地利用人类进化出的各种思维模式，从而提高其推理性能？

一个**固定的、通用的**思维模式很难适应不同问题，就像人类在面对不同类型的问题时，通常会灵活地选择不同的思维方式，如类比推理、演绎推理等。

人类在面对复杂的推理难题时，通常会使用各种认知能力，并且需要与工具、知识和外部环境信息的各个方面进行交互，那语言模型能不能模拟人类的思维流程来解决复杂问题呢？
- 论文：[OlaGPT](https://arxiv.org/abs/2305.16334)
- 代码：[OlaGPT](https://github.com/oladata-team/OlaGPT)

OlaGPT 包括多个认知模块，包括注意力、记忆、推理、学习，以及相应的调度和决策机制；受人类主动学习启发，框架中还包括一个**学习单元**来记录之前的错误和专家意见，并动态参考来提升解决类似问题的能力。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/6ab3a339e2854a0b85402d6e3d8c9790~noop.image)

OlaGPT借鉴了认知架构（cognitive architecture）理论，把认知框架的核心能力建模为`注意力`（attention）、`记忆`（memory）、`学习`（learning）、`推理`（reasoning）、`行动选择`（action selction）。

提出一个适合语言模型解决复杂问题的流程，具体包括六个模块：意图增强模块（注意力）、记忆模块（记忆）、主动学习模块（学习）、推理模块（推理）、控制器模块（行动选择）和**投票**模块。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/5738c24560624a158d92bf58ccb36fc2~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=cw3fQ0rc6LC1ii9%2Bif6aTp%2FIKH4%3D)

人类解决问题的常见有效推理框架，并设计了思维链（CoT）模板；还提出了一个全面的决策机制，可以最大限度地提高模型的准确性。

**意图增强（Intention Enhance）**

注意力是人类认知的一个重要组成部分，识别出相关的信息并过滤掉不相关的数据。

同样地，研究人员为语言模型设计了相应的注意力模块，即意图增强，旨在提取最相关的信息，并在用户输入和模型的语言模式之间建立更强的关联，可以被看作是一个从用户表达习惯到模型表达习惯的优化转换器。

首先通过特定的提示词提前获得LLMs的问题类型，然后重构提问的方式。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/a658e25c9adf4339ab99921399a28946~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=t0lMLBJ%2FWobByPHC6LOKnzWvWos%3D)

比如在问题的开头加上一句「Now give you the XX（问题类型），question and choices:」；为了便于分析，提示中还需要加入「The answer must end with JSON format: Answer: one of options\[A,B,C,D,E\].」
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/3b4f507cfb4f4a7090385d6b7b8bb9a4~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=FPipJqSH%2BL0HIv5XWcLNxLBxt3k%3D)

**记忆（Memory）**

记忆模块在存储各种知识库信息方面起着至关重要的作用，已经有研究证明了当下语言模型在理解最新事实数据方面的局限性，而记忆模块着重于巩固模型尚未内化的知识，并将其作为长期记忆储存在外部库中。

研究人员使用langchain提供的记忆功能进行短期记忆，长期记忆则由基于Faiss的矢量数据库实现。

在查询过程中，其检索功能可以从库中提取相关知识，涵盖了四种类型的记忆库：事实、工具、笔记和思维（thinking），其中事实是现实世界的信息，如常识等；工具包括搜索引擎、计算器和维基百科，可以协助语言模型完成一些无需为条的工作；笔记主要记录一些疑难案例和解决问题的步骤；思考库主要存储由专家编写的人类解决问题的思考模板，专家可以是人类，也可以是模型。

**学习（Learning）**

学习的能力对于人类不断提升自我表现来说至关重要，从本质上讲，所有形式的学习都依赖于经验，语言模型可以从之前的错误中学习，从而实现快速提高推理能力。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/a3f6f32f8f1948cb832072c7e011bb11~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=9TRyX31yvOs2CVuIaPqwCkCFvnQ%3D)

首先，研究人员找出语言模型无法解决的问题；然后在笔记库中记录专家提供的见解和解释；最后选择相关的笔记来促进语言模型的学习，从而可以更有效地处理类似问题。

**推理（Reasoning）**

推理模块的目的是创建基于人类推理过程的多个智能体，从而激发语言模型的潜在思维能力，进而解决推理问题。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/4a36eb80ce674580a272882f74f066ac~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=%2BlQkkOMvaD3ue%2FUy6XcrfQGMgdw%3D)

该模块结合了多种思维模板，参考特定的思维类型，如横向思维、顺序思维、批判性思维和整合性思维，以促进推理任务。

**控制器（Controller）**

控制器模块主要用来处理相关的行动选择，具体包括模型的内部规划任务（如选择某些模块来执行）以及从事实、工具、笔记和思维库中选择。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/93e2d936c4e84a42a4e530ba72b73825~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=l6%2BacGSVoICV0Nc%2FnibV4pLQCig%3D)

首先检索和匹配相关的库，检索到的内容随后被整合到一个模板智能体中，要求语言模型以异步的方式在一个模板下提供回复，就像人类在推理之初可能难以识别所有的相关信息一样，同样很难期望语言模型一开始就做到这一点。

因此，动态检索是根据用户的问题和中间的推理进度来实现的，使用Faiss方法为上述四个库创建嵌入索引，其中各个库的检索策略略有不同。

**投票（voting）**

由于不同的思维模板可能更适合不同类型的问题，研究人员设计了投票模块来提升多个思维模板之间的集成校准能力，并多种投票策略来生成最佳答案以提高性能。

具体的投票方法包括：
1. 语言模型投票：引导语言模型在多个给定的选项中选择最一致的答案，并提供一个理由。
2. regex投票：用正则表达式精确匹配抽取答案以获取投票结果。


在多个推理数据集上进行了严格评估后得到的实验结果表明，OlaGPT超越了此前最先进的基准，证明了其有效性。
1. SC（self-consistency）的性能优于GPT-3.5-turbo，表明在一定程度上采用集成方法确实有助于提高大规模模型的有效性。
2. 文中提出方法的性能超过了SC，在一定程度上证明了思维模板策略的有效性。
  - 不同思维模板的答案表现出相当大的差异，在不同的思维模板下进行投票，最终会比简单地进行多轮投票产生更好的结果。
3. 不同思维模板的效果是不同的，循序渐进的解决方案可能更适合推理型问题。
4. 主动学习模块的性能明显优于零样本方法。
  - 具体来说，随机、检索和组合列表现出更高的性能，即将具有挑战性的案例作为笔记库纳入其中是一种可行的策略。
5. 不同的检索方案在不同的数据集上有不同的效果，总的来说，组合（combine）策略的效果更好。
6. 文中方法明显优于其他方案，这得益于整体框架的合理设计，包括主动学习模块的有效设计；思维模板实现了对不同模型的适应，不同思维模板下的结果是不同的；控制器模块起到了很好的控制作用，选择了与所需内容比较匹配的内容；投票模块设计的不同思维模板的集成方式是有效的。

### 群体智能


#### Camel

[Camel](https://www.camel-ai.org/) 通过 LLM 来模拟用户和 AI 助手，让两个 agent 进行角色扮演（例如一个是业务专家，一个是程序员），然后让他们自主沟通协作来完成一项具体的任务。

这个想法比较直接，不过作者也提到 prompt 的设计还是蛮重要的，否则很容易出现角色转换，重复指令，消息无限循环，有瑕疵的回复，何时终止对话等等问题。具体看项目代码中给出的 prompt 设定，添加了非常多的明确指令来让 agent 按照预想的设定来沟通协作。
- ![](https://pic1.zhimg.com/80/v2-0f871ff3f5b1af49a74d56caa39a785b_1440w.webp?source=1940ef5c)

除了 agent prompt 和运作模式的设计优化外，作者还设计了 prompt 来**自动**生成各种角色，场景诉求等内容。这些内容在自动组成各种角色扮演的场景，就能收集到各个场景下 agent 的交互情况，便于后续做进一步的挖掘分析。[这个网站](https://data.camel-ai.org/) 来探索已经生成的各种 agent 组合之间的对话记录。这个项目代码也做了开源，会是一个非常好的研究 AI agent 社群研究方向的起点。
- ![](https://pic1.zhimg.com/80/v2-3dbb028cb33164d20aaff5ba6b0c1a65_1440w.webp?source=1940ef5c)


#### Generative Agents

【】[如何看本周最火的AutoGPT？](https://www.zhihu.com/question/595382995/answer/2989954125)

沿着这个方向进一步推演，是否可以将多个 agent 组成一个团队，分别扮演不同的角色，是否能更好地解决一些复杂问题，甚至让这个小的“社群”演化出一些更复杂的行为模式甚至新知识的发现？

[Generative Agents](https://arxiv.org/abs/2304.03442) 中，作者将 25 个拥有身份设定的模型 agent 组成了一个虚拟小镇社群，每个 agent 都具有记忆系统，并通过做计划，行动应答，自我反思等机制来让他们自由活动，真正来模拟一个社群的运作。从模拟过程来看这个社群也“涌现”了不少真实社会中的现象，非常有意思。

几个 agent 行为的设定值得学习：
- 每个 agent 的**记忆获取**做得更加细致，会结合**时效性，重要度和相关度**来做相关记忆的召回。相比简单的**向量相似度搜索**来说效果会好很多。
- 记忆**存储**方面也添加了 reflection 步骤，定期对记忆进行**反思总结**，保持 agent 的“目标感”。
- 在 plan 生成方面也做了**多层级递归**，由粗到细生成接下来的行动计划，跟我们的日常思考模式也更接近。
- 通过“**人物采访**”的方式来评估这些行为设定的效果，消融实验中都能发现明显的提升。
- ![](https://picx.zhimg.com/80/v2-4d785241de1d097d8c5ba10b2666fba2_1440w.webp?source=1940ef5c)

一整套 identity，plan， act/react，reflect，memory stream 逻辑挺合理的，与 AutoGPT 的做法可以进行一些互补。当然局限性应该也有不少，比如
- 模拟过程中 agent 之间都是一对一的谈话，而没有会议/广播这种设定。
- 目前模拟运行的时长也有限，比较难确保长时间的运行下 agent 的记忆、行为模式的演化，社群整体目标的探索与推进等方面的效果。

从应用角度来看，目前好像也主要集中在社会活动模拟，游戏应用等。是否能拓展到任务处理，知识探索等更广阔的领域，还有待进一步探索。


# 结束