---
layout: post
title:  大模型代理机器人 LLM Agent
date:   2023-07-13 22:46:00
categories: 大模型
tags: gpt ChatGPT prompt RL 吴恩达 Agent 第一性原理 角色模拟 多模态 kaggle 操作系统 人脑
excerpt: 大模型 LLM 驱动的智能体  
mathjax: true
permalink: /agent
---

* content
{:toc}

# LLM 智能体


## 资讯

- [飞书文档: Agent 技术文章集合](https://agijuejin.feishu.cn/wiki/KAznwTm9giYjzJkbJSjcqF9FnKe)
- [Agent 原理介绍与应用发展思考](https://zhuanlan.zhihu.com/p/654652104)
- 【2023-9-22】米哈游发布了一篇人工智能代理调查论文
  - [LLM-Agent-Paper-List](github.com/WooooDyy/LLM-Agent-Paper-List)
- 【2024-10-13】拾象科技:[AI Agent的千亿美金问题:如何重构10亿知识工作职业，掀起软件生产革命？](https://mp.weixin.qq.com/s/JYu_oXWbWbasT1fcBRo-cA)
- 【2023-10-24】[Agent论文合集:RL-based、LLM-based 前沿研究汇总](https://mp.weixin.qq.com/s/xwbsSId31iFwlKoLWJrZ-A), [知乎](https://zhuanlan.zhihu.com/p/661741663), GitHub [Awesome-Papers-Autonomous-Agent](https://github.com/lafmdp/Awesome-Papers-Autonomous-Agent)
- 【2023-10-20】[从第一性原理看大模型Agent技术](https://mp.weixin.qq.com/s/PL-QjlvVugUfmRD4g0P-qQ)
  - 技术脉络: Model -> Prompt工程 -> Prompt Chain或Flow -> Agent -> 多Agent

【2024-12-16】[微软 CEO 的大胆预言：“AI Agent将替代所有SaaS”](https://mp.weixin.qq.com/s/BD6jbJLAi7xWeMSHDyTLIw)
- 原视频地址：[Youtube](https://www.youtube.com/watch?v=9NtsnzRFJ_o)

微软CEO`萨提亚·纳德拉`宣布应用程序时代的终结,软件开发迎来新方向
- 商业应用程序的概念可能会在智能代理时代消失。　
- 应用程序正在消失,取而代之的是智能代理。　
- 不再有应用程序,也就不再有SaaS(软件即服务), 需要的开发人员会大幅减少。　

SaaS的本质只是数据库之上的一层用户界面和业务逻辑


### HCI

- 【2024-10-28】斯坦福李飞飞最新巨著🔥《AGENT AI》
  - 《[AGENT AI: SURVEYING THE HORIZONS OF MULTIMODAL INTERACTION](https://arxiv.org/pdf/2401.03568)》深入探讨了**多模态人机交互**（Human-Computer Interaction, HCI）的发展状态和未来研究方向。
  - 多模态HCI旨在通过语音、图像、文本、眼动和触觉等多种信息模式来实现人与计算机之间的信息交换，这种交互方式在生理心理评估、办公教育、军事仿真和医疗康复等领域具有广泛的应用前景。

多模态HCI未来的研究方向，包括拓展新的交互方式、设计高效的多模态交互组合、构建小型化交互设备、跨设备分布式交互以及提升开放环境下交互算法的鲁棒性。：
- `[一]` 大数据可视化交互：随着物联网和人工智能技术的发展，人机交互设备已经广泛应用于日常生活中。大数据可视化交互技术通过将抽象数据转换为图形化表征，使用户能够更直观地理解和探索数据。研究者们正在探索如何利用多感知通道来增强数据可视化的交互体验，例如通过触觉和听觉来补充视觉信息，提升用户的沉浸感和参与感。
- `[二]` 基于声场感知的交互：这种交互方式涉及到使用麦克风阵列和机器学习算法来识别特定场景、环境或人体发出的声音。它允许用户通过声音与计算机进行交互，提供了一种非视觉的交互手段。
- `[三]` 混合现实实物交互：混合现实技术结合了物理世界和虚拟世界，使用户能够通过现实世界中的物体与虚拟环境进行交互。这种交互方式在虚拟现实和增强现实中变得越来越重要，它允许用户以更自然的方式与虚拟对象进行互动。
- `[四]` 可穿戴交互：随着智能手表和健康监测设备的普及，可穿戴设备成为了HCI的一个新的研究方向。研究者们正在探索如何通过手势、触摸和皮肤电子技术来实现更自然的交互方式。
- `[五]` 人机对话交互：人机对话交互涉及到语音识别、情感识别、对话系统和语音合成等多个模块。研究者们致力于提高对话系统的性能，使其能够更自然地理解和响应用户的语音输入。


### 论文综述


资料
- 【2023-11】论文 [Igniting Language Intelligence: The Hitchhiker’s Guide From Chain-of-Thought Reasoning to Language Agents](https://arxiv.org/pdf/2311.11797.pdf) 从 CoT 到 Agent


综述:[基于大型语言模型的自主机器人](https://www.zhihu.com/pin/1677734808908824577)
- [A Survey on Large Language Model based Autonomous Agents]()
  - 基于 LLMs 的人工智能代理在社会科学、自然科学和工程学领域的各种应用，以及基于 LLMs 的自主代理常用的评估策略，面临的若干挑战和未来发展方向等
- 论文:[ProAgent: Building Proactive Cooperative AI with Large Language Models]()
  - 【ProAgent:利用大型语言模型构建**主动式合作**AI】
  - 构建具有自适应行为的人工智能是 AGI 研究的一个关键重点。目前开发合作代理主要依赖于**基于学习**的方法，策略泛化在很大程度上取决于过去与特定队友的互动。这些方法限制了代理在面对新队友时重新调整策略的能力。 
  - 该研究提出了 ProAgent 框架，利用大型语言模型（LLMs）能够**预测**队友即将做出的决定，并为自己制定增强型计划。擅长**合作推理**，能够动态调整自己的行为。
  - 此外，ProAgent 框架还具有高度的模块化和可解释性，便于无缝集成，以应对各种协调场景。
  - 在与人类代理模型合作时，与目前最先进的 COLE 相比，ProAgent 的性能平均提高了 10% 以上。
- 论文:[Building Emotional Support Chatbots in the Era of LLMs]()
  - 【在大型语言模型时代，打造情感支持聊天机器人】 
  - 将情感支持融入各种对话场景会带来深远的社会效益，如社交互动、心理健康咨询和客户服务。然而，数据可用性有限、缺乏公认的模型训练范例等一些尚未解决的难题阻碍了这一领域的实际应用。 
  - 该研究介绍了一种新方法，该方法综合了人类的洞察力和大型语言模型（LLM）的计算能力，进而策划了一个广泛的情感支持对话数据集——ExTES。该方法以精心设计的、跨越不同场景的对话集作为生成种子。通过利用 ChatGPT 的上下文学习潜力，我们递归生成了一个 ExTensible 情感支持对话数据集，命名为 ExTES。 对所生成模型的详尽评估证明了该模型在提供情感支持方面的能力，标志着情感支持机器人领域迈出了关键的一步。 
- 论文:[Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models]()
  - 【自我欺骗:反向穿透大型语言模型的语义防火墙】 
  - 以 ChatGPT 为代表的大型语言模型（LLMs）在为各种社会需求提供便利的同时，还降低了生成有害内容的成本。 尽管 LLMs 开发人员部署了语义层面的防御措施，但这些防御措施并非万无一失，一些攻击者通过制作“越狱”提示，使 LLMs 忘记内容防御规则，从而回答任何不当问题。迄今为止，业界和学术界都没有明确解释这些语义级攻击和防御背后的原理。 受通过反向隧道穿透传统防火墙的攻击的启发，该研究提出了一种“自我欺骗”攻击，通过诱导 LLMs 生成有利于越狱的提示来绕过语义防火墙。 他们在七个虚拟场景中用六种语言（英语、俄语、法语、西班牙语、中文和阿拉伯语）生成了共计 2520 个攻击，目标是三种最常见的违规类型:暴力、仇恨和色情。 
  - 结果显示，GPT-3.5-Turbo 和 GPT-4 两个模型的成功率分别为 86.2% 和 67%，失败率分别为 4.7% 和 2.2%，这凸显了拟议攻击方法的有效性。

论文介绍（机翻）
- 长期以来，人类一直追求与甚至超越人类水平的人工智能（AI），而AI代理被认为是这种追求的有前途的载体。AI代理是感知环境、做出决策并采取行动的人工实体。许多努力已经致力于发展智能代理，但它们主要关注算法或训练策略的进步，以增强特定能力或在特定任务上的表现。
- 事实上，社区缺乏的是一个通用且强大的模型，作为设计AI代理的起点，可以适应各种场景。由于它们展示的多功能能力，大型语言模型（LLM）被视为人工通用智能（AGI）的潜在火花，为构建一般化的AI代理提供了希望。许多研究人员已经利用LLM作为构建AI代理的基础，并取得了重大进展。在本文中，对基于LLM的代理进行全面调查。从其哲学起源到在AI中的发展追溯代理的概念，并解释了LLM为何是代理的合适基础。在此基础上，我们提出了一个基于LLM的代理的通用框架，包括三个主要组件:大脑、感知和行动，该框架可以根据不同的应用进行定制。随后，我们探讨了LLM代理在单一代理场景、多代理场景和人代理合作方面的广泛应用。在此基础上，我们深入了解代理社会，探讨基于LLM的代理的行为和个性、代理社会中出现的社会现象以及它们为人类社会提供的见解。

- 【2023-9-11】[一文盘点「AI自主智能体」的构建、应用、评估](https://www.toutiao.com/article/7277401314718466616)
  - 论文:[A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/pdf/2308.11432.pdf)
  - 机构:中国人民大学-高瓴人工智能学院, GitHub地址:[LLM-Agent-Survey](https://github.com/Paitesanshi/LLM-Agent-Survey)
- 基于 LLM 的自主智能体一览，包括工具智能体、模拟智能体、通用智能体和领域智能体
  - ![](https://github.com/Paitesanshi/LLM-Agent-Survey/raw/main/assets/trend.png)
-  Construction of LLM-based Autonomous Agent
  - ![](https://github.com/Paitesanshi/LLM-Agent-Survey/raw/main/assets/architecture-1.png)

## LLM

1981年`希拉里·普特南`在《理性，真理与历史》一书中提出假想，“缸中之脑”:[参考](https://zhuanlan.zhihu.com/p/89931301)
> 有一个科学家实施了这样一个手术，他把别人的大脑切下来，放进一个充满营养液的缸中，营养液可以维持大脑正常运转。大脑的神经末梢连接在了电线上，电线的另一边是一台计算机。这台计算机模拟真实世界的参数，通过电线给大脑传送信息，让大脑保持一切完全正常的感觉。对于大脑来说，似乎人、物体、天空还都存在。

大模型是`瓮中之脑`，《黑客帝国》，《源代码》这类电影很关键的一个预设
> 假如把人脑放到一个充满营养液的罐子里，活下来，那这时候就有一个超级真实的虚幻世界，但却不再能干涉现实。

当前的LLM有一定智力并且拥有大量知识，但除了内容生成这类通用能力，在别的领域还不清楚它到底能干什么

人类日常要处理的任务场景
- **离散**、**孤立**（环境无关）: 无时空依赖
  - 如:编程、下围棋、内容生成
- **连续**、与环境**捆绑**: 环境相关
  - 如:外卖、打车、经营企业

瓮中之脑只能解决**前者**，而绝大多数场景都是**后者**，解决的关键在于AI Agent

所以，AI Agent是大模型与场景间价值传递桥梁

业界认为基于大模型的应用集中在两个方向上:`RAG` 和 `Agent`，无论哪一种应用，设计、实现和优化能够充分利用大模型(LLM)潜力的应用都需要大量的努力和专业知识。

### Agent vs LLM

这类项目绝大多数的主要创新还是在 **prompt 层面**，通过更好的提示词来激发模型的能力，把更多原先需要通过代码来实现的流程“硬逻辑”转化为模型自动生成的“动态逻辑”。

目前语言模型只能响应用户的**查询指令**，实现一些**生成任务**，比如写故事、生成代码等。而以 `AutoGPT`, `GPT-Engineer`和`BabyAGI`等项目为代表的**大型动作模型**（Large-Action Models，`LAM`）将语言模型作为智能体的**核心大脑**，将复杂任务分解，并在每个子步骤实现自主决策，无需用户参与即可解决问题。

LAM的崛起，也标志着语言模型的研发正在走向新阶段


### Agent vs Workflow

AI Agent与 AI Workflow 区别： 
- AI Workflow（工作流）： 一系列**预先定义**的大模型**调用步骤**，像在“轨道上”运行，步骤固定，可预测。每一步骤都有明确的输入和输出 
- AI Agent（智能体）：AI Agent 更加**自主**，由大模型**自行决定**执行多少步骤， 直到问题得到解决。AI Agent会持续循环，例如与客户沟通或迭代代码修改， 步骤数量不固定

### Agent vs Tool


|对比维度|传统AI工具|AI Agent|
| ---- | ---- | ---- |
|任务触发|人类指令触发（如Siri问答）|环境感知自主启动（如特斯拉FSD预判刹车）|
|决策机制|规则驱动（if-else逻辑树）|目标驱动（LLM生成动态策略树）|
|交互模式|单次请求-响应|长期记忆支持的连续对话与协作|
|能力边界|限定场景封闭系统|跨领域经验迁移（如医疗Agent转金融风控）| 


### Agent → AGI

【2023-7-24】[最近](https://zhuanlan.zhihu.com/p/639964649)都不卷大模型了，开始卷 AI Agents
- LLM诞生之初，大家对于其能力的边界还没有清晰的认知，以为有了LLM就可以直通AGI了，路线: LLM -> AGI
- 过了一段时间，发现LLM的既有问题（幻觉问题、容量限制...），导致并不能直接到达AGI，于是路线变成了: LLM -> Agent -> AGI
  - 借助一个或者多个Agent，构建一个新的形态，来继续实现通往AGI的道路。但这条路是否能走通，以及还面临着哪些问题，都是有待进一步验证的。

由于大模型的出现，AI Agents 衍生出了一种新的架构形式: 《LLM Powered Autonomous Agents》
- 将最重要的「任务规划」部分完全交由LLM，而做出这一设计的依据在于默认LLM具有任务分解和反思的能力。

## AI Agent

AI Agent是`大模型`与`场景`间价值传递**桥梁**。
- AI Agent 被认为是 OpenAI 发力的下一个方向

OpenAI 联合创始人 `Andrej Karpathy` 提到:
> “相比模型训练方法，OpenAI 内部目前更关注 Agent 领域的变化，每当有新的 AI Agents 论文出来的时候，内部都会很兴奋并且认真地讨论”

而在更早之前，Andrej 还评价 AutoGPT 是 Prompt Engineering 下一阶段的探索方向。


### AI agent 为什么重要？

原因
1. AI Agent 将使软件行业降低生产成本、提高定制化能力，进入软件的“3D 打印”时代
  - `Software 2.0` 意味着我们可以用大量的数据和算力来解决以前需要大量人力和成本来解决的复杂问题。
  - ak47: AI agent 正是`Software 2.0`的具象化
2. LLM 扮演人类思考的系统 1（`快思考`），AI Agent 扮演人类思考的系统 2（`慢思考`）
  - LLM 反应快，但会出现`幻觉`（hallucination）问题，很像人类的系统 1 中常见的思维谬误和本能反应
  - AI agent 长期目标则是使 LLM 胜任系统2的工作，为 LLM 搭建一套框架来进行深度思考和分析，从而做出更复杂和可靠的决策。
3. 推荐系统让每个人看到个性化的信息， AI Agent 将让每个人有个性化的工作方式，为每一个知识工作者提供 AI 合作伙伴和工作分身

参考
- 【2024-10-13】拾象科技:[AI Agent的千亿美金问题:如何重构10亿知识工作职业，掀起软件生产革命？](https://mp.weixin.qq.com/s/JYu_oXWbWbasT1fcBRo-cA)

### AI Agent 定义

`Agent` 起源于拉丁语中的`Agere`，意思是“to do”。

Agent 可以追溯到**明斯基**的《society of mind》一书。
- 明斯基对Agent的定义有点抽象:“社会中某个个体经过协商后可求得问题的解,这个个体就是agent”。

计算机领域，Agent是一种通过**传感器**感知其环境，并通过执行器作用于该环境的实体，因此，可以把实体定义为一种从感知序列到实体动作的映射。一般认为，Agent是指驻留在某一环境下，能持续自主地发挥作用，具备自主性、反应性、社会性、主动性等特征的计算实体。

智能 是Agent 与环境相互作用的涌现属性。

Agent 核心能力是**完成任务**（achieve goals）、**获取知识**（acquiring knowledge）和**进化**（improve）

AI agent 是有能力**主动**思考和行动的智能体。
- 提需求时，agent 有能力自行**感知**环境、形成**记忆**、**规划**和决策**行动**，甚至与别的 agent 合作实现任务。
- 而 LLM **被动**推理引擎，用户每 prompt 点拨一次才会回复一次。

LLM-based Agent 组件和定义还有模糊的地方，三种理解方式:
1. AI agent 是 AI-native software **开发方法**，不是独立的商业赛道或产品形态。
  - 未来的 AI-native 应用都会有 agent 思路，产品形态可能是 Copilot，也可能还未出现。这一框架下 LLM 是核心推理引擎，而 agent 是使 LLM 扬长避短、结构化思考的**方法论**。
2. 优秀的 AI Agent 产品比传统软件更**灵活**，比 LLM 更**可靠**。
  - 传统软件中有很多规则和启发式算法，带来了高可靠性、确定性，但也使其难以灵活解决长尾问题。优秀的 AI agent 应用需要充分发挥 LLM **推理** (reason)、**扮演** (act) 和**交互** (interact) 的能力。
  - 短期内，这样的应用会牺牲一定软件的可靠性，因此 agent 应用的落地现状并不乐观。但是沿着当前的技术路径走下去，可以预期在长期达到与当前的传统软件接近的可靠性。
3. Agent 和早期的 LLM-based 应用相比，有几个显著差异点:
  - • **合作机制** orchestration:存在多模型、多 agent 分工与交互的机制设计，能实现复杂的工作流。
    - 例如编程场景下可能有 developer agents 和 quality assurance agents，类比开发团队里的工程师和测试；
    - 产品战略场景下可能有 growth agents 和 monetization agents，类比公司中投放和商业化团队在用户规模和商业收入上的多目标博弈。
  - • 与**环境交互** grounding:Agent 能理解自己的不足，并适时从外部寻找合适的工具解决问题。人和动物的区别是人会使用工具，agent 框架能帮助 LLM 认识自己的能力边界，从外部环境中寻找合适的工具。
  - • **个性化记忆** memory:能记忆用户偏好和工作习惯，使用越久越了解用户。未来的 agent 作为人类的工作伙伴会接受大量文本和多模态信息，过程中积累的用户偏好和工作习惯会使产品成为知识工作者最信赖的伙伴。
  - • **主动决策** decision:Agent 有能力在虚拟环境中探索、试错、迭代。目前的 LLM 应用还缺少在环境中连续决策的能力，因为 next token prediction 落子无悔的预测形式和人类的思考方式是截然不同的:开发者在 coding 解题的时候，会对一系列假设方案进行实验总结出最优解，而不是在一开始就能够得到最优解。

四个特点的实现时间由短到长排序:
- •  短期:Orchestration 和 grounding 短期内值得重点探索，当下 LLM 能力还不够强，需要自己与外部环境交互找到合适的工具，并且由用户专家对 AI 的工作流进行设计和编排，使其成为靠谱的合作伙伴。如果当前的应用在这两个方向上没有足够深的编排和实践，不能称为是 AI agent 应用。
- •  中期:Memory 是需要重点强化的方向。随着人类与 AI agent 的信任加深之后，如何让人机协作带来新的数据飞轮是一个相当重要的问题。有了个性化记忆的生成模型，是 Gen AI 时代新形态的推荐引擎。
- •  长期:Decision 则是长期目标，也是需要 OpenAI、Anthropic 等大模型公司一起去解决的问题。目前的 next token prediction 模型和 chat-based UI，目标函数太简单很难让 agent 真正学会主动探索、试错，需要模型复杂推理能力和产品形态的双重进化。

参考
- 【2024-10-13】拾象科技:[AI Agent的千亿美金问题:如何重构10亿知识工作职业，掀起软件生产革命？](https://mp.weixin.qq.com/s/JYu_oXWbWbasT1fcBRo-cA)


### AI Agent 现状

(1) Autonomous Agent 有很强的启发意义，但缺乏可控性，不是未来的商用方向

Autonomous agent 指完全由 LLM 自驱的规划工作流、并完成任务的 agent 产品，最典型的就是2023年3月发布的 AutoGPT 和 BabyAGI

优点:
- •  智能程度和普适性高，能较好的理解和推理复杂的任务并且做出规划
- •  能高效的判断并使用外部工具，整个过程的衔接非常流畅

但随着使用深入，发现其实验性强于实用性，大部分问题并不能真的解决:
- •  效果不稳定，多步推理能力不够:大部分产品 demo 看上去效果惊艳，但是对于抽象复杂的问题，能有效解决的比例不到 10%（让AI自我规划容易产生死循环，或者会出现一步走错，步步走错的问题），只适合解决一些中等难度的问题。这需要等 LLM 的下一次技术突破，尤其是其复杂推理能力的提升
- •  外部生态融合度不高:第三方 api 支持的数量和生态不多（基本以搜索和文件读取功能为主），很难做到比较完整的跨应用生态

这两个缺陷正是 existing company 的优势
- 第一点是 OpenAI/Anthropic 这类 LLM 公司的目标
- 第二点是 Apple/Google/Microsoft 这类有自己软硬件生态、操作系统公司最适合做的。

因此尽管 AutoGPT 是好的思想实验，但通用的 Autonomous agent 并不适合 startup 作为商业方向。

(2) `Agent Framework` 和 `Vertical Agent` 是 AI agent 商业上最可行的两个方向，需要持续探索人机交互的方式

现阶段适合创业公司的机会就是需要**人为干预和设计**的 agent 产品。

目前有两类介入使 Agent 更可控的思路
- 一类是**中间层** `Agent Framework`，提供设计 agent 的 infra 工具，由用户介入为 agent 提供规划思路；
  - 创新点: 模块化设计、适配性(APIs/SDKs)、合作机制设计
  - Agent Platform: 
  - 案例: 2022年在西雅图成立的初创公司 [Fixie AI]()
  - Agent Workflow: 提高可控性最好的方式就是去帮 AI 设计 workflow，把规划职责部分转移给用户
- 一类深入**细分垂类** `Vertical Agent`，运用 agent 思路设计 Copilot 产品，由产品设计者介入是 agent 思路更为可控。
  - 核心竞争力是领域知识针对性和交互反馈: 领域知识、工作流理解、数据反馈和多agent写作
  - 案例: 
    - Coding Agent
      - [Voiceflow]() (特色功能:意图识别管理/信息抽取)
      - 最看好 LLM-first IDE(代表Cursor) 和 Generative UI(代表 Vercel v0)
    - 个人助理: Lindy.ai
    - 写作 Agent:Hyperwriter
    - 数据分析 Agent:Julius AI

### AI Agent 分类

大语言模型（LLM）出现后，其通用文本处理能力使通用 Agent（General Agent）呼之欲出，于是出现了各种`LLM-powered agent`。与此同时，基于`RL`的“传统”Agent仍在发展，还有应用场景广泛的`Multi-Agent`等。
- ![](https://pic3.zhimg.com/80/v2-91043cc84449d67fd94fbfba6aa482b2_1440w.webp)

Agent 是与大模型主动交互的一种重要程序形式，而 Multi-Agent 则是多个Agent利用大模型完成复杂任务的系统机制。

更多论文进展见[文章](https://zhuanlan.zhihu.com/p/661741663)

### AI Agent 评测

#### SuperCLUE

实时榜单
- [SuperCLUE-Agent智能体](https://www.superclueai.com/)
  - GitHub 地址 [SuperCLUE-Agent](https://github.com/CLUEbenchmark/SuperCLUE-Agent)
- SuperCLUE通用榜
- SuperCLUE-Safety

> `GPT-4` > `ChatGLM3-Turbo` > `Claude2-100k` > `GPT-3.5 Turbo` > `Baichuan2-13b-Chat`


资讯
- 【2023-10-19】[SuperCLUE-Agent:Agent智能体中文原生任务评估基准](https://www.cluebenchmarks.com/superclue_agent.html)
- 【2023-11-8】[ChatGLM3刷新智能体中文基准SuperCLUE-Agent最好成绩](https://mp.weixin.qq.com/s/gPT1pIW0UskOcyqbA-ibzA)

10月27日，清华&智谱AI推出了全自研的第三代基座大模型ChatGLM3及相关系列产品
- `ChatGLM3` 集成了自研的 `AgentTuning` 技术，激活了模型智能体能力，尤其在规划和执行方面，相比于 ChatGLM2 提升明显，并且支持工具调用、代码执行、游戏、数据库操作、知识图谱搜索与推理、操作系统等复杂场景。

ChatGLM3 在 SuperCLUE-Agent 评测集上的表现如何？与国内外代表性大模型相比处于什么位置？在各项智能体关键能力上如工具使用、任务规划等任务上的表现如何？

[SuperCLUE-Agent](https://github.com/CLUEbenchmark/SuperCLUE-Agent) 聚焦于Agent能力的**多维度**基准测试，包括3大核心能力、10大基础任务，可用于评估大语言模型在核心Agent能力上的表现，包括: **工具使用**、**任务规划**和**长短期记忆**能力
- （1）**工具使用**:调用api、检索api、规划api和通用工具使用
  - 通用工具:如 搜索引擎、浏览网页、操作本地文件、搜索本地文件、使用数据库等等。
- （2）**任务规划**:任务分解、自我反思、思维链
  - 任务规划: AI Agent将大型任务分解为较小的、可管理的子目标，从而能够高效地处理复杂任务的能力
  - 自我反思: 自我批评和反思，从错误中吸取教训，并为未来的步骤进行改进，从而提高最终结果
  - 思维链: 将困难的任务分解为更小、更简单的步骤
- （3）**长短期记忆**:多文档问答、少样本学习、长程对话
  - 多文档问答: 多个文档中提取并组合答案
  - 长程对话: 用大模型谈论几个话题并在其中切换, 测试方法是检索由多个主题组成的长对话中的开头和中间过程的主题
- ![](https://www.cluebenchmarks.com/static/img/SuperCLUE_Agent.png)
- 评测数据[示例](https://www.cluebenchmarks.com/superclue_agent.html)

结论
- 1:SuperCLUE-Agent基准上，ChatGLM3 在智能体能力上表现不俗，刷新了SuperCLUE-Agent国内模型最高分，暂列榜单首位。
- 2:相比 ChatGLM2，ChatGLM3 有**67.95%**的显著提升。
- 3:ChatGLM3 在任务分解、检索API、通用工具使用、多文档对话、少样本示例学习等任务处于国内头部水平，在自我反思任务上有一定的优化空间。

16个闭源/开源的模型整体表现
- ![](https://www.cluebenchmarks.com/static/img/agent/superclue_agent_performance1024.png)


## RL-based Agent

LLM 出现之前，multi-agent 主要存在于`强化学习`和`博弈论`(game theory) 的相关研究中

RL-based agent 研究相对较少，可能是 RL从头开始训练，缺少很多知识，因此在性能上很难对拼 LLM-based Agent。

但也有工作利用RL在环境中探索、学习的能力和LLM丰富的世界知识，研究如何将 RL-based、LLM-based agent结合。

### RL

强化学习和其他 AI 范式的重要区别

经典三大范式（`监督学习`、`非监督学习`、`强化学习`）中只有`强化学习`的假设是**让 AI 进行自主探索、连续决策**，这个学习方式最接近人类的学习方式，也符合想象中的 AI agent 应该具备的自主行动能力。

强化学习的核心在于"**探索**"（Explore）和"**利用**"（Exploit）之间的权衡。

LLM 在"利用"现有知识上做到了现阶段的极致，而在"探索"新知识方面还有很大潜力，RL 的引入就是为了让 LLM 能通过探索进一步提升推理能力。

RL 过程有两个核心组件。他们之间一直在反复交互，agent 在环境中执行 action，并且根据环境的变化评估 reward:
- • Environment: AI 探索完成任务的环境，当 Alphago 下围棋时，环境就是 19x19 的棋盘。环境会发生变化，AI 会从环境变化中收到 reward value 判断过去的那一系列探索是否有明显的收益，例如距离下围棋胜利是否更接近了。
- • Agent: agent 会根据对环境的观测和感知来输出一个动作，目标是得到更高的 reward。agent 这个概念最早就是来自强化学习。

### self-play

self-play 是让 LLM 同时扮演一个或多个 agent model 去做推理任务，并由另一个 LLM 作为 reward model 来给出打分评价，一定次数后更新 LLM 权重让其多记住做得好的推理方式。

Self-play 是 AlphaZero 等强化学习算法的合成数据方法，最早可以追溯到 1992 年的 TD-Gammon 算法。

这个方法本质是利用 AI 无限的计算能力来补足它数据利用效率不够的短板，更符合当下 AI 的优势。好的 self-play 能合成大量高质量的数据，甚至可能比人类历史上见过的棋局、游戏数更多，用数据量来做到 super human:AlphaGo， Dota Five 都探索出了和人类不一样的游戏套路，并战胜了大部分职业选手。

self-play 给了模型一个自己“卷"自己不断进步的框架，MCTS 方法让模型在连续决策中更容易“打出连招”，self-play+LLM+MCTS 会成为 LLM post-training 中新的范式。

RL 新范式下，LLM 训练的 scaling law 需要被重写。
- 因为训练时计算量不再只是和参数量的上升有关，还多了一个新变量:**self-play 探索时 LLM inference 的计算量**。
- RL本质是用 inference time 换 training time，来解决模型 scale up 暂时**边际收益递减**的现状。

最近 DeepMind 也发布了一篇paper 叫做:
- Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters

新的 scaling law 正在浮现:算力周期性从 scaling 转移到 inference-time compute

【2024-8-30】[LLM的范式转移:RL带来新的 Scaling Law](https://mp.weixin.qq.com/s/JPfgF6UtgIYwWXwNQHOoqQ)

## LLM-based Agent

当前 Agent 文章还是以 LLM-based 方法为主，基于LLM做各种推理、规划等。
- ICLR的投稿中设计LLM-based agent架构来解决特定问题的方法不多，这类研究我会打上【task-specifc】的标签。

### 单 Agent 系统

#### Agent 特点

Agent 一般结构如下图所示:  
- ![图片](https://mmbiz.qpic.cn/mmbiz_png/DE2dk1GjczqUtZyzbHktkrfcia2SwzaEkLzorg1BEj98KeLic9abuMtWHnddWe6tmn7NicpEKpD7W0vwtwmIkZMjw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

<div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers tags lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; modified=\&quot;2023-10-16T12:58:15.069Z\&quot; agent=\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\&quot; etag=\&quot;oopRJ9l2oEmQ21_QZstb\&quot; version=\&quot;21.7.5\&quot;&gt;\n  &lt;diagram name=\&quot;第 1 页\&quot; id=\&quot;VC8KsEmwTz_4FKU3JA4y\&quot;&gt;\n    &lt;mxGraphModel dx=\&quot;1242\&quot; dy=\&quot;789\&quot; grid=\&quot;1\&quot; gridSize=\&quot;10\&quot; guides=\&quot;1\&quot; tooltips=\&quot;1\&quot; connect=\&quot;1\&quot; arrows=\&quot;1\&quot; fold=\&quot;1\&quot; page=\&quot;1\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;827\&quot; pageHeight=\&quot;1169\&quot; math=\&quot;0\&quot; shadow=\&quot;0\&quot;&gt;\n      &lt;root&gt;\n        &lt;mxCell id=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;8V-hR4rmnCvxMIKz6rSl-2\&quot; value=\&quot;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#f5f5f5;fontColor=#333333;strokeColor=none;glass=0;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;260\&quot; y=\&quot;120\&quot; width=\&quot;410\&quot; height=\&quot;510\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;8V-hR4rmnCvxMIKz6rSl-7\&quot; value=\&quot;Agent结构\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontSize=20;strokeWidth=2;fontFamily=Verdana;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;625\&quot; y=\&quot;70\&quot; width=\&quot;120\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-10\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;8V-hR4rmnCvxMIKz6rSl-28\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-6\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;8V-hR4rmnCvxMIKz6rSl-28\&quot; value=\&quot;Sensors\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;shadow=1;fontSize=14;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;517.5\&quot; y=\&quot;160\&quot; width=\&quot;90\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;_ze0iByAne0-O5BmJI7g-5\&quot; value=\&quot;Agent\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontSize=17;strokeWidth=2;fontFamily=Verdana;fontStyle=0;fontColor=#666666;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;310\&quot; y=\&quot;140\&quot; width=\&quot;70\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;_ze0iByAne0-O5BmJI7g-20\&quot; value=\&quot;\&quot; style=\&quot;endArrow=classic;html=1;rounded=0;entryX=0.005;entryY=0.845;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#999999;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryPerimeter=0;\&quot; parent=\&quot;1\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-8\&quot; edge=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-3\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;650\&quot; y=\&quot;520\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;700\&quot; y=\&quot;400\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-11\&quot; value=\&quot;actions\&quot; style=\&quot;edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=15;\&quot; vertex=\&quot;1\&quot; connectable=\&quot;0\&quot; parent=\&quot;_ze0iByAne0-O5BmJI7g-20\&quot;&gt;\n          &lt;mxGeometry x=\&quot;-0.0161\&quot; y=\&quot;-1\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;2\&quot; y=\&quot;-14\&quot; as=\&quot;offset\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-5\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-1\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-2\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-1\&quot; value=\&quot;Condition-action (if-then) rules\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#d5e8d4;strokeColor=#82b366;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;294\&quot; y=\&quot;440\&quot; width=\&quot;120\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-4\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-2\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-3\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-2\&quot; value=\&quot;Action to be done\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;505\&quot; y=\&quot;440\&quot; width=\&quot;115\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-3\&quot; value=\&quot;Actuators\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;517.5\&quot; y=\&quot;530\&quot; width=\&quot;90\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-7\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0.5;entryY=0;entryDx=0;entryDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-6\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-2\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;563\&quot; y=\&quot;390\&quot; /&gt;\n              &lt;mxPoint x=\&quot;563\&quot; y=\&quot;390\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-6\&quot; value=\&quot;What is the world like now&amp;amp;nbsp;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;505\&quot; y=\&quot;270\&quot; width=\&quot;115\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-8\&quot; value=\&quot;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=#d79b00;glass=0;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;760\&quot; y=\&quot;120\&quot; width=\&quot;195\&quot; height=\&quot;510\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-9\&quot; value=\&quot;Environment\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontSize=17;strokeWidth=2;fontFamily=Verdana;fontStyle=0;fontColor=#666666;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;780\&quot; y=\&quot;140\&quot; width=\&quot;130\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-12\&quot; value=\&quot;\&quot; style=\&quot;endArrow=classic;html=1;rounded=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#999999;exitX=-0.01;exitY=0.116;exitDx=0;exitDy=0;exitPerimeter=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-8\&quot; target=\&quot;8V-hR4rmnCvxMIKz6rSl-28\&quot;&gt;\n          &lt;mxGeometry width=\&quot;50\&quot; height=\&quot;50\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;650\&quot; y=\&quot;250\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;803\&quot; y=\&quot;251\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-13\&quot; value=\&quot;percepts\&quot; style=\&quot;edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=15;\&quot; vertex=\&quot;1\&quot; connectable=\&quot;0\&quot; parent=\&quot;yPIZFGMk4KjvjqeKeUAf-12\&quot;&gt;\n          &lt;mxGeometry x=\&quot;-0.0161\&quot; y=\&quot;-1\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;16\&quot; y=\&quot;-14\&quot; as=\&quot;offset\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n      &lt;/root&gt;\n    &lt;/mxGraphModel&gt;\n  &lt;/diagram&gt;\n&lt;/mxfile&gt;\n&quot;}"></div>
<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>


Agent 的主要特性有:
- ● `自主性`（Autonomy） :运行无需人类或其它 Agent 的直接干预，对其自身行为及内部状态进行某种控制。
- ● `社会性`（Social Ability） 能通过某种 通信与其它 Agent（或人类）进行交互。交互主要有三种类型:`协作`（Cooperation）、`协调`（Coordination）和`协商` （Negotiation）。
- ● `反应性`（Reactivity）:能感知环境（可以是物理世界、一个经图形用户接口连接的用户、一系列其 它Agent、Internet 或所有这些的组合），并能对环境的变化及时作出反应。
- ● `主动性`（Pro-activeness）:不但能对环境作出反应，能够积极主动地做出使其目标得以实现的行为。

如果尝试对Agent做一点儿形式化表达，可能是这样的:  
- `Agent` = `platform` + `agent program`  
- `platform` = computing device + sensor + action   
- `agent program` 是 agent function 的真子集

#### LLM Agent

LLM语境下，Agent可以理解为在某种能**自主理解、规划决策、执行复杂任务**的智能体。

Agent 并非ChatGPT升级版，它不仅告诉你“如何做”，更会**帮你去做**。如果`Copilot`是**副驾驶**，那么`Agent`就是**主驾驶**。

自主Agent是由人工智能驱动的程序，当给定目标时，能够自己创建任务、完成任务、创建新任务、重新确定任务列表的优先级、完成新的顶级任务，并循环直到达到目标。

最直观的公式
> `Agent` = `LLM` + Planning + Feedback + Tool use

Agent 让 LLM 具备目标实现能力，并通过自我激励循环来实现这个目标。
- **并行**（同时使用多个提示，试图解决同一个目标）和**单向**（无需人类参与对话）。

大模型替代了传统 agent 中的规则引擎以及知识库，Agent提供了并寻求推理、观察、批评和验证的对话通道。特别是当配置了正确的提示和推理设置时，单个LLM就可以显示出广泛的功能 ，不同配置的Agent之间的对话可以帮助以模块化并以互补的方式将这些广泛的LLM功能结合起来。

开发人员可以轻松、快速地创建具有不同角色的Agent，例如，使用Agent来编写代码、执行代码、连接人工反馈、验证输出等。通过选择和配置内置功能的子集，Agent的后端也可以很容易地进行扩展，以允许更多的自定义行为。


#### 常见单 Agent 系统

基于大模型的常见单Agent 系统包括:
- `AutoGPT`:AutoGPT是一个AI代理的开源实现，它试图自动实现一个给定的目标。它遵循单Agent范式，使用了许多有用的工具来增强AI模型，并且不支持Multi-Agent协作。
- `ChatGPT`+ (code interpreter or plugin) :ChatGPT是一种会话AI Agent，现在可以与code interpreter或插件一起使用。code interpreter使ChatGPT能够执行代码，而插件通过管理工具增强了ChatGPT。
- `LangChain Agent`:LangChain是开发基于LLM应用的通用框架。LangChain有各种类型的代理，ReAct Agent是其中一个著名的示例。LangChain所有代理都遵循单Agent范式，并不是天生为交流和协作模式而设计的。
- `Transformers Agent`:Transformers Agent 是一个建立在Transformer存储库上的实验性自然语言API。它包括一组经过策划的工具和一个用来解释自然语言和使用这些工具的Agent。与 AutoGPT类似，它遵循单Agent范式，不支持Agent间的协作。


### Multi-Agent

Multi-Agent System (MAS, 多智能体系统) 由多个自主个体组成的**群体系统**，目标是通过个体间的相互信息通信和交互作用。

Multi-Agent 由一系列相互作用的Agent及其相应的组织规则和信息交互协议构成，内部的各个Agent之间通过相互通信、合作、竞争等方式，完成单个Agent不能完成的，大量而又复杂的工作，是“系统的系统”。



#### Multi-Agent 特点

Multi-Agent 系统主要特点:
- **自主性**。在 Multi-Agent 系统中，每个Agent都能管理自身的行为并做到自主地合作或者竞争。
- **容错性**。Agent 可以共同形成合作的系统用以完成独立或者共同的目标，如果某几个智能体出现了故障，其他智能体将自主地适应新的环境并继续工作，不会使整个系统陷入故障状态。
- **灵活性**和**可扩展性**。Multi-Agent系统本身采用分布式设计，Agent具有高内聚低耦合的特性，使得系统表现出极强的可扩展性。
- **协作能力**。Multi-Agent 系统是分布式系统，Agent之间可以通过合适的策略相互协作完成全局目标。


#### Multi-Agent RL

Multi-agent 系统相比于 single-agent 更加复杂，因为每个 agent 在和环境交互的同时也在和其他 agent 直接或者间接交互。

因此，Multi-agent 强化学习要比 single-agent 的建模和优化**更困难**，难点：
- 由于多个 agent 在环境中进行**实时动态**交互，并且每个 agent 在不断学习并更新自身策略，因此在每个 agent 视角下，环境**非稳态**（non-stationary），即 agent 在相同状态下采取相同动作，得到的状态转移和奖励信号的分布可能在不断改变；
- 多个 agent 训练可能是**多目标**，不同 agent 需要最大化自己的利益；
- **训练评估**的复杂度会增加，可能需要大规模分布式训练来提高效率，例如 Ray 框架
- ![](https://pic2.zhimg.com/v2-109e3a68654aebeba044e99ad330c061_1440w.jpg)


multi-agent RL 可以被定义为随机博弈问题，用元组 `(N, S, A, R, P, γ)`
- `N`: 智能体集合，即智能体数量
- `S`: 所有Agent**状态**集合，`S=S1*S2*S3*...*Sn`
- `A`: 所有Agent**动作**集合，`A=A1*A2*A3*...*An`
- `R`: 所有Agent**奖励函数**集合，`R=R1*R2*R3*...*Rn`
- `P`: 环境**状态转移概率**，`S X A -> Ω(S)`
- `γ`: 衰减因子

multi-agent 强化学习的目标: 每个 agent 学习一个策略来最大化其自身的累积奖励

求解范式
- 完全**中心化**方法（fully centralized）: 所有 agent 状态聚合在一起当作全局超级状态，动作连起来作为联合动作
  - 优点: 超级agent，环境依旧稳态，可保证单agent收敛性
  - 缺点: 所有信息暴力拼接导致维度爆炸, 训练复杂度提升，难以推广到agent数量多/环境大的场景
- 完全**去中心化**方法（fully decentralized）: 每个 agent 在自己的环境里独立学习，不受其他agent影响
  - 优点: 扩展容易
  - 缺点: 环境非稳态，训练不一定收敛
  - 案例: IPPO (Independent PPO)
- 中心化训练去中心化执行 （centralized training with decentralized execution, CTDE）: 介于**中心化**和**去中心化**之间, 训练时使用**全局信息**达到更好的效果，而执行时每个agent完全独立行动，达到去中心化效果
  - 案例: DDPG(multi-agent DDPG)

此时的Agent主要使用深度神经网络

#### Multi-Agent LLM

基于大模型的应用领域中，当复杂任务被分解成更简单的子任务时，LLM已经被证明了拥有解决复杂任务的能力。Multi-Agent 的通信与协作可以通过“对话”这一直观的方式实现这种子任务的分拆和集成。

为了使基于大模型的Agent适合于Multi-Agent的对话，每个Agent都可以进行对话，它们可以接收、响应和响应消息。当配置正确时 ，Agent可以自动与其他代理进行多次对话，或者在某些对话轮次中请求人工输入，从而通过人工反馈形成RLHF。可对话的Agent设计利用了LLM通过聊天获取反馈并取得进展的强大能力，还允许以模块化的方式组合LLM的功能。

协作型 Multi-Agent 

主要优点：
- 专业知识增强：系统内的每个 agent 都拥有各自领域的专业知识， 广泛的专业知识可以帮助生成的结果全面且准确。
- 提高问题解决能力：解决复杂的问题通常需要采取多方面的方法。 LLM-based multi-agent 系统通过综合多个 agent 的优势，提供单个 LLM 难以匹敌的解决方案。
- 稳健性和可靠性：冗余和可靠性是人工智能驱动的解决方案的关键因素。 LLM-based multi-agent 系统可降低单点故障的风险，确保持续运行并减少出现错误或不准确的可能性。
- 适应性：在动态的世界中，适应性至关重要。 LLM-based multi-agent 系统可以随着时间的推移而发展，新的代理无缝集成以应对新出现的挑战。

multi-agent LLM 

#### Multi-Agent 分类

Multi-Agent 系统（MAS） 主要可以分成以下类别:
- 独立型: 离散型、协作涌现型、竞争型
- 协作型: 相互通讯型（联合规划型和谈判型）、无通讯型
- 非零和型: 与竞争型和无通讯型相关

对比分析

| 维度 | 协作型 | 竞争型 |
| ---- | ---- | ---- |
| 系统目标 | 整体 | 个体 |
| 主流结构 | 中心化 | 去中心化 |
| agent功能 | 相对分散 | 相对同质 |
| agent关系 | 相互依赖 | 相互独立 |
| 是否自运行 | 否 | 是 |
| 系统资源 | 通常不共享 | 共享 | 

cooperative 协作型 vs adversarial 竞争型
- ![](https://picx.zhimg.com/v2-9f18606c948a1d945a8eed994e264811_1440w.jpg)



##### 图解

<div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers tags lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; modified=\&quot;2023-10-16T13:04:34.215Z\&quot; agent=\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\&quot; etag=\&quot;Twmu-MA36whEqS3YeIpp\&quot; version=\&quot;21.7.5\&quot;&gt;\n  &lt;diagram name=\&quot;第 1 页\&quot; id=\&quot;VC8KsEmwTz_4FKU3JA4y\&quot;&gt;\n    &lt;mxGraphModel dx=\&quot;1242\&quot; dy=\&quot;789\&quot; grid=\&quot;1\&quot; gridSize=\&quot;10\&quot; guides=\&quot;1\&quot; tooltips=\&quot;1\&quot; connect=\&quot;1\&quot; arrows=\&quot;1\&quot; fold=\&quot;1\&quot; page=\&quot;1\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;827\&quot; pageHeight=\&quot;1169\&quot; math=\&quot;0\&quot; shadow=\&quot;0\&quot;&gt;\n      &lt;root&gt;\n        &lt;mxCell id=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;8V-hR4rmnCvxMIKz6rSl-7\&quot; value=\&quot;Multi-Agent分类\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontSize=20;strokeWidth=2;fontFamily=Verdana;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;305\&quot; y=\&quot;90\&quot; width=\&quot;180\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-17\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-14\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-16\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-18\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0.5;entryY=0;entryDx=0;entryDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-14\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-15\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-14\&quot; value=\&quot;Multi-Agent系统\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;335\&quot; y=\&quot;150\&quot; width=\&quot;115\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-22\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-15\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-21\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-23\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0.5;entryY=0;entryDx=0;entryDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-15\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-19\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-24\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0.5;entryY=0;entryDx=0;entryDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-15\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-20\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-15\&quot; value=\&quot;独立型\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;185\&quot; y=\&quot;240\&quot; width=\&quot;80\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-27\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0.5;entryY=0;entryDx=0;entryDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-16\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-25\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-28\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-16\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-26\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-16\&quot; value=\&quot;协作型\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;560\&quot; y=\&quot;240\&quot; width=\&quot;80\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-19\&quot; value=\&quot;离散型\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;90\&quot; y=\&quot;330\&quot; width=\&quot;80\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-31\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-20\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-29\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-20\&quot; value=\&quot;竞争型\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;280\&quot; y=\&quot;330\&quot; width=\&quot;80\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-21\&quot; value=\&quot;协作涌现型\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;180\&quot; y=\&quot;330\&quot; width=\&quot;90\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-30\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;yPIZFGMk4KjvjqeKeUAf-25\&quot; target=\&quot;yPIZFGMk4KjvjqeKeUAf-29\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-25\&quot; value=\&quot;无通讯型\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;500\&quot; y=\&quot;330\&quot; width=\&quot;90\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-26\&quot; value=\&quot;相互通讯型\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;610\&quot; y=\&quot;330\&quot; width=\&quot;90\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;yPIZFGMk4KjvjqeKeUAf-29\&quot; value=\&quot;非零和型\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=none;shadow=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;390\&quot; y=\&quot;330\&quot; width=\&quot;90\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n      &lt;/root&gt;\n    &lt;/mxGraphModel&gt;\n  &lt;/diagram&gt;\n&lt;/mxfile&gt;\n&quot;}"></div>
<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>


##### 协作型

协作型
- 不同 agent 是系统的不同环节，承担不同的功能，共同为了系统的整体目标而服务。

参考 
- [LLM 时代的 multi-agent 系统](https://zhuanlan.zhihu.com/p/665644399)
- 【2023-8-14】香港科技大学和北大 [CHATEVAL: TOWARDS BETTER LLM-BASED EVALUATORS THROUGH MULTI-AGENT DEBATE](https://arxiv.org/pdf/2308.07201.pdf)
- 【2024-3-22】MIT [Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?](https://arxiv.org/pdf/2309.15943)
- 【2024-5-27】浙大 [Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View](https://arxiv.org/pdf/2310.02124)

协作机制：
- 每一个 agent 都有不同个体特质、思维模式和合作策略；
- agent 之间的**辩论**和 agent **反思**可提高 agent 表现；
- agent **数量**和**策略**的平衡是协作关键因素
- LLM agents 协作机制和人类社会心理学相似，如 从众和少数服从多数。

多个不同特征的 agent 组成多样化的机器社会。 这些 agent 通过多轮**相互辩论**或**自我反思**来完成任务，辩论和反思的组合构成 agent 的策略。
- **多样的协作策略组合**对结果具有重要的积极作用；
- **思维模式排序**对于协作机制至关重要；
- 持续的反思会增加**不确定性**（模型幻觉提高）

![](https://pic2.zhimg.com/v2-d5f17137be6fc92c45c25d7847234a4d_1440w.jpg)


##### 竞争型

竞争型 multi-agent 
- 每个 agent 相对平等，通过与不同个体间的信息交流和各自的活动，以实现各自不同的目标

竞争型 multi-agent 系统：
- 人类、LLM APIs、local LLMs 都是各自独立的 agent（即 Player）
- 所有 agent 共享同一个环境和评价规则
- 所有 agent 共享同一系统资源，并处于互相竞争和博弈中
- ![](https://pic1.zhimg.com/v2-fb845f75a51b73cad982a213748b1ef4_1440w.jpg)

以 LLM 为基础的 agent 作为 player，甚至和人类竞争，是因为 LLM-based agent 具有以下特征：
- **反应性**（Reactivity）：Agent 反应能力指对环境中的即时变化和刺激做出快速反应的能力。多模态融合技术可以扩展语言模型的感知空间，使其能够快速处理来自环境的视觉和听觉信息。这些进步使 LLMs 能够有效地与真实世界的物理环境互动，并在其中执行任务。
- **主动性**（Pro-activeness）：积极主动指Agent不仅仅会对环境做出反应，还能积极主动地采取以目标为导向的行动。LLMs 具有很强的概括推理和规划能力，如逻辑推理和数学推理。同样也以目标重拟、任务分解和根据环境变化调整计划等形式显示了规划的新兴能力。
- **社会能力**（Social Ability）：社交能力指一个Agent通过某种Agent交流语言与其他Agent（包括人类）进行交互的能力。大型语言模型具有很强的自然语言交互能力，如理解和生成能力。这种能力使它们能够以可解释的方式与其他模型或人类进行交互，这构成了LLM-based Agent的社会能力的基石。

案例主要在虚拟的世界观之中，如：角色扮演类的游戏、对人类世界的模拟等场景中。

从单个 agent 的视角来看，其运行机制：为确保有效交流，**自然语言交互能力**至关重要。agent 接收感知模块处理的信息后，大脑模块首先转向存储，在知识中检索并从记忆中回忆。这些结果有助于 Agent 制定计划、进行推理和做出明智的决定。

此外，大脑模块还能以摘要、矢量或其他数据结构的形式记忆 Agent 过去的观察、思考和行动。同时，它还可以更新常识和领域知识等知识，以备将来使用。LLM-based Agent还可以利用其固有的概括和迁移能力来适应陌生场景。
- ![](https://pica.zhimg.com/v2-41f13bc21981edaeddf3521cf50d77d6_1440w.jpg)

LLM-based Agent 由个体和群体社会活动组成的复杂系统，在合作与竞争并存的环境中表现出了自发的社会行为。新出现的行为相互交织，形成了社会互动。
- 基础个体行为
- 动态群体行为


#### LLM 开发框架


Multi-Agent 系统

基于大模型的常见 Multi-Agent 系统包括:
- `BabyAGI`: BabyAGI 是Python实现的人工智能任务管理系统的示例。用了多个基于LLM的代理。
  - 例如，有一个Agent用于基于上一个任务的目标和结果创建新任务，有一个Agent用于确定任务列表的优先级，还有一个用于完成任务/子任务的Agent。BabyAGI作为一个Multi-Agent系统，采用静态Agent对话模式，一个预定义的Agent通信顺序。
- `CAMEL`: CAMEL 是 agent 通信框架。它演示了如何使用角色扮演来让聊天Agent相互通信以完成任务。它还记录了Agent的对话， 以进行行为分析和能力理解，并采用初始提 示技术来实现代理之间的自主合作。但是，CAMEL本身不支持工具的使用，比如代码执行。虽然它被提议作为多代理会话的基础设施，但它只支持静态会话模式。
- `Multi-Agent Debate`: Multi-Agent Debate 试图构建具有多代理对话的LLM应用程序，是鼓励LLM中发散思维的有效方式，并改善了LLM的事实性和推理。在这两种工作中 ，多个LLM推理实例被构建为多个Agent来解决与Agent争论的问题。每个Agent都是一个LLM推理实例，而不涉及任何工具或人员，并且Agent间的对话需要遵循预定义的顺序。
- `MetaGPT`: MetaGPT 是一种基于 Multi-Agent 对话框架的LLM自动软件开发应用程序。他们为各种gpt分配不同的角色来协作开发软件，针对特定场景制定专门的解决方案。
- Autogen: 用于简化 LLM 工作流的编排、优化和自动化的开发框架。它提供了可定制和可对话的Agent，利用 LLM 的最强功能，如 GPT-4，同时通过与人和工具集成以及通过自动聊天在多个Agent之间进行对话来解决它们的局限性。

Autogen 使用 Multi-Agent 会话启用复杂的基于 LLM 的工作流

构建复杂 Multi-Agent 会话系统可以归结为:
- 定义一组具有专门功能和角色的Agent。
- 定义Agent之间的交互行为，例如，当一个代理从另一个代理接收到消息时应该回复什么。

这两个步骤都是模块化的，使这些Agent可重用和可组合。例如，要构建一个基于代码的问答系统，可以设计Agent及其交互，这样的系统可以减少应用程序所需的手动交互次数。

AutoGen 中的Agent具有由 LLM、人工、工具或这些元素混合启用的功能。例如:
- 可以通过高级推理特性轻松配置Agent中 LLM 的使用和角色(通过组聊天自动解决复杂任务)。
- 人工智能和监督可以通过具有不同参与级别和模式的Agent来实现，例如，使用 GPT-4 + 多个人工用户的自动任务解决。
- Agent具有对 LLM 驱动代码/函数执行的本机支持，例如，通过代码生成、执行和调试自动解决任务，使用提供的工具作为函数。

Autogen 在 github上提供了很多有意思的示例，[agentchathumanfeedback.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchathumanfeedback.ipynb)

详见 [智能体开发框架](agent_arch)

### Agent 决策流程

#### 人类决策逻辑

人们高效完成一项任务非常成功的经验总结
- 基于PDCA模型，将完成一项任务进行拆解，按照作出计划、计划实施、检查实施效果
- 然后将成功的纳入标准，不成功的留待下一循环去解决。

`PDCA`思维模型
- `Plan` 计划 -> `Do` 执行 -> `Check` 检查结果 -> `Action` 处理（纠正偏差）
- ![](https://pic4.zhimg.com/80/v2-96b37fe27e029de5ae97420063e0609f_1440w.webp)


#### 人脑 与 智能体

【2025-3-31】加拿大蒙特利尔等发布论文
- [ADVANCES AND CHALLENGES IN FOUNDATION AGENTS](https://arxiv.org/pdf/2504.01990)
- Github [awesome-foundation-agents](https://github.com/FoundationAgents/awesome-foundation-agents)

Agent 的设计、评估以及持续改进面临着错综复杂、多方面的挑战。

全面概述将智能体置于一个受`大脑`启发的模块化架构中，融合`认知科学`、`神经科学`和`计算研究`的原理。四个关联部分。
- 首先，智能体模块化基础, 将认知、感知和操作模块, 对应到人类大脑的类似功能上，并阐释记忆、世界建模、奖励处理和类情感系统等核心组件。
- 其次，自我提升和自适应进化机制，智能体如何自主优化自身能力，适应动态环境，并通过自动化优化范式实现持续学习，这些范式包括新兴的自动化机器学习（AutoML）以及由大语言模型驱动的优化策略。
- 第三，**协作式**和**进化式**多智能体系统，考察智能体之间的交互、合作以及社会结构中涌现出的集体智能，强调其与人类社会动态的相似之处。
- 最后，构建安全、可靠且有益的人工智能系统的关键必要性，强调内在和外在的安全威胁、伦理一致性、稳健性，以及在现实世界中进行可信部署所必需的切实可行的缓解策略。

通过综合模块化人工智能架构以及不同学科的见解，本综述识别出了关键的研究空白、挑战和机遇，鼓励开展能够使技术进步与显著的社会效益相协调的创新。


#### Agent 决策

基于PDCA模型进行 规划、执行、评估和反思。
- **规划能力**（Plan）-> **分解任务**:Agent大脑把大的任务拆解为更小的，可管理的子任务，这对有效的、可控的处理好大的复杂的任务效果很好。
- **执行能力**（Done）-> 使用**工具**:Agent能学习到在模型内部知识不够时（比如:在pre-train时不存在，且之后没法改变的模型weights）去调用外部API，比如:获取实时的信息、执行代码的能力、访问专有的信息知识库等等。这是一个典型的平台+工具的场景，我们要有生态意识，即我们构建平台以及一些必要的工具，然后大力吸引其他厂商提供更多的组件工具，形成生态。
- **评估能力**（Check）-> **确认执行结果**:Agent要能在任务正常执行后判断产出物是否符合目标，在发生异常时要能对异常进行分类（危害等级），对异常进行定位（哪个子任务产生的错误），对异常进行原因分析（什么导致的异常）。这个能力是通用大模型不具备的，需要针对不同场景训练独有的小模型。
- **反思能力**（Action）-> 基于评估结果**重新规划**:Agent要能在产出物符合目标时及时结束任务，是整个流程最核心的部分；同时，进行归因分析总结导致成果的主要因素，另外，Agent要能在发生异常或产出物不符合目标时给出应对措施，并重新进行规划开启再循环过程。
- ![](https://pic3.zhimg.com/80/v2-284842dc92028b66c2f2b3b0a7d6e7fa_1440w.webp)

LLM作为一种智能代理，引发了人们对人工智能与人类工作的关系和未来发展的思考。它让我们思考人类如何与智能代理合作，从而实现更高效的工作方式。而这种合作方式也让我们反思人类自身的价值和特长所在。


Agent决策流程
- `感知`（Perception）→ `规划`（Planning）→ `行动`（Action）

具体
- `感知`（Perception）是指Agent从环境中收集信息并从中提取相关知识的能力。
- `规划`（Planning）是指Agent为了某一目标而作出的决策过程。
- `行动`（Action）是指基于环境和规划做出的动作。

解读
- Agent通过**感知**环境，收集信息并提取相关知识。
- 然后通过**规划**为了达到某个目标做出决策。
- 最后，通过行动基于环境和规划做出具体的动作。

Policy 是 Agent 做出行动的核心决策，而行动又为进一步感知提供了观察的前提和基础，形成了一个自主的闭环学习过程。


### Agent 爆发

Agent 大爆发
- 3月21日，Camel发布。
- 3月30日，AutoGPT发布。
- 4月3日，BabyAGI发布。
- 4月7日，西部世界小镇发布。
- 5月27日，英伟达AI智能体Voyager接入GPT-4后，直接完胜了AutoGPT。通过自主写代码，它完全独霸了《我的世界》，可以在游戏中进行全场景的终身学习，根本无需人类插手。
- 同一时间，商汤、清华等共同提出了通才AI智能体 Ghost in the Minecraft (GITM)，它同样能够通过自主学习解决任务，表现优异。这些表现优异的AI智能体，简直让人看到了AGI+智能体的雏形

### AI Agent 是桥梁

【2023-8-23】[AI Agent:大模型与场景间的价值之桥，但不适合当纯技术看](https://mp.weixin.qq.com/s/doPGtqvInASGLpE3M9jslA)

怎么理解AI Agent和特征？
1. 可重用通行定义，基于感知进行智能判断并采取行动。（陆奇的大模型世界观说）
  - 要和IoT、现有各种系统做深度结合，不可能是Lilian Wen 图里的简单工具的概念, 感知范围大小事实上也定义了AI Agent的范围
1. 价值序列的初始化
  - 不是感知，而是原则，是绝对必须的输入，但似乎很少被提及。
1. 三个核心输入输出上都要接受变化。
  - 感知和行动的风格肯定要根据不同的公司要有微调，比如同样是招聘的Agent，不可能期望用感知、行动和价值序列都固定的产品解决所有公司的问题。
1. 一组算法的组合
  - 大模型与其它算法、领域模型、记忆、规划能力形成一套新的内核，这种内核要有通用性，否则一个是不匹配大模型的通用能力
  - 大模型能力已经通用化了，再配上通用的结构，这种通用能力就能够彻底发挥，相当于给瓮中之脑加了一个终结者的身体。

AI Agent即**系统型超级应用**。
- 解决具体问题所以是个应用，但具有通用性，而达成通用性的手段其实和过去的操作系统非常类似，并且以大模型为根基。

从西部世界类的元宇宙Agent到具身智能全是Agent。

Agent 有很多种
- 最基础的和来的最快的应该是纯数字，**无场景**或者**场景极为单薄**的AI Agent。
  - 元宇宙型的Agent，谷歌和斯坦福要干的现实版西部世界就是这类。如果放在游戏里就是元宇宙里的智能NPC。这类Agent最大的建设性在于给元宇宙注入生气，最大的破坏性则在于对上古社区的影响可能不咋正向，包括抖音。
- 第二种Agent则要与**现实场景**结合，可能是纯粹数字的，也可能不是。比如招聘、营销、空调管理、运维状态监控等。
- 第三种则是**具身机器人**。和上一种的区别是完全控制自己的一套外设，上一个则更多的是一种粘合。

这三类都会解决**连续运转**场景问题，只不过后两个在**现实世界**使劲，第一个在**虚拟世界**使劲。



## 智能体组件

【2023-7-11】[下一代语言模型范式LAM崛起！AutoGPT模式席卷LLM，三大组件全面综述:规划、记忆和工具](https://zhuanlan.zhihu.com/p/642902065)


### 图解

<!-- draw.io diagram -->
<div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers tags lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; agent=\&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\&quot; version=\&quot;24.8.4\&quot;&gt;\n  &lt;diagram id=\&quot;xdYpP7w1t2VaaceZiyqw\&quot; name=\&quot;第 1 页\&quot;&gt;\n    &lt;mxGraphModel dx=\&quot;1235\&quot; dy=\&quot;1793\&quot; grid=\&quot;1\&quot; gridSize=\&quot;10\&quot; guides=\&quot;1\&quot; tooltips=\&quot;1\&quot; connect=\&quot;1\&quot; arrows=\&quot;1\&quot; fold=\&quot;1\&quot; page=\&quot;1\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;827\&quot; pageHeight=\&quot;1169\&quot; math=\&quot;0\&quot; shadow=\&quot;0\&quot;&gt;\n      &lt;root&gt;\n        &lt;mxCell id=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;KTwht3HF3Dpf_-XckZrt-1\&quot; value=\&quot;Agent 智能体架构\&quot; style=\&quot;text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=21;rotation=0;strokeWidth=3;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;946.79\&quot; y=\&quot;-60\&quot; width=\&quot;267\&quot; height=\&quot;33\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-82\&quot; value=\&quot;协作\&quot; style=\&quot;edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];labelBackgroundColor=none;fontStyle=0;fontColor=#333333;fontSize=15;\&quot; vertex=\&quot;1\&quot; connectable=\&quot;0\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1127.84\&quot; y=\&quot;663\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;3_X8OGEKLLMm8I1NppsK-71\&quot; value=\&quot;2024-11-11&amp;lt;div&amp;gt;wqw547243068@163.com&amp;lt;/div&amp;gt;\&quot; style=\&quot;edgeLabel;html=1;align=left;verticalAlign=middle;resizable=0;points=[];labelBackgroundColor=none;fontStyle=0;fontColor=#333333;fontSize=15;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot; connectable=\&quot;0\&quot;&gt;\n          &lt;mxGeometry x=\&quot;709.47\&quot; y=\&quot;650\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-140\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;exitX=0.5;exitY=1;exitDx=0;exitDy=0;dashed=1;dashPattern=1 2;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-128\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-134\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-156\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-128\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-145\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-128\&quot; value=\&quot;Tools&amp;lt;div style=&amp;quot;font-size: 14px;&amp;quot;&amp;gt;工具&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#FFB570;strokeColor=none;shadow=1;fontStyle=1;fontSize=14;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;915.25\&quot; y=\&quot;445\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-130\&quot; value=\&quot;\&quot; style=\&quot;verticalLabelPosition=bottom;aspect=fixed;html=1;shape=mxgraph.salesforce.bots;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1059.23\&quot; y=\&quot;402.5\&quot; width=\&quot;42.11\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-135\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-131\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-128\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-136\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-131\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-133\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-137\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-131\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-132\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-138\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-131\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-134\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-131\&quot; value=\&quot;Agent&amp;amp;nbsp;&amp;lt;div style=&amp;quot;font-size: 14px;&amp;quot;&amp;gt;智能体&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#CCFFCC;strokeColor=#82b366;dashed=1;dashPattern=1 1;shadow=1;fontStyle=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1082.8700000000001\&quot; y=\&quot;442.5\&quot; width=\&quot;63.44\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-153\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-132\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-142\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-154\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-132\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-142\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-132\&quot; value=\&quot;&amp;lt;div&amp;gt;Memory&amp;lt;/div&amp;gt;记忆\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#9933FF;strokeColor=none;shadow=1;fontStyle=1;fontSize=14;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1074.97\&quot; y=\&quot;278\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-161\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-133\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-149\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;1330\&quot; y=\&quot;463\&quot; /&gt;\n              &lt;mxPoint x=\&quot;1330\&quot; y=\&quot;397\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-133\&quot; value=\&quot;&amp;lt;div&amp;gt;Planning&amp;lt;/div&amp;gt;规划\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#FF6666;strokeColor=none;shadow=1;fontStyle=1;fontSize=14;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1224.53\&quot; y=\&quot;445\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-134\&quot; value=\&quot;&amp;lt;div&amp;gt;Action&amp;lt;/div&amp;gt;行动\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#6666FF;strokeColor=none;shadow=1;fontStyle=1;fontSize=14;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1074.96\&quot; y=\&quot;540\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-141\&quot; value=\&quot;&amp;lt;div&amp;gt;Short-term Memory&amp;lt;/div&amp;gt;短期记忆\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#e1d5e7;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;964.97\&quot; y=\&quot;200\&quot; width=\&quot;136.38\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-142\&quot; value=\&quot;&amp;lt;div&amp;gt;Long-term Memory&amp;lt;/div&amp;gt;短期记忆\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#e1d5e7;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1127.84\&quot; y=\&quot;200\&quot; width=\&quot;136.38\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-143\&quot; value=\&quot;Calendar&amp;lt;div style=&amp;quot;font-size: 14px;&amp;quot;&amp;gt;日历&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;780.63\&quot; y=\&quot;330\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-144\&quot; value=\&quot;Calculator&amp;lt;div style=&amp;quot;font-size: 14px;&amp;quot;&amp;gt;计算器&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;780.63\&quot; y=\&quot;390\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-145\&quot; value=\&quot;&amp;lt;div&amp;gt;CodeInterpreter&amp;lt;/div&amp;gt;代码执行\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;738.13\&quot; y=\&quot;445\&quot; width=\&quot;121.75\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-146\&quot; value=\&quot;&amp;lt;div&amp;gt;Search&amp;lt;/div&amp;gt;搜索\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;780.63\&quot; y=\&quot;510\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-147\&quot; value=\&quot;More&amp;lt;div&amp;gt;其它&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;780.63\&quot; y=\&quot;570\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-148\&quot; value=\&quot;&amp;lt;div&amp;gt;Reflection&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;反射&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#f8cecc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1360\&quot; y=\&quot;315\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-149\&quot; value=\&quot;&amp;lt;div&amp;gt;Self-critics&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;反思&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#f8cecc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1360\&quot; y=\&quot;380\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-150\&quot; value=\&quot;&amp;lt;div&amp;gt;Chain of thought&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;思维链&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#f8cecc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1360\&quot; y=\&quot;445\&quot; width=\&quot;119.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-151\&quot; value=\&quot;&amp;lt;div&amp;gt;Reflection&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;反射&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#f8cecc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1360\&quot; y=\&quot;502\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-152\&quot; value=\&quot;&amp;lt;div&amp;gt;Sub-goal decomposition&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;子目标拆解&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#f8cecc;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1360\&quot; y=\&quot;565\&quot; width=\&quot;179.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-155\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0.5;entryY=1;entryDx=0;entryDy=0;exitX=0.5;exitY=0;exitDx=0;exitDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-132\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-141\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1125.01\&quot; y=\&quot;290\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1206.01\&quot; y=\&quot;245\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-157\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=1;entryY=0.5;entryDx=0;entryDy=0;exitX=0;exitY=0.5;exitDx=0;exitDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-128\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-144\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;925\&quot; y=\&quot;473\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;870\&quot; y=\&quot;473\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-158\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=1;entryY=0.5;entryDx=0;entryDy=0;exitX=0;exitY=0.5;exitDx=0;exitDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-128\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-143\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;925\&quot; y=\&quot;473\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;870\&quot; y=\&quot;418\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-159\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=1;entryY=0.5;entryDx=0;entryDy=0;exitX=0;exitY=0.5;exitDx=0;exitDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-128\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-146\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;935\&quot; y=\&quot;483\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;880\&quot; y=\&quot;428\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-160\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=1;entryY=0.5;entryDx=0;entryDy=0;exitX=0;exitY=0.5;exitDx=0;exitDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-128\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-147\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;945\&quot; y=\&quot;493\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;890\&quot; y=\&quot;438\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-162\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-133\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-148\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1310\&quot; y=\&quot;460\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1361\&quot; y=\&quot;473\&quot; as=\&quot;targetPoint\&quot; /&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;1330\&quot; y=\&quot;463\&quot; /&gt;\n              &lt;mxPoint x=\&quot;1330\&quot; y=\&quot;333\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-163\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-133\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-150\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1324\&quot; y=\&quot;483\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1371\&quot; y=\&quot;483\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-164\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-133\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-151\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1334\&quot; y=\&quot;493\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1381\&quot; y=\&quot;493\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-165\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-133\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-152\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1344\&quot; y=\&quot;503\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1391\&quot; y=\&quot;503\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-166\&quot; value=\&quot;Agent&amp;amp;nbsp;&amp;lt;div style=&amp;quot;font-size: 14px;&amp;quot;&amp;gt;智能体&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#CCFFCC;strokeColor=#82b366;dashed=1;dashPattern=1 1;shadow=1;fontStyle=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;957.53\&quot; y=\&quot;630\&quot; width=\&quot;63.44\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-167\&quot; value=\&quot;Agent&amp;amp;nbsp;&amp;lt;div style=&amp;quot;font-size: 14px;&amp;quot;&amp;gt;智能体&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#CCFFCC;strokeColor=#82b366;dashed=1;dashPattern=1 1;shadow=1;fontStyle=1;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1232.44\&quot; y=\&quot;630\&quot; width=\&quot;63.44\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-170\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;dashed=1;dashPattern=1 2;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-132\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-133\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;965\&quot; y=\&quot;490\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1085\&quot; y=\&quot;568\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-171\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;dashed=1;dashPattern=1 2;entryX=0.5;entryY=0;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-132\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-148\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1164\&quot; y=\&quot;388\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1274\&quot; y=\&quot;455\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-172\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=1;exitDx=0;exitDy=0;startArrow=classic;startFill=1;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-131\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-166\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1093\&quot; y=\&quot;473\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1005\&quot; y=\&quot;473\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-173\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=1;exitDx=0;exitDy=0;startArrow=classic;startFill=1;entryX=0;entryY=0.25;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-131\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-167\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1093\&quot; y=\&quot;493\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1029\&quot; y=\&quot;640\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-174\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;startArrow=classic;startFill=1;entryX=1;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-167\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-166\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;1103\&quot; y=\&quot;503\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;1040\&quot; y=\&quot;650\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-179\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-175\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-176\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-182\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;exitX=0.5;exitY=1;exitDx=0;exitDy=0;strokeWidth=2;strokeColor=#999999;dashed=1;dashPattern=1 2;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-175\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-178\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-175\&quot; value=\&quot;Perception&amp;lt;div&amp;gt;感知&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#3399FF;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;strokeWidth=2;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;907.1\&quot; y=\&quot;15\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-180\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;bMsFhyuYCdBvBACusGag-176\&quot; target=\&quot;bMsFhyuYCdBvBACusGag-177\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-176\&quot; value=\&quot;&amp;lt;div&amp;gt;Planning&amp;lt;/div&amp;gt;规划\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#3399FF;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;strokeWidth=2;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1072.0700000000002\&quot; y=\&quot;15\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-177\&quot; value=\&quot;&amp;lt;div&amp;gt;Action&amp;lt;/div&amp;gt;行动\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#3399FF;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;strokeWidth=2;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1242.8400000000001\&quot; y=\&quot;15\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;bMsFhyuYCdBvBACusGag-178\&quot; value=\&quot;&amp;lt;div&amp;gt;Observation&amp;lt;/div&amp;gt;观察\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#3399FF;strokeColor=none;shadow=1;fontStyle=0;fontSize=14;strokeWidth=2;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1077.85\&quot; y=\&quot;95\&quot; width=\&quot;79.25\&quot; height=\&quot;35\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n      &lt;/root&gt;\n    &lt;/mxGraphModel&gt;\n  &lt;/diagram&gt;\n&lt;/mxfile&gt;\n&quot;}"></div>
<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>




### 整体结构

三个关键组件，即`规划`、`记忆`和`工具`
- ![](https://pic2.zhimg.com/80/v2-df16b538d5ecf9d68aea1fbfb77e03ad_1440w.webp)

【2023-6-23】`翁丽莲`（Weng Lilian）博文([LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/))详细介绍了Agent架构
- OpenAI 应用主管
- 现在是 OpenAI 的 Head of Safety Systems

在一个由LLM驱动的自主代理系统中，LLM充当代理的大脑，并辅以几个关键组成部分:
- `规划`（Planning）
  - 子目标与分解（Subgoal and decomposition）:代理将大型任务分解为更小、更易于处理的子目标，从而实现对复杂任务的高效处理。
  - 反思与完善（Reflection and refinement）:代理可以对过去的行动进行自我批评和自我反思，从错误中吸取教训，并为未来的步骤进行改进，从而提高最终结果的质量。
- `记忆`（Memory）
  - 短期记忆（Short-term memory）:作者认为所有上下文学习（参考 [提示工程](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)Prompt Engineering）都是利用模型的短期记忆来学习。
  - 长期记忆（Long-term memory）:这为代理提供了在长时间嘞保留和回忆（无限）信息的能力，通常通过利用外部向量存储和快速检索来实现。
- `工具使用`（Tool use）
  - 代理程序学会调用外部API获取模型权重中缺失的额外信息（通常在预训练后很难更改），包括当前信息、代码执行能力、访问专有信息源等。

In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:
- `Planning`
  - Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.
  - Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.
- `Memory`
  - Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.
  - Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.
- `Tool use`
  - The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.
- ![](https://lilianweng.github.io/posts/2023-06-23-agent/agent-overview.png)

【2024-7-24】陈旭博士 中国人民大学 准聘副教授 [Al Agent--大模型时代重要落地方向](https://mp.weixin.qq.com/s/IfFctgdiLW7mnnFkc118jw), 大语言模型 Agent 的构成，主要分为以下 4 个模块:
- 增加 `画像`



### 理想 Agent 框架是什么

Agent 系统有以下几个模块组成


#### 0. 画像

画像模块:主要描述 Agent 背景信息

画像模块的主要内容和生成策略。
- （1）画像内容，主要基于 3 种信息:人口统计信息、个性信息和社交信息。
- （2）生成策略:主要采用 3 种策略来生成画像内容:
  - **手工设计**方法:自行通过指定的方式，将用户画像的内容写入大模型的 prompt 中；适用于 Agent 数量比较少的情况；
  - **大模型生成**方法:首先指定少量画像，并将其作为示例，进而使用大语言模型生成更多的画像；适用于大量 Agent 的情况；
  - **数据对齐**方法:需要根据事先指定的数据集中人物的背景信息作为大语言模型的 prompt，进而做相应的预测。


#### 1. 记忆

LLM 是**无状态**（stateless）的，大参数量使产品无法基于每一次交互的经验来更新模型的内部参数。不过由于 LLM 能理解大量语义信息，Agent 系统可以在模型之外建立一个记录信息的记忆系统，来模仿人类大脑那样从过往的经验中学习正确的工作模式。

以下分类根据医学中人类的几种记忆方式类比，将 AI agent 的记忆系统分为短期记忆与三种长期记忆:
- **短期**记忆:
  - • `工作记忆`（Working Memory）:这一轮决策所需要用到的所有信息。其中包括上下文内容，例如从长期记忆中检索到的知识；也包括 LLM context 以外的信息，例如 function call 时使用其他能力所产生的数据
- **长期**记忆:
  - • `事件记忆`（Episodic Memory）:Agent 对过去多轮决策中所发生事情的记忆。每一次 LLM 有了新的行为和结果，agent 都会把内容写进情节记忆。例如在 Generative agents 小镇中，虚拟小镇的 agent 居民会把自己每天看到的事、说过的话计入事件记忆。要使得用户得到个性化的使用体验，这一部分的优化是至关重要的。
  - • `语义记忆`（Semantic Memory）:Agent 对自身所在世界的语义知识记忆，一般通过外部向量存储和检索来调用。这一部分记忆可以用类似知识图谱的思路，使 agent 之间的知识更方便共享和更新。同样以 Generative agents 小镇为例，agent 居民会记忆其他居民的爱好、生日等信息，这都是语义记忆。
  - • `程序记忆`（Procedural Memory）:在一些特定场景下，agent 执行操作的 workflow 会通过代码的形式在框架中写出来。这类记忆使部分行为能够按照更可控的工作流来执行。以 Generative agents 小镇类比，agent 居民会有自己的行为习惯，比如每天晚上要去某条街散步等等。


#### 2. 行动

面对不同任务，Agent 系统有一个完整的行动策略集，在决策时可以选择需要执行的行动。

以下罗列几个最常见、重要的行为，实际应用中根据不同场景会有补充和优先级的差异:
- • `工具使用`:智人与其他动物的重要区别是其使用工具的能力，而 LLM 同样可以通过这一点来扬长避短。AI Agents 可以通过文档和数据集教会 agent 如何调用外部工具的 API，来补足 LLM 自身的弱项。例如复杂的数学计算就不是 LLM 的长处，调用 Calculator() 可以事半功倍。
- • `职责扮演`:AI agent 系统中，不同 LLM 需要进行分工的机制设计。就像在工厂和公司制中常常出现的角色配合和博弈那样，LLM 之间也需要各司其职，按照各自的职责去完成任务，形成一个完整的协同组织。
- • `记忆检索`:指的是从长期记忆中找到与本次决策相关的信息，将其放到工作记忆、交给 LLM 处理的过程。
- • `推理`:从短期工作记忆生成新知识，并将其存入长期记忆中
- • `学习`:将新的知识和对话历史加入长期记忆，让 Agent 更了解用户
- • `编程`:AI agent 可以实现很多长尾的开发需求，让软件变得接近定制。而编程是最适合 AI agent 去自己迭代和收集反馈（是否能有效执行）的环境，因为能自己形成反馈的闭环

动作模块
- 动作目标:有些 Agent 的目标是完成某个任务，有些是交流，有些是探索。
- 动作生成:有些 Agent 是依靠记忆回想生成动作，有些是按照原有计划执行特定的动作。
- 动作空间:有些动作空间是工具的集合，有些是基于大语言模型自身知识，从自我认知的角度考虑整个动作空间。
- 动作影响:包括对环境的影响、对内在状态的影响，以及对未来新动作的影响。

#### 3. 规划/决策

前面提到很多行动可以由 Agent 进行规划和执行，而决策这一步就是从中选择最为合适的一个行为去执行。
- • `事前规划`:LLM 能够将一个大目标分解为较小的、可执行的子目标，以便高效的处理复杂任务。对于每一个目标，评估使用不同行为方案的可行性，选择其中期望效果最好的一个。
- • `事后反思`:Agents 可以对过去的行为进行自我批评和反省，从错误中吸取经验教训，并加入长期记忆中帮助 agent 之后规避错误、更新其对世界的认知。这一部分试错的知识将被加入长期记忆中


参考
- 【2024-10-13】拾象科技:[AI Agent的千亿美金问题:如何重构10亿知识工作职业，掀起软件生产革命？](https://mp.weixin.qq.com/s/JYu_oXWbWbasT1fcBRo-cA)

### Planning 规划

Planning 指
- **任务分解**（把大任务划分成小任务，进而达到解决复杂问题的目标）；
- **反思和提炼**（基于已有动作做自我批评和自我反思，从错误中学习优化接下来的动作）。

任务分解的典型实现方案是 COT（Chain of thought）和 TOT（Tree of thoughts）。

一项复杂任务包括多个子步骤，智能体需要提前将任务分解，并进行规划。

规划模块
- **无反馈**规划:大语言模型在做推理的过程中无需外界环境的反馈。这类规划进一步细分为三种类型:
  - **单路**推理，仅一次大语言模型调用就输出完整的推理步骤；
  - **多路**推理，借鉴众包思想，让大语言模型生成**多个**推理路径，进而确定最佳路径；
  - 借用外部的规划器。
- **有反馈**规划:外界环境提供反馈，而大语言模型需要基于环境的反馈进行下一步以及后续的规划。这类规划反馈的提供者来自三个方面:**环境**反馈、**人类**反馈和**模型**反馈。

#### 综述

LLM在智能体规划中应用，综述文章
- 【2024-2-5】中科大 [Understanding the planning of LLM agents: A survey](https://arxiv.org/pdf/2402.02716)
- 解读: [五个维度，详解LLM-based Agent中的规划（planning）能力](https://zhuanlan.zhihu.com/p/704354580)

传统工作主要依赖符号方法或强化学习方法, 比如 领域定义语言 `PDDL` (Planning Domain Definition Language), 这类方法存在若干不足:
- 符号方法要求将灵活的**自然语言**转化成**符号模型**, 少不了人工专家介入
  - 这种方法对错误容忍度低,稍有不慎就导致任务失败
- 强化学习方法通过深度神经网络模型模拟策略网络、奖励模型. 
  - 需要大量与环境交互的样本学习, 代价较高且不易实施

大型语言模型（LLM）标志着一个范式转变。LLM在多个领域取得了显著的成功，展示了在推理、工具使用、规划和指令跟随方面的重要智能。这种智能为将LLM作为代理的认知核心提供了可能性，从而有潜力提高规划能力。

提出新颖的分类法，将现有 LLM-智能体规划方法分成了五大类:**任务分解**、**多计划选择**、**外部模块辅助规划**、**反思与细化**以及**记忆增强规划**。
- **任务分解**: 把大蛋糕切成小块，让智能体一步步地解决。
- **多计划选择**: 给智能体一个“选择轮”，生成多个计划，然后挑一个最好的来执行。
- **外部模块**辅助规划: 借助**外部规划器**，有个军师在旁边出谋划策。
- **反思与细化**: 执行计划的过程中，能够停下来反思，然后改进计划。
- **记忆增强**规划: 一个记忆面包，记住过去经验，为将来规划提供帮助。

![](https://pic2.zhimg.com/80/v2-46a02481c04f6faed4e5142d8d36ccb5_1440w.webp)

这些方法并不孤立，相互交织，共同提升智能体的规划能力。


#### (1) 任务分解

任务分解 (Task Decomposition)

两个关键步骤:
- “**分解**”: 将复杂任务分解成若干个子任务；
- “**子计划**”: 为每个子任务制定计划。

任务分解方法主要分为两类:
- **分解优先** (Decomposition-First Methods): 将任务分解为子目标，依次为每个子目标制定规。
- **交错分解**方法（Interleaved Decomposition Methods）: 在**任务分解**和**子任务规划**之间进行交错，每次只揭示当前状态的一个或两个子任务。

##### 分解优先

“分解优先” 把所有拼图碎片按颜色或形状分类，然后再逐个拼起来。
- 优点: 子任务与原任务之间的联系更强，减少了任务遗忘和幻想的风险。
- 缺点: 如果某个步骤出错，可能会影响到整个任务的完成。
- 案例: “HuggingGPT” 和 “Plan-and-Solve”

案例
- • `HuggingGPT`: LLM作为控制器，负责将人类输入的任务分解为子任务，选择模型，并生成最终响应。
- • `Plan-and-Solve`: 将原始的“**让我们逐步思考**”转变为两步提示指令:“**我们首先制定计划**”和“**我们执行计划**”。
- • `ProgPrompt`: 将自然语言描述的任务转化为**编码**问题，将每个动作形式化为函数，每个对象表示为变量。


##### 交错分解

“交错分解”: 一边**分解**任务，一边**制定**计划，边拼图边调整拼图碎片的位置。
- 优点: 可根据环境反馈**动态调整**任务分解，提高了容错性。
- 缺点: 如果任务太过复杂，可能会导致智能体在后续的子任务和子计划中**偏离原始目标**。

案例: 类似于ReAct, COT, POT都可以归类到这个范畴中。
- • `Chain-of-Thought` (`CoT`): 通过构建的轨迹指导LLM推理复杂问题，利用LLM的推理能力进行任务分解。
- • `Zero-shot CoT`: 使用“**让我们逐步思考**”的指令，解锁LLM的零样本推理能力。
- • `ReAct`: 将**推理**和**规划**解耦，交替进行推理（思考步骤）和规划（行动步骤）。

##### CoT

思维链（Chain of Thought, CoT） 是一种 prompt 方法，通过编写每步推理逻辑（推理链），解决复杂问题。

COT 已然成为「诱导模型推理」的标准提示技术，可以增强解决复杂任务时的模型性能。
- 通过「Think step by step」，模型可以利用更多测试时计算（test-time computation）将任务分解为更小、更简单的子步骤，并能够解释模型的思维过程。

具体可用 Zero-shot 和 Few-shot 的 COT

Zero-shot COT 的实现分为两步:
- 第一步输入 `Let's think step by step` 得到推理链
- 第二步输入 `推理链`+ `Therefore, the answer is` 得到最终答案
- 合并成一步:`Let's work this out it a step by step to be sure we have the right answer`


```s
Question: Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill?
Answer: It takes Tom 30*4 = <<30*4=120>>120 minutes to climb the hill.
It takes Tom 120/60 = <<120/60=2>>2 hours to climb the hill.
So the answer is 2.
===
Question: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost $9.50, and the shoes cost $92. Jack has $40. How much more money does Jack need?
Answer: The total cost of two pairs of socks is $9.50 x 2 = $<<9.5*2=19>>19.
The total cost of the socks and the shoes is $19 + $92 = $<<19+92=111>>111.
Jack need $111 - $40 = $<<111-40=71>>71 more.
So the answer is 71.
===
Question: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?
Answer:
```


##### ToT

**思想之树**（Tree of Thoughts）在每个子步骤中探索多种推理可能性来扩展CoT。
- 首先将问题分解为多个思维步，并在每个步骤内生成多个思路，从而创建出一个**树结构**解决方案；
  - 搜索过程可以是BFS（广度优先搜索）或DFS（深度优先搜索），其中每个状态由分类器（经由提示）或多数投票来评估。
- 任务分解可以通过简单提示，如「Steps for XYZ.\n1.」，「What are the subgoals for achieving XYZ」 ；
- 或是使用任务相关指令，如「Write a story outline」可以用于写小说；也可以由人输入。


##### 思考

总结
- • **分解优先**
  - 优势: 创建了子任务与原始任务之间的**强关联**，降低了**任务遗忘**和**幻觉**风险。
  - 劣势: 需要额外的调整机制，以避免某个步骤的错误导致整体失败。
- • 交错分解
  - 优势: 根据环境反馈动态调整分解，提高了容错性。
  - 劣势: 但对于复杂任务，过长的轨迹可能导致LLM产生**幻觉**，偏离原始目标。

挑战: 尽管任务分解显著提高了智能体解决复杂任务能力，但挑战依然存在。
- 任务分解带来的**额外开销**，需要更多的推理和生成，增加了时间和计算成本。
- 对于那些被分解成数十个子任务的**高度复杂任务**，智能体**上下文长度限制**可能会导致**规划轨迹遗忘**。

所以，任务分解给智能体的一本“任务攻略”，教它如何一步步解决问题。
- 但这本攻略也不是万能的，需要智能体有足够的“智慧”去理解和运用，同时还要避免在复杂情况下迷失方向。

未来的研究，或许能找到更高效、更可靠的任务分解方法，让智能体在面对复杂任务时更加游刃有余。


#### (2) 多计划选择

多规划选择 (Multi-Plan Selection)

由于任务**复杂性** 和 LLM固有的**不确定性**，给定任务，LLM代理可能会生成多种不同的规划。

如何让机器像人类一样，面对复杂任务时，能够**生成多种可能**解决方案，并从中**选择最优**的计划呢？

这正是“多计划选择”（Multi-Plan Selection）要解决的问题。

智能Agent面对复杂问题，可能会生成多个计划。但是，这些计划可能各不相同，甚至有些可能根本不可行。

怎么办? **多计划选择**, 两个主要步骤:**多计划生成**和**最优计划选择**。
- 多计划生成阶段: LLMs 尝试生成一系列可能的计划。
- 最优计划选择阶段: Agent 从多个候选计划中选择一个最好的

多计划选择理论上很美，却面临着一些挑战。
- 首先，增加**计算需求**，尤其是对于大模型来说，计算成本可能会非常高。
- 依赖于LLM来评估计划的性能，这本身还存在不确定性，需要进一步的验证和调整。


##### 多计划生成

头脑风暴阶段尽可能多地提出解决方案。

主流方法利用生成模型在解码过程中的不确定性，比如通过**温度采样**或**top-k采样**获得多个不同推理路径。
- Tree-of-thought 提到了2种生成 planing 策略:sample、propose
  - 采样 `sample`: 策略与 Self-consistency 策略一致，在解码过程中，LLM会采样多个plan。
  - 提议 `propose`: 提示中使用**少量示例**指导LLM生成各种plan。

多规划生成涉及利用生成模型解码过程中的不确定性，通过不同采样策略来产生多个候选规划。
- `Self-consistency`: 采用简单直觉，即复杂问题的解决方案很少是唯一的。通过温度采样、top-k采样等策略，获得多个不同的推理路径。
- `Tree-of-Thought` (ToT) : 提出“采样”和“提议”两种策略来生成规划。LLM在解码过程中会采样多个规划，并通过少量示例提示生成各种规划。
- `Graph-of-Thought` (GoT) : 在ToT的基础上增加了思想的转换，支持任意思想的聚合。
- `LLM-MCTS` 和 `RAP`: 利用LLM作为启发式策略函数，通过蒙特卡洛树搜索（`MCTS`）算法来获取多个潜在动作。


##### 最优计划选择

最优计划选择阶段，Agent 从多个候选计划中选择一个最好。

候选规划中选择最优规划时，采用了多种**启发式**搜索算法, 比如 简单的**多数投票**策略，或者利用**树结构**来辅助多计划搜索

案例
- • `Self-consistency`: 使用简单的**多数投票**策略，将得票最多的规划视为**最优选择**。
- • `Tree-of-Thought` (ToT) : 支持**树搜索**算法，如**广度优先搜索**（BFS）和**深度优先搜索**（DFS），使用LLM评估多个动作并选择最优动作。
- • `LLM-MCTS` 和 `RAP`: 也用**树结构**辅助多规划搜索，但采用MCTS算法进行搜索。
- • `LLM A`: 利用经典的**A算法**协助LLM搜索，使用当前位置到目标位置的切比雪夫距离作为启发式成本函数来选择最优路径。

##### 思考

总结
- • 多规划选择的可扩展性显著优势在于提供广阔搜索空间中广泛探索潜在解决方案的能力。
- • 然而，这种优势伴随着计算需求的增加，尤其是对于具有大量token计数或计算的模型，这在资源受限的情况下尤为重要。
- • LLM 在规划评估中的作用引入了新的挑战，因为LLM在任务排名方面的表现仍在审查中，需要进一步验证和微调其在此特定情境下的能力。
- • LLM 的随机性质为选择过程增加了随机性，可能影响所选规划的一致性和可靠性。



#### (3) 外部规划器

外部规划器辅助规划 (External Planner-Aided Planning)

语音助手解决问题时，常常面临一些复杂环境约束, 单纯的LLM可能就会有点力不从心
- 比如解决数学问题或生成可接受的行动方案。

尽管LLM在推理和任务分解方面展现出了强大的能力，但在面对具有复杂约束的环境时，例如数学问题求解或生成可执行动作，仍然面临挑战。

根据引入的规划器类型，分为两类: 
- • `符号规划器`（Symbolic Planner）: 基于形式化模型，如PDDL，使用符号推理来找到从初始状态到目标状态的最优路径。
- • `神经规划器`（Neural Planner）: 通过强化学习或模仿学习技术训练的深度模型，针对特定领域展现出有效的规划能力。

External Planner-Aided Planning，整体略复杂，很少会用到


虽然, LLM 在推理和任务分解方面展现出了强大的能力，但在面对**复杂约束**的环境时，借助外部规划器的规划方法显得尤为重要。LLM在这里主要扮演**辅助**角色，其主要功能包括解析文本反馈，为规划提供额外的推理信息，特别是解决复杂问题时。

这种结合统计AI和LLM的方法，有望成为未来人工智能发展的主要趋势。


##### 基于符号

符号规划器一直是自动化规划领域的基石

基于明确的符号化模型
- 比如 PDDL 模型，用符号推理来找出从初始状态到目标状态的最优路径。
- `LLM+P` 通过结合基于PDDL的符号规划器，增强了LLM的规划能力，用LLM的语义理解能力和编码能力，把问题组织成文本语言提示输入LLM，然后利用Fast-Downward求解器进行规划。
- 如果环境**动态交互式**，`LLM-DP`接收到环境反馈后，将信息形式化为`PDDL`语言，然后用 BFS 求解器生成计划。
- 此外，`LLM+PDDL` 也在用PDDL语言形式化任务，并且加入了手动验证步骤，以防LLM生成的PDDL模型出现问题。

符号规划器代表工作: 
- • `LLM+P`: 通过结合基于PDDL的符号规划器，使用LLM将问题组织成PDDL语言格式，并利用Fast Downward solver进行规划。
- • `LLM-DP`: 特别为动态交互环境设计，将环境反馈信息形式化为PDDL语言，并使用BFS solver生成规划。
- • `LLM+PDDL`: 在LLM生成的PDDL模型中增加手动验证步骤，并提出使用LLM生成的规划作为局部搜索规划器的初始启发式解。
- • `LLM+ASP`: 将自然语言描述的任务转换为ASP问题，然后使用ASP求解器CLINGO生成规划。

规划过程中，提出的计划可作为局部搜索规划器的初始启发式解，加快搜索进程。

##### 基于神经网络

神经规划器是**深度模型**，通过`强化学习`或`模仿学习`技术在收集的规划数据上训练，表现出在特定领域内有效的规划能力。
- `DRRN`通过`强化学习`将规划过程建模为`马尔可夫决策过程`，训练策略网络来获得深度决策模型。
- 而 Decision Transformer 则通过`模仿学习`，让一个transformer模型复制人类的决策行为。

问题复杂且少见时，这些小模型可能因训练数据不足而表现不佳。

有些研究将LLM与轻量级神经网络规划器结合起来，以进一步提升规划能力。

神经规划器代表工作: 
- `CALM`: 早期提出将语言模型与基于RL的神经规划器结合起来，语言模型处理文本环境信息，生成系列候选动作，然后 `DRRN` 策略网络重新对这些候选动作进行排序，最终选择最优动作。
- `SwiftSage` 利用`认知心理学`中的`双过程理论`，将规划过程分为`慢思考`和`快思考`。
  - `慢思考`涉及复杂推理和理性思考，而`快思考`则类似于通过长期训练发展起来的本能响应。
  - 当执行计划时出现错误，表明问题更复杂时，Agent会切换到慢思考过程，LLM基于当前状态进行推理和规划。这种快思考和慢思考的结合在效率方面非常有效。

##### 思考

总结: 
- • 这些策略中，LLM主要扮演**支持**角色，主要功能包括: 解析文本反馈并提供额外的推理信息以协助规划，特别是在解决复杂问题时。
- • 传统符号AI系统在构建符号模型时**复杂且依赖于人类专家**，而LLM可以加速这一过程，有助于更快更优地建立符号模型。
- • 符号系统优势包括**理论完备性**、**稳定性**和**可解释性**。将统计AI与LLM结合，有望成为未来人工智能发展的主要趋势。

#### (4) 自我反思

**自我反思**和**自我修正**（Reflection and Refinemen）

为什么要反思和修正?
- LLM 规划过程中可能会产生**幻觉**，或因为理解不足而陷入“**思维循环**”
- 这时候停下来回头看看，总结一下哪里出了问题，然后进行调整，就能更好地继续前进。
- 反思和总结失败有助于代理纠正错误并在后续尝试中打破循环

自我反思让自主智能体改进过去的行动决策、纠正之前的错误来迭代改进，在可以试错的现实任务中非常有用。

反馈和改进是规划过程中不可或缺的组成部分，增强了LLM Agent 规划的**容错**能力和**错误纠正**能力。

如何反思、修正? 
- 迭代过程:`生成`、`反馈`和`改进`。

案例
- • `Self-refine`: 利用迭代过程，包括生成、反馈和精炼。在每次生成后，LLM为计划产生反馈，促进基于反馈的调整。
  - 论文 [Self-Refine: Iterative Refinement with Self-Feedback]()
- • `Reflexion`: 扩展 `ReAct` 方法，通过引入**评估器**来评估轨迹。LLM 检测到错误时生成自我反思，帮助纠正错误。
- • `CRITIC`: 使用**外部**工具，如**知识库**和**搜索引擎**，验证LLM生成的动作。然后利用外部知识进行自我纠正，显著减少事实错误。
  - 论文 [CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing]()
- • `InteRecAgent`: 使用称为ReChain的自我纠正机制。LLM用于评估由交互推荐代理生成的响应和工具使用计划，总结错误反馈，并决定是否重新规划。
- • `LEMA`: 首先收集错误的规划样本，然后使用更强大的GPT-4进行纠正。纠正后的样本随后用于**微调LLM代理**，从而在各种规模的LLaMA模型上实现显著的性能提升。


`ReAct` 通过将动作空间扩展为任务相关的离散动作和语言空间的组合，在LLM中集成了推理和动作，其中动作使得LLM能够与环境交互（例如使用维基百科搜索API），而语言空间可以让LLM以自然语言的方式生成推理轨迹。

ReAct 提示模板包含了LLM思考的明确步骤

在知识密集型任务和决策任务的实验中，ReAct 比只用 Act（移除Thought）的基线模型效果更好。
- ![](https://pic3.zhimg.com/80/v2-01b151d354d1cba34993a8d0f87ef50e_1440w.webp)


挑战
- LLM 自我反思，目前还没有证明这种**文本形式更新最终能够让LLM达到指定目标**。
- 毕竟, 人类学习时，既要反思，还要老师或他人的指导，才能更有效地学习和进步。

LLM通过自我反思和修正，不仅能够提高自身的**容错**能力和**错误纠正**能力，而且还能在复杂问题规划中表现得更加出色。


##### 思考

总结
• 自我反思策略类似于强化学习的原则，其中代理作为决策者，环境反馈触发策略网络的更新。然而，与深度强化学习通过修改模型参数来更新不同，在LLM代理中，这种更新是通过LLM自身的自我反思来实现的，最终形成文本形式的反馈。

• 这些文本反馈可以作为长期和短期记忆，通过提示影响代理后续的规划输出。然而，目前还没有确凿的证据证明这种文本形式的更新最终能够使LLM代理达到特定目标。


#### (5) 记忆增强

记忆增强规划 (Memory-Augmented Planning)

**记忆**是提升规划能力的关键, 帮助代理从经验中学习并适应新情境。

通过记忆来增强 LLM-Agents 规划能力的方法: **RAG记忆** 和 **参数记忆**


##### RAG 记忆

RAG，检索增强生成，用检索到的信息来增强LLM最新知识。

把过去的经验存储在记忆中，需要时检索出来。

RAG-based Memory（基于RAG的记忆）：
- • 概念：使用`检索增强生成`（Retrieval-Augmented Generation, `RAG`）技术，将记忆以**文本**形式存储，并在需要时检索出来辅助规划。
- • 方法：如 `MemoryBank`、`TiM` 和 `RecMind`，通过文本编码模型将记忆编码为向量，并建立索引结构，以便在规划时检索与当前任务相关的经验。

这些记忆通常以**文本**、**表格**形式或**知识图谱**等形式存储。
- 有的系统把人类每天行为经验以文本形式存储起来，然后基于相关性和新鲜度来检索记忆。
- 还有的系统用**向量编码模型**将每个记忆编码成**向量**，并建立索引结构，以便在检索时快速找到相关信息。

案例
- `Generative Agents`:以文本形式存储类似于人类的日常经验，并根据当前情况的相关性和新鲜度来检索记忆。
- `MemoryBank`、`TiM` 和 `RecMind`:将每个记忆编码成向量，并使用索引结构（如FAISS库）来组织这些向量。检索时，使用当前状态的描述作为查询来检索记忆池中的记忆。区别在于更新记忆的方式不同。
- `MemGPT`:借鉴了计算机架构中的**多级存储**概念，将LLM 上下文视为RAM，并将额外的存储结构视为磁盘。LLM 自主决定是检索历史记忆还是将当前上下文保存到存储中。
- `REMEMBER`:将历史记忆以Q值表的形式存储，每个记录是一个包含环境、任务、动作和Q值的元组。在检索时，会同时检索正面和负面记忆，以便LLM根据环境和任务的相似性生成计划。

##### 参数记忆

参数记忆通过**微调LLM**，将 Agent历史经验样本嵌入到模型参数中。

Embodied Memory（体现记忆）：
- • 概念：通过微调（fine-tuning）LLM，将代理的历史经验样本嵌入到模型参数中，从而增强记忆能力。
- • 方法：如 `CALM` 和 `TDT`，这些方法使用从代理与环境交互中收集的数据来微调模型，使其能够记住与规划相关的信息，并在规划任务中表现更好。

这些经验样本通常来自Agent与环境的交互，可能包括关于环境的常识知识、与任务相关的先验知识，以及成功或失败的经验。

虽然微调一个大参数的模型成本很高，但通过PEFT，可以通过只训练一小部分参数来降低成本并加快速度。

##### 思考

两种方法都有优势和局限性。

记忆更新方式：
- 基于 RAG 方法提供了**实时、低成本**的外部记忆更新，主要在自然语言文本中，但依赖于检索算法的准确性。
- 而 FineTuning 微调通过**参数修改**提供了更大的记忆容量，但**记忆更新成本高**，并且在保留细节方面存在挑战。

总结
- • 记忆增强的LLM代理在规划中表现出更强的**增长潜力**和**容错**能力，但记忆生成在很大程度上依赖于LLM自身的生成能力。
- • 通过自我生成的记忆来提升较弱LLM代理的能力仍然是一个具有挑战性的领域。


#### 规划能力评估

如何评估 (Evaluation) 代理的规划能力? 

几种主流基准测试方法：
- **交互式游戏环境**（Interactive Gaming Environments）：
  - • 提供基于代理动作的实时多模态反馈，如文本和视觉反馈。
  - • 例如Minecraft，代理需要收集材料制作工具以获得更多奖励，常用评价指标是代理创建的工具数量。
- 基于**文本**的交互环境（Text-based interactive environments）：
  - • 代理位于用自然语言描述的环境中，动作和位置有限。
  - • 常用评价指标是成功率或获得的奖励，例如ALFWorld和ScienceWorld。
- **交互式检索环境**（Interactive Retrieval Environments）：
  - • 模拟人类在现实生活信息检索和推理的过程。
  - • 代理可以与搜索引擎和其他网络服务交互，通过搜索关键词或执行点击、前进、后退操作来获取更多信息，完成问答任务或信息检索任务。
- **交互式编程环境**（Interactive Programming Environments）：
  - • 模拟程序员与计算机之间的交互，测试代理解决计算机相关问题的规划能力。
  - • 代理需要与计算机交互，通过编写代码或指令来解决问题。


#### Tool Use 工具使用

使用复杂工具是人类高智力的体现，创造、修改和利用外部物体来完成超出身体和认知极限的事情，同样，为LLM配备外部工具也可以显著扩展模型功能。
- 一只海獭漂浮在水中时，用岩石劈开贝壳的图片。虽然其他一些动物可以使用工具，但其复杂性无法与人类相比。
- ![](https://pic4.zhimg.com/80/v2-9ce6576b421bda75019d1cf9a4bd4167_1440w.webp)

[MRKL](https://arxiv.org/pdf/2205.00445.pdf)（模块化推理、知识和语言），是一个神经符号架构的自主智能体，包含一组「专家」模块和一个用作路由器（router）的通用语言模型，以路由查询到最合适的专家模块。

每个模块可以神经网络，也可以是符号模型，例如数学计算器、货币转换器、天气API

研究人员做了一个微调语言模型以调用计算器的实验，使用算术作为测试用例，结果表明，解决verbal数学问题比解决明确陈述的数学问题更难，因为LLM（7B Jurassic 1-large 模型）不能可靠地为基本算术提取正确的参数，也凸显了符号工具的重要性，以及了解何时利用何种工具的重要性。

TALM（工具增强语言模型）和 [Toolformer](https://arxiv.org/pdf/2302.04761.pdf) 都是微调语言模型以学习使用外部工具API

**MRKL**（[Karpas et al. 2022](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2205.00445)）是“模块化推理、知识和语言（Modular Reasoning, Knowledge and Language）”的缩写，是一种用于自主代理的神经符号结构。提出了MRKL系统，其中包含一系列“专家”模块，通用的LLM作为路由将查询路由到最合适的专家模块。这些模块可以是神经网络（如深度学习模型）或符号化的（如数学计算器、货币转换器、天气API）。

对LLM进行了一项实验，用算术作为测试案例，对其进行了微调，以便能够调用计算器。实验结果显示，相对于明确陈述的数学问题，解决口头数学问题更加困难，因为LLMs（7B Jurassic1-large 模型）无法可靠地提取基本算术的正确参数。这些结果强调了在外部符号化工具能够可靠工作时，**了解何时以及如何使用这些工具非常重要**，这取决于LLM的能力。

**TALM**（Tool Augmented Language Models; [Parisi et al. 2022](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2205.12255)）和**Toolformer**（[Schick et al. 2023](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2302.04761)）都通过微调语言模型来学习使用外部工具API。数据集根据新增的API调用注释是否能提高模型输出的质量来进行扩展。请参阅Prompt Engineering文章中的“[External APIs](https://link.zhihu.com/?target=https%3A//lilianweng.github.io/posts/2023-03-15-prompt-engineering/%23external-apis)”部分以获取更多详细信息。

[ChatGPT插件](https://link.zhihu.com/?target=https%3A//openai.com/blog/chatgpt-plugins)和[OpenAI API函数调用](https://link.zhihu.com/?target=https%3A//platform.openai.com/docs/guides/gpt/function-calling)是LLM在实践中能够使用工具的很好例子。工具API的集合可以由其他开发者提供（如插件）或自定义（如函数调用）。

**HuggingGPT**（[Shen et al. 2023](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2303.17580)）是一个框架，利用ChatGPT作为任务规划器，根据模型描述选择HuggingFace平台上可用的模型，并根据执行结果总结回应。

ChatGPT插件和OpenAI API函数调用也是增强语言模型使用工具能力的例子，其中工具API的集合可以由其他开发人员提供（如插件）或自定义（如函数调用）。

[API-Bank](https://arxiv.org/pdf/2304.08244.pdf)是用于评估工具增强型LLM性能的基准，包含53个常用的API工具，一个完整的工具增强的LLM工作流，以及264个标注对话，用到568次API调用。

API的选择非常多样化，包括搜索引擎、计算器、日历查询、智能家居控制、日程管理、健康数据管理、账户认证工作流等。
由于API数量众多，LLM首先可以访问API搜索引擎，找到合适的API调用，然后使用相应的文档进行调用。

在API-Bank工作流程中，LLM需要做出三次决策，每一步都可以评估决策的准确性:
1. 是否需要API调用；
2. 确定要调用的正确API:如果不够好，则LLM需要迭代地修改API输入（例如决定搜索引擎API的搜索关键字）；
3. 基于API结果的响应:如果结果不满意，则模型可以选择改善并再次调用。

该基准可以在三个层次上评估智能体的工具使用能力。
- 层次1:评估调用API的能力
  - 给定API的描述，模型需要确定是否调用给定的API，正确调用并正确响应API返回；
- 层次2:检查检索API的能力
  - 模型需要搜索可能解决用户需求的API，并通过阅读文档学习如何使用。
- 层次3:评估规划API的能力，而非检索和调用
  - 如果用户请求不明确（例如安排小组会议、预订旅行的航班/酒店/餐厅），模型可能不得不进行多次API调用来解决。


LLM 调用外部工具的应用模式
- ![](https://pic1.zhimg.com/80/v2-670e38abdbd8e6686adcc2c35aea66c2_1440w.webp?source=1940ef5c)

OpenAI 的 Jack Rae 和 Ilya Sutskever 在之前的分享中也分别提到了 **压缩即智慧** 的理念。对于模型的“压缩率”来说，如果能更有效地使用这些“外部工具”，就能大幅提升很多特定任务 next token 预测的准确率。

提升压缩率的手段
- ![](https://picx.zhimg.com/80/v2-06fe12da5124690ef0bcba8b8531d618_1440w.webp?source=1940ef5c)

这个方向很有价值
- 从“有效数据”角度看，人类执行各类任务使用工具，甚至思维过程等数据会有非常高的价值。
- 从模型训练角度来看，如何能在过程中把模型利用工具的能力也体现在 loss function 里，可能也是个很有趣的方向


#### 角色扮演


如何提升 Agent 角色扮演能力

Agent 最重要的功能是通过扮演某种角色，来完成特定的任务，或者完成各种各样的模拟，因此 Agent 的角色扮演能力至关重要。
- （1）Agent 角色扮演能力定义
  - Agent 角色扮演能力分为两个维度:
  - 角色和 Agent 行为关系
  - 角色在环境中演化机制
- （2）Agent 角色扮演能力评估
  - 定义了角色扮演能力之后，接下来要对 Agent 角色扮演能力，从以下两个方面进行评估:
  - 角色扮演评估指标
  - 角色扮演评估场景
- （3）Agent 角色扮演能力提升
  - 在评估的基础上，需要进一步对 Agent 的角色扮演能力进行提升，有如下两种方法:
  - 通过 Prompt 提升角色扮演能力:该方法本质是通过设计 prompt 来激发原有大语言模型的能力；
  - 通过微调提升角色扮演能力:该方法通常是基于外部的数据，重新对大语言模型进行 finetune，来提升角色扮演能力。

#### 规划能力

如何提升 Agent 推理/规划能力

- （1）Agent 任务分解能力
  - 子任务定义和拆解
  - 任务执行最优顺序
- （2）Agent 推理和外界反馈融合
  - 设计推理过程中外界反馈的融入机制:让 Agent 和环境形成互相交互的整体；
  - 提升 Agent 对外界反馈的响应能力:一方面需要 Agent 真实应对外界环境，另一方面需要 Agent 能够对外界环境提出问题并寻求解答方案。

多 Agent 高效协同机制

- （1）多 Agents 合作机制
  - Agents 不同角色定义
  - Agents 合作机制设计
- （2）多 Agents 辩论机制
  - Agents 辩论机制设计
  - Agents 辩论收敛条件确定

### Memory 记忆

记忆模块像 Agent大脑，帮助它积累经验、自我进化，让它的行为更加一致、合理和有效。

记忆模块主要记录 Agent 行为，并为未来 Agent 决策提供支撑

- （1）**记忆结构**
  - 统一记忆:仅考虑短期记忆，不考虑长期记忆；
  - 混合记忆:长期记忆和短期记忆相结合
- （2）**记忆形式**:主要基于以下 4 种形式
  - 语言
  - 数据库
  - 向量表示
  - 列表
- （3）**记忆内容**:常见3 种操作:
  - 记忆读取
  - 记忆写入
  - 记忆反思


agentic memory is represented as:
- `Sensory memory` or short-term holding of inputs which is not emphasized much in agents. 
- `Short-term memory` which is the LLM context window 
- `Long-term memory` which is the external storage such as RAG or knowledge graphs.


#### 人类记忆

设计灵感来自人类记忆过程的**认知科学研究**。

人类记忆发展:
- 从感觉记忆开始，它记录**感知输入**；
- 然后是**短期记忆**，暂时保持信息；
- 最后是**长期记忆**，在更长的时间内巩固信息。

【2024-7-4】张亚勤: 人类拥有`DNA记忆`、`短期记忆`、`海马体记忆`、`皮层记忆`、`长期记忆`。

记忆定义为用于获取、存储、保留和后续检索信息的过程，人类大脑中主要有三种类型的记忆。
1. `感官记忆`（Sensory memory）
  - 这种记忆处于记忆的最早阶段，提供了在原始刺激结束后保留感官信息（视觉，听觉等）印象的能力，通常只持续几秒钟。
  - 感官记忆的子类别包括**图标记忆**（视觉）、**回声记忆**（听觉）和**触觉记忆**（触觉）。
2. `短时记忆`（STM）或**工作记忆**（Working Memory）
  - 存储了当下能意识到的所有信息，以及执行复杂的认知任务（如学习和推理）所需的信息，大概可以存储7件事，持续20-30秒。
3. `长期记忆`（LTM）
  - 顾名思义，LTM可以将信息存储相当长的时间，范围从几天到几十年不等，具有基本上无限的存储容量。LTM有两种亚型:
  - 1）显式/**陈述性**记忆，即对事实和事件的记忆，指那些可以有意识地回忆起来的记忆，包括**情景记忆**（事件和经验）和**语义记忆**（事实和概念）。
  - 2）隐式/**程序性**记忆，这种类型的记忆是**无意识**的，包括自动执行的技能和例程，比如骑自行车或在键盘上打字。
- ![](https://pic4.zhimg.com/80/v2-39913cb59e36e3ee0729cad2b56c98bb_1440w.webp)

#### Agent 记忆

Agent 记忆结构设计也借鉴了这些人类记忆特点。
- 短期记忆类似 受限于transformers**上下文窗口**的输入信息。
- 而长期记忆则类似于**外部向量存储**，Agent可以根据需要快速查询检索。

记忆来源: 智能体记忆内容的出处。

三种类型记忆来源:
- **内部任务信息**（Inside-trial Information）: 当前任务执行信息
  - 单个任务或交互过程中收集的数据。仅与当前正在进行的任务有关。
  - 一个对话人物, Agent 要记住上下文信息,以便生成连贯的回应
- **跨任务信息**（ Cross-trial Information ）: 历史任务重的长期积累学习
  - 跨越了多个任务或交互过程，它包括了Agent在不同任务中积累的经验、学到的教训以及可能的模式识别
  - 旅行计划中, Agent 从用户预订过的机票酒店,用户反馈 这类跨任务信息优化改进执行策略
- **外部知识**（External Knowledge）
  - Agent 与环境交互之外的信息。
  - 可能是通过API调用、数据库查询或访问在线资源（如维基百科）等方式获得的

对应到语言模型概念:
1. 作为原始输入（包括文本、图像或其他形式）的学习嵌入表征的`感官记忆`;
2. `短期记忆`就是**上下文学习**（in-context learning），非常短且影响范围有限，受到Transformer的上下文窗口长度的限制。
3. `长期记忆`作为智能体在查询时可用的外部向量存储，可通过快速检索访问。


##### 长期记忆

获取长期记忆的方法最常见的方式是通过“语义搜索”。
- 用一个 embedding 模型，将所有的记忆文本都转化为一个向量。而后续跟模型的交互信息也可以通过同样的 embedding 模型转化为向量，计算相似度来找到最相似的记忆文本。最后再将这些记忆文本拼接到 prompt 里，作为模型的输入。
- ![](https://pic1.zhimg.com/80/v2-3742a095fdd75d3c3a66faecbb690575_1440w.webp?source=1940ef5c)
- 这类方法最热门的开源项目可以参考 OpenAI 官方的 [ChatGPT Retrieval Plugin](https://github.com/openai/chatgpt-retrieval-plugin) 和 Jerry Liu 的 [LlamaIndex](https://github.com/jerryjliu/llama_index)。

这种拓展模型记忆的模式相比人类大脑的运作方式来说感觉还有些“粗糙”，所谓的长期与短期记忆（包括 LangChain 与 LlamaIndex 中一些更复杂的实现），仍然是比较“hard coded”的感觉。如果未来在模型 context size 上有突破性的研究进展，那么当前的这类模式或许就不再需要了。


##### 永久记忆


【2025-1-3】[“AI将在2025拥有永久记忆”，谷歌前 CEO 施密特预测道](https://mp.weixin.qq.com/s/lXHxivjZ3UGh9E_kIBgWKw)

前谷歌CEO埃里克·施密特（Eric Schmidt）在最近的预测中指出，未来几年内，人工智能（AI）将迎来三项革命性的突破，这些突破不仅有可能实现，而且其影响力“被低估而非高估”。

当前AI的上下文窗口相当于“短期记忆”。然而，2025年，AI将实现“永久记忆”，这将彻底改变其信息处理和存储的方式。

【2024-8-9】谷歌研究院最新发表的论文《Leave No Context Behind》提出了一种名为“无限注意力”的新方法，使AI能够像一个永不疲倦的助手，持续读取和记忆大量信息，只保留最关键的内容。
- 论文 [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/pdf/2404.07143)
- [解读](https://zhuanlan.zhihu.com/p/692221106)

将基于transformer的大型语言模型（LLMs）扩展到**无限长的输入**，同时保持**有界**的内存和计算。
- 一个关键组成部分是一种新的注意力技术，称为 `Infini-attention`。
- Infini-attention将压缩记忆整合到传统的注意力机制中，并在单个变换器块中构建了掩蔽的局部注意力和长期的线性注意力机制。
- 在长上下文语言建模基准测试、1M序列长度的密钥上下文块检索和500K长度的书籍摘要任务上，使用1B和8B LLMs展示了我们方法的有效性。引入了最小有界的内存参数，并实现了LLMs的快速流式推理。

这种“永久记忆”技术将广泛应用于多个领域。

例如，教育领域的AI导师能够记住学生的学习进度和偏好，提供个性化的教学方案；医疗领域的AI助手可以长期跟踪患者的健康数据，提供持续的健康管理建议。此外，施密特预测，这项技术将在科学研究中发挥重要作用，帮助研究人员记住和整合大量文献和实验数据，加速科研进程。

#### 记忆存储

记忆如何保存?

**记忆形式**:主要基于以下 4 种形式
- 语言
- 数据库
- 向量表示
- 列表

记忆格式上，可以**自然语言**或**嵌入向量**形式存储。

文本形式的记忆和参数形式的记忆同样也是各有千秋，它们适合不同的应用场景。
- 如果要快速回忆最近的对话，文本形式可能更合适；
- 而如果要存储大量知识，或者需要一个稳定可靠的知识库，参数形式可能更有优势。

各种记忆形式案例总结
- [Agent memory大揭秘:5种记忆形态，轻松拿捏](https://mp.weixin.qq.com/s?__biz=MzkxNjcyNTk2NA==&mid=2247483923&idx=1&sn=d83a98a68c8f3b1b7185f352832b085d)

##### (1) 文本形式

分析
- 好处: 易于理解和实现，而且读写速度都很快。
- 但是，如果记忆太长，就会占用很多空间，影响处理速度。

文本形式记忆可进一步细分为几种类型:
- 存储完整的交互信息: ReAct
- 最近的交互信息
- 检索到的交互信息和外部知识。

MemGPT 分别体现出了短期和召回记忆；

Qwen-Agent中，通过 chatml 特有多轮格式`<im_start>` `<im_end>`进行分割历史的会话，最后一轮才加上ReAct的prompt。

##### (2) 参数形式

这种方式更高级。不直接存储文字，而是把记忆转换成模型参数，就像是把知识压缩成精华。
- 好处: 不受文本长度限制，而且存储效率更高。
- 但是，写入时可能需要更多的计算，而且解释起来也不如文本形式直观。

参数形式的记忆则涉及更复杂的技术，比如: fine-tuning 和 editing。
- 微调可以帮助模型快速学习特定领域的知识
- 而知识编辑则可以精确地更新或删除某些记忆，避免影响其他无关的知识。

经典 [Character-LLM: A Trainable Agent for Role-Playing]()，用微调方式，


#### 记忆原理


##### 记忆流程

Agent通过**记忆阅读**、**记忆写入**和**记忆反思**三个关键操作与外部环境进行交互。
- 记忆阅读: 提取有意义的信息, 以增强Agent的行动；
- 记忆写入: 将感知到的环境信息存储在记忆中；
- 记忆反思: 模拟了人类审视和评估自己的认知、情感和行为过程的能力。

【2024-4-21】人民大学高瓴学院, Memory 设计综述 [A Survey on the Memory Mechanism of Large Language Model based Agents](https://arxiv.org/pdf/2404.13501)

人民大学关于memory设计的最新资料:[LLM_Agent_Memory_Survey](https://github.com/nuster1128/LLM_Agent_Memory_Survey)

【2024-8-13】[Agent memory大揭秘:轻松搞定记忆写入、管理、读取](https://mp.weixin.qq.com/s/67q1nLDXiB8ypHnIYk4VuA)

记忆操作像 LLM大脑，三个部分组成:记忆写入、记忆管理和记忆读取。
- 记忆写入: LLM短期记忆, 接收到新信息时(聊天),以特殊编码方式存入"大脑"
  - `MemGPT`: 自我指导是否写入记忆,智能体根据上下文决定是否更新
  - `MemoGPT`: 聊天时做总结, 提取对话片段的主题, 关键词形式保存,便于查找, topic,summary,dialogues
- 记忆管理: LLM长期记忆, 整理短期记忆信息;信息归类, 找出最重要的部分,忘掉次要信息,保持大脑的清晰、高效
  - `MemoryBank`: 智能体从对话内容中提炼每日大事记, 同时不断评估，生成个性特征
  - `Voyager`: 智能体根据环境反馈优化记忆
  - `Generative Agents`: 智能体自我反思，获取更高层次的信息. 从事件信息中生成抽象想法
  - `GITM`: 记忆模块中总结多个计划的关键行动, 建立各种情况下的共同参考计划, 提取最重要的行动步骤
- 记忆读取: 使用LLM记忆解决问题
  - `ChatDB`: SQL操作完成记忆阅读
  - `MPC`: 从记忆池里检索相关记忆, 使用思维链示例方式，忽略次要信息
  - `ExpeL`: 用Faiss向量库作为记忆池, 找出与当前任务最相似的k个成功示例.

##### 如何设计 Agent 记忆机制

Agent 和大语言模型最大的不同: Agent 能够在环境中不断进行自我演化和自我学习；而这其中，记忆机制扮演了非常重要的角色。

从 3 个维度来分析 Agent 的记忆机制:
- （1）Agent 记忆机制设计, 常见有以下两种记忆机制:
  - 基于**向量检索**的记忆机制
  - 基于 **LLM 总结**的记忆机制
- （2）Agent 记忆能力评估，主要需要确定以下两点:
  - 评估指标
  - 评估场景
- （3）Agent 记忆机制演化分析:
  - 记忆机制的演化
  - 记忆机制的自主更新



##### 最大内积搜索 Maximum Inner Product Search (MIPS)

外部记忆可以缓解有限注意力span的限制，常用的操作是将信息嵌入表征保存到支持快速最大内积搜索（MIPS）的向量存储数据库中。

为了优化检索速度，一般都会选择近似最近邻（ANN，approximate nearest neighbors）算法返回前k个最近邻节点，牺牲一点准确性以换取巨大的速度提升。

常用的ANN算法包括: LSH（Locality-Sensitive Hashing），ANNOY, HNSW， FAISS, ScaNN

快速MIPS的几种常见ANN算法选择:[更多](https://ann-benchmarks.com/)
-   **[LSH](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Locality-sensitive_hashing)**（Locality-Sensitive Hashing）:它引入了一种哈希函数，使得相似的输入项在很大概率下被映射到相同的桶中，而桶的数量远远小于输入项的数量。
-   **[ANNOY](https://link.zhihu.com/?target=https%3A//github.com/spotify/annoy)**（Approximate Nearest Neighbors Oh Yeah）:核心数据结构是随机投影树，是一组二叉树，其中每个非叶节点表示将输入空间分成两半的超平面，每个叶节点存储一个数据点。树是独立且随机构建的，因此在某种程度上模拟了哈希函数。ANNOY搜索在所有树中进行，通过迭代搜索与查询最接近的一半，并汇总结果。这个想法与KD树有很大的关联，但可扩展性更强。
-   **[HNSW](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.09320)**（Hierarchical Navigable Small World）:它受到小世界网络（ [small world networks](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Small-world_network)）的启发，其中大多数节点可以通过少数步骤到达任何其他节点；例如，社交网络中的“六度分隔”特性。HNSW构建了这些小世界图的分层结构，其中底层包含实际数据点。中间层创建了快捷方式以加快搜索速度。在执行搜索时，HNSW从顶层的随机节点开始，并向目标节点导航。当无法再靠近时，它会下降到下一层，直到达到底层。上层的每次移动都有可能在数据空间中覆盖较大的距离，而下层的每次移动则会提高搜索质量。
-   **[FAISS](https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/faiss)**（Facebook AI Similarity Search）:它基于这样的假设，在高维空间中，节点之间的距离遵循高斯分布，因此应该存在数据点的聚类。FAISS通过将向量空间划分为聚类，并在聚类内部进行量化的方式来应用向量量化。搜索首先使用粗糙的量化方法寻找聚类候选项，然后再使用更精细的量化方法进一步查找每个聚类内的数据。
-   **[ScaNN](https://link.zhihu.com/?target=https%3A//github.com/google-research/google-research/tree/master/scann)**（Scalable Nearest Neighbors）:ScaNN的主要创新在于各向异性向量量化。它将数据点 $x_i$ 量化为 $\tilde{x}_i$ ，使得内积 $\langle q, x_i \rangle$ 尽可能与原始距离 $\angle q, \tilde{x}_i$ 相似，而不是选择最接近的量化质心点。


#### 业界案例


各个 记忆实现案例 在以上版面的分布对比
- [Agent memory大揭秘:记忆从哪儿来？](https://mp.weixin.qq.com/s?__biz=MzkxNjcyNTk2NA==&mid=2247483916&idx=1&sn=4e72c6c675df1c5559aba88128cbd61b)

实际应用中
- 有些系统只模拟人类的**短期记忆**，通过上下文学习实现，记忆信息直接写在prompt中。
- 而有些系统则采用了hybird memory（**混合记忆架构**），明确模拟了人类的短期和长期记忆。**短期记忆**暂时缓冲**最近**的感知，而**长期记忆**则随着时间的推移巩固重要信息。


### Prompt 设计

绝大多数的主要创新还是在 prompt 层面，通过更好的提示词来激发模型的能力，把更多原先需要通过代码来实现的流程“硬逻辑”转化为模型自动生成的“动态逻辑”。

### Prompt 设计范式

prompt 设计模式
- CoT prompt，在给出指令的过程中，同时也给出执行任务过程的拆解或者样例。这个应该很多人都用过，“let's think step by step”  
- “自我审视”，提醒模型在产出结果之前，先自我审视一下，看看是否有更好的方案。也可以拿到结果后再调用一下模型强制审视一下。比如 AutoGPT 里的“Constructively self-criticize your big-picture behavior constantly”。
- 分而治之，大家在写 prompt 的时候也发现，越是具体的 context 和目标，模型往往完成得越好。所以把任务拆细再来应用模型，往往比让它一次性把整个任务做完效果要好。利用外部工具，嵌套 agent 等也都是这个角度，也是 CoT 的自然延伸。
- 先计划，后执行。BabyAGI，HuggingGPT 和 Generative Agents 都应用了这个模式。也可以扩展这个模式，例如在计划阶段让模型主动来提出问题，澄清目标，或者给出一些可能的方案，再由人工 review 来进行确认或者给出反馈，减少目标偏离的可能。
- 记忆系统，包括短期记忆的 scratchpad，长期记忆的 memory stream 的存储、加工和提取等。这个模式同样在几乎所有的 agent 项目里都有应用，也是目前能体现一些模型的实时学习能力的方案。

这些模式与人类认知和思考模式有很相似，历史上也有专门做 [cognitive architecture](https://cogarch.ict.usc.edu/) 相关的研究，从记忆，世界认知，问题解决（行动），感知，注意力，奖励机制，学习等维度来系统性思考智能体的设计。个人感觉目前的 LLM agent 尝中，在奖励机制（是否有比较好的目标指引）和学习进化（是否能持续提升能力）这两方面还有很大的提升空间。或许未来 RL 在模型 agent 这方的应用会有很大的想象空间，而不仅仅是现在主要用来做“价值观对齐”。
- ![](https://pica.zhimg.com/80/v2-8fd3707e43ddc4afa367ce855ab84205_1440w.webp?source=1940ef5c)


#### AutoGPT Prompt

AutoGPT 是提示词应用模式当前比较先进的“集大成者”, 相比经典的 reason + act 模式
- `Constraints & Resources`
  - 模型的输入 context size 有限制，所以需要把重要的信息保存到文件里。
  - 长期记忆的管理功能，当前这类复杂 prompt 生成的解决任务的流程往往比较冗长，没有这类长期记忆的管理很容易就会导致模型的输出变得不连贯协调。
  - 模型是“没有联网”的，所有的知识只更新到训练数据的截止日期。所以也明确告诉模型可以通过网络搜索来获取更多时效性的外部信息。
- `Commands` 各类工具的选择上丰富多样，所以 AutoGPT 能够完成多种不同任务
  - 几大类，包括搜索、浏览网页相关，启动其它的 GPT agent，文件读写操作，代码生成与执行等。
  - 跟 HuggingGPT 有些类似，因为目前 GPT 模型对于越具体，细致的任务，生成的表现就越精确和稳定。所以这种“分而治之”的思路，是很有必要的。
- `Performance Evaluation` 模型整体思考流程的指导原则，思考逻辑也非常符合人类的思考，决策与反馈迭代的过程。
  - 包括:对自己的能力与行为的匹配进行 review，大局观与自我反思，结合长期记忆对决策动作进行优化，以及尽可能高效率地用较少的动作来完成任务
- `Response` 格式的限定也是对前面思维指导原则的具体操作规范说明
  - 格式上来看，也是综合了几种模式，包括需要把自己的想法写出来，做一些 reasoning 获取相关背景知识，生成有具体步骤的 plan，以及对自己的思考过程进行 criticism 等
  - 注意:大段 response 是模型一次交互生成的，而不像一些其它框架中会把计划，审视，动作生成等通过多轮模型交互来生成。
- `人工介入` 
  - 模型很容易会把问题复杂化或者在执行计划层面“跑偏”。
  - 所以在具体执行过程中，AutoGPT 也允许用户来介入，对于每一个具体执行步骤提供额外的输入来指导模型行为。
  - 经过人工反馈输入后，模型会重新生成上述的 response，以此往复

AutoGPT 核心 prompt 如下:

```py
You are Guandata-GPT, 'an AI assistant designed to help data analysts do their daily work.'
Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.

GOALS:

1. 'Process data sets'
2. 'Generate data reports and visualizations'
3. 'Analyze reports to gain business insights'

Constraints:
1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.
2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.
3. No user assistance
4. Exclusively use the commands listed in double quotes e.g. "command name"

Commands:
1. Google Search: "google", args: "input": "<search>"
2. Browse Website: "browse_website", args: "url": "<url>", "question": "<what_you_want_to_find_on_website>"
3. Start GPT Agent: "start_agent", args: "name": "<name>", "task": "<short_task_desc>", "prompt": "<prompt>"
4. Message GPT Agent: "message_agent", args: "key": "<key>", "message": "<message>"
5. List GPT Agents: "list_agents", args: 
6. Delete GPT Agent: "delete_agent", args: "key": "<key>"
7. Clone Repository: "clone_repository", args: "repository_url": "<url>", "clone_path": "<directory>"
8. Write to file: "write_to_file", args: "file": "<file>", "text": "<text>"
9. Read file: "read_file", args: "file": "<file>"
10. Append to file: "append_to_file", args: "file": "<file>", "text": "<text>"
11. Delete file: "delete_file", args: "file": "<file>"
12. Search Files: "search_files", args: "directory": "<directory>"
13. Evaluate Code: "evaluate_code", args: "code": "<full_code_string>"
14. Get Improved Code: "improve_code", args: "suggestions": "<list_of_suggestions>", "code": "<full_code_string>"
15. Write Tests: "write_tests", args: "code": "<full_code_string>", "focus": "<list_of_focus_areas>"
16. Execute Python File: "execute_python_file", args: "file": "<file>"
17. Generate Image: "generate_image", args: "prompt": "<prompt>"
18. Send Tweet: "send_tweet", args: "text": "<text>"
19. Do Nothing: "do_nothing", args: 
20. Task Complete (Shutdown): "task_complete", args: "reason": "<reason>"

Resources:
1. Internet access for searches and information gathering.
2. Long Term memory management.
3. GPT-3.5 powered Agents for delegation of simple tasks.
4. File output.

Performance Evaluation:
1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.
2. Constructively self-criticize your big-picture behavior constantly.
3. Reflect on past decisions and strategies to refine your approach.
4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.

You should only respond in JSON format as described below 
Response Format: 
{
    "thoughts": {
        "text": "thought",
        "reasoning": "reasoning",
        "plan": "- short bulleted\n- list that conveys\n- long-term plan",
        "criticism": "constructive self-criticism",
        "speak": "thoughts summary to say to user"
    },
    "command": {
        "name": "command name",
        "args": {
            "arg name": "value"
        }
    }
} 
Ensure the response can be parsed by Python json.loads
```

## LAM 应用


### 总结

- AI技术的自动化范式 —— AutoGPT
- 基于Agents的自动化团队——GPTeam，许多流程都可以被自动化执行。市场调研、问卷调查、品牌计划等等，都可以由AI来完成。
- 自动化品牌营销公司——AutoCorp


### AI 应用分析

【2023-5-20】[AI应用赛道全解析](https://kylezgq.github.io/generative/ai/2023/05/20/AI%E5%BA%94%E7%94%A8%E5%B1%82%E8%B5%9B%E9%81%93%E5%85%A8%E8%A7%A3%E6%9E%90.html)， [公众号](https://mp.weixin.qq.com/s/yuobgsf-_O_-xCQ5mPAYWA)

产品、平台都可以被称之为`应用层`。
- 比如移动互联网的淘宝、滴滴、美团等，同样比如现在的 MidJourney、ChatGPT 等。

何为 “AI 应用层产品”？
- “`AI 应用层产品`不是 AI 产品，而是`应用层产品`”。就像 “产品经理并不是搞 AI 的，而是搞产品的”。 

AI 应用层产品 整体划分成了两类，一类是 AI-Enabled，另一类是 AI-Native
- AI-Enabled
- AI-Native

问题1:目前的 timing 是应用层的投资阶段吗？
- 十年移动互联网时期，最优秀的应用层公司其实是 2~4 年后出现的。目前海外 AI 市场的融资很热，这里面存在泡沫。今年能看到一些独角兽公司都在估值回调，一些创业公司拿了一些钱却还没找到 PMF, 较为危险，很容易在泡沫破裂之际一同覆灭。
- 更看好做 2B 方向的产品，AI 在 2B 市场的想象力比 2C 更大

问题2:为什么要在**应用层**赛道挖掘投资机会，价值和潜力在哪里？
- 对比移动互联网，价值捕获最多的一层都是做**应用（平台）**，比如微信、美团、滴滴等。 
- 拾象发布过一篇研报，里面有一组投研数据:**应用层**和 **Infra 层**各占整个行业的 20% 价值，大头是**模型层**（60%）。
- 当下的 AI 时代，应用层当如何演变，这也是存在非共识的地方。

5 月份按照场景拆分，分析 AI 应用层的价值，当时的观点基本被掀翻了:
- 产品接入大模型能否有壁垒，当时忽视了场景数据的获取和团队工程能力这两个因素。
- 创业公司和巨头的竞争，巨头一定更有优势？现在来看不见得，这涉及到赛道现有巨头是否愿意做以及能否做。比如，一些尚未被解决的需求，可能巨头还看不上，这正是创业公司的机会。

从 近200 个 AI 产品中挑了几个有代表性的产品绘制这张[图](https://kylezgq.github.io/assets/images/2023-05-20-02.PNG)。
- ![](https://kylezgq.github.io/assets/images/2023-05-20-02.PNG)
- 横轴是产品的上线时间，纵轴是产品对用户 workflow 的冲击。
- 横轴好理解，标注了 3 个重要的时间节点以作时间维度上的区分。
- 纵轴从 workflow 的角度去想，是因为:
  - 这一波 AI，有了大模型的存在，用户对工具/产品的使用流程发生了根本性的变化。
  - 大模型出现以前，用户需要会使用某一类工具（技术门槛，如 Excel、PhotoShop、MySQL 等），才能达到想要的目的。现在大语言模型在用户和工具 / 产品之间，作为一种中介，理解用户的指令（自然语言）并转化为软件的操作（计算机指令）。
- 全新的 `AI-Native` 工作流。与之对应的便是 `AI-Enabled` 工作流。
  - Notion AI 属于 `AI-Enabled` 产品，Jasper、Tome、Gamma、Github Copilot、Microsoft 365 Copilot 等属于 `AI-Native` 产品。

- 应用场景: 内容创作和企业服务是最多的，其次是面向开发者的产品。
- 产品上线时间: GPT-3 发布前后以横向产品为主。由于技术层面的模型性能欠佳，所以基本都是做文案生成、图像生成。GPT4 发布后，模型性能的提升带动产品在垂直领域的发展，如法律、金融、医疗等。

把应用层市场进一步拆分可以得到 4 个小赛道，分别是`内容生成`（Content Creation）、`Copilot`、`AI Agent` 和通用型的`类搜索引擎`。
- 应用场景来看，如前面提到一样，内容创作中的文案编辑、客户支持、以及企业协作是主要场景，其次是聊天机器人。
- 面向的用户群体来看，目前 2B 和 2C 的用户界限尚不清晰，大部分是 2B 场景。
- ![](https://kylezgq.github.io/assets/images/2023-05-20-03.PNG)

其中看好: Copilot 和 AI Agent 中的 Chatbot（聊天机器人）。
- 针对 Copilot 赛道进一步非穷尽式地拆分: 开发者工具（Development tools）、组织协作（生产力工具） 和 垂直场景。
- ![](https://kylezgq.github.io/assets/images/2023-05-20-04.PNG)
- 开发者工具层面，无代码 / 低代码是核心叙事。传统产品如 Webflow，以及这一波 AI 下诞生的初创公司:无代码建站、无代码创建应用程序等。
- 组织协作层面，各个体量的公司都有在参与。几乎都是海外 SaaS 公司在布局。

Chatbot 赛道
- 聊天机器人，插上大脑（大模型）和手脚（AutoGPT）就是 AI Agent。这个赛道的产品以 2C 为主。

按照输出效果（Output）来分析，大致是 \[数据准确性，娱乐休闲性\]。 
- 追求数据准确和严谨性的产品，例如 Perplexity、Neeva（被 Snowflake 收购）、SciSpace 等；
- 追求用户娱乐休闲体验的产品，例如 Character.ai、Replika、Inworld 等。
- ![](https://kylezgq.github.io/assets/images/2023-05-20-05.PNG)


### 科学发现

科学发现
- [ChemCrow](https://arxiv.org/abs/2304.05376)系统内的语言模型通过13个专家设计的工具进行能力增强，可以完成跨有机合成、药物发现和材料设计的任务。

LangChain中实现的工作流程包括了在ReAct和MRKL中描述的机制，并将CoT推理与任务相关的工具相结合:
- 语言模型先提供一个工具名称列表、用途描述以及有关预期输入/输出的详细信息；然后指示模型在必要时使用提供的工具回答用户给定的提示，指令要求模型遵循ReAct格式，即Thought, Action, Action Input, Observation
- 实验结果来看，用语言模型评估的话，GPT-4和ChemCrow的性能几乎相当；但当人类专家评估时，在特定解决方案的完成和化学正确性进行的实验结果显示，ChemCrow的性能远远超过GPT-4

实验结果表明，使用LLM来评估需要深入专业知识领域的性能存在问题，可能会导致LLM不知道内在缺陷，无法很好地判断任务结果正确性。

另一篇[论文](https://arxiv.org/abs/2304.05332)研究了语言模型处理复杂科学实验的自主设计、规划和性能，可以使用工具浏览互联网、阅读文档、执行代码、调用机器人实验API以及利用其他语言模型。

当用户请求「develop a novel anticancer drug」时，模型会返回了以下推理步骤:
1. 询问抗癌药物发现的当前趋势；
2. 选择目标；
3. 要求一种靶向这些化合物的scaffold；
4. 一旦找出化合物，模型再尝试合成。

论文还讨论了风险，特别是非法药物和生物武器，研究人员开发了一套包含已知化学武器制剂清单的测试集，并要求合成，11项请求中有4项（36%）被接受；在这7个被拒绝的样本中，5例发生在网络搜索之后，2例仅基于提示词就拒绝。


## AI Agent 效果


### AI Agent 评测

【2023-8-22】[智谱ChatGLM发布:AgentBench:基础模型 Agent 评测](https://www.toutiao.com/article/7270001966061781545/), [公众号](https://mp.weixin.qq.com/s/-l85RMutoWMRLDPerM9Ziw)

哪些模型更适合作为 agent，其表现又如何？还没有一个合适的评测能够去衡量。

智谱提出了 AgentBench。
- 一个多维演进基准测试，包括 8 个不同环境，可以用来评估 LLMs 在多回合开放式生成环境中的推理和决策能力。

具体如下:
- 操作系统（OS）:考察 LLM 在 bash 环境进行文件操作、用户管理等能力。
- 数据库（DB）:考察 LLM 利用 SQL 对给定数据库进行操作的能力。
- 知识图谱（KG）:考察 LLM 利用工具从知识图谱中获取复杂知识的能力。
- 卡牌对战（DCG）:考察 LLM 作为玩家，根据规则和状态进行卡牌对战的策略决策能力。
- 情景猜谜（LTP）:这个游戏需要 LLM 针对谜题进行提问，从而猜出答案，能够考察 LLM 的横向思维能力。
- 家居（HH）:在模拟的家庭环境下，LLM 需要完成一些日常任务，主要考察 LLM 将复杂的高级目标拆解为一系列简单行动的能力。
- 网络购物（WS）:在模拟的在线购物环境中，LLM 需要按照需求完成购物，主要考察 LLM 的自主推理和决策能力。
- 网页浏览（WB）:在模拟网页环境中，LLM需要根据指令完成跨网站的复杂任务，考察 LLM 作为 Web agent的能力。


初步选择了25个闭源/开源的模型，通过API或Docker的方式进行测试。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/c7f7d46c16ee4d6e8dc6bda61c1919a7~tplv-tt-origin-asy2:5aS05p2hQEFJ56eR5oqA5YiG5Lqr6YOo.image)

经过对 25 个语言模型的测试，发现:
- 顶级商业语言模型在复杂环境中表现出色，与开源模型存在显著差距。

另一方面，v0.2 版本的 ChatGLM2 在几个闭源模型的对比中，评测分数并不是很好，这需要着重改进。

数据集、环境和集成评估包已在这里发布:[AgentBench](https://github.com/THUDM/AgentBench)

主要结论:
- 结论一:顶级 LLM 已经具备了处理真实世界环境交互的强大能力。
  - GPT-4 在 AgentBench 的 8 个数据集中有 7 个表现最佳；在「家居（HH）」上，也实现了 78% 的成功率，这表明它在这种情况下具有实际可用性。而其他基于 API 的 LLM，虽然表现相对较差，但或多或少能够解决一些问题，这表明这些模型有具备这种能力的潜力。
- 结论二:大多数开源 LLM 在 AgentBench 中的表现远不如基于 API 的 LLM（平均分为 0.42 对比 2.24）。
  - 即使是能力最强的开源模型 openchat-13b-v3.2 也与 gpt-3.5-turbo 存在明显的性能差距。这个结果与网上存在的一些开源 LLM 许多声称可以与 gpt-3.5-turbo 和 gpt-4 相媲美，有很大的不符。对于开源的 LLM，它们在诸如知识图谱（KG）、卡牌对战（DCG）和家居（HH）等具有挑战性的任务中通常无法解决任何问题。

通过深入分析评测结果，LLM-as-agent 需要应对以下几个问题:
- **动作有效性**。评估过程中，我们发现模型并不总是在遵循指令。换句话说，模型的预期输出并不总是处于环境可以接受的输入空间中。几种常见的错误包括:1）模型没有理解指令，所以也就没有输出动作；2）模型输出了动作，但却是错误的或不完整的。所以如何确保动作有效，是一个需要改进的方向。
- **长上下文**。一些开源模型的上下文长度只有 2k tokens，这会极大地影响它们在交互任务中的表现，有些任务需要较长的指令和反馈，这有可能会超过上下文长度，导致模型忽略了可能的有用信息。因此，扩展上下文长度可能会提高多轮对话的性能。
- **多轮一致性**。有些任务（例如家居）需要很多轮对话，但每轮对话都比较简短。这导致一些模型在多轮对话中会丢失掉自己的角色。最常见的错误就是输出道歉并表示无法回答。所以，如何在多轮对话中保持一致性，是一个具有挑战性的工作。
- **代码训练**的平衡。相比 ChatGLM-6B，codegeex2-6b-chat 是用更多的代码数据训练出来的，我们通过对比发现，前者在 OS、DB、KG、WS 等方面明显优于后者，然而在需要逻辑推理的 情景猜谜（LTP）上性能却下降不少。而另一方面，进行了代码训练的 wizardcoder 的表现却并不是很好。我们的推测是，代码训练的单轮格式减弱了其多轮能力。因此，用代码数据训练，可以提高部分能力，但同时也会导致其他能力的减弱。


## 智能体应用


### 局限性

当前模型 agent 的问题和局限性。例如:
- **记忆召回**问题。如果只是做简单的 embedding 相似性召回，很容易发现召回的结果不是很好。这里应该也有不少可以改进的空间，例如前面提到的 Generative Agents 里对于记忆的更细致的处理，LlamaIndex 中对于 index 结构的设计也有很多可以选择与调优的地方。
- **错误累积**问题。网上给出的很多例子应该都是做了 cherry-picking 的，实际上模型总体表现并没有那么惊艳，反而经常在前面一些步骤就出现了偏差，然后逐渐越跑越远……这里一个很重要的问题可能还是任务拆解执行，外部工具利用等方面的高质量训练数据相对匮乏。这应该也是 OpenAI 为啥要自己来做 plugin 体系的原因之一。
- **探索效率**问题。对于很多简单的场景，目前通过模型 agent 来自行探索并完成整个解决过程还是比较繁琐耗时，agent 也很容易把问题复杂化。考虑到 LLM 调用的成本，要在实际场景落地使用也还需要在这方面做不少优化。一种方式可能是像 AutoGPT 那样可以中途引入人工的判断干预和反馈输入。
- **任务终止与结果验证**。在一些开放性问题或者无法通过明确的评估方式来判断结果的场景下，模型 agent 的工作如何终止也是一个挑战。这也回到了前面提到的，执行 task 相关的数据收集与模型训练以及强化学习的应用或许可以帮助解决这个问题。


### Agent 设计模式

大模型落地两种模式:`Copilot`模式和`Agent`模式。
- `Copilot` 模式:人机交互以**人类为主导**，AI只是作为助手，部分流程由AI通过对话交互或SDK方式完成。
  - AI Copilot 可能在特定领域（如编程、写作、驾驶等）提供帮助，通过与人类的交互来提高效率和创造力。AI Copilot 可能更多地依赖于人类的输入和指导，而不是完全自主地完成任务。
- `Agent` 模式:人类作为人工智能导师/教练的角色，设计目标并监督结果，大模型充分发挥自身推理能力，实现推理规划，并调用合适的工具和接口，实现行动执行，最后给予结果反馈。

agent和copilot的区别主要体现在:**交互方式**、**任务执行**和**独立性**等方面。
- **交互方式**:
  - copilot 要用户给出清晰明确的prompt，即需要用户具体详细地描述任务或问题，copilot才能根据prompt给出有用的回答。
  - 而大模型agent交互方式更为灵活，根据给定目标自主思考并做出行动，无需用户给出过于详细明确的prompt。
- **任务执行**:
  - copilot在接收到清晰明确的prompt后，可以协助完成一些任务，但它的执行能力相对有限。
  - 而大模型agent则可以根据目标自主规划并执行任务，还能连接多种服务和工具来达成目标，执行任务的能力更强。
- **独立性**:
  - copilot被视为一个“副驾驶”，在完成任务时更多的是起辅助作用，需要用户的引导。
  - 而大模型agent则更像一个初级的“主驾驶”，具有较强的独立性，可以根据目标自主思考和行动。


总结
- AI Agent 更强调**自主性**和**独立**完成任务的能力
- 而 AI Copilot 更侧重于作为人类的**助手**，协助完成特定任务。

场景
- Copilot模式更适合**简单知识交互类**场景，而Agent模式则更适合企业内部**复杂任务**场景，帮助企业尽可能提高劳动生产力


#### Anthropic

详见站内专题: [LLM应用范式](llm_dev)

#### Andrew NG


【2024-3-27】吴恩达
- [解读](https://mp.weixin.qq.com/s/6Jn4-3KPoffsYGrrvYX6vg)
- [Agent才是大模型的最终归宿？](https://mp.weixin.qq.com/s/Y8zj7aWOcyGxNepIV82VQA)
- [Agentic Workflow:AI重塑了我的工作流](https://mp.weixin.qq.com/s/XzEUpUbbWHazAq-OD4EbMA)

2024年3月，初创公司 Cognition 基于大模型开发出首个AI软件工程师Devin，Devin几乎能完成普通软件工程师能做的所有事情，包括搭建环境、编码、调试；更离谱的是，Devin成功通过了一家AI公司的面试。
- Devin没有开源代码，不过随后就有一个团队为了复刻Devin，开发了[OpenDevin](https://github.com/OpenDevin/OpenDevin)，从代码中可见，其核心就是Agent。

通过agent workflow，人工智能能够胜任的任务种类将会大幅扩展。

吴恩达团队实验，让 AI 去写一些代码并运行，最终对比不同模型和工作流程得出结果的性能。结果如下:
- GPT-3.5 模型:准确率 48%
- GPT-4 模型:准确率 67% 
- GPT-3.5 + Agent:高于 GPT-4 模型的表现
- GPT-4 + Agent:表现远高于 GPT-4 模型，非常出色

吴恩达提到的四种 Agent 设计模式: `Reflection`、`Tool Use`、`Planning`、`Multiagent`
- `反思`（reflection）: Agent 审视和修正自己生成的输出
  - 两个 Agent, 一个负责 Coding，另一个负责 Code Review。
  - 让大模型仔细检查输出内容的准确性、结构规范性等，并且给出评论
  - agent会利用外部组件运行代码，执行单元测试，代码Review，甚至与另一个Agent进行对抗来逐渐提升代码质量
- `工具使用`（Tool use）: AI Agent会与外部组件相连接，使用各种工具来分析、收集信息
  - 例如，执行网络搜索作为上下文输入，基于LLM输出执行发送预警邮件操作。
- `规划`（Planning）: Agent 分解复杂任务并按计划执行
  - 类似于思维链模式，按照逻辑顺序组织和评估信息，形成一系列的思考步骤。
  - 这种方法特别适用于**复杂问题**，因为能够帮助人们逐步分析问题，从而得出合理的结论或解决方案。
  - 任务: 生成一张女孩读书的图像，并且女孩的姿势与男孩一致，最后输出描述图像的音频。
  - Agent 规划: 第一步, 确定男孩的姿势，可能在huggingface上找到一个合适的模型来提取这个姿势，接下来使用controlNet模型来合成一个女孩的图像，然后使用图像到文本的模型，最后使用语音合成。
- `多智能体协作`（Multiagent collaboration）: 多个 Agent 扮演不同角色合作完成任务
  - 将一个复杂任务进行分解，让不同语言模型扮演不同的角色，比如公司CEO、设计师、产品经理或测试员，这些"代理"会相互协作，共同开发游戏等复杂程序。
  - AI客服回答售前，售中，售后三种不同类型的问题。
  - 先基于预训练模型微调出三个专业模型，分别用于回答售前，售中，售后问题
  - 然后，再通过一个LLM判断用户的提问属于售前，售中，售后哪一种，最后调用对应的专业大模型。

### 协作案例


#### AI 客服

AI客服回答售前，售中，售后三种不同类型的问题。
- 先基于预训练模型微调出三个专业模型，分别用于回答售前，售中，售后问题
- 然后，再通过一个LLM判断用户的提问属于售前，售中，售后哪一种，最后调用对应的专业大模型。

```py
import openai, os

openai.api_key = os.environ.get("OPENAI_API_KEY")

from langchain.prompts import PromptTemplate
from langchain.llms import OpenAIChat
from langchain.chains import LLMChain

llm = OpenAIChat(max_tokens=2048, temperature=0.5)
# 问题路由: 大模型能准确地判断出用户提问所属类型。
multiple_choice = """
请针对 >>> 和 <<< 中间的用户问题，选择一个合适的工具去回答她的问题。只要用A、B、C的选项字母告诉我答案。
如果你觉得都不合适，就选D。

>>>{question}<<<

我们有的工具包括:
A. 一个能够查询商品信息，为用户进行商品导购的工具
B. 一个能够查询订单信息，获得最新的订单情况的工具
C. 一个能够搜索商家的退换货政策、运费、物流时长、支付渠道、覆盖国家的工具
D. 都不合适
"""
multiple_choice_prompt = PromptTemplate(template=multiple_choice, input_variables=["question"])
choice_chain = LLMChain(llm=llm, prompt=multiple_choice_prompt, output_key="answer")

question = "我想买一件衣服，但是不知道哪个款式好看，你能帮我推荐一下吗？" # A. 一个能够查询商品信息，为用户进行商品导购的工具。
question = "我有一张订单，订单号是 2022ABCDE，一直没有收到，能麻烦帮我查一下吗？" # B. 一个能够查询订单信息，获得最新的订单情况的工具
print(choice_chain(question)) 

# ----- 工具调用 -------
def search_order(input: str) -> str:
    return "订单状态:已发货；发货日期:2023-01-01；预计送达时间:2023-01-10"

def recommend_product(input: str) -> str:
    return "红色连衣裙"

def faq(intput: str) -> str:
    return "7天无理由退货"

tools = [
    Tool(
        name = "Search Order",func=search_order,
        description="useful for when you need to answer questions about customers orders"
    ),
    Tool(name="Recommend Product", func=recommend_product,
         description="useful for when you need to answer questions about product recommendations"
    ),
    Tool(name="FAQ", func=faq,
         description="useful for when you need to answer questions about shopping policies, like return policy, shipping policy, etc."
    )
]

agent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)

question = "我想买一件衣服，但是不知道哪个款式好看，你能帮我推荐一下吗？"
result = agent.run(question)
print(result)

```


两点:
1. 目前人们还是习惯“及时反馈”: 输入命令能够尽快反馈。
  - 虽然通过 Agent 能显著的提升效果，但是在 Agent 工作流程中，往往需要等待几分钟甚至几个小时，才能得到响应，人能不能接受这一点, 或者说习惯培养的难度到底有多大，还不得而知。
2.  快速生成 token 也很重要，即使用质量略低但速度更快的语言模型，通过更多轮次的迭代，也可能比使用更高质量但速度较慢的模型获得更好的结果。

<iframe width="560" height="315" src="https://www.youtube.com/embed/sal78ACtGTc?si=N1GsJ7QcaHTUG-W0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>




## 智能体实现

业界各种最新的智能体

【2023-7-14】[构建你的第一个 LLM APP 所需了解的一切](https://zhuanlan.zhihu.com/p/643233392)

通过上下文注入构建你自己的聊天机器人
- ![](https://pic2.zhimg.com/80/v2-2117b4f8b6ec27c5396603b53203ec21_1440w.webp)

智能体分类
- 单智能体: AutoGPT, ChatGPT(code+plugin), Langchain Agent, Transformers Agent
- 多智能体: BabyAGI, CAMEL, Multi-Agent Debate, MetaGPT, AutoGenß

### 单智能体实现

#### 2023.4 BabyAGI

相对于 AutoGPT ，BabyAGI 是一个相对更聚焦在“思维流程”方面尝试的项目，并**没有**添加对各种外部工具利用的支持。

##### BabyAGI 原理

其核心逻辑非常简单:
- 从任务列表中获取排在第一位的任务。
- 获取任务相关的“记忆”信息，由任务执行 agent 来执行这个任务，获取结果。目前这个执行就是一个简单的 LLM 调用，不涉及外部工具。
- 将返回结果再存放到**记忆存储**中。基于当前信息，如整体目标，最近一次执行结果，任务描述，还未执行的任务列表等，生成**新任务**。
- 将新任务添加到任务列表中，再判断所有任务的优先级，重新排序。
- ![](https://user-images.githubusercontent.com/21254008/235015461-543a897f-70cc-4b63-941a-2ae3c9172b11.png)

这个过程就是在模拟作者一天真实的工作流程。
- 早上起来看下有哪些任务要做，白天做任务拿反馈，晚上再看下基于反馈有没有新的任务要加进来，然后重新排下优先级。
- ![](https://picx.zhimg.com/80/v2-fc665ce2bae6d064e16ae444a5096ff0_1440w.webp?source=1940ef5c)

整个项目的代码量很少，相关的 prompts 也比较简单易懂
- ![](https://picx.zhimg.com/80/v2-a6a493784c7c41d553390551e64f9f4d_1440w.webp?source=1940ef5c)

##### BabyAGI 进化

进化版本
- BabyASI 借鉴了 AutoGPT 添加了对 search，代码执行等工具的支持。理论上，如果这个 ASI（Artificial Super Intelligence）真的足够聪明，甚至可以产生代码给自己做 prompt 优化，流程改造，甚至持续的模型训练等，让 GPT 自己开发未来的 GPT，想想是不是很带感 。


#### HuggingGPT


##### HuggingGPT 简介

【2023-4-3】[HuggingGPT:一个ChatGPT控制所有AI模型，自动帮人完成AI任务](https://www.toutiao.com/article/7217680526839202307),浙大与微软亚研院的合作成果. 
- [paper](https://arxiv.org/abs/2303.17580)
- 项目已开源，名叫「贾维斯」,钢铁侠里的AI管家贾维斯（[JARVIS](https://github.com/microsoft/JARVIS)）。
- 和3月份刚发布的Visual ChatGPT的思想非常像:后者HuggingGPT，主要是可调用的模型范围扩展到了更多，包括数量和类型。

##### HuggingGPT 原理

如果说 BabyAGI 更多的是探索了 plan & execution 这个应用 LLM 的模式，那么 HuggingGPT 相对早一些的工作更多地展示了在“外部工具”这个层面的想象空间。

其核心运作逻辑也是**计划加上执行**，只不过在执行工具层面，可以利用丰富的“领域专业模型”来协助 LLM 更好地完成复杂任务
- ![](https://picx.zhimg.com/80/v2-512ef42cf0d983c518d3d47d2638dbbe_1440w.webp?source=1940ef5c)

语言是通用的接口。于是，HuggingGPT就诞生了。工程流程分为四步:
- 首先，任务规划。ChatGPT将用户的需求解析为任务列表，并确定任务之间的执行顺序和资源依赖关系。
- 其次，模型选择。ChatGPT根据HuggingFace上托管的各专家模型的描述，为任务分配合适的模型。
- 接着，任务执行。混合端点（包括本地推理和HuggingFace推理）上被选定的专家模型根据任务顺序和依赖关系执行分配的任务，并将执行信息和结果给到ChatGPT。
- 最后，输出结果。由ChatGPT总结各模型的执行过程日志和推理结果，给出最终的输出。

请求:
> 请生成一个女孩正在看书的图片，她的姿势与example.jpg中的男孩相同。然后请用你的声音描述新图片。

可以看到HuggingGPT是如何将它拆解为6个子任务，并分别选定模型执行得到最终结果的。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/cef09fd55855447c80ebe387c3376566~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681185700&x-signature=gCXw63eEBk%2FqSX6NUbCm2SAJLQo%3D)

用gpt-3.5-turbo和text-davinci-003这俩可以通过OpenAI API公开访问的变体，进行了实测。如下图所示:
- 在任务之间存在资源依赖关系的情况下，HuggingGPT可以根据用户的抽象请求正确解析出具体任务，完成图片转换。

在音频和视频任务中，它也展现了组织模型之间合作的能力，通过分别并行和串行执行两个模型的方式，完了一段“宇航员在太空行走”的视频和配音作品。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/e6db6415cbd348d19d0ddaa6cd25ec3a~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681185700&x-signature=5kpy%2Bz12gyq1MjUpDM2ewDVw4pU%3D)

还可以集成多个用户的输入资源执行简单的推理，比如在以下三张图片中，数出其中有多少匹斑马。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/809148f403f3486eae6f7e1f9c172116~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681185700&x-signature=x8aBJK6pJiGy309RKsS%2Bm52Kw2w%3D)


#### AutoGPT

【2023-4-12】
- [AutoGPT 太火了，无需人类插手自主完成任务](https://mp.weixin.qq.com/s/bV1tPc7hNn2z06YOpzyanw)
- [拥有自我意识的AI:AutoGPT](https://juejin.cn/post/7236594708301840441)

##### AutoGPT 介绍

[AutoGPT](https://news.agpt.co/) 的研究开始走进大众视野。
-  2023年3月30日，Toran Bruce Richards 发行 AutoGPT，一个实验性开源应用程序，利用 OpenAI 的GPT-4语言模型来创建**完全**自主和可定制的 AI 代理
  - Toran 是一名游戏开发商，并创立了一家名为 Significant Gravitas 的游戏公司
- AutoGPT 相当于给基于 GPT 的模型一个内存和一个身体。
  - 可以把一项任务交给 AI 智能体，自主地提出一个计划，然后执行计划。
  - 此外其还具有互联网访问、长期和短期内存管理、用于文本生成的 GPT-4 实例以及使用 GPT-3.5 进行文件存储和生成摘要等功能。
- AutoGPT 从根本上改变了 AI 与人类之间的交互方式，人类不再需要发挥积极作用，同时仍然保持与 ChatGPT 等其他 AI 应用程序相同或更好的结果质量。

AutoGPT 用处很多，可用来分析市场并提出交易策略、提供客户服务、进行营销等其他需要持续更新的任务。
- [GitHub 地址](https://github.com/torantulino/auto-gpt)

##### AutoGPT 工作原理

AutoGPT 如何工作？

AutoGPT 基于自主 AI 机制工作，其中 AI 系统创建不同的 AI 代理来满足特定任务，其中包括:
- **任务创建**代理: 在 AutoGPT 上输入目标时，第一个与任务创建代理交互的 AI 代理。根据目标创建一个任务列表以及实现这些目标的步骤，并将其发送给优先级代理。
- 任务**优先级**代理: 收到任务列表后，优先级 AI 代理会确保顺序正确且符合逻辑，然后再将其发送给执行代理。
- 任务**执行**代理: 完成优先级排序后，执行代理将一个接一个地完成任务。这涉及利用 GPT-4、互联网和其他资源来获得结果。
- ![架构图](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aba630117cd6408bb1fb8a1265fdf520~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

当执行代理完成所有任务，结果不理想时，它可以与**任务创建代理**通信，创建新的任务列表。三个代理之间的迭代循环，直到完成所有用户定义的目标。

AI 代理的行为也显示在用户界面上，将它们分为四组:思想、推理、计划、评判。
- `思想`（THOUGHTS） :AI 代理分享它对目标的想法。
- `推理`（REASONING） :AI 代理推理如何开展并实现它的想法。
- `计划`（PLAN） :AI代理通过分析，列举了所要完成任务的计划。
- `评判`（CRITICISM） :AI进行自我评判，纠正错误并克服任何限制问题。

通过共享此计算流程，AutoGPT 可以进行反复尝试论证，并进行针对性的优化处理，可以在没有任何用户干预的情况下克服所遇到的所有问题。

模块图, 源自 [AGI-MAP](https://github.com/ziwang-com/AGI-MAP)
- ![arch](https://user-images.githubusercontent.com/11691791/236591588-3aaa6a6e-bbf5-42cd-84c3-a21f76e8f2a7.png)

特斯拉前 AI 总监、刚刚回归 OpenAI 的 Andrej Karpathy 也为其大力宣传，并在推特赞扬:「AutoGPT 是 prompt 工程的下一个前沿。」

##### AutoGPT 的局限性

一些关键限制
1. **成本高昂**
  - 虽然功能令人惊叹，但 AutoGPT 的实用性可能会让你失望。由于 `AutoGPT` 使用昂贵的 `GPT-4` 模型，因此即使是小任务，完成每个任务的成本也可能很高。这主要是因为 AutoGPT 在特定任务的步骤中会多次使用 `GPT-4`。
2. 经常陷入**循环**
  - 用户在使用 AutoGPT 时面临的最常见问题是它陷入循环。如果这种情况持续超过几分钟，则可能意味着你必须重新启动该过程。发生这种情况是因为 AutoGPT 依赖 GPT-4 来正确定义和分解任务。因此，如果底层LLM返回结果不足以让 AutoGPT 采取任何行动就会出现反复尝试的问题。
3. 数据**安全性**
  - 由于AutoGPT经过充分授权，能自主运行并访问你的系统和互联网，例如使用twitter账号，登录github，使用搜索引擎等，因此你的数据可能会被泄露。
  - AutoGPT没有安全代理，所以使用 AutoGPT 时必须小心，如果没有给出正确的说明和安全指南，你不能让模型继续运行。

##### 实践

实践
- [AUTOGPT INSTALLATION AND FEATURES](https://autogpt.net/autogpt-installation-and-features/)

```sh
# 准备Python 3.8以上的环境, 安装minicoda
# source  ~/.bash_profile
conda create -n py310 python=3.10 # 创建 3.10环境
conda activate py310 # 激活环境
# 下载autogpt代码
git clone https://github.com/Torantulino/Auto-GPT.git
cd 'Auto-GPT'
pip install -r requirements.txt
# 配置文件
mv .env.template .env
vim .env # 填入 openai key 到变量 OPENAI_API_KEY
# python scripts/main.py
# python scripts/main.py --debug # 调试模式
# python scripts/main.py --speak # use TTS for Auto-GPT
python3 scripts/main.py # 多个虚拟环境时，为了避免干扰
python -m autogpt
```

【2023-7-7】
- 复制 默认的 env文件，只更新里面的openai_api_key

```sh
cp .env.template .env
```

命令，详见[指南](https://docs.agpt.co/)

```sh
./run.sh --help     # on Linux / macOS
./run.sh --debug # 打印日志
# Run Auto-GPT with a different AI Settings file shell
./run.sh --ai-settings <filename>
# Run Auto-GPT with a different Prompt Settings file shell
./run.sh --prompt-settings <filename>
# Specify a memory backend
./run.sh --use-memory  <memory-backend>
./run.sh --speak # 启动tts 语音播报
./run.sh --continuous # 100% 自动化，无需手动确认，有一定风险
./run.sh --gpt3only # 只用 gpt3
./run.sh --gpt4only # 只用 gpt4 ，更贵

```

##### 使用

AutoGPT配置
- 为AI取一个名字 \[Name]，一个角色定位\[Role]，同时你可以为它制定目标\[Goals]（最多5个目标，如果你仅有一个目标就直接回车）。
- 制定完成目标以后，AutoGPT会进行自主思考并分析你的目标\[THOUGHTS]，思考完成后开始理解并推理如何去完成这个目标\[REASONING]，然后开始自主拆解成具体的计划\[PLAN]，最后会提出评判\[CRITICISM] 用以保证 AI 代理纠正错误并作出正确的决断。
- 完成以上的行为规划后，AutoGPT会提示它将要作出的指令和动作[NEXT ACTION]， 里面包含具体执行的命令\[COMMAND]和参数\[ARGUMENTS]，用户可以在此时可以对风险命令进行识别，避免出现数据泄露等预期外的风险，这里可以通过y或者n进行授权或者拒绝AutoGPT接下来的指令动作。
- ![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/23d2ddb5b3ab4638af1aa22913f29142~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)
- AutoGPT会通过以上步骤，进行多次循环，由于AutoGPT可以存储上下文和历史经验，所以每一次都会根据反馈结果进行更深入的思考，制定出更优的方案，最后列举他要执行的计划，反复尝试和补充，直到达到你预期的目标。
- AutoGPT会通过以上步骤，进行多次循环，由于AutoGPT可以存储上下文和历史经验，所以每一次都会根据反馈结果进行更深入的思考，制定出更优的方案，最后列举他要执行的计划，反复尝试和补充，直到达到你预期的目标。


效果
- ![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ac14513a95c49b5a7bb87f2050ee170~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

##### COA

【2023-5-18】从 COT 到 COA
- `COT`(Chain of Thought，思维链) 
- `COA`(Chain of Action，行为链):AutoGPT为代表，将自然语言表达的目标分解 为子任务，并利用互联网和其他工具自动迭代地尝试实现这些目标。

特点
- 自主化决策，任务链自动化
- 知行一体，参数外挂，泛化学习，
- 动态适应和灵活反应
- AI从模拟人类思维到模拟人类行为， 人主要负责设定目标、审批预算、 调整关键行动链

优点
- •自主任务分解  
- •上下文适应性
- •泛化多功能优化 
- •智能响应 
- •协同学习 
- •动态知识整合

缺点
- 语义鸿沟
- 依赖风险 
- 计算成本过高
- 透明度缺失

##### CoVe

【2023-9-20】META AI提出CoVe
- [Chain-of-Verification Reduces Hallucination in Large Language Models](https://arxiv.org/abs/2309.11495)

一种CoVe(链式验证)的方式来减少大模型的幻觉

该论文研究了大语言模型在解决‘幻觉’问题上的能力，提出了一种链式验证（CoVe）方法，通过该方法模型首先起草初始回答，然后计划验证问题来核实起草结果，独立回答这些问题以避免受到其他回答的影响，最终生成验证后的回答。实验证明CoVe方法降低了在各种任务中出现的幻觉，包括基于Wikidata的列表问题，闭书型MultiSpanQA和长文本生成等。

按照这种Prompt模板
1. Generate baseline response
2. Generate a verification plan (set of questions)
3. Execute plan (answer questions)
4. Generate Final Verified Response (using answers)


#### AgentGPT -- AutoGPT 改进

`AgentGPT`:浏览器中直接部署自主 AI 智能体
- [项目主页](https://agentgpt.reworkd.ai)
- [GitHub 地址](https://github.com/reworkd/AgentGPT)

近日，又有开发者对 AutoGPT 展开了新的探索尝试，创建了一个可以在浏览器中组装、配置和部署自主 AI 智能体的项目 ——`AgentGPT`。项目主要贡献者之一为亚马逊软件工程师 Asim Shrestha

`AgentGPT` 允许自定义 AI 命名，执行任何想要达成的目标。自定义 AI 会思考要完成的任务、执行任务并从结果中学习，试图达成目标。
- 如下为 demo 示例:HustleGPT，设置目标为创立一个只有 100 美元资金的初创公司。

用户在使用该工具时，同样需要输入自己的 OpenAI API 密钥。AgentGPT 目前处于 beta 阶段，并正致力于长期记忆、网页浏览、网站与用户之间的交互。

```sh
git clone https://github.com/reworkd/AgentGPT.git
cd AgentGPT
./setup.sh
```

【2023-4-15】[免费的AutoGPT替代网站](https://zhuanlan.zhihu.com/p/622083666)
- 第一个是最火的AutoGPT，性能最强的，但是安装起来也挺麻烦的，并且还需要各种API的权限，小白不建议。
- 第二个AgentGPT，需要OpenAI的API，操作简单，在网页输入key就可以用。
- 第三个和第四个暂时是免费的，想体验的可以赶紧了。

| 名称 | 方案 | 特点 | 链接 |
| --- | --- | --- | --- |
| [AutoGPT](https://github.com/Torantulino/Auto-GPT) | 最复杂 | 需要安装开源代码 |  |
| [AgentGPT](https://agentgpt.reworkd.ai/) | 需要Token | 通过简单的网页访问易于使用，具有相对简单的功能 | |
| [Cognosys](http://cognosys.ai/) | 不需要Token | 性能不错，具有明确的任务组织 | |
| [Godmode](https://godmode.space/) | 不需要Token | 操作更加直观，每个步骤需要用户权限 | |

#### Adept

Adept 和 Inflection 这两家早期团队想以自然语言为基础，为用户打造新的 **LUI** （语言为基础的 UI）方式。

#### Inflection

待定

#### GPT4Free

【2023-5-4】[GPT4Free](https://github.com/xtekky/gpt4free) ([discord](https://discord.com/invite/gpt4free)地址) 通过You.com、Quora和CoCalc等网站（OpenAI付费用户）提供的各种API地址，免费使用GPT-4和GPT-3.5模型。
- GPT4Free 脚本会先访问 https://you.com/api/streamingSearch，并传送各种参数过去，然后获取返回的JSON并对其进行格式化。
- GPT4Free仓库还有从Quora、Forefront和TheB等其他网站获取数据的脚本，任何开发者都可以基于这些脚本制作自己的聊天机器人。

实测:
- 安装
  - 要求 Python 3.8以上
  - 修改 requirements.txt 文件
- [The requirements.txt need to be updated](https://github.com/xtekky/gpt4free/issues/419)

```yml
# pypasser # 原始
pypasser>=0.0.5 # 指定版本，否则 pip install -r requirements.txt 提示冲突
```

UI部署正常，但点击“Think”后，出现新的[错误](https://github.com/xtekky/gpt4free/issues/406):
- Please make sure you are using a valid cloudflare clearance token and user agent.


安装

```sh
pip install gpt4free
```

程序调用

```py
import gpt4free
from gpt4free import Provider, quora, forefront

# usage You
response = gpt4free.Completion.create(Provider.You, prompt='Write a poem on Lionel Messi')
print(response)
# usage Poe
token = quora.Account.create(logging=False)
response = gpt4free.Completion.create(Provider.Poe, prompt='Write a poem on Lionel Messi', token=token, model='ChatGPT')
print(response)
# usage forefront
token = forefront.Account.create(logging=False)
response = gpt4free.Completion.create(
    Provider.ForeFront, prompt='Write a poem on Lionel Messi', model='gpt-4', token=token
)
print(response)
print(f'END')
# usage theb
response = gpt4free.Completion.create(Provider.Theb, prompt='Write a poem on Lionel Messi')
print(response)
# usage cocalc
response = gpt4free.Completion.create(Provider.CoCalc, prompt='Write a poem on Lionel Messi', cookie_input='')
print(response)
```

错误信息
> tls_client.exceptions.TLSClientExeption: failed to do request: Get "https://you.com/api/streamingSearch?q=Write+a+poem+on+Lionel+Messi&page=1&count=10&safeSearch=Moderate&onShoppingPage=False&mkt=&responseFilter=WebPages%2CTranslations%2CTimeZone%2CComputation%2CRelatedSearches&domain=youchat&queryTraceId=77ebaf4c-ba0c-4035-bad6-1dafc27fdc14&chat=%5B%5D": dial tcp 192.133.77.59:443: i/o timeout (Client.Timeout exceeded while awaiting headers)


#### OlaGPT

【2023-8-9】[首个模拟人类认知的思维框架OlaGPT:推理能力最高提升85%](https://www.toutiao.com/article/7241109327501689381)

模型在对话上的表现实在是太像人类了，以至于产生了语言模型具有「思维能力」的错觉。基于高概率语言模式的再现与期望中的「通用人工智能」还有很大差距。
1. 某些情况下**生成内容毫无意义**，或者偏离了人类的价值偏好，甚至会给出一些非常危险的建议，目前的解决方案是引入人类反馈的强化学习（RLHF）对模型输出进行排序。
2. 语言模型的**知识仅限于**在训练数据中明确提到的概念和事实。

面对复杂问题时，语言模型也无法像人类一样适应变化的环境、利用现有的知识或工具、反思历史教训、分解问题，以及使用人类在长期进化中总结出的思维模式（如类比、归纳推理和演绎推理等）来解决问题。

当前大多数研究中，大模型主要是在**特定提示**的引导下生成**思维链**来执行推理任务，没有考虑**人类认知框架**，使得语言模型解决复杂推理问题的能力与人类之间仍然存在着显着的差距。

让语言模型模拟人脑处理问题的过程还有许多系统难题:
1. 如何系统地**模仿和编码**人类认知框架中的主要模块，同时以可实现的方式根据人类的通用推理模式进行调度？
2. 如何引导语言模型像人类一样进行**主动学习**，即从历史错误或专家对困难问题的解决方案中学习和发展？虽然重新训练模型对纠正后的答案进行编码可能是可行的，但显然成本很高而且不灵活。
3. 如何让语言模型灵活地利用人类进化出的各种思维模式，从而提高其推理性能？

一个**固定的、通用的**思维模式很难适应不同问题，就像人类在面对不同类型的问题时，通常会灵活地选择不同的思维方式，如类比推理、演绎推理等。

人类在面对复杂的推理难题时，通常会使用各种认知能力，并且需要与工具、知识和外部环境信息的各个方面进行交互，那语言模型能不能模拟人类的思维流程来解决复杂问题呢？
- 论文:[OlaGPT](https://arxiv.org/abs/2305.16334)
- 代码:[OlaGPT](https://github.com/oladata-team/OlaGPT)

OlaGPT 包括多个认知模块，包括注意力、记忆、推理、学习，以及相应的调度和决策机制；受人类主动学习启发，框架中还包括一个**学习单元**来记录之前的错误和专家意见，并动态参考来提升解决类似问题的能力。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/6ab3a339e2854a0b85402d6e3d8c9790~noop.image)

OlaGPT借鉴了认知架构（cognitive architecture）理论，把认知框架的核心能力建模为`注意力`（attention）、`记忆`（memory）、`学习`（learning）、`推理`（reasoning）、`行动选择`（action selction）。

提出一个适合语言模型解决复杂问题的流程，具体包括六个模块:意图增强模块（注意力）、记忆模块（记忆）、主动学习模块（学习）、推理模块（推理）、控制器模块（行动选择）和**投票**模块。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/5738c24560624a158d92bf58ccb36fc2~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=cw3fQ0rc6LC1ii9%2Bif6aTp%2FIKH4%3D)

人类解决问题的常见有效推理框架，并设计了思维链（CoT）模板；还提出了一个全面的决策机制，可以最大限度地提高模型的准确性。

**意图增强（Intention Enhance）**

注意力是人类认知的一个重要组成部分，识别出相关的信息并过滤掉不相关的数据。

同样地，研究人员为语言模型设计了相应的注意力模块，即意图增强，旨在提取最相关的信息，并在用户输入和模型的语言模式之间建立更强的关联，可以被看作是一个从用户表达习惯到模型表达习惯的优化转换器。

首先通过特定的提示词提前获得LLMs的问题类型，然后重构提问的方式。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/a658e25c9adf4339ab99921399a28946~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=t0lMLBJ%2FWobByPHC6LOKnzWvWos%3D)

比如在问题的开头加上一句「Now give you the XX（问题类型），question and choices:」；为了便于分析，提示中还需要加入「The answer must end with JSON format: Answer: one of options\[A,B,C,D,E\].」
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/3b4f507cfb4f4a7090385d6b7b8bb9a4~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=FPipJqSH%2BL0HIv5XWcLNxLBxt3k%3D)

**记忆（Memory）**

记忆模块在存储各种知识库信息方面起着至关重要的作用，已经有研究证明了当下语言模型在理解最新事实数据方面的局限性，而记忆模块着重于巩固模型尚未内化的知识，并将其作为长期记忆储存在外部库中。

研究人员使用langchain提供的记忆功能进行短期记忆，长期记忆则由基于Faiss的矢量数据库实现。

在查询过程中，其检索功能可以从库中提取相关知识，涵盖了四种类型的记忆库:事实、工具、笔记和思维（thinking），其中事实是现实世界的信息，如常识等；工具包括搜索引擎、计算器和维基百科，可以协助语言模型完成一些无需为条的工作；笔记主要记录一些疑难案例和解决问题的步骤；思考库主要存储由专家编写的人类解决问题的思考模板，专家可以是人类，也可以是模型。

**学习（Learning）**

学习的能力对于人类不断提升自我表现来说至关重要，从本质上讲，所有形式的学习都依赖于经验，语言模型可以从之前的错误中学习，从而实现快速提高推理能力。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/a3f6f32f8f1948cb832072c7e011bb11~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=9TRyX31yvOs2CVuIaPqwCkCFvnQ%3D)

首先，研究人员找出语言模型无法解决的问题；然后在笔记库中记录专家提供的见解和解释；最后选择相关的笔记来促进语言模型的学习，从而可以更有效地处理类似问题。

**推理（Reasoning）**

推理模块的目的是创建基于人类推理过程的多个智能体，从而激发语言模型的潜在思维能力，进而解决推理问题。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/4a36eb80ce674580a272882f74f066ac~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=%2BlQkkOMvaD3ue%2FUy6XcrfQGMgdw%3D)

该模块结合了多种思维模板，参考特定的思维类型，如横向思维、顺序思维、批判性思维和整合性思维，以促进推理任务。

**控制器（Controller）**

控制器模块主要用来处理相关的行动选择，具体包括模型的内部规划任务（如选择某些模块来执行）以及从事实、工具、笔记和思维库中选择。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/93e2d936c4e84a42a4e530ba72b73825~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692155129&x-signature=l6%2BacGSVoICV0Nc%2FnibV4pLQCig%3D)

首先检索和匹配相关的库，检索到的内容随后被整合到一个模板智能体中，要求语言模型以异步的方式在一个模板下提供回复，就像人类在推理之初可能难以识别所有的相关信息一样，同样很难期望语言模型一开始就做到这一点。

因此，动态检索是根据用户的问题和中间的推理进度来实现的，使用Faiss方法为上述四个库创建嵌入索引，其中各个库的检索策略略有不同。

**投票（voting）**

由于不同的思维模板可能更适合不同类型的问题，研究人员设计了投票模块来提升多个思维模板之间的集成校准能力，并多种投票策略来生成最佳答案以提高性能。

具体的投票方法包括:
1. 语言模型投票:引导语言模型在多个给定的选项中选择最一致的答案，并提供一个理由。
2. regex投票:用正则表达式精确匹配抽取答案以获取投票结果。


在多个推理数据集上进行了严格评估后得到的实验结果表明，OlaGPT超越了此前最先进的基准，证明了其有效性。
1. SC（self-consistency）的性能优于GPT-3.5-turbo，表明在一定程度上采用集成方法确实有助于提高大规模模型的有效性。
2. 文中提出方法的性能超过了SC，在一定程度上证明了思维模板策略的有效性。
  - 不同思维模板的答案表现出相当大的差异，在不同的思维模板下进行投票，最终会比简单地进行多轮投票产生更好的结果。
3. 不同思维模板的效果是不同的，循序渐进的解决方案可能更适合推理型问题。
4. 主动学习模块的性能明显优于零样本方法。
  - 具体来说，随机、检索和组合列表现出更高的性能，即将具有挑战性的案例作为笔记库纳入其中是一种可行的策略。
5. 不同的检索方案在不同的数据集上有不同的效果，总的来说，组合（combine）策略的效果更好。
6. 文中方法明显优于其他方案，这得益于整体框架的合理设计，包括主动学习模块的有效设计；思维模板实现了对不同模型的适应，不同思维模板下的结果是不同的；控制器模块起到了很好的控制作用，选择了与所需内容比较匹配的内容；投票模块设计的不同思维模板的集成方式是有效的。


#### SuperAGI

[SuperAGI](https://superagi.com/)

[SuperAGI](https://github.com/TransformerOptimus/SuperAGI)
- ![](https://camo.githubusercontent.com/91e90c90adbeb243ae9e8c43f310d626db758037d323c18726b66297dbbcf6ea/68747470733a2f2f73757065726167692e636f6d2f77702d636f6e74656e742f75706c6f6164732f323032332f30352f4c696768742d64617368626f6172642e706e67)

SuperAGI - A dev-first open source autonomous AI agent framework. Enabling developers to build, manage & run useful autonomous agents quickly and reliably.


#### XAgent -- 面壁智能

【2023-10-17】[全面超越AutoGPT，面壁智能联合清华 NLP实验室打造大模型“超级英雄”—— XAgent](https://mp.weixin.qq.com/s/uFt-0F4d-4_xY1zkIOJ83Q)

国内领先的人工智能大模型公司 面壁智能 又放大招，联合 清华大学 NLP 实验室 共同研发并推出 大模型 “超级英雄”——XAgent。
通过任务测试，XAgent在真实复杂任务的处理能力已 全面超越 AutoGPT。

GitHub 正式开源:
- ➤  [地址](https://github.com/OpenBMB/XAgent)
- ➤  [案例展示地址](https://x-agent.net)
- ➤  [博客地址](https://blog.x-agent.net)

XAgent 是一个可以实现自主解决复杂任务的全新 AI 智能体，以 LLM 为核心，能够理解人类指令、制定复杂计划并自主采取行动。
- 传统智能体通常受到**人类定制规则**的限制，只能在限定范围内解决问题。更像是为人类所用的“工具”，而不是真正的“自主智能体”，难以自主解决复杂问题。
- XAgent 被赋予了 **自主规划和决策** 的能力，使它能够独立运行，发现新的策略和解决方案，不受人类预设的束缚。

(1) “左右脑”协同，双循环机制 
- 正如人类具备“左脑”和“右脑”，在处理复杂任务时通常从“宏观”和“微观”两个视角进行考虑，既要针对全局进行统筹和规划，也要从执行层面来考量。

面壁智能和清华大学在 XAgent 的设计中创新地引入了一种 “双循环机制”:
- 外循环:负责全局任务规划，将复杂任务分解为可操作的简单任务。
- 内循环:负责局部任务执行，专注于细节。

(2) 人机协作:智能体交互新范式

- 虽然 AutoGPT 在一定程度上突破了传统 GPT 模型的局限性，但它仍然存在死循环、错误调用等执行出错的现象，需要人工干预才能解决。
- 而 XAgent 在设计之初就针对相关问题进行了考量，并引入了专为增强人机协作的交互机制:它可以自主与用户进行交互，并向人类发出干预和指导的请求。

对于一个智能体而言，“是否能够与人类协作” 也是体现其智能程度的重要指标。
- 首先，XAgent 具备 直观的界面，用户可以直接覆盖或修改它提出的建议，从而将AI效率与人类的直觉和专业知识有效结合。
- 其次，在面临陌生挑战的情况下，XAgent 具备“向人类寻求帮助”能力，它会向用户征求实时反馈、建议或指导，确保即使在不确定的领域，智能体也能发挥出最佳作用。

(3) 高效通信语言，超强工具调用

无论“双循环”的运转机制，还是“人机协作”的交互能力，在 XAgent 的总体设计中，面壁智能和清华团队着重考虑的是智能体的稳定、高效和安全等核心特性。
而 结构化的通信方式 同样是建立强大、稳定智能体的重要因素之一。
XAgent 采用 Function Call 作为其内部的通信语言，具备结构化、标准化、统一化等优势。
- 结构化:Function Call 具备清晰且严谨的格式，可以明确表述所需内容，从而最小化了潜在的错误。
- 标准化:Function Call 可以将与外部工具的交互过程标准化，提供一种通用语言，使智能体具备使用和整合多种工具的能力，解决复杂任务。
- 统一化:通过将信息摘要、任务规划、工具执行等所有环节转化为特定的 Function Call 形式，确保每个环节均以统一的方式进行处理，从而简化系统设计。

此外，工具调用也是评价 AI Agent 是否具备解决复杂问题的重要能力之一。
XAgent 在设计中原创了工具执行引擎 ToolServer，可以实现更安全、高效、可扩展的工具执行能力。


经过在一系列任务中的测试可以看到（如下图a、b所示），基于 GPT-4 的 XAgent 表现效果在所有基准测试中都超过了原始的 GPT-4，并全面超越了 AutoGPT。

这些任务需要 Agent 推理规划和使用外部工具的能力，包括:用搜索引擎回答问题的能力（FreshQA+HotpotQA）、Python 编程能力（MBPP）、数学推理能力（MATH）、交互式编程能力（InterCode）、具身推理能力（ALFWorld）、真实复杂任务等。

### 多智能体实现

群体智能

单一AI代理已经无法满足日益增长的需求。而多Agent让多个AI代理能够协同工作，共同完成复杂的任务

这就是为什么 Autogen 的出现如此重要。

AutoGen总结对比

|维度|`AutoGen`|`Multi-Agent Debate`|`CAMEL`|`BabyAGI`|`MetaGPT`|
|---|---|---|---|---|---|
|架构通用性|✅|❌|✅|❌|❌|
|对话模式|静态/动态|静态|静态|静态|静态|
|执行LLM代码|✅|❌|❌|❌|✅|
|人工介入|✅|❌|❌|❌|❌|


#### 资料

2024年，EMNLP 收录港科大论文，研究社会游戏（如Avalon）里的Agent行为
- 论文 [LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay](https://aclanthology.org/2024.emnlp-main.7.pdf)
- 代码 [LLM-Game-Agent](https://github.com/3DAgentWorld/LLM-Game-Agent)


#### 多智能体问题

第一篇 MAS 失败原因分析综述
- 【2025-3-17】伯克利 [Why Do Multi-Agent LLM Systems Fail?](https://export.arxiv.org/pdf/2503.13657)
- [multi-agent-systems-failuretaxonomy](https://github.com/multi-agent-systems-failuretaxonomy/MASFT)
- 解读 [三个Agent没水喝](https://mp.weixin.qq.com/s/qbilxmgrEqZfzUoMIz4f2Q)


鉴于llm处理复杂、多步骤任务以及与不同环境实时互动的能力，由大语言模型（LLM）驱动的 agent 系统，尤其是多 agent 系统（MAS），被认为非常适合用来解决现实世界中的问题，越来越多地应用在各个领域中，如软件工程、药物发现、科学模拟，以及通用 agent 系统。

Multi-Agent Systems (MAS) 多智能体系统通过多个LLM驱动的智能体相互协作完成任务，MAS 广受关注，热度越来越高，但与单智能体相比，在流行的评测集上取得的成绩相对有限。
- 多 agent 系统却在处理实际问题时，相对于单Agent更易出错。AppWorld 故障率可高达 86.7%。

有必要深入分析下 MAS 的失败原因

- 选用5个热门MAS系统，150多个任务上评测
- 6个人工专家来标注评测结果
- ![](https://arxiv.org/html/2503.13657/x1.png)


|MAS|Agentic Architecture|Purpose of the System|
| ---- | ---- | ---- |
|MetaGPT (Hong et al., 2023)|Assembly Line|Simulating the SOPs of different roles in Software Companies to create open - ended software applications|
|ChatDev (Qian et al., 2023)|Hierarchical Workflow|Simulating different Software Engineering phases like (design, code, QA) through simulated roles in a software engineering company|
|HyperAgent (Phan et al., 2024)|Hierarchical Workflow|Simulating a software engineering team with a central Planner agent coordinating with specialized child agents (Navigator, Editor, and Executor)|
|AppWorld (Trivedi et al., 2024)|Star Topology|Tool - calling agents specialized to utility services (ex: Gmail, Spotify, etc.) being orchestrated by a supervisor to achieve cross - service tasks|
|AG2 (Wu et al., 2024a)|N/A - Agentic Framework|An open - source programming framework for building agents and managing their interactions| 


|MAS名称（测试环境）|成功率|失败率|
| ---- | ---- | ---- |
|MetaGPT (ProgramDev)|66.0%|34.0%|
|ChatDev (ProgramDev)|25.0%|75.0%|
|HyperAgent (SWE-Bench Lite)|25.3%|74.7%|
|AppWorld (Test-C)|13.3%|86.7%|
|AG2 (GSM-Plus)|84.8%|15.2%| 


提炼出MAS的14种失败模式
- 流程规划、任务划分: 
- 智能体协作: 
  - 智能体在讨论没意义的内容，效率低
  - 忽略关键信息
- 任务验证: 不少系统里验证机制失效
  - manus 能协作漂亮的文档，但内容很容易出错 -- 使用低质量内容，错误进一步放大

14 种故障模式，划分为 3 大类：
- （1）规范和系统设计故障；
- （2）Agent 间错位；
- （3）任务验证和终止。



反思
- 智能体不该简单模仿人类工作方式
- MAS 使用场景: 群体智慧场景，如 Google 的 Co-Scientist, Agent之间相互辩论、启发、对抗，更有价值

论文贡献
- 提出首个基于经验的多 agent 系统**故障分类法**——MASFT，理解和缓解多 agent 系统故障提供了一个结构化框架。
  - MASFT 将多 agent 系统的执行划分为 3 个阶段：**执行前**、**执行中**和**执行后**，确定每个细粒度故障模式可能发生的多 agent 系统执行阶段。
- 同时，开发可扩展的 “LLM-as-a-judge” **评估管道**，用于分析新的多 agent 系统性能和诊断故障模式。
- 针对 agent 规范、对话管理和验证策略，进行干预研究，尽管将任务完成率提高了 14%，但仍未能完全解决多 agent 系统故障问题，这凸显了结构性多 agent 系统重新设计的必要性。

开源：
- 150 多个标注的多 agent 系统会话轨迹；
- 可扩展的 LLM-as-a-judge 评估管道和 150 多个轨迹的 LLM 标注；
- 15 个选定轨迹的详细专家标注。


#### 发展趋势


论文: 多智能体协作机制综述
- 【2025-1-10】爱尔兰科克大学 [Multi-Agent Collaboration Mechanisms: A Survey of LLMs](https://arxiv.org/pdf/2501.06322)
- [解说](https://mp.weixin.qq.com/s/_igFqVXtlrdQzNfhZcrorA)

协作机制：**参与者**（涉及Agent）、**类型**（例如，合作、竞争或合作竞争）、**结构**（例如，点对点、集中式或分布式）、**策略**（例如，基于角色或基于模型）和**协调协议**。

未来 5 年内多Agent平台的两大发展趋势：
- （1）多Agent 平台的技术架构演进
- （2）模型能力的提升

多 Agent 架构正从**简单设置**演变为更加**分布式、分层和混合**框架，以协调大量Agent。

近期研究将这些架构分类为 `扁平式`（点对点）、`分层式`（树状监督）、`团队式`、`社会式` 或 `混合式`

每种架构在可扩展性、灵活性和效率上各有权衡。未来的平台可能会结合这些模式，使Agent能够根据不同任务进行自组织。新框架展示了如何将Agent组织成动态网络或专业化小组，以提升集体问题求解能力。随着系统规模扩大，这种分布式方法预计将增强系统的鲁棒性和性能。

多Agent协作的核心是高效通信。当前研究正推动Agent间通信协议向更**自适应、稳健且高效**利用带宽的方向发展，特别是在Agent数量增长的情况下。

关于基于 LLM 的**多智能体系统**（MAS）的研究强调，系统级设计（即Agent如何通信以及它们共享哪些目标）和内部通信策略必须进行优化，以实现真正的集体智能。

结构化消息传递、共享“黑板”记忆，甚至隐式信号都可以被利用，以减少误解并提高协作效率。

**多智能体强化学习**（MARL）将继续成为训练Agent群体，使其通过经验不断改进的关键技术。

MARL 在可扩展性和鲁棒性方面的优势，使Agent能够在**共享环境**中**共同学习协调**策略。

未来，研究人员正致力于使 MARL Agent 在部署后变得更自适应和自我优化。

一个值得关注的趋势是将 基于 LLM 推理 整合到强化学习（RL）循环中 —— Agent 可在训练过程中交换信息或学习通信协议，以提升协作能力。
- 例如，语言条件 MARL 正在被探索，使Agent能够发展出一种共享的“语言”来协调策略，结合深度强化学习（Deep RL）与类人通信的优势。

未来五年内，预计会出现以下重要进展：
- 多智能体**自博弈**（Self-Play）技术：Agent 通过相互对抗或协作来提升自身能力；
- **元学习**（Meta-Learning）：Agent 学会优化自己的学习算法，提高学习效率；
- **终身学习**（Lifelong Learning）：Agent 团队能够在多Agent环境中动态适应新任务。

这些自我优化能力将由将 MAS 视为一个不断进化的生态系统的新型架构所支持，可能引领我们迈向人工集体智能（Artificial Collective Intelligence），即Agent群体整体的学习能力超越个体能力的总和。

增强推理、自主性与适应性

多Agent平台将更深层次的推理能力和更高的自主性。

大型语言模型（LLMs）已被用于自主Agent的“大脑”，支持其执行复杂的规划和决策任务。

将 LLM 与**长期记忆**和**规划**模块结合，可打造出能够规划、记忆并进行类人适应的Agent。
- 例如，近期的生成式Agent实验表明，多个基于 LLM 的Agent在沙盒环境中可以自主模拟可信的**社交行为**（如仅凭初始提示就共同组织一场聚会），展现出涌现的协作能力与适应性。
- 在未来几年将看到Agent在理解上下文、推理多步问题和动态调整行为方面的能力进一步提升。

当前，LLM 驱动的Agent已经能够推理目标并做出情境决策，而这些能力将通过更优的认知架构进一步优化。

以下技术将得到更广泛应用：
- 链式思维提示：增强推理能力，使Agent能够进行更复杂的逻辑推导；
- 逻辑推理增强：提升Agent在推理问题上的准确性；
- 外部工具调用：让Agent可以调用计算器、代码解释器等外部工具，以提升执行能力。

这些技术的结合将进一步提升多Agent系统的自主性，减少对人类干预的依赖，使Agent能够主动判断任务需求并协作完成任务。多Agent系统中 LLM 的演进大型语言模型（LLMs）自身也在不断发展，以更好地支持多Agent环境。

当前趋势表明，与其使用单一的大型通用模型，更倾向于部署多个专门化的 LLM Agent进行交互。基于 LLM 的Agent协作使其能够解决单个模型难以独立完成的任务，利用多样化的专业知识和集体问题求解能力。

当前 LLM 在多Agent环境中的局限性，将Agent视为一种**数字物种**，可能需要新的训练方法。

一个关键挑战是，通用 LLM 设计初衷并非针对多Agent交互，因此可能导致：
- 产生错误信息或不一致性（**幻觉**），这些错误会在Agent间**级联放大**；
- 缺乏共识构建机制，导致决策不稳定或Agent之间难以达成一致。

未来的发展方向可能包括专为多Agent协作优化的 **LLM 变体**或**微调**模型，提升其在与其他 AI 交互时的可靠性。
- 例如，正在探索能考虑其他Agent观点、维护一致的共享世界状态的 LLM。通过调整模型架构和训练方法，AI 研究社区正努力打造更加协作透明、团队意识更强、适应性更高的 AI “团队成员”。

与“通用型”Agent不同，当前的发展方向是 **Agent专精化**
- 每个Agent通过微调或提示配置，被设计为擅长完成某一特定职能，并在一个协调框架中协同工作。

Agent角色的自动分配方法
- 2024 年的一项研究提出了一种“自动Agent生成”框架，可以在一个大任务中自动生成处理子任务的专用Agent。




#### Camel

[Camel](https://www.camel-ai.org/) 通过 LLM 来模拟用户和 AI 助手，让两个 agent 进行角色扮演（例如一个是业务专家，一个是程序员），然后让他们自主沟通协作来完成一项具体的任务。

这个想法比较直接，不过作者也提到 prompt 的设计还是蛮重要的，否则很容易出现角色转换，重复指令，消息无限循环，有瑕疵的回复，何时终止对话等等问题。具体看项目代码中给出的 prompt 设定，添加了非常多的明确指令来让 agent 按照预想的设定来沟通协作。
- ![](https://pic1.zhimg.com/80/v2-0f871ff3f5b1af49a74d56caa39a785b_1440w.webp?source=1940ef5c)

除了 agent prompt 和运作模式的设计优化外，作者还设计了 prompt 来**自动**生成各种角色，场景诉求等内容。这些内容在自动组成各种角色扮演的场景，就能收集到各个场景下 agent 的交互情况，便于后续做进一步的挖掘分析。[这个网站](https://data.camel-ai.org/) 来探索已经生成的各种 agent 组合之间的对话记录。这个项目代码也做了开源，会是一个非常好的研究 AI agent 社群研究方向的起点。
- ![](https://pic1.zhimg.com/80/v2-3dbb028cb33164d20aaff5ba6b0c1a65_1440w.webp?source=1940ef5c)


#### Generative Agents

[如何看本周最火的AutoGPT？](https://www.zhihu.com/question/595382995/answer/2989954125)

沿着这个方向进一步推演，是否可以将多个 agent 组成一个团队，分别扮演不同的角色，是否能更好地解决一些复杂问题，甚至让这个小的“社群”演化出一些更复杂的行为模式甚至新知识的发现？

[Generative Agents](https://arxiv.org/abs/2304.03442) 中，作者将 25 个拥有身份设定的模型 agent 组成了一个虚拟小镇社群，每个 agent 都具有记忆系统，并通过做计划，行动应答，自我反思等机制来让他们自由活动，真正来模拟一个社群的运作。从模拟过程来看这个社群也“涌现”了不少真实社会中的现象，非常有意思。

几个 agent 行为的设定值得学习:
- 每个 agent 的**记忆获取**做得更加细致，会结合**时效性，重要度和相关度**来做相关记忆的召回。相比简单的**向量相似度搜索**来说效果会好很多。
- 记忆**存储**方面也添加了 reflection 步骤，定期对记忆进行**反思总结**，保持 agent 的“目标感”。
- 在 plan 生成方面也做了**多层级递归**，由粗到细生成接下来的行动计划，跟我们的日常思考模式也更接近。
- 通过“**人物采访**”的方式来评估这些行为设定的效果，消融实验中都能发现明显的提升。
- ![](https://picx.zhimg.com/80/v2-4d785241de1d097d8c5ba10b2666fba2_1440w.webp?source=1940ef5c)

一整套 identity，plan， act/react，reflect，memory stream 逻辑挺合理的，与 AutoGPT 的做法可以进行一些互补。当然局限性应该也有不少，比如
- 模拟过程中 agent 之间都是一对一的谈话，而没有会议/广播这种设定。
- 目前模拟运行的时长也有限，比较难确保长时间的运行下 agent 的记忆、行为模式的演化，社群整体目标的探索与推进等方面的效果。

从应用角度来看，目前好像也主要集中在社会活动模拟，游戏应用等。是否能拓展到任务处理，知识探索等更广阔的领域，还有待进一步探索。


#### 斯坦福小镇

【2023-8-10】斯坦福25个AI智能体「小镇」终于开源，《西部世界》真来了！斯坦福爆火「小镇」开源，25个AI智能体恋爱交友
- 项目地址:[generative_agents](https://github.com/joonspk-research/generative_agents)

Smallville 沙盒世界小镇中，区域会被标记。根节点描述整个世界，子节点描述区域（房屋、咖啡馆、商店），叶节点描述对象（桌子、书架）。
智能体会记住一个子图，这个子图反映了他们所看到的世界的各个部分。

25个AI智能体不仅能在这里上班、闲聊、social、交友，甚至还能谈恋爱，而且每个Agent都有自己的个性和背景故事。

智能体John Lin的种子记忆就是这样的——
- John Lin是一名药店店主，十分乐于助人，一直在寻找使客户更容易获得药物的方法。
- John Lin的妻子Mei Lin是大学教授，儿子Eddy Lin正在学习音乐理论，他们住在一起，John Lin非常爱他的家人。
- John Lin认识隔壁的老夫妇Sam Moore和Jennifer Moore几年了，John Lin觉得Sam Moore是一个善良的人。
- John Lin和他的邻居山本百合子很熟。John Lin知道他的邻居TamaraTaylor和Carmen Ortiz，但从未见过他们。
- John Lin和Tom Moreno是药店同事，也是朋友，喜欢一起讨论地方政治等等。

John Lin度过的一天早晨:6点醒来，开始刷牙、洗澡、吃早餐，在出门工作前，他会见一见自己的妻子Mei和儿子Eddy。

这些智能体相互之间会发生社会行为。当他们注意到彼此时，可能会进行对话。随着时间推移，这些智能体会形成新的关系，并且会记住自己与其他智能体的互动。

一个有趣的故事是，在模拟开始时，一个智能体的初始化设定是自己需要组织一个情人节派对。
随后发生的一系列事情，都可能存在失败点，智能体可能不会继续坚持这个意图，或者会忘记告诉他人，甚至可能忘了出现。

英伟达高级科学家Jim Fan评论道——
> 斯坦福智能体小镇是2023年最激动人心的AI Agent实验之一。我们常常讨论单个大语言模型的新兴能力，但是现在有了多个AI智能体，情况会更复杂、更引人入胜。
> 一群AI，可以演绎出整个文明的演化进程。



### 智能体框架

详见站内专题 [Agent 开发框架](agent_arch)


## Agent LLM

训练专门适配Agent的LLM


### 工具调用

如何提升工具调用能力

#### ToolBench 


[面壁智能](https://modelbest.cn/) OpenBMB 的 [ToolBench](https://github.com/OpenBMB/ToolBench)
- [ToolEval Leaderboard](https://openbmb.github.io/ToolBench/)

ToolBench 包含**单工具**和**多工具**场景。
- 多工具场景进一步分为**类别内**多工具和**集合内**多工具。
- 在数据创建过程中使用DFSDT方法

从 RapidAPI 爬取超过16000个API，并且为之构造了真实的人类指令。

ToolBench 上经过微调的强大模型 ToolLLaMA

数据集构建方法、模型训练、评测的整体概览
- ![](https://github.com/OpenBMB/ToolBench/raw/master/assets/overview.png)


#### α-UMi

【2024-1-30】 `α-UMi` 是一个用于工具学习的多LLM协作智能体。
- 将单个LLM能力分解为三个组件，即`规划器`（planner）、`调用器`（caller）和`总结器`（summarizer）。
- 在智能体执行的每一步中，`规划器`根据系统状态为当前步骤生成理由，并选择`调用器`或`总结器`来生成下游输出。`调用器`由理由指导，并负责调用特定的工具进行交互。`总结器`在规划器的指导下，根据执行轨迹制定最终的用户答案


要点
- 使小型LLMs能够协作，并在工具学习中超越性能强大的闭源大型语言模型。
- 比单LLM智能体系统更灵活的提示设计。
- 两阶段的**全局到局部渐进式微调**（GLPFT）用于成功训练多模型协作智能体。



#### 微软 AgentOptimizer

【2024-2-17】 微软 [AgentOptimizer](https://microsoft.github.io/autogen/blog/2023/12/23/AgentOptimizer/), 按照 Agent 范式去微调 LLM
- 论文: [Offline Training of Language Model Agents with Functions as Learnable Weights](https://arxiv.org/pdf/2402.11359)
- [agentchat_agentoptimizer.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_agentoptimizer.ipynb)
- ML训练: 训练集上,通过loss更新模型权重
- Agent训练: 历史function call数据集上,通过Agent的策略更新模型权重
- ![](https://microsoft.github.io/autogen/assets/images/agentoptimizer-33b265dcca6cff56bbf0c9d63ff389f9.png)


### 规划能力


#### LUMOS

Allen AI 发布 [LUMOS](https://allenai.github.io/lumos/)
- 论文 [Agent LUMOS:Unified and Modular Training for Open-Source Language Agents](https://arxiv.org/pdf/2311.05657)
- GitHub [lumos](https://github.com/allenai/lumos)
- [LUMOS : Training framework for Open-source LLM-based Agents](https://medium.com/@techsachin/lumos-training-framework-for-llm-based-agents-47704d4ba3a3)


Lumos 具有统一数据格式、模块化设计和开源LLMs语言代理。

Lumos 统一复杂的交互任务，并与基于 GPT-4/3.5 和更大的开源代理实现了具有竞争力的性能。

Lumos 由以下模块组成:
- Planning Module: 规划模块:将复杂任务分解为一系列用自然语言编写的高级子目标。
- Grounding Module: 接地模块:将规划模块生成的高级子目标转换为低级可执行操作。
- Execution Module: 执行模块:将动作解析为一系列外部工具，包括 API、小型神经模型以及与相关工具和外部环境交互的虚拟模拟器。


## Agent 应用

详见站内专题 [智能体应用](agent_usecase)




# 结束