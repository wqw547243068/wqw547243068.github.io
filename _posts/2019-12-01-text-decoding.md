---
layout: post
title:  "æ–‡æœ¬ç”Ÿæˆä¹‹åºåˆ—è§£ç ä¸“é¢˜ - Decoding Strategy in Text Generation"
date:   2019-12-01 21:39:00
categories: å¤§æ¨¡å‹
tags: gpt è§£ç  è´ªå¿ƒ æ¨æµ‹è§£ç  é›†æŸæœç´¢ æ¸©åº¦ é‡‡æ · å¤šé¡¹å¼ å¯¹æ¯” moe
excerpt: æ–‡æœ¬ç”Ÿæˆé‡Œçš„åºåˆ—è§£ç ä¸“é¢˜ç¬”è®°
author: é¹¤å•¸ä¹å¤©
mathjax: true
permalink: /text_decoding
---

* content
{:toc}

# æ–‡æœ¬ç”Ÿæˆä¹‹åºåˆ—è§£ç 


## åºåˆ—è§£ç 

ç”Ÿæˆå¼ä»»åŠ¡æ¯”æ™®é€šçš„åˆ†ç±»ã€taggingç­‰NLPä»»åŠ¡å¤æ‚ä¸å°‘ã€‚
- ![](https://uploads-ssl.webflow.com/5fdc17d51dc102ed1cf87c05/60adb96dd09ceb13f5d35c3f_sequence.png)
- Seq2Seqæ¨¡å‹ä¸­ï¼ŒRNN Encoderå¯¹è¾“å…¥å¥å­è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆä¸€ä¸ªå¤§å°å›ºå®šçš„hidden state $h_c$
- ç»“åˆå…ˆå‰ç”Ÿæˆçš„ç¬¬1åˆ°t-1ä¸ªè¯ $x_{1~t-1}$, RNN Decoderä¼šç”Ÿæˆå½“å‰ç¬¬tä¸ªè¯çš„hidden state $h_t$
- æœ€åé€šè¿‡softmaxå‡½æ•°å¾—åˆ°ç¬¬tä¸ªè¯ $x_t$ çš„ è¯æ±‡æ¦‚ç‡åˆ†å¸ƒ vocabulary probability distribution $P(x|x_{1:t-1})$

ç”Ÿæˆæ—¶æ¨¡å‹ä¸€ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªæ—¶é—´æ­¥ä¾æ¬¡è¾“å‡ºï¼Œå‰é¢æ—¶é—´æ­¥çš„ç»“æœå½±å“åé¢æ—¶é—´æ­¥çš„ç»“æœã€‚å³æ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œæ¨¡å‹ç»™å‡ºçš„éƒ½æ˜¯<font color='blue'>åŸºäºå†å²ç”Ÿæˆç»“æœçš„æ¡ä»¶æ¦‚ç‡</font>ã€‚
- ç”Ÿæˆå®Œæ•´çš„å¥å­ï¼Œéœ€è¦ä¸€ä¸ªç§°ä¸º`è§£ç `çš„é¢å¤–åŠ¨ä½œæ¥èåˆæ¨¡å‹å¤šä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼Œä½¿å¾—æœ€ç»ˆåºåˆ—çš„æ¯ä¸€æ­¥æ¡ä»¶æ¦‚ç‡è¿ä¹˜èµ·æ¥æœ€å¤§ã€‚
- åˆ†æ
  - æ¯ä¸€ä¸ªæ—¶é—´æ­¥å¯èƒ½çš„è¾“å‡ºç§ç±»ç§°ä¸º`å­—å…¸å¤§å°`(vocabulary sizeï¼Œç”¨Vè¡¨ç¤º)
  - è¿›è¡ŒTæ­¥éšæœºçš„ç”Ÿæˆå¯èƒ½è·å¾—çš„ç»“æœæ€»å…±æœ‰$V_T$ç§ã€‚
  - ä»¥ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆä¸ºä¾‹ï¼ŒVçš„å€¼å¤§çº¦æ˜¯**5000-6000**ï¼Œå³å¸¸ç”¨æ±‰å­—çš„ä¸ªæ•°ã€‚
- åŸºæ•°è¾ƒå¤§ï¼Œéå†æ•´ä¸ªç”Ÿæˆç©ºé—´æ˜¯ä¸ç°å®çš„ã€‚


## LLM è§£ç 


### LLM è§£ç åŸç†

å¤§æ¨¡å‹è®­ç»ƒå¥½ä¹‹åï¼Œå¦‚ä½•å¯¹è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè§£ç ï¼ˆdecodeï¼‰ï¼Ÿ
- ç”Ÿæˆè¾“å‡ºæ–‡æœ¬ï¼šæ¨¡å‹é€ä¸ªé¢„æµ‹æ¯ä¸ª token ï¼Œç›´åˆ°è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶ï¼ˆå¦‚ç»ˆæ­¢ç¬¦å·æˆ–æœ€å¤§é•¿åº¦ï¼‰ã€‚æ¯ä¸€æ­¥æ¨¡å‹ç»™å‡ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œè¡¨ç¤ºä¸‹ä¸€ä¸ªå•è¯é¢„æµ‹ã€‚
- ã€2023-8-4ã€‘[å¤§æ¨¡å‹æ–‡æœ¬ç”Ÿæˆâ€”â€”è§£ç ç­–ç•¥ï¼ˆTop-k & Top-p & Temperatureï¼‰](https://zhuanlan.zhihu.com/p/647813179)

ä¾‹å¦‚ï¼Œå¦‚æœè¾“å…¥æ–‡æœ¬æ˜¯â€œæˆ‘æœ€å–œæ¬¢çš„â€ï¼Œé‚£ä¹ˆæ¨¡å‹å¯èƒ½ä¼šç»™å‡ºä¸‹é¢çš„æ¦‚ç‡åˆ†å¸ƒï¼š
- ![](https://pic3.zhimg.com/80/v2-f9fd705b9f498b5ccfb0cb7d547a522e_1440w.webp)

æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªå•è¯å‘¢ï¼Ÿä»¥ä¸‹æ˜¯å‡ ç§å¸¸ç”¨çš„æ–¹æ³•ï¼š
-   **è´ªå¿ƒè§£ç **ï¼ˆGreedy Decodingï¼‰ï¼šç›´æ¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å•è¯ã€‚è¿™ç§æ–¹æ³•ç®€å•é«˜æ•ˆï¼Œä½†æ˜¯å¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬è¿‡äºå•è°ƒå’Œé‡å¤ã€‚
-   **éšæœºé‡‡æ ·**ï¼ˆRandom Samplingï¼‰ï¼šæŒ‰ç…§æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸€ä¸ªå•è¯ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¢åŠ ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œä½†æ˜¯å¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬ä¸è¿è´¯å’Œæ— æ„ä¹‰ã€‚
-   **Beam Search**ï¼šç»´æŠ¤ä¸€ä¸ªå¤§å°ä¸º k çš„å€™é€‰åºåˆ—é›†åˆï¼Œæ¯ä¸€æ­¥ä»æ¯ä¸ªå€™é€‰åºåˆ—çš„æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ k ä¸ªå•è¯ï¼Œç„¶åä¿ç•™æ€»æ¦‚ç‡æœ€é«˜çš„ k ä¸ªå€™é€‰åºåˆ—ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¹³è¡¡ç”Ÿæˆçš„è´¨é‡å’Œå¤šæ ·æ€§ï¼Œä½†æ˜¯å¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬è¿‡äºä¿å®ˆå’Œä¸è‡ªç„¶ã€‚

è¿™äº›æ–¹æ³•å„æœ‰å„çš„é—®é¢˜ï¼Œè€Œ `top-k é‡‡æ ·`å’Œ `top-p é‡‡æ ·`æ˜¯ä»‹äº`è´ªå¿ƒè§£ç `å’Œ`éšæœºé‡‡æ ·`ä¹‹é—´ï¼Œç›®å‰å¤§æ¨¡å‹è§£ç ç­–ç•¥ä¸­å¸¸ç”¨çš„æ–¹æ³•ã€‚

```json
{
 "top_k": 10,
 "temperature": 0.95,
 "num_beams": 1,
 "top_p": 0.8,
 "repetition_penalty": 1.5,
 "max_tokens": 30000,
 "message": [
        {
 "content": "ä½ å¥½ï¼",
 "role": "user"
        }
    ]
}
```

- `è´ªå¿ƒç­–ç•¥`ï¼Œé‚£ä¹ˆé€‰æ‹©çš„ token å¿…ç„¶å°±æ˜¯â€œå¥³å­©â€
  - ![](https://pic3.zhimg.com/80/v2-5927f0103a6e1ea3030548f9c2c8cb2e_1440w.webp)
  - é—®é¢˜: å®¹æ˜“é™·å…¥é‡å¤å¾ªç¯
- `Top-k é‡‡æ ·`: â€œè´ªå¿ƒç­–ç•¥â€çš„ä¼˜åŒ–
  - ä»æ’åå‰ k çš„ token ä¸­æŠ½æ ·ï¼Œå…è®¸åˆ†æ•°/æ¦‚ç‡è¾ƒé«˜çš„token æœ‰æœºä¼šè¢«é€‰ä¸­ã€‚è¿™ç§æŠ½æ ·å¸¦æ¥çš„éšæœºæ€§æœ‰åŠ©äºæé«˜ç”Ÿæˆè´¨é‡ã€‚
  - æ¯æ­¥åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ªå•è¯ä¸­è¿›è¡Œéšæœºé‡‡æ ·ï¼Œè€Œä¸è€ƒè™‘å…¶ä»–ä½æ¦‚ç‡çš„å•è¯ã€‚
  - ä¾‹å¦‚ï¼Œå¦‚æœ k=2ï¼Œé‚£ä¹ˆåªä»å¥³å­©ã€é‹å­ä¸­é€‰æ‹©ä¸€ä¸ªå•è¯ï¼Œè€Œä¸è€ƒè™‘å¤§è±¡ã€è¥¿ç“œç­‰å…¶ä»–å•è¯ã€‚è¿™æ ·é¿å…é‡‡æ ·åˆ°ä¸€äº›ä¸åˆé€‚æˆ–ä¸ç›¸å…³çš„å•è¯ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ä¿ç•™ä¸€äº›æœ‰è¶£æˆ–æœ‰åˆ›æ„çš„å•è¯ã€‚
  - ![](https://pic3.zhimg.com/80/v2-84999dc8b60cf679844f2a73b9c3d7e2_1440w.webp)
  - é€šè¿‡è°ƒæ•´ k çš„å¤§å°ï¼Œå³å¯æ§åˆ¶é‡‡æ ·åˆ—è¡¨çš„å¤§å°ã€‚â€œ`è´ªå¿ƒç­–ç•¥`â€å…¶å®å°±æ˜¯ k = 1 çš„ `top-k é‡‡æ ·`ã€‚
  - ![](https://pic1.zhimg.com/80/v2-1a7e2450809497727140e44ca8932edc_1440w.webp)


### LLMç¨³å®šè¾“å‡º

Temperature æ§åˆ¶æ¨¡å‹è¾“å‡ºå†…å®¹ç¨³å®šæ€§ï¼Œå› ä¸º LLM çš„è¾“å‡ºæ˜¯é€šè¿‡â€œæ¦‚ç‡â€æ¥æ’åºã€‚
- å¦‚æœå¯¹åŒä¸€ä¸ªé—®é¢˜æƒ³è¦æ¯æ¬¡è¾“å‡ºå®Œå…¨ä¸€è‡´çš„å†…å®¹ï¼Œ`temperature = 0`ã€‚
- è€Œå¦‚æœæƒ³æå‡ LLM è¾“å‡ºå†…å®¹çš„â€œ**åˆ›æ„æ€§**â€ï¼ŒæŠŠ temperature å¾€ä¸Šå¢åŠ 
- ä¸€èˆ¬ temperature åœ¨ `[0,1]` èŒƒå›´è·å¾—çš„ç»“æœå¯ç”¨ï¼Œå¤§äº1å¯èƒ½ç»“æœå°±ä¸å¯ç”¨ã€‚
- æœ€å¥½æ˜¯æŒ‰ä¸åŒåœºæ™¯æ¥é…ç½® temperature çš„æ•°å€¼ï¼Œä¾‹å¦‚å†™è¯—å°±éœ€è¦æ›´é«˜çš„ temperature æ•°å€¼


## è§£ç åŸç†

ã€2019-6-16ã€‘[æ–‡æœ¬ç”Ÿæˆä¸­çš„decoding strategyæ•´ç†](https://zhuanlan.zhihu.com/p/68383015)

### è§£ç ç­–ç•¥

æ–‡æœ¬ç”Ÿæˆ decoding strategy ä¸»è¦åˆ†ä¸ºä¸¤å¤§ç±»ï¼š
- ï¼ˆ1ï¼‰ `Argmax Decoding`: ä¸»è¦åŒ…æ‹¬ beam search, class-factored softmax ç­‰
  - å¦‚æœvocabulary sizeè¾ƒå¤§ï¼Œè¾¾åˆ°äº†**50k**ç”šè‡³**150k**ï¼Œåœ¨softmaxå±‚çš„è¿ç®—é‡å°±ä¼šå˜å¾—éå¸¸å¤§, éœ€è¦é™ä½å¤æ‚åº¦
  - â‘  `Class-factored Softmax`ï¼šå°†åŸæœ¬çš„softmax layeræ‰©å±•ä¸ºä¸¤å±‚ï¼š
    - ç¬¬ä¸€å±‚ä¸º**clusterå±‚**ï¼Œæ¯ä¸ªclusterä¸­åŒ…å«ä¸€ç»„è¯­æ„ç›¸è¿‘çš„è¯ï¼Œæ¯ä¸ªè¯åªå‡ºç°åœ¨ä¸€ä¸ªclusterä¸­ï¼›
    - ç¬¬äºŒå±‚ä¸º**wordå±‚**ï¼Œè¾“å‡ºæœ€ådecodeçš„è¯ã€‚
    - å°½ç®¡clusterå±‚å’Œwordå±‚åˆ†åˆ«åŒ…å«ä¸€ä¸ªsoftmax layerï¼Œä½†æ¯ä¸€å±‚softmaxçš„åˆ†æ¯éƒ¨åˆ†çš„è®¡ç®—é‡éƒ½å¤§å¤§ç¼©å°äº†ã€‚
    - clusterçš„é€‰å–å¯¹decodingçš„æ•ˆæœæœ‰å¾ˆå¤§çš„å½±å“ï¼Œæ‰€ä»¥éœ€è¦é€‰æ‹©åˆé€‚çš„**èšç±»ç®—æ³•**æ¥pre-trainé«˜è´¨é‡çš„clusterï¼Œè®ºæ–‡ä¸­é€‰ç”¨çš„æ˜¯Brown clusterã€‚
    - è¯¦è§è®ºæ–‡ï¼š[Pragmatic Neural Language Modelling in Machine Translation](https://arxiv.org/abs/1412.7119)
  - â‘¡ `Pointer-generator Network`
    - ä¸€å±‚softmax layerï¼Œä½†å¼•å…¥äº†ä¸€ä¸ªéå¸¸å¼ºå¤§çš„copy networkï¼Œæ¨¡å‹è®­ç»ƒé€Ÿåº¦å’Œç”Ÿæˆå¥å­çš„è´¨é‡éƒ½**æ˜¾è‘—**é«˜äºSeq2Seq + Standard Softmaxã€‚
    - é¦–å…ˆå»ºç«‹ä¸€ä¸ªå¾ˆå°ï¼ˆå¦‚5kï¼‰çš„é«˜é¢‘è¯vocabulary
    - ç„¶åå»ºç«‹ä¸€ä¸ªAttention layerï¼Œå¾—åˆ°è¾“å…¥å¥å­çš„Attention distribution
    - åœ¨decodingé˜¶æ®µï¼Œè‹¥vocabularyä¸­ä¸å­˜åœ¨éœ€è¦decodeçš„è¯ xtï¼Œåˆ™ç›´æ¥ä»è¾“å…¥å¥å­çš„Attention distributionä¸­copy xt çš„attention weightä½œä¸º p(xt)ã€‚è¯¦è§è®ºæ–‡ï¼š[Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)
- ï¼ˆ2ï¼‰`Stochastic Decoding`: ä¸»è¦åŒ…æ‹¬ temperature sampling, top-k samplingç­‰
  - é—®é¢˜ï¼šArgmax Decodingå¸¸å¸¸ä¼šå¯¼è‡´æ¨¡å‹ç”Ÿæˆé‡å¤çš„å¥å­ï¼Œå¦‚ "<span style='color:blue'>I don't know. I don't know. I don't know....</span>"ã€‚
  - å› ä¸ºæ¨¡å‹ä¸­ï¼š`p(know|I don't) < p(know|I don't know. I don't)`
  - è§£å†³ï¼šdecodingè¿‡ç¨‹ä¸­å¼•å…¥randomness
  - ä½†æ˜¯è®ºæ–‡ï¼ˆ[The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)ï¼‰æŒ‡å‡ºï¼Œsampling from full vocabulary distributionç”Ÿæˆçš„å¥å­ä¼šéå¸¸çš„æ‚ä¹±æ— ç« ï¼Œå› ä¸ºå½“vocabulary sizeéå¸¸å¤§æ—¶ï¼Œæ¯ä¸ªè¯çš„probabilityéƒ½ä¼šå˜å¾—å¾ˆå°ï¼Œè¿™æ—¶æ¨¡å‹ä¼šæœ‰éå¸¸é«˜çš„å¯èƒ½æ€§sampleåˆ°ä¸€ä¸ªtail distributionä¸­çš„è¯ï¼Œä¸€æ—¦sampleåˆ°äº†tail distributionä¸­ä¸€ä¸ªå’Œå‰æ–‡éå¸¸ä¸ç›¸å…³çš„è¯ï¼Œå¾ˆæœ‰å¯èƒ½æ¥ä¸‹æ¥çš„è¯éƒ½å—å…¶å½±å“ï¼Œä½¿å¾—å¥å­è„±ç¦»åŸæœ¬çš„æ„æ€ã€‚
  - å› æ­¤éœ€è¦sampling from truncated vocabulary distributionï¼Œæ¯”è¾ƒå¸¸è§çš„ç®—æ³•ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§ï¼š
  - â‘  `Temperature Sampling`
    - softmaxä¸­å¼•å…¥ä¸€ä¸ªtemperature tæ¥æ”¹å˜vocabulary probability distributionï¼Œä½¿å…¶æ›´åå‘high probability words
    - é€šè¿‡è°ƒæ•´tçš„å¤§å°ï¼Œå°±å¯ä»¥é¿å…sampling from tail distributionã€‚
    - å½“ t -> 0 æ—¶ï¼Œå°±å˜æˆäº†greedy decodingï¼›
    - å½“ t -> âˆ æ—¶ï¼Œå°±å˜æˆäº†uniform samplingã€‚
  - â‘¡ `Top-k Sampling`
    - æ›´ç®€å•æœ‰æ•ˆ
    - decodingè¿‡ç¨‹ä¸­ï¼Œä» æ¦‚ç‡åˆ†å¸ƒP ä¸­é€‰å–æ¦‚ç‡æœ€é«˜çš„å‰kä¸ªtokensï¼Œæ¦‚ç‡ç´¯åŠ å¾—åˆ° pâ€˜ï¼Œå†å°† P è°ƒæ•´ä¸º Pâ€™=P/p', æœ€åä» P' ä¸­sampleä¸€ä¸ªtokenä½œä¸ºoutput token
    - è®ºæ–‡ï¼š[Hierarchical Neural Story Generation](https://arxiv.org/abs/1805.04833)
    - é—®é¢˜ï¼šå¸¸æ•°kæ˜¯æå‰ç»™å®šçš„å€¼ï¼Œå¯¹äºé•¿çŸ­å¤§å°ä¸ä¸€ï¼Œè¯­å¢ƒä¸åŒçš„å¥å­ï¼Œæˆ‘ä»¬å¯èƒ½æœ‰æ—¶éœ€è¦æ¯”kæ›´å¤šçš„tokensã€‚
    - ![](https://pic2.zhimg.com/80/v2-414643d3b320b8048dc2f3cd682d3c85_1440w.webp)
    - è®¾k=10
      - ç¬¬ä¸€å¥è¯"She said, 'I never"åé¢å¯ä»¥è·Ÿçš„é€‰é¡¹èƒ½æœ‰å¾ˆå¤§çš„diversityï¼Œæ­¤æ—¶10ä¸ªtokensæˆ–è®¸ä¸è¶³ä»¥åŒ…å«å…¨éƒ¨å¯èƒ½çš„é€‰æ‹©ï¼›
      - è€Œç¬¬äºŒå¥è¯"I ate the pizza while it was still"åé¢å¯ä»¥è·Ÿçš„é€‰é¡¹åˆ™ä¸èƒ½æœ‰å¤ªå¤§çš„diversityï¼Œå¦åˆ™ä¼šä½¿å¾—æ•´ä¸ªå¥å­å«ä¹‰è¡¨è¾¾é”™ä¹±ï¼Œæ­¤æ—¶10ä¸ªtokenså°±å˜å¾—è¿‡å¤šäº†ï¼Œä¼šè®©æ¨¡å‹é™·å…¥sample from tail distributionçš„é£é™©ã€‚
  - â‘¢ `Top-p Sampling` (top kæ”¹è¿›) -- nuclear sampling
    - é’ˆå¯¹top kçš„é—®é¢˜ï¼ŒTop-p Sampling åŸºäºTop-k Samplingï¼Œå°† p' è®¾ä¸ºä¸€ä¸ªæå‰å®šä¹‰å¥½çš„å¸¸æ•° p'âˆˆ(0,1)ï¼Œè€Œselected tokensæ ¹æ®å¥å­history distributionçš„å˜åŒ–è€Œæœ‰æ‰€ä¸åŒã€‚è¯¦è§è®ºæ–‡ï¼šThe Curious Case of Neural Text Degeneration
  - æœ¬è´¨ä¸Š`Top-p Sampling`å’Œ`Top-k Sampling`éƒ½æ˜¯ä»truncated vocabulary distributionä¸­sample tokenï¼ŒåŒºåˆ«åœ¨äº**ç½®ä¿¡åŒºé—´çš„é€‰æ‹©**ã€‚

ä¸¤ç±» decoding strategy ä¸»è¦åŒºåˆ«: å¦‚ä½•ä»vocabulary probability distribution $P(x\|x_{1:t-1})$ ä¸­é€‰å–ä¸€ä¸ªè¯ $x_t$ï¼š
- Argmax Decodingçš„åšæ³•æ˜¯é€‰æ‹©è¯è¡¨ä¸­æ¦‚ç‡æœ€å¤§çš„è¯ï¼Œå³ $x_t=argmax P(x\|x_{1:t-1})$;
- Stochastic Decodingåˆ™æ˜¯åŸºäºæ¦‚ç‡åˆ†å¸ƒéšæœºsampleä¸€ä¸ªè¯ $x_t$ï¼Œå³ $x_t~P(x\|x_{1:t-1})$ã€‚

é—®é¢˜ï¼š`top-k`/`top-p` ä¸ `beam search`åŒºåˆ«
- Top-pæ²¡æœ‰å’Œbeam searchä¸€æ ·çš„**å€™é€‰åºåˆ—**ï¼Œå®ƒä»…åœ¨**å½“å‰**time stepé‡‡æ ·

ã€2021-1-2ã€‘ç¿ä¸½è²çš„åšå®¢ï¼š[Controllable Neural Text Generation](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/)

Since the final layer of the model predicts logits o over the vocabulary space, the next token can be sampled by applying softmax with temperature T. The probability of sampling the i-th token is

$p_i \propto \frac{\exp(o_i / T)}{\sum_j \exp(o_j/T)}$
 
A low temperature would make the distribution sharper and a high value makes it softer.

Decoding Strategies
- **Greedy search**: Always pick the next token with the _highest_ probability, equivalent to setting temperature $T=0$. However, it tends to create repetitions of phrases, even for well-trained models.
  - è´ªå¿ƒè§£ç ç›¸å½“äº $T=0$, å®¹æ˜“å¯¼è‡´çŸ­è¯­é‡å¤
- **Beam search**: It essentially does breadth-first search, one token per tree level, but with a limited bandwidth. At each level of the search tree, beam search keeps track of n (named â€œbeam widthâ€) best candidates and expands all the successors of these candidates in the next level. Beam search could stop expanding a node if it hits the EOS (end-of-sentence) token.
   - å®½åº¦ä¼˜å…ˆæœç´¢, é‡åˆ°ç»“æŸç¬¦ï¼ˆEOSï¼‰åœæ­¢ï¼›é›†æŸæœç´¢ä¸ä¿è¯æœ€ä¼˜ç”Ÿæˆç»“æœ
   - However, maximization-based decoding does not guarantee high-quality generation.
   - ![](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/beam_search_less_surprising.png)
   - Fig. 1. The probability assigned to the next token by beam search versus by humans. The human selected tokens have much higher variance in predicted probability and thus more surprising. (Image source: [Holtzman et al. 2019](https://arxiv.org/abs/1904.09751))
- **Top-k sampling** `Top-ké‡‡æ ·` ([Fan et al., 2018](https://arxiv.org/abs/1805.04833)): At each sampling step, only the top k most likely tokens are selected and the probability mass is redistributed among them. In [Fan et al., 2018](https://arxiv.org/abs/1805.04833), the authors proposed to use _top-k random sampling_ where the next token is randomly selected among the top k most likely candidates and they argued that this approach can generate more novel and less repetitive content than beam search.
  - æ¯æ­¥è§£ç æ—¶ï¼Œåªé€‰æ‹© top-k å¯èƒ½æ€§çš„tokenï¼Œå†é‡æ–°è®¡ç®—æ¦‚ç‡åˆ†å¸ƒã€‚
  - å¥½å¤„ï¼šæ¯”beam searchæ›´å®¹æ˜“ç”Ÿæˆæ–°é¢–ã€å°‘é‡å¤çš„å†…å®¹
- **Nucleus sampling** `Top-pé‡‡æ ·` ([Holtzman et al. 2019](https://arxiv.org/abs/1904.09751)): Also known as â€œTop-p samplingâ€. One drawback of top-k sampling is that the predefined number k does not take into consideration how _skewed_ the probability distribution might be. The nucleus sampling selects the smallest set of top candidates with the cumulative probability exceeding a threshold (e.g. 0.95) and then the distribution is rescaled among selected candidates.
  - Top-ké‡‡æ ·çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯kå€¼é€‰å–æœªè€ƒè™‘æ¦‚ç‡åˆ†å¸ƒæ˜¯å¦å€¾æ–œã€‚
  - Top-pé‡‡æ ·é€‰æ‹©è¶…è¿‡ä¸€å®šé˜ˆå€¼ï¼ˆå¦‚0.95ï¼‰çš„æœ€å°å­—ç¬¦é›†åˆï¼Œé‡æ–°è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
  - Both top-k and nucleus sampling have less repetitions with a proper set of hyperparameters.
- **Penalized sampling** ([Keskar et al. 2019](https://arxiv.org/abs/1909.05858)): To avoid the common failure case of generating duplicate substrings, the [CTRL](https://arxiv.org/abs/1909.05858) paper proposed a new sampling method to penalize repetitions by discounting the scores of previously generated tokens. The probability distribution for the next token with repetition penalty is defined as:
  - $p_i = \frac{\exp(o_i / (T \cdot \mathbb{1}(i \in g)))}{\sum_j \exp(o_j / (T \cdot \mathbb{1}(j \in g)))} \quad \mathbb{1}(c) = \theta \text{ if the condition }c\text{ is True else }1$
  - ä¸€ç§æƒ©ç½šé‡å¤å­ä¸²çš„é‡‡æ ·æ–¹æ³•ï¼Œè€ƒè™‘ä¹‹å‰ç”Ÿæˆè¿‡çš„å­—ç¬¦
  - where g contains a set of previously generated tokens, ğŸ™1(.) is an identity function. Î¸=1.2 is found to yield a good balance between less repetition and truthful generation.


### GPT-2

huggingface é‡Œçš„ GPT-2 ä»£ç 

```py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "gpt2-xl"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)

import pandas as pd

input_txt = "Transformers are the"
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
iterations = []
n_steps = 8 # è¿›è¡Œ8æ­¥è§£ç 
choices_per_step = 5 # æ¯ä¸€æ­¥å€™é€‰æ•°é‡

with torch.no_grad():# evalæ¨¡å¼
    for _ in range(n_steps):# æ¯æ­¥è§£ç 
        iteration = dict()
        iteration["Input"] = tokenizer.decode(input_ids[0]) # æç¤ºæ–‡æœ¬
        output = model(input_ids=input_ids) # å°†æç¤ºæ–‡æœ¬è¾“å…¥åˆ°æ¨¡å‹è¿›è¡Œè§£ç 
        # Select logits of the first batch and the last token and apply softmax
        next_token_logits = output.logits[0, -1, :]
        next_token_probs = torch.softmax(next_token_logits, dim=-1)
        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)
        # Store tokens with highest probabilities
        for choice_idx in range(choices_per_step): # æ¦‚ç‡æœ€å¤§çš„äº”ä¸ªtoken
            token_id = sorted_ids[choice_idx]
            token_prob = next_token_probs[token_id].cpu().numpy()
            token_choice = (
                f"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)" # å–ç™¾åˆ†å·ä¸¤ä½æ•°
            )
            iteration[f"Choice {choice_idx+1}"] = token_choice
        # Append predicted next token to input
        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1) # å°†æ¦‚ç‡æœ€å¤§çš„å­—ç¬¦æ‹¼æ¥åˆ°æç¤ºæ–‡æœ¬
        iterations.append(iteration)
# è¾“å‡ºåºåˆ—è§£ç ç»“æœ
pd.DataFrame(iterations)
```


## è§£ç æ–¹æ³•

### æ€»ç»“

ä»¥ ç®€åŒ–ç‰ˆ ä¸­è‹±ç¿»è¯‘ä»»åŠ¡ä¸ºä¾‹
>- ä¸­æ–‡è¾“å…¥ï¼š"æˆ‘" "æ¨" "ä½ "
>- è‹±æ–‡è¾“å‡ºï¼š"I" "H" "U", å‡è®¾è¾“å‡ºå­—å…¸åªæœ‰3ä¸ª
>- ç›®æ ‡ï¼šå¾—åˆ°æœ€ä¼˜çš„ç¿»è¯‘åºåˆ— I-H-U

è§£ç æ–¹æ³•
- ï¼ˆ1ï¼‰`exhaustive/brute search`ï¼ˆ`ç©·ä¸¾æœç´¢`/æš´åŠ›æœç´¢ï¼‰ï¼šéå†æ‰€æœ‰å¯èƒ½å¾—è¾“å‡ºåºåˆ—ï¼Œæœ€åé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„åºåˆ—è¾“å‡º
  - ç¤ºä¾‹ï¼šä¸€å…± $3^3=27$ ç§æ’åˆ—ç»„åˆ
  - ç©·ä¸¾æœç´¢èƒ½ä¿è¯**å…¨å±€æœ€ä¼˜**ï¼Œä½†è®¡ç®—å¤æ‚åº¦å¤ªé«˜ï¼Œå½“è¾“å‡ºè¯å…¸ç¨å¾®å¤§ä¸€ç‚¹æ ¹æœ¬æ— æ³•ä½¿ç”¨ã€‚
- ï¼ˆ2ï¼‰`greedy search` `è´ªå¿ƒæœç´¢`ï¼šæ¯æ­¥é€‰å–æ¦‚ç‡**æœ€å¤§**çš„è¯ï¼Œå±€éƒ¨æœ€ä¼˜
  - ç¤ºä¾‹ï¼š1ç§ç»„åˆ
    - ç¬¬1ä¸ªæ—¶é—´æ­¥ï¼šç¿»è¯‘"æˆ‘"ï¼Œå‘ç°å€™é€‰"I"çš„æ¡ä»¶æ¦‚ç‡æœ€å¤§ä¸º0.6ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªæ­¥é•¿ç›´æ¥ç¿»è¯‘æˆäº†"I"ã€‚
    - ç¬¬2ä¸ªæ—¶é—´æ­¥ï¼šç¿»è¯‘"æˆ‘æ¨"ï¼Œå‘ç°IIæ¦‚ç‡0.2ï¼ŒIHæ¦‚ç‡0.7ï¼ŒIUæ¦‚ç‡0.1ï¼Œæ‰€ä»¥é€‰æ‹©IHä½œä¸ºå½“å‰æ­¥é•¿æœ€ä¼˜ç¿»è¯‘ç»“æœã€‚
    - ç¬¬3ä¸ªæ—¶é—´æ­¥ï¼šç¿»è¯‘"æˆ‘æ¨ä½ "ï¼Œå‘ç°IHIæ¦‚ç‡0.05ï¼ŒIHHæ¦‚ç‡0.05ï¼ŒIHUæ¦‚ç‡0.9ï¼Œæ‰€ä»¥é€‰æ‹©IHUä½œä¸ºæœ€ç»ˆçš„ç¿»è¯‘ç»“æœã€‚
    - ![](https://pic4.zhimg.com/80/v2-ade0d00a227b00c232dffad522566d9b_1440w.webp)
  - è´ªå¿ƒç®—æ³•æ¯æ­¥é€‰æ‹©å½“å‰æœ€å¥½çš„é€‰æ‹©ï¼Œå¸Œæœ›é€šè¿‡å±€éƒ¨æœ€ä¼˜ç­–ç•¥æœŸæœ›äº§ç”Ÿå…¨å±€æœ€ä¼˜è§£ã€‚ä½†æ˜¯è´ªå¿ƒç®—æ³•æ²¡æœ‰ä»æ•´ä½“æœ€ä¼˜ä¸Šè€ƒè™‘ï¼Œå¹¶ä¸èƒ½ä¿è¯æœ€ç»ˆä¸€å®šå…¨å±€æœ€ä¼˜ã€‚ä½†æ˜¯ç›¸å¯¹ç©·ä¸¾æœç´¢ï¼Œæœç´¢æ•ˆç‡å¤§å¤§æå‡ã€‚
- ï¼ˆ3ï¼‰`beam search` `é›†æŸæœç´¢`ï¼šä½¿ç”¨æ¡ä»¶æ¦‚ç‡ï¼Œæ¯æ­¥é€‰å–æ¦‚ç‡æœ€å¤§çš„top kä¸ªè¯ï¼ˆbeam widthï¼‰
  - beam searchæ˜¯å¯¹greedy searchçš„ä¸€ä¸ªæ”¹è¿›ç®—æ³•ï¼Œç›¸å¯¹greedy searchæ‰©å¤§äº†æœç´¢ç©ºé—´ï¼Œä½†è¿œè¿œä¸åŠç©·ä¸¾æœç´¢æŒ‡æ•°çº§çš„æœç´¢ç©ºé—´ï¼Œæ˜¯æŠ˜ä¸­æ–¹æ¡ˆ
  - beam searchæœ‰ä¸€ä¸ªè¶…å‚æ•° beam sizeï¼ˆ**æŸå®½**ï¼‰ï¼Œè®¾ä¸º k
    - æ¯æ­¥é€‰å–å½“å‰æ¡ä»¶æ¦‚ç‡æœ€å¤§çš„kä¸ªè¯ï¼Œå½“åšå€™é€‰è¾“å‡ºåºåˆ—çš„ç¬¬ä¸€ä¸ªè¯ã€‚ä¹‹åæ¯ä¸ªæ—¶é—´æ­¥ï¼ŒåŸºäºä¸Šæ­¥çš„è¾“å‡ºåºåˆ—ï¼ŒæŒ‘é€‰å‡ºæ‰€æœ‰ç»„åˆä¸­æ¡ä»¶æ¦‚ç‡æœ€å¤§çš„kä¸ªï¼Œä½œä¸ºè¯¥æ—¶é—´æ­¥ä¸‹çš„å€™é€‰è¾“å‡ºåºåˆ—ã€‚å§‹ç»ˆä¿æŒkä¸ªå€™é€‰ã€‚æœ€åä»kä¸ªå€™é€‰ä¸­æŒ‘å‡ºæœ€ä¼˜çš„ã€‚
    - ç¬¬1æ­¥: Iå’ŒHçš„æ¦‚ç‡æ˜¯top2ï¼Œæ‰€ä»¥å°†Iå’ŒHåŠ å…¥åˆ°å€™é€‰è¾“å‡ºåºåˆ—ä¸­ã€‚
    - ç¬¬2æ­¥: ä»¥Iå¼€å¤´æœ‰ä¸‰ç§å€™é€‰ { II, IH, IU }ï¼Œä»¥Hå¼€å¤´æœ‰ä¸‰ç§å€™é€‰ { HI, HH, HU } ä»è¿™6ä¸ªå€™é€‰ä¸­æŒ‘å‡ºæ¡ä»¶æ¦‚ç‡æœ€å¤§çš„2ä¸ªï¼Œå³IHå’ŒHIï¼Œä½œä¸ºå€™é€‰è¾“å‡ºåºåˆ—
    - ç¬¬3æ­¥: åŒç†ä»¥IHå¼€å¤´æœ‰ä¸‰ç§å€™é€‰ {IHI, IHH, IHU}ï¼Œä»¥HIå¼€å¤´æœ‰ä¸‰ç§å€™é€‰ {HII, HIH, HIU}ã€‚ä»è¿™6ä¸ªå€™é€‰ä¸­æŒ‘å‡ºæ¡ä»¶æ¦‚ç‡æœ€å¤§çš„2ä¸ªï¼Œå³IHHå’ŒHIUï¼Œä½œä¸ºå€™é€‰è¾“å‡ºåºåˆ—ã€‚
    - 3æ­¥ç»“æŸ, ç›´æ¥ä»IHHå’ŒIHUä¸­æŒ‘é€‰å‡ºæœ€ä¼˜å€¼IHUä½œä¸ºæœ€ç»ˆçš„è¾“å‡ºåºåˆ—ã€‚
    - ![](https://pic4.zhimg.com/80/v2-e28eda027a639a9034cb1c39a291056b_1440w.webp)
- ï¼ˆ4ï¼‰`æ¸©åº¦é‡‡æ ·`æ–¹æ³•ï¼ˆTemperature Sampling Methodsï¼‰
- æ€»ç»“
  - beam searchä¸ä¿è¯å…¨å±€æœ€ä¼˜ï¼Œä½†æ˜¯æ¯”greedy searchæœç´¢ç©ºé—´æ›´å¤§ï¼Œä¸€èˆ¬ç»“æœæ¯”greedy searchè¦å¥½ã€‚
  - greedy search å¯ä»¥çœ‹åšæ˜¯ beam size = 1æ—¶çš„ beam searchã€‚

### è´ªå¿ƒ Greedy Search

`è´ªå¿ƒæœç´¢`ï¼Œæ¯æ­¥éƒ½å–æ¡ä»¶æ¦‚ç‡**æœ€å¤§**çš„è¯è¾“å‡ºï¼Œå†å°†ä»å¼€å§‹åˆ°å½“å‰æ­¥çš„ç»“æœä½œä¸ºè¾“å…¥ï¼Œè·å¾—ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼Œç›´åˆ°æ¨¡å‹ç»™å‡ºç”Ÿæˆç»“æŸçš„æ ‡å¿—ã€‚
- ç¤ºä¾‹ï¼Œç”Ÿæˆåºåˆ—: \[A,B,C\]
  - ![img](http://www.wuyuanhao.com/wp-content/uploads/2020/03/greedy.png)

å‚æ•°è®¾ç½®ï¼š
- do_sample = False, num_beams = 1

åˆ†æ
- ä¼˜ç‚¹: åŸæ¥æŒ‡æ•°çº§åˆ«çš„æ±‚è§£ç©ºé—´ç›´æ¥å‹ç¼©åˆ°äº†ä¸é•¿åº¦çº¿æ€§ç›¸å…³çš„å¤§å°ã€‚ï¼ˆæŒ‡æ•°çº§â†’çº¿æ€§çº§ï¼‰
- ç¼ºç‚¹ï¼š
  - 1ã€ç”Ÿæˆæ–‡æœ¬é‡å¤
  - 2ã€ä¸æ”¯æŒç”Ÿæˆå¤šæ¡ç»“æœã€‚ å½“ num_return_sequences å‚æ•°è®¾ç½®å¤§äº1æ—¶ï¼Œä»£ç ä¼šæŠ¥é”™ï¼Œè¯´greedy searchä¸æ”¯æŒè¿™ä¸ªå‚æ•°å¤§äº1
  - ç”±äºä¸¢å¼ƒäº†ç»å¤§å¤šæ•°çš„å¯èƒ½è§£ï¼Œè¿™ç§å…³æ³¨å½“ä¸‹çš„ç­–ç•¥<font color='red'>æ— æ³•ä¿è¯æœ€ç»ˆåºåˆ—æ¦‚ç‡æ˜¯æœ€ä¼˜çš„</font>

```py
def greedy_decode(model, input, max_length):
    output = input
    for _ in range(max_length):
        # ä¸ºæ¨¡å‹çš„ä¸‹ä¸€ä¸ªå•è¯ç”Ÿæˆé¢„æµ‹
        predictions = model(output)
        # ä½¿ç”¨argmaxæ¥é€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªå•è¯
        next_word = torch.argmax(predictions, dim=-1)
        # å°†é€‰æ‹©çš„å•è¯æ·»åŠ åˆ°è¾“å‡ºä¸­
        output = torch.cat((output, next_word), dim=-1)
    # åœ¨ç”Ÿæˆå®Œæˆåè¿”å›è¾“å‡º
    return output
```


### é›†æŸæœç´¢ Beam Search

Beam searchæ˜¯å¯¹è´ªå¿ƒç­–ç•¥ä¸€ä¸ªæ”¹è¿›ã€‚
- æ€è·¯ï¼šç¨å¾®æ”¾å®½ä¸€äº›è€ƒå¯Ÿçš„èŒƒå›´ã€‚
  - æ¯æ­¥ä¸å†åªä¿ç•™å½“å‰åˆ†æ•°æœ€é«˜çš„1ä¸ªè¾“å‡ºï¼Œè€Œæ˜¯ä¿ç•™num_beamsä¸ªã€‚æ¯æ­¥é€‰æ‹©num_beamsä¸ªè¯ï¼Œå¹¶ä»ä¸­æœ€ç»ˆé€‰æ‹©å‡ºæ¦‚ç‡æœ€é«˜çš„åºåˆ—ã€‚
  - ç¬¬1æ­¥é€‰å–å½“å‰æ¡ä»¶æ¦‚ç‡æœ€å¤§çš„ k ä¸ªè¯ã€‚ä¹‹åæ¯ä¸ªæ—¶é—´æ­¥åŸºäºä¸Šä¸ªæ­¥é•¿çš„è¾“å‡ºåºåˆ—ï¼ŒæŒ‘é€‰å‡ºæ‰€æœ‰ç»„åˆä¸­æ¡ä»¶æ¦‚ç‡æœ€å¤§çš„ k ä¸ªï¼Œä½œä¸ºè¯¥æ—¶é—´æ­¥é•¿ä¸‹çš„å€™é€‰è¾“å‡ºåºåˆ—ã€‚å§‹ç»ˆä¿æŒ k ä¸ªå€™é€‰ã€‚æœ€åä» k ä¸ªå€™é€‰ä¸­æŒ‘å‡ºæœ€ä¼˜çš„ã€‚
  - å½“ num_beams=1 æ—¶é›†æŸæœç´¢å°±é€€åŒ–æˆäº†**è´ªå¿ƒæœç´¢**ã€‚
- ç¤ºä¾‹
  - æ¯ä¸ªæ—¶é—´æ­¥æœ‰ABCDEå…±5ç§å¯èƒ½çš„è¾“å‡ºï¼Œå³v=5v=5ï¼Œå›¾ä¸­çš„num_beams=2ï¼Œä¹Ÿå°±æ˜¯è¯´æ¯ä¸ªæ—¶é—´æ­¥éƒ½ä¼šä¿ç•™åˆ°å½“å‰æ­¥ä¸ºæ­¢æ¡ä»¶æ¦‚ç‡æœ€ä¼˜çš„2ä¸ªåºåˆ—
  - ![](http://www.wuyuanhao.com/wp-content/uploads/2020/03/beam-search.png)
  - ![](https://pic2.zhimg.com/80/v2-a760198d6b851fc38c8d21830d1f27c9_1440w.webp)
  - åœ¨ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ï¼ŒAå’ŒCæ˜¯æœ€ä¼˜çš„ä¸¤ä¸ªï¼Œå› æ­¤å¾—åˆ°äº†ä¸¤ä¸ªç»“æœ\[A],\[C]ï¼Œå…¶ä»–ä¸‰ä¸ªå°±è¢«æŠ›å¼ƒäº†ï¼›
  - ç¬¬äºŒæ­¥ä¼šåŸºäºè¿™ä¸¤ä¸ªç»“æœç»§ç»­è¿›è¡Œç”Ÿæˆï¼Œåœ¨Aè¿™ä¸ªåˆ†æ”¯å¯ä»¥å¾—åˆ°5ä¸ªå€™é€‰äººï¼Œ\[AA],\[AB],\[AC],\[AD],\[AE]ï¼ŒCä¹ŸåŒç†å¾—åˆ°5ä¸ªï¼Œæ­¤æ—¶ä¼šå¯¹è¿™10ä¸ªè¿›è¡Œç»Ÿä¸€æ’åï¼Œå†ä¿ç•™æœ€ä¼˜çš„ä¸¤ä¸ªï¼Œå³å›¾ä¸­çš„\[AB]å’Œ\[CE]ï¼›
  - ç¬¬ä¸‰æ­¥åŒç†ï¼Œä¹Ÿä¼šä»æ–°çš„10ä¸ªå€™é€‰äººé‡Œå†ä¿ç•™æœ€å¥½çš„ä¸¤ä¸ªï¼Œæœ€åå¾—åˆ°äº†\[ABD],\[CED]ä¸¤ä¸ªç»“æœã€‚
  - ![](https://pic1.zhimg.com/80/v2-964bce7699b8ae813346015dc11c3e60_1440w.webp)

å‚æ•°è®¾ç½®ï¼š
- do_sample = False, num_beams>1

åˆ†æ
- beam searchåœ¨æ¯æ­¥éœ€è¦è€ƒå¯Ÿçš„å€™é€‰äººæ•°é‡æ˜¯è´ªå¿ƒæœç´¢çš„num_beamså€
- BSæ˜¯ä¸€ç§**æ—¶é—´**æ¢**æ€§èƒ½**çš„æ–¹æ³•ã€‚
- ä¼šé‡åˆ°è¯¸å¦‚è¯è¯­**é‡å¤**é—®é¢˜

ç¼ºç‚¹ï¼š
- è™½ç„¶ç»“æœæ¯”è´ªå¿ƒæœç´¢æ›´æµç•…ï¼Œä½†æ˜¯ä»ç„¶å­˜åœ¨ç”Ÿæˆé‡å¤çš„é—®é¢˜



```py
def beam_search_decode(model, input, max_length, k):
    output = [(input, 0)]  # initialize beam with the input and its score
    for _ in range(max_length):
        all_candidates = []  # list to store all sentence candidates at this step
        for sentence, score in output:
            # Get next word probabilities
            predictions = model(sentence)
            # Get the k most probable next words
            top_k_scores, top_k_words = torch.topk(predictions, k, dim=-1)
            # create new candidates with the top_k words and add their score
            for i in range(k):
                candidate = torch.cat((sentence, top_k_words[i].unsqueeze(0)), dim=-1)
                all_candidates.append((candidate, score + top_k_scores[i]))
        # Sort all candidates by score
        ordered = sorted(all_candidates, key=lambda tup:tup[1], reverse=True)
        # Select the best k candidates
        output = ordered[:k]
    # Return the sentence of the best candidate
    return output[0][0]
```

ä»£ç å®ç°
- tensorflow æŠŠ decoder ä» BasicDecoder æ¢æˆ BeamSearchDecoder
- å› ä¸ºç”¨äº† Beam Searchï¼Œæ‰€ä»¥ decoder çš„è¾“å…¥å½¢çŠ¶éœ€è¦åš K å€çš„æ‰©å±•ï¼Œtile_batch å°±æ˜¯ç”¨æ¥å¹²è¿™ä¸ªã€‚å¦‚æœå’Œä¹‹å‰çš„ AttentionWrapper æ­é…ä½¿ç”¨çš„è¯ï¼Œè¿˜éœ€è¦æŠŠencoder_outputs å’Œ sequence_length éƒ½ç”¨ tile_batch åšä¸€ä¸‹æ‰©å±•

```py
tokens_go = tf.ones([config.batch_size], dtype=tf.int32) * w2i_target["_GO"]
decoder_cell = tf.nn.rnn_cell.GRUCell(config.hidden_dim)

if useBeamSearch > 1:
	decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=useBeamSearch)	
	decoder = tf.contrib.seq2seq.BeamSearchDecoder(decoder_cell, decoder_embedding, tokens_go, w2i_target["_EOS"],  decoder_initial_state , beam_width=useBeamSearch, output_layer=tf.layers.Dense(config.target_vocab_size))
else:
	decoder_initial_state = encoder_state
	decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, decoder_initial_state, output_layer=tf.layers.Dense(config.target_vocab_size))
			
decoder_outputs, decoder_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(decoder, maximum_iterations=tf.reduce_max(self.seq_targets_length))
```


### åºåˆ—æ‰©å±•

- åºåˆ—æ‰©å±•æ˜¯beam searchçš„æ ¸å¿ƒè¿‡ç¨‹
- ![](http://www.wuyuanhao.com/wp-content/uploads/2020/03/seqextend-1024x695.png)


### Multinomial samplingï¼ˆå¤šé¡¹å¼é‡‡æ ·ï¼‰

æ–¹å¼ï¼š
- æ¯æ­¥æ ¹æ®æ¦‚ç‡åˆ†å¸ƒéšæœºé‡‡æ ·å­—ï¼ˆæ¯ä¸ªæ¦‚ç‡>0çš„å­—éƒ½æœ‰è¢«é€‰ä¸­çš„æœºä¼šï¼‰ã€‚

å‚æ•°ï¼š
- do_sample = True, num_beams = 1

ä¼˜ç‚¹ï¼š
- è§£å†³äº†ç”Ÿæˆé‡å¤çš„é—®é¢˜ï¼Œä½†æ˜¯å¯èƒ½ä¼šå‡ºç°ç”Ÿæˆçš„æ–‡æœ¬ä¸å‡†å®ˆåŸºæœ¬çš„è¯­æ³•


### Beam-search multinomial sampling

æ–¹å¼ï¼š
- ç»“åˆäº†Beam-searchå’Œmultinomial samplingçš„æ–¹å¼ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ä»num_beamsä¸ªå­—ä¸­é‡‡æ ·

å‚æ•°ï¼š
- do_sample = True, num_beams > 1


### ç›´æ¥ç”Ÿæˆ

ä»¥llamaæ¨¡å‹ä¸ºä¾‹

```py
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
# --------- æ¨¡å‹/åˆ†è¯å™¨åˆå§‹åŒ– ----------
model_name = "llama-2-7b-hf" # ç”¨ä½ çš„æ¨¡å‹çš„åœ°å€
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)
# --------- åˆ†è¯ ----------
text = "say"
inputs = tokenizer(text, return_tensors="pt") # åˆ†è¯åæ˜¯1ä¸ªtoken, è¯è¡¨ä¸­ä½ç½®æ˜¯ 1827
print(f"inputs:{inputs}")
# ç»“æœ
# inputs:{'input_ids': tensor([[   1, 1827]]), 'attention_mask': tensor([[1, 1]])}
# --------- ç›´æ¥ç”Ÿæˆ ----------
logits = model.forward(input_ids) # æ¨¡å‹è¾“å‡º, [batch_size, sequence_length, vocab_size]
print("Logits Shape:", logits.logits.shape) # Logits Shape: torch.Size([1, 2, 32000])
print(f"logits:{logits.logits}")
# logits:tensor([[[-12.9696,  -7.4491,  -0.4354,  ...,  -6.8250,  -8.0804,  -7.5782],
#          [-11.3775, -10.1338,  -2.3563,  ...,  -6.7709,  -6.5252,  -8.9753]]],
#        device='cuda:0', grad_fn=<UnsafeViewBackward0>)
```

logits æ˜¯æ¨¡å‹çš„æœ€åè¾“å‡ºï¼Œæ˜¯ä¸€ä¸ªå¼ é‡ï¼ˆtensorï¼‰ï¼Œç»´åº¦æ˜¯`[batch_size, sequence_length, vocab_size]`
- ç¤ºä¾‹ä¸­, batch_size=1ï¼Œsequence_length=2

é—®é¢˜: åªè¾“å…¥äº†ä¸€ä¸ª`say`ï¼ˆ1ä¸ªtokenï¼‰ï¼Œä¸ºä»€ä¹ˆæ˜¯2ä¸ªtoken? 
- è¾“å…¥æ¨¡å‹å‰ llama tokenizerè‡ªåŠ¨æ·»åŠ ä¸€ä¸ªbos token â€”â€”`<s>` ï¼ˆå¼€å§‹ç¬¦ï¼‰, å®é™…è¾“å…¥é•¿åº¦å°±æ˜¯2ä¸ªtokenï¼ˆ`<s>` + `say`ï¼‰
- llama æ¨ç†è¿‡ç¨‹å¹¶æ²¡æœ‰å¢åŠ ï¼ˆæ”¹å˜ï¼‰è¾“å…¥åºåˆ—é•¿åº¦ï¼Œæœ€åä¸€ä¸ªtokençš„ logits è¾“å‡ºé¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡ï¼Œvocab_size æ˜¯è¯å…¸çš„å¤§å°ï¼Œllama æ˜¯32000ï¼Œåœ¨ç¬¬2ä¸ªsequenceé‡Œæ‰¾åˆ°åœ¨32000è¯è¡¨ä¸­å“ªä¸ªtokençš„æ¦‚ç‡æœ€å¤§

```py
# åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šï¼ˆ32000ï¼‰è¿›è¡Œæ±‚æœ€å¤§å€¼æ“ä½œï¼Œè¿”å›å…·æœ‰æœ€é«˜åˆ†æ•°çš„è¯æ±‡ç´¢å¼•
next_token = torch.argmax(logits.logits, dim=-1).reshape(-1)[1]
print(f"next_token:{next_token}")
# next_token:tensor([22172], device='cuda:0')
next_word = tokenizer.decode(next_token)
print(f"next_word:{next_word}") # next_word:hello
```

æœ€åä¸€ä¸ªç»´åº¦ä¸Šï¼ˆ32000ï¼‰è¿›è¡Œæ±‚æœ€å¤§å€¼æ“ä½œï¼Œå¹¶è¿”å›å…·æœ‰æœ€é«˜åˆ†æ•°çš„è¯æ±‡ç´¢å¼•ï¼Œåœ¨è¯è¡¨ä¸­çš„ä½ç½®æ˜¯22172ï¼Œæ¥ä¸‹æ¥å°±æ˜¯è§£ç è¯¥token

å°† next_word é¢„æµ‹å‡ºæ¥åçš„æµç¨‹: 
- å°†â€œhelloâ€åŠ åˆ°â€œsayâ€åé¢å˜æˆâ€œsay helloâ€
- è¿­ä»£ä¸Šè¿°æµç¨‹ç›´åˆ°ç”Ÿæˆeos_tokenï¼ˆç»ˆæ­¢è¯ï¼‰

æ•´ä¸ªé¢„æµ‹ä¹Ÿå°±å®Œæˆäº†ï¼Œè¿™å°±æ˜¯è‡ªå›å½’è¿‡ç¨‹ã€‚



### æ€»ç»“


Huggingface å…±æœ‰8ç§è§£ç ç­–ç•¥

```py
model.generate()
```

æ€»ç»“
- greedy decoding **è´ªå¿ƒ**è§£ç ç­–ç•¥: æœ€åŸå§‹ã€ç®€å•, æ¯æ­¥é€‰æ‹©é¢„æµ‹æ¦‚ç‡æœ€é«˜çš„token
- beam search **é›†æŸ**è§£ç ç­–ç•¥: æˆ–**æŸæœç´¢**, æ¯æ­¥é€‰æ‹©å¤šä¸ªå€™é€‰, ç®€ç§° bs
- multinomial sampling **å¤šé¡¹å¼é‡‡æ ·**è§£ç ç­–ç•¥: 
  - é€šè¿‡å„ç§æ”¹å˜ logits å‚æ•°ï¼ˆmultinomial samplingï¼Œtemperatureï¼Œtop_kï¼Œtop_pç­‰ï¼‰å®ç°ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§
- contrastive search **å¯¹æ¯”æœç´¢**ç­–ç•¥: å¼•å…¥**å¯¹æ¯”åº¦æƒ©ç½š**çš„æœç´¢æ–¹æ³•
  - å½“å‰tokenä¸å‰é¢tokenç›¸ä¼¼æ€§å¤§,å°±å‡å°‘ç”Ÿæˆæ¦‚ç‡ï¼Œè§£å†³é‡å¤é—®é¢˜
- constrained beam-search decoding **å—é™æŸæœç´¢**è§£ç 
  - è§£ç æœç´¢è¿‡ç¨‹ä¸­,å¼•å…¥è‡ªå®šä¹‰è¯è¡¨, å¼ºåˆ¶ç”ŸæˆæŒ‡å®šè¯è¡¨çš„token
- beam-search multinomial sampling: bs æ”¹è¿›, å¼•å…¥**å¤šé¡¹å¼é‡‡æ ·**
- diverse beam-search decoding: **åˆ†ç»„** beam-search è§£ç æ–¹å¼
- assisted decoding **è¾…åŠ©è§£ç **: ç”¨å¦ä¸€ä¸ªæ¨¡å‹ï¼ˆç§°ä¸ºè¾…åŠ©æ¨¡å‹ï¼‰çš„è¾“å‡ºæ¥è¾…åŠ©ç”Ÿæˆæ–‡æœ¬ï¼Œä¸€èˆ¬æ˜¯**å€ŸåŠ©è¾ƒå°æ¨¡å‹æ¥åŠ é€Ÿç”Ÿæˆå€™é€‰ token**

### è´ªå¿ƒè§£ç  greedy decoding

è´ªå¿ƒè§£ç ç­–ç•¥æœ€ç»å…¸ã€æœ€åŸå§‹
- åœ¨`model.generate()`ä¸­ï¼Œå½“ `num_beams` ç­‰äº 1 ä¸” `do_sample` ç­‰äº False æ—¶è¿›å…¥æ­¤æ¨¡å¼
- ä¹Ÿå¯ç›´æ¥ä½¿ç”¨`model.greedy_search()`

æ¯æ­¥é€‰æ‹©é¢„æµ‹æ¦‚ç‡æœ€é«˜çš„tokenä½œä¸ºä¸‹ä¸€ä¸ªtokenï¼Œä»è€Œç”Ÿæˆæ–‡æœ¬ï¼Œå’Œä¹‹å‰çš„forwordæ˜¯ä¸€æ ·çš„

é—®é¢˜
- è¿™ç§æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬**å•ä¸€**å’Œ**å±€éƒ¨æœ€ä¼˜**ã€‚

æ³¨æ„
- æ­¤ç­–ç•¥ä¸èƒ½ç”¨ temperatureï¼Œtop_kï¼Œtop_p ç­‰æ”¹å˜ logits å‚æ•°ã€‚


#### ä¼˜ç¼ºç‚¹

è´ªå©ªæœç´¢ç¼ºç‚¹ï¼š
- å€¾å‘äºäº§ç”Ÿ**é‡å¤**åºåˆ—
- å¯èƒ½ä¼šé”™è¿‡æ•´ä½“æ¦‚ç‡è¾ƒé«˜çš„å•è¯åºåˆ—ï¼Œåªæ˜¯å› ä¸ºé«˜æ¦‚ç‡çš„å•è¯åˆšå¥½åœ¨ä½æ¦‚ç‡çš„å•è¯ä¹‹å‰ã€‚

è§£æ³•ï¼šé›†æŸæœç´¢


#### å®ç°1

ä¸¤ç§æ–¹æ¡ˆ
- model.generate()
- model.greedy_search()

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import torch

model_name = "llama-2-7b-hf" # ä½ æ¨¡å‹çš„ä½ç½®
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

text = "say hello to"
inputs = tokenizer(text, return_tensors="pt")
print(f"inputs:{inputs}")
# inputs:{'input_ids': tensor([[    1,  1827, 22172,   304]]), 'attention_mask': tensor([[1, 1, 1, 1]])}
input_ids = inputs["input_ids"].to("cuda")

# å¯åŠ¨æ–¹å¼â‘  model.generate
generation_output = model.generate(
    input_ids=input_ids,
    num_beams = 1,
    do_sample = False,
    return_dict_in_generate=True,
    max_new_tokens=3,
)
# å¯åŠ¨æ–¹å¼â‘¡, ç›´æ¥æŒ‡å®šä½¿ç”¨å…¶å‡½æ•° model.greedy_search
generation_output = model.greedy_search(
    input_ids=input_ids,
    num_beams = 1,
    do_sample = False,
    return_dict_in_generate=True,
    max_length = 7
)

# è§£ç 
print("query:", text)
for i, output_sequence in enumerate(generation_output.sequences):
    output_text = tokenizer.decode(output_sequence, skip_special_tokens=True)
    print(f"Generated sequence {i+1}: {output_text}")

# ç»“æœ
# query: say hello to
# Generated sequence 1: say hello to the newest
```

#### å®ç°2

```py
# ï¼ˆ1ï¼‰è´ªå©ªæœç´¢
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)
print(tokenizer.decode(output[0]))
# Transformers are the most popular toy line in the world,
# æ‰©å¤§é•¿åº¦
max_length = 128
input_txt = """In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n
"""
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)
print(tokenizer.decode(output_greedy[0]))
# Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
# In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.â€‹
```



### é›†æŸè§£ç  beam-search decoding


#### åŸç†

é›†æŸæœç´¢æ¯æ­¥è§£ç æ—¶, ä¸é€‰**æ¦‚ç‡æœ€é«˜**æ ‡è®°ï¼Œè€Œæ˜¯è®°å½•**å‰bä¸ª**æœ€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå…¶ä¸­, bè¢«ç§°ä¸º`æ³¢æŸ`æˆ–`è·¯å¾„ä¸ªæ•°`ã€‚
- ä¸‹ä¸€ç»„é›†æŸçš„é€‰æ‹©æ˜¯è€ƒè™‘ç°æœ‰é›†æŸçš„æ‰€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªæ ‡è®°çš„æ‰©å±•ï¼Œå¹¶é€‰æ‹©bä¸ªæœ€å¯èƒ½çš„æ‰©å±•ã€‚
- è¿™ä¸ªè¿‡ç¨‹é‡å¤è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°**æœ€å¤§é•¿åº¦**æˆ–**EOSæ ‡è®°**
- ç„¶åæ ¹æ®å¯¹æ•°æ¦‚ç‡å¯¹bä¸ªæ³¢æŸè¿›è¡Œæ’åºï¼Œé€‰æ‹©æœ€å¯èƒ½çš„åºåˆ—
- ![](https://pica.zhimg.com/80/v2-ef3522dfec91840dcad6642981722b18_1440w.webp?source=1940ef5c)

ä¸ºä»€ä¹ˆç”¨`å¯¹æ•°æ¦‚ç‡`è€Œä¸æ˜¯`æ¡ä»¶æ¦‚ç‡`å¯¹åºåˆ—è¿›è¡Œè¯„åˆ†ï¼Ÿ
- è®¡ç®—ä¸€ä¸ªåºåˆ—çš„æ€»ä½“æ¦‚ç‡ `P(y1ï¼Œy2ï¼Œ...ï¼Œyt|x)` æ¶‰åŠè®¡ç®—æ¡ä»¶æ¦‚ç‡ `P(yt|y < t,x)` çš„ä¹˜ç§¯ã€‚ç”±äºæ¯ä¸ªæ¡ä»¶æ¦‚ç‡é€šå¸¸æ˜¯ `[0ï¼Œ1]` èŒƒå›´å†…çš„å°æ•°å­—ï¼Œå–ä¹˜ç§¯ä¼šå¯¼è‡´æ€»æ¦‚ç‡å¾ˆå®¹æ˜“å‡ºç°**ä¸‹æº¢**ã€‚ä¸èƒ½å†ç²¾ç¡®åœ°è¡¨ç¤ºè®¡ç®—çš„ç»“æœã€‚
- ![](https://pic1.zhimg.com/80/v2-06d671883015295f2a493fb4f550f897_1440w.webp?source=1940ef5c)
- äºæ˜¯ï¼Œä½¿ç”¨`å¯¹æ•°æ¦‚ç‡`æ›¿æ¢`æ¡ä»¶æ¦‚ç‡`


BSæµç¨‹ï¼š
- åˆå§‹åŒ–ï¼šè®¾å®šä¸€ä¸ªå®½åº¦å‚æ•° beam widthï¼Œè¡¨ç¤ºæ¯æ­¥ä¿ç•™çš„æœ€ä¼˜å€™é€‰è§£çš„æ•°é‡ã€‚
- é€’å½’è¿‡ç¨‹ï¼šä»èµ·å§‹çŠ¶æ€å¼€å§‹ï¼Œæ¨¡å‹ä¼šé¢„æµ‹ç¬¬ä¸€ä¸ªè¯è¯­çš„æ‰€æœ‰å¯èƒ½é€‰é¡¹ï¼Œå¹¶æ ¹æ®å®ƒä»¬çš„æ¦‚ç‡ä¿ç•™å‰beam widthä¸ªæ¦‚ç‡æœ€é«˜çš„é€‰é¡¹ä½œä¸ºå€™é€‰è·¯å¾„ã€‚
- æ‰©å±•è·¯å¾„ï¼šå¯¹æ¯ä¸ªä¿ç•™ä¸‹æ¥çš„å€™é€‰è·¯å¾„ï¼Œæ¨¡å‹æ¥ç€é¢„æµ‹ç¬¬äºŒä¸ªè¯è¯­ï¼Œå°†å½“å‰è¯è¯­æ·»åŠ åˆ°ä¹‹å‰è·¯å¾„ä¸Šï¼Œå¹¶è®¡ç®—æ–°çš„å®Œæ•´è·¯å¾„çš„æ¦‚ç‡ã€‚å†æ¬¡ä¿ç•™æ¦‚ç‡æœ€é«˜çš„beam widthæ¡è·¯å¾„ã€‚
- è¿­ä»£æ±‚è§£ï¼šé‡å¤æ­¥éª¤3çš„è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°ç»ˆæ­¢æ¡ä»¶ï¼ˆå¦‚é‡åˆ°ç»“æŸç¬¦å·æˆ–è€…è¾¾åˆ°é¢„è®¾çš„æœ€å¤§é•¿åº¦ï¼‰ã€‚
- æœ€åï¼ŒBeam Searchè¿”å›çš„æ˜¯æ•´ä¸ªæœç´¢è¿‡ç¨‹ä¸­æ‰¾åˆ°çš„æœ€é«˜æ¦‚ç‡è·¯å¾„ä½œä¸ºæœ€ç»ˆè¾“å‡ºåºåˆ—ã€‚

#### BS ä¸ ç›´æ¥é‡‡æ ·

ä¸`ç›´æ¥é‡‡æ ·`ï¼ˆSamplingï¼‰çš„åŒºåˆ«ï¼š

Beam Searchï¼š
- ä¸€ç§**ç¡®å®šæ€§**ç­–ç•¥ï¼Œæ€»æ˜¯é€‰æ‹©**æ¦‚ç‡æœ€é«˜**çš„è‹¥å¹²é€‰é¡¹ç»§ç»­è¿›è¡Œä¸‹ä¸€è½®ç”Ÿæˆã€‚
- ä¿è¯ç”Ÿæˆç»“æœå…·æœ‰è¾ƒé«˜çš„æ¦‚ç‡è´¨é‡ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²ä¸€å®šçš„**å¤šæ ·æ€§**ï¼Œå› ä¸ºä¸æ˜¯æ‰€æœ‰ä½æ¦‚ç‡çš„åºåˆ—éƒ½è¢«è€ƒè™‘ã€‚
- å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆäºè®­ç»ƒæ•°æ®ä¸­å‡ºç°é¢‘ç‡è¾ƒé«˜çš„æ¨¡å¼ï¼Œäº§ç”Ÿâ€œåƒµåŒ–â€æˆ–â€œæœºæ¢°â€çš„è¾“å‡ºã€‚

ç›´æ¥é‡‡æ ·ï¼ˆRandom Sampling æˆ– Top-k Sampling ç­‰ï¼‰ï¼š
- ä¸€ç§**éšæœº**ç­–ç•¥ï¼Œæ¯æ¬¡ç”Ÿæˆæ–°è¯æ—¶ï¼Œå¯ä»¥æŒ‰ç…§è¯æ±‡è¡¨ä¸­æ¯ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œ**éšæœºæŠ½æ ·**ã€‚
- é‡‡æ ·æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆæ›´åŠ **å¤šæ ·åŒ–**çš„è¾“å‡ºï¼Œæ›´æœ‰å¯èƒ½æ¢ç´¢åˆ°æ–°é¢–å’Œæœªè§çš„åºåˆ—ç»„åˆï¼Œæœ‰åŠ©äºè§£å†³Beam Searchå¯èƒ½å¯¼è‡´çš„è¿‡äºä¿å®ˆçš„é—®é¢˜ã€‚
- ç›´æ¥é‡‡æ ·çš„**ä¸ç¡®å®šæ€§è¾ƒå¤§**ï¼Œç”Ÿæˆç»“æœä¸ä¸€å®šæ˜¯å…¨å±€æœ€ä¼˜è§£ï¼Œè€Œä¸”å¯¹äºè¾ƒå·®çš„æ¦‚ç‡åˆ†å¸ƒå¯èƒ½å‡ºç°ç”Ÿæˆç»“æœè´¨é‡è¾ƒä½çš„æƒ…å†µã€‚

æ€»ç»“
- Beam Search æ—¨åœ¨å¯»æ‰¾**æœ€å¤§æ¦‚ç‡è·¯å¾„**ï¼Œç¡®ä¿ç”Ÿæˆç»“æœçš„åˆç†æ€§å’Œå‡†ç¡®æ€§
- è€Œ**ç›´æ¥é‡‡æ ·**åˆ™é€šè¿‡å¼•å…¥éšæœºæ€§æ¥å¢å¼ºè¾“å‡ºçš„**å¤šæ ·æ€§**å’Œ**åˆ›é€ æ€§**ï¼Œä¸¤è€…åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥æ ¹æ®éœ€æ±‚æƒè¡¡ç²¾åº¦å’Œå¤šæ ·æ€§æ¥è¿›è¡Œé€‰æ‹©ã€‚


#### BS ä¼˜ç¼ºç‚¹

ä¼˜ç‚¹ï¼š
- ç”Ÿæˆ**å¤šæ ·æ€§**ï¼š é€šè¿‡å¢åŠ num_beamsæŸå®½ï¼ŒæŸæœç´¢å¯ä»¥ä¿ç•™æ›´å¤šçš„å€™é€‰åºåˆ—ï¼Œä»è€Œç”Ÿæˆæ›´å¤šæ ·åŒ–çš„ç»“æœã€‚
- æ‰¾åˆ°**è¾ƒä¼˜**è§£ï¼š å¢åŠ num_beamsæŸå®½æœ‰åŠ©äºä¿ç•™æ›´å¤šå¯èƒ½çš„å€™é€‰åºåˆ—ï¼Œä»è€Œæ›´æœ‰å¯èƒ½æ‰¾åˆ°æ›´ä¼˜çš„è§£ç ç»“æœï¼Œè¿™åœ¨ç”Ÿæˆä»»åŠ¡ä¸­æœ‰åŠ©äºé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜è§£
- æ§åˆ¶è¾“å‡º**æ•°é‡**ï¼š é€šè¿‡è°ƒæ•´num_beamsæŸå®½ï¼Œå¯ä»¥ç²¾ç¡®æ§åˆ¶ç”Ÿæˆçš„å€™é€‰åºåˆ—æ•°é‡ï¼Œä»è€Œå¹³è¡¡ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§å’Œæ•°é‡ã€‚

ç¼ºç‚¹ï¼š
- è®¡ç®—**å¤æ‚åº¦**ï¼š éšç€num_beamsæŸå®½çš„å¢åŠ ï¼Œè®¡ç®—å¤æ‚åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œè¾ƒå¤§çš„æŸå®½ä¼šå¯¼è‡´è§£ç è¿‡ç¨‹å˜å¾—æ›´åŠ è€—æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šã€‚
- å¿½ç•¥æ¦‚ç‡è¾ƒä½çš„åºåˆ—ï¼š å¢åŠ num_beamsæŸå®½å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›ä½æ¦‚ç‡çš„å€™é€‰åºåˆ—è¢«å¿½ç•¥ï¼Œå› ä¸ºæœç´¢è¿‡ç¨‹å€¾å‘äºé›†ä¸­åœ¨æ¦‚ç‡è¾ƒé«˜çš„è·¯å¾„ä¸Šï¼Œä»è€Œå¯èƒ½é”™è¿‡ä¸€äº›æ½œåœ¨çš„ä¼˜è´¨è§£ã€‚
- ç¼ºä¹å¤šæ ·æ€§ï¼š å°½ç®¡å¢åŠ num_beamsæŸå®½å¯ä»¥å¢åŠ ç”Ÿæˆç»“æœçš„å¤šæ ·æ€§ï¼Œä½†æŸæœç´¢ä»ç„¶å¯èƒ½å¯¼è‡´ç”Ÿæˆçš„ç»“æœè¿‡äºç›¸ä¼¼ï¼Œå› ä¸ºå®ƒå€¾å‘äºé€‰æ‹©æ¦‚ç‡è¾ƒé«˜çš„è·¯å¾„ã€‚


#### å®šä¹‰

æ­¥éª¤
- å®šä¹‰ä¸€ä¸ª BeamSearchNode ç±»
- ç„¶åç»™å‡ºæ¥ä¸‹æ¥ç”Ÿæˆtokençš„æ¦‚ç‡ï¼Œç®€å•èµ·è§ç»™ä¸€ä¸ªå›ºå®šçš„æ¦‚ç‡

```py
class BeamSearchNode:
    def __init__(self, sequence, score):
        self.sequence = sequence  # ç”Ÿæˆçš„åºåˆ—
        self.score = score  # åˆ†æ•°ï¼ˆæ¦‚ç‡ï¼‰
# ç¤ºä¾‹ï¼šä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡å‡½æ•°ï¼Œç®€å•ä½¿ç”¨å›ºå®šæ¦‚ç‡
def simple_next_word_probs(sequence):
    if sequence[-1] == "<end>":
        return {}
    return {"apple": 0.3, "like": 0.35, "peach": 0.2, "banana": 0.15}

def beam_search(initial_sequence, next_word_probs_func, num_beams, max_sequence_length):
    # åˆå§‹åŒ–åˆå§‹èŠ‚ç‚¹ï¼Œä¸”åˆ†æ•°ä¸º1
    initial_node = BeamSearchNode(sequence=initial_sequence, score=1.0)
    candidates = [initial_node]

    final_candidates = []  # æœ€ç»ˆçš„å€™é€‰åºåˆ—
    # åªè¦å€™é€‰èŠ‚ç‚¹åˆ—è¡¨ä¸ä¸ºç©ºï¼Œä¸” final_candidates ä¸­çš„å€™é€‰èŠ‚ç‚¹æ•°é‡è¿˜æ²¡æœ‰è¾¾åˆ°æŒ‡å®šçš„æŸå®½åº¦ï¼Œå°±ç»§ç»­è¿›è¡Œæœç´¢
    while candidates and len(final_candidates) < num_beams:
        # å€™é€‰èŠ‚ç‚¹æ’åº
        candidates.sort(key=lambda x: -x.score)
        current_node = candidates.pop(0)
        # å½“èŠ‚ç‚¹åºåˆ—æœ«å°¾ç”Ÿæˆç»“æŸç¬¦å·ï¼ˆå¦‚"<end>"ï¼‰ï¼Œæˆ–è€…å½“ç”Ÿæˆçš„åºåˆ—é•¿åº¦è¾¾åˆ°æœ€å¤§é™åˆ¶æ—¶ç»ˆæ­¢èŠ‚ç‚¹çš„æ‰©å±•
        if current_node.sequence[-1] == "<end>" or len(current_node.sequence) >= max_sequence_length:
            final_candidates.append(current_node)
        else:
            # è·å–ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡ï¼Œæˆ‘ä»¬çš„ä¾‹å­è¿”å›çš„æ˜¯å›ºå®šçš„æ¦‚ç‡
            next_words_probs = next_word_probs_func(current_node.sequence) 
            # ç”Ÿæˆæ–°çš„å€™é€‰åºåˆ—ï¼Œå¹¶è®¡ç®—åˆ†æ•°           
            for next_word, next_word_prob in next_words_probs.items():
                new_sequence = current_node.sequence + [next_word]
                new_score = current_node.score * next_word_prob
                new_node = BeamSearchNode(sequence=new_sequence, score=new_score)
                candidates.append(new_node)

    return [candidate.sequence for candidate in final_candidates]
```

ä½¿ç”¨

```py
initial_sequence = ["<start>", "I"]
num_beams = 3
max_sequence_length = 3
result = beam_search(initial_sequence, simple_next_word_probs, num_beams, max_sequence_length)

for idx, sequence in enumerate(result):
    print(f"Sentence {idx + 1}: {' '.join(sequence)}")
```

#### å®ç°


beam search é›†æŸè§£ç ç­–ç•¥
- åœ¨ `model.generate()` ä¸­æ˜¯å½“ `num_beams` å¤§äº 1 ä¸” `do_sample` ç­‰äº False æ—¶ä½¿ç”¨
- ä¹Ÿå¯è°ƒç”¨ `model.beam_search()` æ¥å®ç°



```py
import torch.nn.functional as F
# å¯¹æ•°æ¦‚ç‡
def log_probs_from_logits(logits, labels):
    logp = F.log_softmax(logits, dim=-1)
    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)
    return logp_label
# åºåˆ—æ€»å¯¹æ•°æ¦‚ç‡
def sequence_logprob(model, labels, input_len=0):
    with torch.no_grad():
        output = model(labels)
        log_probs = log_probs_from_logits(output.logits[:, :-1, :], labels[:, 1:]) # ä¸ç®—é¦–å°¾æ ‡è®°ï¼Œéæ¨¡å‹ç”Ÿæˆ
        # åªéœ€è¦å°†æ¯ä¸ªæ ‡è®°çš„å¯¹æ•°æ¦‚ç‡ç›¸åŠ 
        seq_log_prob = torch.sum(log_probs[:, input_len:])
    return seq_log_prob.cpu().numpy()
# è°ƒç”¨
logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))
print(tokenizer.decode(output_greedy[0]))
print(f"\nlog-prob: {logp:.2f}")
# beam search, 5ä¸ª
output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)
logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))
print(tokenizer.decode(output_beam[0]))
print(f"\nlog-prob: {logp:.2f}")
```

æ³¢æŸè¶Šå¤šï¼Œå¾—åˆ°çš„ç»“æœå°±è¶Šå¥½ï¼›ç„¶è€Œï¼Œç”Ÿæˆè¿‡ç¨‹ä¼šå˜å¾—æ›´æ…¢

ç”¨é›†æŸæœç´¢å¾—åˆ°çš„å¯¹æ•°æ¦‚ç‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰æ¯”ç”¨ç®€å•çš„è´ªå©ªè§£ç å¾—åˆ°çš„è¦å¥½ã€‚
- ç„¶è€Œï¼Œé›†æŸæœç´¢ä¹Ÿå—åˆ°é‡å¤æ–‡æœ¬çš„å½±å“ã€‚

ä¸€ä¸ªè§£å†³æ–¹æ³•
- <span style='color:blue'>ç”¨ no_repeat_ngram_size å‚æ•°æ–½åŠ ä¸€ä¸ª n-gramæƒ©ç½š</span>ï¼Œè·Ÿè¸ªå“ªäº›n-gramå·²ç»è¢«çœ‹åˆ°ï¼Œå¹¶å°†ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡è®¾ç½®ä¸ºé›¶ï¼Œå¦‚æœå®ƒå°†äº§ç”Ÿä¸€ä¸ªä»¥å‰çœ‹åˆ°çš„n-gram

```py
output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2) 
logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0])) 
print(tokenizer.decode(output_beam[0])) 
print(f"\nlog-prob: {logp:.2f}")
```

åœæ­¢é‡å¤åï¼Œå°½ç®¡äº§ç”Ÿäº†è¾ƒä½çš„åˆ†æ•°ï¼Œä½†æ–‡æœ¬ä»ç„¶æ˜¯è¿è´¯çš„ã€‚

å¸¦n-gramæƒ©ç½šçš„é›†æŸæœç´¢æ˜¯ä¸€ç§å¾ˆå¥½çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨å…³æ³¨**é«˜æ¦‚ç‡æ ‡è®°**ï¼ˆç”¨æŸæœç´¢ï¼‰å’Œ**å‡å°‘é‡å¤**ï¼ˆç”¨n-gramæƒ©ç½šï¼‰ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ª**å¹³è¡¡ç‚¹**
- é€šå¸¸ç”¨äºæ€»ç»“æˆ–æœºå™¨ç¿»è¯‘ç­‰äº‹å®æ­£ç¡®æ€§å¾ˆé‡è¦çš„åº”ç”¨ä¸­ã€‚å½“äº‹å®çš„æ­£ç¡®æ€§ä¸å¦‚ç”Ÿæˆçš„è¾“å‡ºçš„å¤šæ ·æ€§é‡è¦æ—¶ï¼Œä¾‹å¦‚åœ¨å¼€æ”¾é¢†åŸŸçš„é—²èŠæˆ–æ•…äº‹ç”Ÿæˆä¸­ï¼Œå¦ä¸€ç§å‡å°‘é‡å¤åŒæ—¶æé«˜å¤šæ ·æ€§çš„æ–¹æ³•æ˜¯ä½¿ç”¨æŠ½æ ·ã€‚


### å¤šæ ·å¼é‡‡æ · multinomial sampling

**å¤šé¡¹å¼é‡‡æ ·**è§£ç ç­–ç•¥
- åœ¨ `model.generate()` ä¸­ï¼Œå½“ `num_beams` ç­‰äº 1 ä¸” `do_sample` ç­‰äº True æ—¶è¿›å…¥æ­¤æ¨¡å¼
- ä¹Ÿå¯ç”¨ `model.sample()`

è¯¥ç­–ç•¥é€šè¿‡å„ç§æ”¹å˜ logits çš„å‚æ•°ï¼ˆmultinomial samplingï¼Œtemperatureï¼Œtop_kï¼Œtop_pç­‰ï¼‰ä»è€Œå®ç°ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ã€‚

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from transformers import (
    LogitsProcessorList,
    TopKLogitsWarper,
    TopPLogitsWarper,
    TemperatureLogitsWarper,
    )

import torch
model_name = "llama-2-7b-hf" # ä½ æ¨¡å‹çš„ä½ç½®
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

text = "say hello to"
inputs = tokenizer(text, return_tensors="pt")
print(f"inputs:{inputs}")
# inputs:{'input_ids': tensor([[    1,  1827, 22172,   304]]), 'attention_mask': tensor([[1, 1, 1, 1]])}
input_ids = inputs["input_ids"].to("cuda")
# æ–¹å¼ä¸€: model.generate
generation_output = model.generate(
    input_ids=input_ids,
    num_beams = 1,
    do_sample = True,
    temperature = 1.2,
    top_k = 100,
    top_p = 0.6,
    return_dict_in_generate=True,
    max_length=7,
)

# æ–¹å¼äºŒ: model.sample
# sampleå®ç°
logits_warper = LogitsProcessorList(
    [
      TopKLogitsWarper(100),
      TemperatureLogitsWarper(1.2),
      TopPLogitsWarper(0.6)
    ]
)
generation_output = model.sample(
    input_ids=input_ids,
    logits_warper=logits_warper,
    return_dict_in_generate=True,
    max_length=7,
)

print("query:", text)
for i, output_sequence in enumerate(generation_output.sequences):
    output_text = tokenizer.decode(output_sequence, skip_special_tokens=True)
    print(f"Generated sequence {i+1}: {output_text}")

# æ³¨æ„è¿™ç§æ–¹å¼æ¯æ¬¡ç»“æœéƒ½å¯èƒ½ä¸ä¸€æ ·
# query: say hello to
# Generated sequence 1: say hello to our new intern
```

### å¯¹æ¯”æœç´¢ contrastive search

**å¯¹æ¯”æœç´¢**ç­–ç•¥
- åœ¨ `model.generate()` ä¸­ï¼Œå½“ `penalty_alpha` å¤§äº 0 ä¸” `top_k`>1å¤§äº 1 æ—¶ä½¿ç”¨

è¿™æ˜¯ä¸€ç§å¼•å…¥**å¯¹æ¯”åº¦æƒ©ç½š**çš„æœç´¢æ–¹æ³•ï¼Œpenalty_alpha æƒ©ç½šå› å­å‚æ•°ï¼Œåªæœ‰åœ¨contrastive searchæ˜¯æ‰ä¼šç”¨åˆ°ã€‚

è¿™ç§è§£ç ç­–ç•¥æ˜¯ 2022å¹´ [A Contrastive Framework for Neural Text Generation]() è®ºæ–‡ä¸­æå‡ºæ¥çš„æ–¹æ³•

Huggingfaceå·²ç»å®ç°ï¼Œç®€å•åŸç†ï¼š
- ç”Ÿæˆçš„token ä»æ¨¡å‹é¢„æµ‹çš„æœ€ä½³å€™é€‰ï¼ˆtop kï¼‰ä¸­è€Œæ¥ï¼›
- åœ¨ç”Ÿæˆtokenæ—¶ï¼Œå½“å‰tokenåº”è¯¥èƒ½ä¸å‰é¢ç”Ÿæˆçš„å†…å®¹ä¿æŒ**å¯¹æ¯”æ€§**ï¼ˆæˆ–å·®å¼‚æ€§ï¼‰ï¼Œå…¶å®ç°å°±æ˜¯è‹¥å½“å‰ç”Ÿæˆçš„token ä¸ä¹‹å‰çš„åºåˆ—tokenç›¸ä¼¼åº¦å¾ˆå¤§ï¼Œå°±å‡å°‘å…¶æ•´ä½“æ¦‚ç‡å€¼ï¼Œè¿›è€Œå‡å°‘å®ƒè¢«è§£ç å‡ºæ¥çš„å¯èƒ½æ€§ï¼Œé¿å…é‡å¤è§£ç çš„é—®é¢˜ã€‚

æ ¸å¿ƒä»£ç 

```py
def ranking(context_hidden, next_hidden, next_top_k_ids, next_top_k_probs, alpha):
    '''
       è¯¥å‡½æ•°æ˜¯å®ç°Contrastive Searchä¸­next tokené¢„æµ‹ä¸­å€™é€‰tokençš„æ’åºåˆ†æ•°ï¼Œåˆ†æ•°æœ€å¤§å¯¹åº”tokenä¸ºè¾“å‡ºç»“æœ
        context_hidden: beam_width x context_len x embed_dim ,ç”¨äºè®¡ç®—ç›¸ä¼¼åº¦ï¼Œæ˜¯å…¬å¼ä¸­x_jé›†åˆè¡¨å¾å‘é‡
        next_hidden: beam_width x 1 x embed_dimï¼Œç”¨äºè®¡ç®—ç›¸ä¼¼åº¦ï¼Œæ˜¯å…¬å¼ä¸­å€™é€‰token v çš„è¡¨å¾å‘é‡
        next_top_k_ids: beam_width x 1ï¼Œè®°å½•å€™é€‰tokençš„ç¼–ç 
        next_top_k_probsï¼Œå€™é€‰tokençš„æ¨¡å‹é¢„æµ‹æ¦‚ç‡
        alphaï¼Œæƒ©ç½šå‚æ•°
    '''
    beam_width, context_len, embed_dim = context_hidden.size()
    assert next_hidden.size() == torch.Size([beam_width, 1, embed_dim])
    norm_context_hidden = context_hidden / context_hidden.norm(dim=2, keepdim=True) 
    norm_next_hidden = next_hidden / next_hidden.norm(dim=2, keepdim=True)
    cosine_matrix = torch.matmul(norm_context_hidden, norm_next_hidden.transpose(1,2)).squeeze(-1) #è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    assert cosine_matrix.size() == torch.Size([beam_width, context_len])
    scores, _ = torch.max(cosine_matrix, dim = -1) #è¾“å‡ºå…¬å¼ç¬¬äºŒé¡¹å€¼
    assert scores.size() == torch.Size([beam_width])
    next_top_k_probs = next_top_k_probs.view(-1)  #è¾“å‡ºå…¬å¼ç¬¬ä¸€é¡¹å€¼
    scores = (1.0 - alpha) * next_top_k_probs - alpha * scores  #å¯¹åº”å…¬å¼æ•´ä½“è®¡ç®—
    _, selected_idx = torch.topk(scores, k = 1)
    assert selected_idx.size() == torch.Size([1])
    selected_idx = selected_idx.unsqueeze(0)
    assert selected_idx.size() == torch.Size([1,1])
    next_id = torch.gather(next_top_k_ids, dim = 0, index=selected_idx)
    assert next_id.size() == torch.Size([1,1])
    return next_id
```

ä½¿ç”¨æ–¹æ³•

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import torch
model_name = "llama-2-7b-hf" # ä½ æ¨¡å‹çš„ä½ç½®
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)


text = "say hello to"
inputs = tokenizer(text, return_tensors="pt")
print(f"inputs:{inputs}")
input_ids = inputs["input_ids"].to("cuda")

# (1) model.generate
generation_output = model.generate(
    input_ids=input_ids,
    penalty_alpha = 0.5,
    top_k = 30,
    return_dict_in_generate=True,
    max_new_tokens=3,
)

# (2) ç›´æ¥ä½¿ç”¨å…¶å‡½æ•° model.contrastive_search
generation_output = model.contrastive_search(
    input_ids=input_ids,
    penalty_alpha = 0.5,
    top_k = 30,
    return_dict_in_generate=True,
    max_new_tokens=3,
)

print("query:", text)
for i, output_sequence in enumerate(generation_output.sequences):
    output_text = tokenizer.decode(output_sequence, skip_special_tokens=True)
    print(f"Generated sequence {i+1}: {output_text}")

# ç»“æœ
# inputs:{'input_ids': tensor([[    1,  1827, 22172,   304]]), 'attention_mask': tensor([[1, 1, 1, 1]])}
# query: say hello to
# Generated sequence 1: say hello to 20
```

### è¾…åŠ©è§£ç  Assisted decoding

**è¾…åŠ©è§£ç **ï¼Œç”¨å¦ä¸€ä¸ªæ¨¡å‹ï¼ˆç§°ä¸ºè¾…åŠ©æ¨¡å‹ï¼‰çš„è¾“å‡ºæ¥è¾…åŠ©ç”Ÿæˆæ–‡æœ¬ï¼Œä¸€èˆ¬æ˜¯**å€ŸåŠ©è¾ƒå°æ¨¡å‹æ¥åŠ é€Ÿç”Ÿæˆå€™é€‰ token**
- è¾…åŠ©æ¨¡å‹å¿…é¡»å…·æœ‰ä¸ç›®æ ‡æ¨¡å‹å®Œå…¨ç›¸åŒçš„**åˆ†è¯å™¨**ï¼ˆtokenizerï¼‰


å±äºæ¨æµ‹è§£ç çš„ä¸€ç§å®ç°, è¯¦è§ç«™å†…ä¸“é¢˜ [LLMæ¨ç†åŠ é€Ÿä¸­çš„æ¨æµ‹è§£ç ](llm_opt)


#### å®ç°

ç®€å•å®ç°ï¼Œé€šè¿‡llama7Bè¾…åŠ©ç”Ÿæˆllama13Bï¼Œä¸€èˆ¬æ¥è¯´è¾…åŠ©æ¨¡å‹è¦å¾ˆå°ï¼Œè¿™é‡Œåªæ˜¯ç®€å•å®éªŒï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import torch
model_name = "llama-2-13b-hf" # ä½ è‡ªå·±æ¨¡å‹çš„ä½ç½®
assistant_model_name = "llama-2-7b-hf" # ä½ è‡ªå·±æ¨¡å‹çš„ä½ç½®
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
assistant_model = AutoModelForCausalLM.from_pretrained(assistant_model_name, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

text = "say hello to"
inputs = tokenizer(text, return_tensors="pt")
print(f"inputs:{inputs}")
# inputs:{'input_ids': tensor([[    1,  1827, 22172,   304]]), 'attention_mask': tensor([[1, 1, 1, 1]])}
input_ids = inputs["input_ids"].to("cuda")

generation_output = model.generate(
    assistant_model=assistant_model,
    input_ids=input_ids,
    num_beams = 1,
    do_sample = False,
    return_dict_in_generate=True,
    max_length=7,
)

print("query:", text)
for i, output_sequence in enumerate(generation_output.sequences):
    output_text = tokenizer.decode(output_sequence, skip_special_tokens=True)
    print(f"Generated sequence {i+1}: {output_text}")

# ç»“æœ
# query: say hello to
# Generated sequence 1: say hello to the newest
```



### Beam Search æ”¹è¿›

Beam Search è™½ç„¶æ¯”è´ªå¿ƒæœ‰æ‰€æ”¹è¿›ï¼Œä½†è¿˜æ˜¯ä¼šç”Ÿæˆ<span style='color:red'>ç©ºæ´ã€é‡å¤ã€å‰åçŸ›ç›¾</span>çš„æ–‡æœ¬ã€‚
- è¯•å›¾æœ€å¤§åŒ–åºåˆ—æ¡ä»¶æ¦‚ç‡çš„è§£ç ç­–ç•¥ä»æ ¹ä¸Šå°±æœ‰é—®é¢˜

äººç±»é€‰æ‹©çš„è¯ï¼ˆæ©™çº¿ï¼‰å¹¶ä¸æ˜¯åƒæœºå™¨é€‰æ‹©çš„ï¼ˆè“çº¿ï¼‰é‚£æ ·æ€»æ˜¯é‚£äº›æ¡ä»¶æ¦‚ç‡æœ€å¤§çš„è¯ã€‚
- æ€»æ˜¯é€‰æ‹©æ¦‚ç‡å¤§çš„è¯ä¼šå‘ç”Ÿæ­£åé¦ˆä»è€Œé™·å…¥é‡å¤ï¼Œä»ç”Ÿæˆçš„ç»“æœä¹Ÿå¯ä»¥çœ‹å‡ºï¼Œæœºå™¨ç”Ÿæˆçš„ç»“æœæœ‰å¤§é‡é‡å¤ã€‚
- ![](http://www.wuyuanhao.com/wp-content/uploads/2020/03/probability.png)
- å‚è€ƒï¼š[è§£è¯»Beam Search (2/2)](http://www.wuyuanhao.com/2020/03/23/%e8%a7%a3%e8%af%bbbeam-search-2/)
- ä»¥ä¸‹æ€è·¯ä¸»è¦æºè‡ªICLR 2020è®ºæ–‡ï¼š[ã€ŠThe Curious Case of Neural Text Degenerationã€‹](https://arxiv.org/abs/1904.09751)

å¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬å‘¢ï¼Ÿç”¨åˆ°äº†ã€Œè§£ç å™¨ã€ï¼šä¸€ç§ç”¨äºä»è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„ç®—æ³•ã€‚

ç›®å‰ä¸»æµè§£ç å™¨æœ‰ä¸‰ç§ï¼šã€Œ`è´ªå©ªè§£ç `ã€ï¼ˆGreedy Decodingï¼‰ã€ã€Œ`é›†æŸæœç´¢`ã€ï¼ˆBeam Searchï¼‰ã€ã€Œ`åŸºäºæŠ½æ ·`ã€ï¼ˆSampling-basedï¼‰ã€‚
- ã€Œ`è´ªå©ªè§£ç `ã€è§£ç å™¨æ¯ä¸€æ­¥é‡‡ç”¨ argmax æ¥ç”Ÿæˆç›®æ ‡å¥å­ï¼Œç®€å•ç²—æš´ï¼Œä½†æ˜¯ç”±äºç¼ºä¹å›æº¯ï¼Œè¾“å‡ºå¯èƒ½ä¼šå¾ˆå·®ï¼Œä¼šå‡ºç°å¥å­ä¸åˆè¯­æ³•ã€ä¸è‡ªç„¶ã€æ²¡æœ‰æ„ä¹‰ç­‰é—®é¢˜ï¼Œä¸»è¦æ˜¯å› ä¸ºå¹¶ä¸æ˜¯æ¯ä¸€æ­¥è§£ç çš„æ¦‚ç‡æœ€å¤§ï¼Œæ•´ä½“ç»“æœçš„æ¦‚ç‡å°±ä¼šæœ€å¤§ï¼Œæ¦‚ç‡æœ€å¤§çš„å¥å­ï¼Œåœ¨å…¶ä¸­æŸä¸€æ­¥è§£ç çš„æ¦‚ç‡å¯èƒ½ä¸æ˜¯æœ€å¤§çš„ï¼›
- ã€Œ`é›†æŸæœç´¢`ã€è§£ç å™¨çš„æ¯ä¸€æ­¥éƒ½è¦è·Ÿè¸ª Beam Size ä¸ªæœ€æœ‰å¯èƒ½çš„éƒ¨åˆ†åºåˆ—ï¼Œä¸åªæ˜¯å¯»æ‰¾å½“å‰æ­¥æ¦‚ç‡æœ€å¤§çš„åºåˆ—ã€‚è¾¾åˆ°åœæ­¢æ¡ä»¶åï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åºåˆ—ï¼Œå½“ç„¶æœ€ç»ˆç»“æœä¹Ÿä¸ä¸€å®šæ˜¯**æœ€ä½³**åºåˆ—ï¼Œå› ä¸ºå­˜åœ¨ä¸€ä¸ªé€‰æ‹©èŒƒå›´ï¼Œæ‰€ä»¥ä¼˜äºè´ªå©ªè§£ç ã€‚
  - é›†æŸæœç´¢çš„å…³é”®æ˜¯ Beam Size çš„ç¡®å®šï¼Œå°çš„ Beam Size ä¼šæœ‰ä¸è´ªå©ªè§£ç ç›¸ä¼¼çš„é—®é¢˜ï¼ˆæé™ Beam Size = 1ï¼‰ï¼ŒBeam Size è¶Šå¤§è¡¨ç¤ºè€ƒè™‘çš„å‡è®¾è¶Šå¤šï¼Œè®¡ç®—é‡ä¹Ÿå°±è¶Šå¤§ï¼Œå¤§ Beam Size ä¼šä½¿è¾“å‡ºå¤ªè¿‡é€šç”¨ã€å¤ªä¸‡é‡‘æ²¹ï¼ŒèŠå¤©ä¼šå¾ˆæ— èŠã€è¢«ç»ˆç»“ï¼ŒåŒæ—¶å¯¹äºç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNMTï¼‰ï¼Œå¢å¤§ Beam Size è¿‡å¤šä¼šé™ä½ BLUE å¾—åˆ†ï¼›
- ã€Œ`åŸºäºæŠ½æ ·`ã€æ–¹æ³•åˆå¯ä»¥åˆ†ä¸ºã€Œ**çº¯é‡‡æ ·**ã€ï¼ˆPure Samplingï¼‰å’Œã€Œ**å¤´éƒ¨é‡‡æ ·**ã€ï¼ˆTop-n Samplingï¼‰
  - çº¯é‡‡æ ·æ˜¯åœ¨æ¯ä¸ªæ­¥éª¤ tï¼Œä»æ¦‚ç‡åˆ†å¸ƒ Pt ä¸­éšæœºé‡‡æ ·ä»¥è·å¾—ä¸‹ä¸€ä¸ªå•è¯ã€‚
  - å¤´éƒ¨é‡‡æ ·æ˜¯åœ¨æ¯ä¸ªæ­¥éª¤ tï¼Œä» Pt ä¸­éšæœºæŠ½æ ·ï¼Œä»…é™äºå‰ n ä¸ªæœ€å¯èƒ½çš„å•è¯
  - å½“ n = 1 æ—¶ï¼Œå³ä¸º`è´ªå©ªè§£ç `ï¼Œn = V æ—¶ï¼Œå³ä¸º`çº¯é‡‡æ ·`
  - å¢åŠ  n å¯ä»¥è·å¾—æ›´åŠ å¤šæ ·åŒ–ã€é£é™©æ›´é«˜çš„è¾“å‡ºï¼Œå‡å°‘ n å¯ä»¥è·å¾—æ›´åŠ å®‰å…¨ã€é€šç”¨çš„è¾“å‡ºã€‚

è¿™ä¸‰ç§è§£ç å™¨ä¸­
- `è´ªå©ªè§£ç `æ˜¯ä¸€ç§æ¯”è¾ƒç®€å•çš„æ–¹æ³•ï¼Œè¾“å‡ºè´¨é‡ä¹Ÿæ¯”è¾ƒä½
- `é›†æŸæœç´¢`è¾“å‡ºè´¨é‡æ¯”`è´ªå©ªè§£ç `æ›´é«˜ï¼Œä½†æ˜¯å¦‚æœ beam size å¤ªå¤§ï¼Œå°†è¿”å›ä¸åˆé€‚çš„è¾“å‡º
- åŸºäºæŠ½æ ·çš„æ–¹æ³•å¯ä»¥è·å¾—æ›´å¤šçš„**å¤šæ ·æ€§**å’Œ**éšæœºæ€§**ï¼Œé€‚åˆå¼€æ”¾å¼ã€åˆ›é€ æ€§çš„åˆ›ä½œï¼Œä¾‹å¦‚è¯—æ­Œæ•…äº‹ç”Ÿæˆï¼Œé€šè¿‡ top-n é‡‡æ ·å¯ä»¥æ§åˆ¶å¤šæ ·æ€§çš„å¼ºå¼±ã€‚



#### å—é™æŸæœç´¢ constrained beam-search decoding

**å—é™æŸæœç´¢è§£ç **
- ç”¨ `model.generate()`: å½“ `constraints` ä¸ä¸º None æˆ– `force_words_ids` ä¸ä¸º None æ—¶è¿›å…¥è¯¥æ¨¡å¼ï¼Œè€Œä¸”è¦æ±‚ `num_beams` è¦å¤§äº1ï¼ˆæœ¬è´¨è¿˜æ˜¯æŸæœç´¢ï¼‰ï¼Œ`do_sample` ä¸ºFalseï¼Œ`num_beam_groups`ä¸º1ï¼Œå¦åˆ™å°±ä¼šæŠ›å‡ºï¼š

```sh
"`num_beams` needs to be greater than 1 for constrained generation."
"`do_sample` needs to be false for constrained generation."
"`num_beam_groups` not supported yet for constrained generation."
```

è¿™ä¸ªè§£ç ç­–ç•¥æ ¸å¿ƒæ˜¯ beam searchï¼Œåªä¸è¿‡åœ¨searchä¸­åŠ å…¥**è‡ªå®šä¹‰è¯è¡¨**ï¼Œå¼ºåˆ¶å…¶ç”Ÿæˆæä¾›è¯è¡¨

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import torch

model_name = "llama-2-7b-hf" # ä½ æ¨¡å‹çš„ä½ç½®
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

text = "say hello to"
inputs = tokenizer(text, return_tensors="pt")
print(f"inputs:{inputs}")
# inputs:{'input_ids': tensor([[    1,  1827, 22172,   304]]), 'attention_mask': tensor([[1, 1, 1, 1]])}
input_ids = inputs["input_ids"].to("cuda")

# generateå®ç°
generation_output = model.generate(
    input_ids=input_ids,
    num_beams = 3,
    num_return_sequences=3,
    return_dict_in_generate=True,
    max_new_tokens=3,
)

print("query:", text)
for i, output_sequence in enumerate(generation_output.sequences):
    output_text = tokenizer.decode(output_sequence, skip_special_tokens=True)
    print(f"Generated sequence {i+1}: {output_text}")

# ç»“æœ
# query: say hello to
# Generated sequence 1: say hello to your new favorite
# Generated sequence 2: say hello to your new best
# Generated sequence 3: say hello to our newest
```

åŠ ä¸Šäº†çº¦æŸä¹‹åï¼Œå³ç»™å®šè¯è¡¨`["my"]`ï¼š

```py
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import torch
model_name = "llama-2-7b-hf" # ä½ æ¨¡å‹çš„ä½ç½®
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

text = "say hello to"
inputs = tokenizer(text, return_tensors="pt")
print(f"inputs:{inputs}")
input_ids = inputs["input_ids"].to("cuda")

force_words = ["my"] # è‡ªå®šä¹‰è¯è¡¨
force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids

generation_output = model.generate(
    input_ids=input_ids,
    force_words_ids = force_words_ids,
    num_beams = 3,
    num_return_sequences=3,
    return_dict_in_generate=True,
    max_new_tokens=3,
)

print("query:", text)
for i, output_sequence in enumerate(generation_output.sequences):
    output_text = tokenizer.decode(output_sequence, skip_special_tokens=True)
    print(f"Generated sequence {i+1}: {output_text}")

# ç»“æœ    
# inputs:{'input_ids': tensor([[    1,  1827, 22172,   304]]), 'attention_mask': tensor([[1, 1, 1, 1]])}
# query: say hello to
# Generated sequence 1: say hello to my little friend
# Generated sequence 2: say hello to your new favorite
# Generated sequence 3: say hello to your new my
```

ç»“æœå¾ˆæ˜æ˜¾ï¼Œç”Ÿæˆä¸­å‡ºç°äº†æˆ‘ä»¬çš„é™åˆ¶è¯è¡¨â€œmyâ€ã€‚


#### æŸæœç´¢+å¤šé¡¹å¼é‡‡æ · beam-search multinomial sampling

beam-search ä¸­åœ¨å®ç°é‡‡æ ·çš„æ–¹å¼
- åœ¨ model.generate() ä¸­ï¼Œå½“ num_beams å¤§äº 1 ä¸” do_sample ç­‰äº True æ—¶ä½¿ç”¨ï¼Œå…¶å®å°±æ˜¯åœ¨ beam search ä¸­åŠ å…¥äº†**å¤šæ ·åŒ–**é‡‡æ ·æ–¹å¼

#### DBS diverse beam-search decoding

åˆ†ç»„ beam-search è§£ç æ–¹å¼ï¼Œä¸Šè¿°åœ¨è§£é‡Šnum_beam_groups
- åœ¨ model.generate() ä¸­ï¼Œå½“ num_beams å¤§äº 1 ï¼Œ num_beam_groups å¤§äº 1 ï¼Œdiversity_penalty å¤§äº0ï¼Œdo_sample ç­‰äº False æ—¶è¿›å…¥æ­¤æ¨¡å¼ã€‚

beam searchç®—æ³•çš„æ”¹è¿›ï¼Œå«åš Diverse Beam Search (`DBS`)ï¼Œæ ¸å¿ƒå°±æ˜¯**åˆ†ç»„**æœºåˆ¶
- num_beams=2, num_beam_groups=2, åˆ†æˆ2ä¸ªç»„ï¼Œæ¯ä¸ªç»„é‡Œçš„beamå¯ä»¥ç›¸ä¼¼ï¼Œä½†ç»„å’Œç»„ä¹‹é—´è¦æœ‰è¶³å¤Ÿçš„å¤šæ ·æ€§ï¼Œå¼•å…¥äº†å¤šæ ·æ€§åˆ†æ•°

è®ºæ–‡å›¾è§£
- ![](https://pic1.zhimg.com/80/v2-86e01c651cac143e5e734df48f78dae0_1440w.webp)


## è§£ç å‚æ•°


å¤§æ¨¡å‹ç”Ÿæˆæ—¶çš„å‚æ•°è®¾ç½®å–å†³äºå…·ä½“ä»»åŠ¡å’Œæ¨¡å‹ã€‚

å¸¸è§å‚æ•°åŒ…æ‹¬ï¼š
- **æ¸©åº¦**ï¼ˆTemperatureï¼‰ï¼šæ§åˆ¶ç”Ÿæˆçš„æ–‡æœ¬çš„éšæœºæ€§ã€‚è¾ƒä½çš„æ¸©åº¦å€¼å°†å¯¼è‡´ç”Ÿæˆæ›´ä¿å®ˆçš„æ–‡æœ¬ï¼Œè€Œè¾ƒé«˜çš„æ¸©åº¦å€¼å°†å¯¼è‡´æ›´å¤šæ ·åŒ–çš„æ–‡æœ¬ã€‚
- **Top-ké‡‡æ ·**ï¼šä»…ä»æ¦‚ç‡æœ€é«˜çš„kä¸ªè¯ä¸­é‡‡æ ·ï¼Œä»¥å‡å°‘ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚
- **Top-pé‡‡æ ·**ï¼šä»ç´¯ç§¯æ¦‚ç‡è¶…è¿‡pçš„è¯ä¸­è¿›è¡Œé‡‡æ ·ï¼Œè¿™æœ‰åŠ©äºç”Ÿæˆæ›´ç›¸å…³çš„æ–‡æœ¬ã€‚
- **æœ€å¤§ç”Ÿæˆé•¿åº¦**ï¼šæŒ‡å®šç”Ÿæˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦ã€‚

ã€2024-4-26ã€‘[LLMå¤§è¯­è¨€æ¨¡å‹ä¹‹Generate/Inferenceï¼ˆç”Ÿæˆ/æ¨ç†ï¼‰ä¸­å‚æ•°ä¸è§£ç ç­–ç•¥åŸç†åŠå…¶ä»£ç å®ç°](https://zhuanlan.zhihu.com/p/653926703)

LLMå¤§è¯­è¨€æ¨¡å‹Generate/Inferenceç”Ÿæˆæˆ–è€…è¯´æ¨ç†æ—¶ï¼Œæœ‰å¾ˆå¤šçš„å‚æ•°å’Œè§£ç ç­–ç•¥

Huggingface å¸¸ç”¨å‚æ•°
1. temperature: æ¸©åº¦, é€šè¿‡è°ƒæ•´æ¨¡å‹è¾“å‡ºçš„logitsæ¦‚ç‡åˆ†å¸ƒæ¥æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„**éšæœºæ€§**å’Œ**å¤šæ ·æ€§**
2. top_p
3. top_k
4. repetition_penalty
5. no_repeat_ngram_size
6. do_sample
7. num_beams
8. num_beam_groups
9. diversity_penalty
10. length_penalty
11. use_cache

å…¶ä»–ç®€å•ã€å°‘è§å‚æ•°
1. num_return_sequences
2. max_length
3. max_new_tokens
4. min_length
5. min_new_tokens
6. early_stopping
7. bad_words_ids
8. force_words_ids
9. constraints

### BS

BS ç›¸å…³å‚æ•°

#### num_beams

num_beams ç”¨äº**æŸæœç´¢**ï¼ˆbeam searchï¼‰ç®—æ³•ï¼Œå…¶ç”¨é€”æ˜¯**æ§åˆ¶ç”Ÿæˆçš„å¤šä¸ªå€™é€‰å¥å­çš„æ•°é‡ï¼Œ**è¯¥

å‚æ•°æ§åˆ¶çš„æ˜¯æ¯ä¸ªç”Ÿæˆæ­¥è¦ä¿ç•™çš„ç”Ÿæˆç»“æœçš„æ•°é‡ï¼Œç”¨äºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¢åŠ å¤šæ ·æ€§æˆ–ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„ç»“æœã€‚

ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š
- åœ¨æ¯ä¸ªç”Ÿæˆæ­¥ï¼Œå¯¹äºå‰ä¸€ä¸ªç”Ÿæˆä¸­çš„æ‰€æœ‰ç”Ÿæˆç»“æœï¼Œåˆ†åˆ«åŸºäºæ¦‚ç‡ä¿ç•™å‰ k ä¸ªæœ€å¯èƒ½çš„ç»“æœï¼ˆk å³ num_beams å‚æ•°çš„å€¼ï¼‰ã€‚
- å°†æ‰€æœ‰æ‰©å±•åçš„ç”Ÿæˆç»“æœï¼ŒæŒ‰ç…§å…¶å¯¹åº”çš„æ¦‚ç‡åˆ†æ•°é‡æ–°è®¡ç®—åˆ†æ•°å¹¶è¿›è¡Œæ’åºï¼Œå¹¶ä¿ç•™å‰ k ä¸ªæœ€å¯èƒ½çš„ç»“æœã€‚
- å¦‚æœå·²ç»ç”Ÿæˆäº†ç»“æŸç¬¦ï¼Œåˆ™å°†å…¶å¯¹åº”çš„ç»“æœä¿ç•™ä¸‹æ¥ã€‚
- é‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´åˆ°ç”Ÿæˆæ‰€æœ‰çš„ç»“æœæˆ–è¾¾åˆ°æœ€å¤§é•¿åº¦ã€‚

#### num_beam_groups

num_beam_groups æ˜¯ DBS ç›¸å…³å‚æ•°, è¡¨ç¤ºåˆ†æˆå¤šå°‘ç»„


#### diversity_penalty

DBS ç›¸å…³å‚æ•°

diversity_penalty å¤šæ ·æ€§æƒ©ç½šå‚æ•°
- åªæœ‰å¯ç”¨â€œnum_beam_groupsâ€ï¼ˆ**ç»„æŸæœç´¢**ï¼‰æ—¶æ‰æœ‰æ•ˆï¼Œåœ¨è¿™äº›ç»„ä¹‹é—´åº”ç”¨å¤šæ ·æ€§æƒ©ç½šï¼Œä»¥ç¡®ä¿æ¯ä¸ªç»„ç”Ÿæˆçš„å†…å®¹å°½å¯èƒ½ä¸åŒã€‚

#### length_penalty


é•¿åº¦æƒ©ç½šå‚æ•° length_penalty ä¹Ÿç”¨äº**æŸæœç´¢**è¿‡ç¨‹ä¸­
- å€™é€‰åºåˆ—çš„å¾—åˆ†é€šè¿‡å¯¹æ•°ä¼¼ç„¶ä¼°è®¡è®¡ç®—å¾—åˆ°ï¼Œå³å¾—åˆ†æ˜¯**è´Ÿå¯¹æ•°ä¼¼ç„¶**ã€‚

length_penalty ä½œç”¨: å°†ç”Ÿæˆåºåˆ—çš„é•¿åº¦åº”ç”¨äºå¾—åˆ†çš„åˆ†æ¯ï¼Œä»è€Œå½±å“å€™é€‰åºåˆ—çš„å¾—åˆ†
- å½“ length_penalty > 1.0 æ—¶ï¼Œè¾ƒé•¿çš„åºåˆ—å¾—åˆ°æ›´å¤§çš„æƒ©ç½šï¼Œé¼“åŠ±ç”Ÿæˆè¾ƒçŸ­çš„åºåˆ—ï¼›
- å½“ length_penalty< 1.0 æ—¶ï¼Œè¾ƒçŸ­çš„åºåˆ—å¾—åˆ°æ›´å¤§çš„æƒ©ç½šï¼Œé¼“åŠ±ç”Ÿæˆè¾ƒé•¿çš„åºåˆ—
- é»˜è®¤ä¸º1ï¼Œä¸å—æƒ©ç½šã€‚


### éšæœºé‡‡æ ·(sampling)

- éšæœºé‡‡æ ·ï¼šæ ¹æ®è§£ç å™¨è¾“å‡ºçš„è¯å…¸ä¸­æ¯ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒéšæœºæŠ½æ ·ã€‚
  - ç›¸æ¯”äºæŒ‰æ¦‚ç‡â€œæå°–â€ï¼Œè¿™æ ·ä¼šå¢å¤§æ‰€é€‰è¯çš„èŒƒå›´ï¼Œå¼•å…¥æ›´å¤šçš„éšæœºæ€§ã€‚

è°·æ­Œå¼€æ”¾å¼èŠå¤©æœºå™¨äººMeenaé‡‡ç”¨çš„æ–¹å¼ï¼Œè®ºæ–‡ç»“è®ºæ˜¯ï¼š
- è¿™ç§éšæœºé‡‡æ ·çš„æ–¹æ³•è¿œå¥½äºBeam Searchã€‚
- ä½†è¿™å…¶å®ä¹Ÿæ˜¯æœ‰æ¡ä»¶çš„ï¼Œéšæœºé‡‡æ ·å®¹æ˜“äº§ç”Ÿå‰åä¸ä¸€è‡´çš„é—®é¢˜ã€‚
- è€Œåœ¨å¼€æ”¾é—²èŠé¢†åŸŸï¼Œç”Ÿæˆæ–‡æœ¬çš„ é•¿åº¦éƒ½æ¯”è¾ƒçŸ­ ï¼Œè¿™ç§é—®é¢˜å°±è¢«è‡ªç„¶çš„æ·¡åŒ–äº†ã€‚

#### do_sample

æ˜¯å¦å¯¹æ¨¡å‹è®¡ç®—å‡ºæ¥çš„æ¦‚ç‡è¿›è¡Œ**å¤šé¡¹å¼é‡‡æ ·**

`å¤šé¡¹å¼é‡‡æ ·`ï¼ˆMultinomial Samplingï¼‰æ˜¯ä¸€ç§ç”¨äºä»ä¸€ä¸ªå…·æœ‰å¤šä¸ªå¯èƒ½ç»“æœçš„ç¦»æ•£æ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡ŒéšæœºæŠ½æ ·çš„æ–¹æ³•

å¤šé¡¹å¼é‡‡æ ·çš„æ­¥éª¤å¦‚ä¸‹ï¼š
- é¦–å…ˆï¼Œæ ¹æ®æ¦‚ç‡åˆ†å¸ƒå¯¹åº”çš„æ¦‚ç‡ï¼Œä¸ºæ¯ä¸ªå¯èƒ½ç»“æœåˆ†é…ä¸€ä¸ªæŠ½æ ·æ¦‚ç‡ã€‚è¿™äº›æŠ½æ ·æ¦‚ç‡ä¹‹å’Œå¿…é¡»ä¸º1ã€‚
- ç„¶åï¼Œåœ¨è¿›è¡Œä¸€æ¬¡æŠ½æ ·æ—¶ï¼Œä¼šæ ¹æ®è¿™äº›æŠ½æ ·æ¦‚ç‡æ¥é€‰æ‹©ä¸€ä¸ªç»“æœã€‚å…·ä½“åœ°ï¼Œä¼šç”Ÿæˆä¸€ä¸ªéšæœºæ•°ï¼Œç„¶åæ ¹æ®æŠ½æ ·æ¦‚ç‡é€‰æ‹©ç»“æœã€‚æŠ½æ ·æ¦‚ç‡è¶Šé«˜çš„ç»“æœï¼Œè¢«é€‰ä¸­çš„æ¦‚ç‡ä¹Ÿå°±è¶Šå¤§ã€‚
- æœ€ç»ˆï¼Œè¢«é€‰ä¸­çš„ç»“æœå°±æ˜¯è¿™æ¬¡æŠ½æ ·çš„è¾“å‡ºã€‚

åœ¨å¤šé¡¹å¼é‡‡æ ·ä¸­ï¼Œæ¦‚ç‡é«˜çš„ç»“æœæ›´æœ‰å¯èƒ½è¢«é€‰ä¸­ï¼Œä½†ä¸åŒäºç¡®å®šæ€§çš„é€‰æ‹©ï¼Œæ¯ä¸ªç»“æœä»ç„¶æœ‰ä¸€å®šçš„æ¦‚ç‡è¢«é€‰ä¸­ã€‚è¿™ä½¿å¾—æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å…·æœ‰ä¸€å®šçš„éšæœºæ€§ï¼Œä½†åˆå—åˆ°æ¦‚ç‡çš„æ§åˆ¶ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´åŠ å¤šæ ·ä¸”ç¬¦åˆæ¦‚ç‡åˆ†å¸ƒçš„æ–‡æœ¬ã€‚

```py
import torch
probs = torch.tensor([[0.2559, 0.5154, 0.0571, 0.1716]])
next_token = torch.multinomial(probs, num_samples=1)
print("next_token:", next_token)
# ç»“æœ
next_token: tensor([[1]])
```

do_sample å‚æ•°é€šè¿‡å¤šæ ·å¼é‡‡æ ·ä¼šæœ‰ä¸€å®šçš„éšæœºæ€§ï¼Œè¿™ç§éšæœºæ€§å¯¼è‡´äº†ç”Ÿæˆçš„æ–‡æœ¬æ›´åŠ å¤šæ ·åŒ–ï¼Œå› ä¸ºæ¨¡å‹æœ‰æœºä¼šé€‰æ‹©æ¦‚ç‡è¾ƒä½ä½†ä»ç„¶å¯èƒ½çš„è¯ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥äº§ç”Ÿä¸°å¯Œã€æœ‰è¶£ã€åˆ›æ–°çš„æ–‡æœ¬ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²ä¸€äº›æ–‡æœ¬çš„å‡†ç¡®æ€§ã€‚

æ³¨
- do_sample=Falseï¼Œä¸è¿›è¡Œé‡‡æ ·ã€‚åœ¨Huggingfaceä¸­ï¼Œdo_sample å‚æ•°æœ‰æ›´é«˜çš„å«ä¹‰, å³åšä¸åšå¤šæ ·åŒ–é‡‡æ ·
- do_sample=Falseï¼Œtemperatureï¼Œtop_kï¼Œtop_p è¿™äº›å‚æ•°æ˜¯ä¸èƒ½å¤Ÿè¢«è®¾ç½®çš„ï¼Œåªæœ‰ do_sample=True æ—¶æ‰èƒ½å¤Ÿè¢«è®¾ç½®


### æ¸©åº¦ Temperature

Temperature Parameter è¶…å‚æ•°ç›´è¯‘ä¸ºâ€œ**æ¸©åº¦ç³»æ•°**â€ 
- å‚è€ƒ:[æ¸©åº¦ç³»æ•°Temperature Parameterçš„è®²äººè¯è§£é‡Š](https://zhuanlan.zhihu.com/p/544432496)

Temperature é‡‡æ ·å—**ç»Ÿè®¡çƒ­åŠ›å­¦**å¯å‘ï¼Œé«˜æ¸©æ„å‘³ç€æ›´å¯èƒ½é‡åˆ°ä½èƒ½æ€ã€‚
- å°†è®¡ç®—è¿‡ç¨‹çœ‹åšçƒ§æ°´ï¼Œæ¸©åº¦è¶Šé«˜ï¼Œæ°´æ²¸è…¾è¶Šå‰§çƒˆï¼Œç±»æ¯”**ä¿¡æ¯ç†µå¢å‡**

Temperature é‡‡æ ·ä¸­çš„æ¸©åº¦ä¸`ç»å°”å…¹æ›¼`åˆ†å¸ƒæœ‰å…³. 
- æ¦‚ç‡æ¨¡å‹ä¸­ï¼Œlogits æ‰®æ¼”ç€**èƒ½é‡**è§’è‰²ï¼Œé€šè¿‡å°† logits é™¤ä»¥æ¸©åº¦æ¥å®ç°**æ¸©åº¦é‡‡æ ·**ï¼Œç„¶åå°†å…¶è¾“å…¥ Softmax å¹¶è·å¾—é‡‡æ ·æ¦‚ç‡ã€‚
- æœ¬è´¨: åœ¨ Softmax å‡½æ•°ä¸Šæ·»åŠ äº†**æ¸©åº¦**ï¼ˆTï¼‰è¿™ä¸ªå‚æ•°ã€‚Logits æ ¹æ®æ¸©åº¦å€¼è¿›è¡Œç¼©æ”¾ï¼Œç„¶åä¼ é€’åˆ° Softmax å‡½æ•°ä»¥è®¡ç®—æ–°çš„æ¦‚ç‡åˆ†å¸ƒã€‚
- è¶Šä½æ¸©åº¦ä½¿æ¨¡å‹å¯¹å…¶é¦–é€‰è¶Š**æœ‰ä¿¡å¿ƒ**ï¼Œè€Œé«˜äº1çš„æ¸©åº¦ä¼š**é™ä½ä¿¡å¿ƒ**ã€‚
- 0æ¸©åº¦ç›¸å½“äº **argmax ä¼¼ç„¶**ï¼Œè€Œæ— é™æ¸©åº¦ç›¸å½“äº**å‡åŒ€é‡‡æ ·**ã€‚

ç‰¹æ€§
- æ¸©åº¦ç³»æ•°è¶Šå¤§ï¼Œç†µå°±è¶Šé«˜ï¼Œæ··ä¹±ç¨‹åº¦è¶Šé«˜ï¼Œé‚£ä¹ˆå‡½æ•°è¾“å‡ºçš„å„ç±»åˆ«æ¦‚ç‡å·®è·ä¼šè¶Šæ¥è¶Šå°ï¼ˆå› ä¸ºå·®è·è¶Šå°é‚£ä¹ˆçœ‹å‡ºæœ€ä¼˜ç»“æœä¹Ÿå°±è¶Šå›°éš¾ï¼Œå¯¹åº”äºç†µè¶Šé«˜ï¼‰ï¼Œæ›²çº¿ä¹Ÿä¼šæ„ˆå‘å¹³æ»‘ã€‚
- ç›¸åï¼Œæ¸©åº¦ç³»æ•°è¶Šå°ï¼Œå‡½æ•°æ›²çº¿ä¹Ÿä¼šæ„ˆå‘é™¡å³­ã€‚

â€œ**æˆ‘å–œæ¬¢æ¼‚äº®çš„___**â€ ä¾‹å­ä¸­ï¼Œåˆå§‹æ¸©åº¦ T=1 ï¼Œç›´è§‚çœ‹ä¸€ä¸‹ T å–ä¸åŒå€¼ä¸‹æ¦‚ç‡ä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼š
- ![](https://pic3.zhimg.com/80/v2-e1673506371968d79a2059575a39d426_1440w.webp)
- éšç€æ¸©åº¦çš„é™ä½ï¼Œæ¨¡å‹æ„ˆæ¥æ„ˆè¶Šå€¾å‘é€‰æ‹©â€å¥³å­©â€œï¼›
- éšç€æ¸©åº¦çš„å‡é«˜ï¼Œåˆ†å¸ƒå˜å¾—è¶Šæ¥è¶Šå‡åŒ€ã€‚
- å½“ T=50 æ—¶ï¼Œé€‰æ‹©â€è¥¿ç“œâ€œçš„æ¦‚ç‡å·²ç»ä¸é€‰æ‹©â€å¥³å­©â€œçš„æ¦‚ç‡ç›¸å·®æ— å‡ äº†ã€‚
  - ![](https://pic1.zhimg.com/80/v2-1e1ffb0ff8d227083f5b578ce28707d0_1440w.webp)

æ¸©åº¦ä¸æ¨¡å‹çš„â€œåˆ›é€ åŠ›â€æœ‰å…³?
- éä¹Ÿã€‚æ¸©åº¦åªæ˜¯è°ƒæ•´å•è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚
- å®è§‚æ•ˆæœ: **ä½æ¸©æ¨¡å‹æ›´å…·ç¡®å®šæ€§ï¼Œè€Œé«˜æ¸©ä¸é‚£ä¹ˆç¡®å®š**ã€‚

æ¸©åº¦ç³»æ•°å–å€¼è®¾è®¡ç±»æ¯”è‡ªä¿¡å¿ƒå¤§å°ï¼š
- æ¸©åº¦ç³»æ•°å¤§ï¼ˆæ›²çº¿å˜å¾—å¹³æ»‘ï¼ŒT>1ï¼‰: å¯¹äºç®—æ³•ç»“æœä¸è‡ªä¿¡ -- `çŸ¥è¯†è’¸é¦`
  - ä¸ç›¸ä¿¡å½“å‰çš„ç»“æœæ˜¯æœ€ä¼˜çš„ï¼Œé€šè¿‡æ·»åŠ å¤§çš„æ¸©åº¦ç³»æ•°ï¼Œå°† softmax è¾“å‡ºåçš„æ›²çº¿å˜å¾—å¹³æ»‘ï¼Œé‚£ä¹ˆç¨å¾®é™¡å³­çš„ç»“æœå’Œä¸é™¡å³­çš„ç»“æœæ‰€ä½“ç°å‡ºæ¥çš„æ•ˆæœæ˜¯å·®åˆ«ä¸å¤§ã€‚å› æ­¤æƒ³è¦æ˜ç¡®è·å¾—ç»“æœ, éœ€è¦è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒï¼Œç›´åˆ°æ¨¡å‹è®­ç»ƒå¾—åˆ°ä¸€ä¸ªéå¸¸é™¡å³­çš„è¾“å‡ºï¼Œç»è¿‡softmaxä¹‹åæ‰èƒ½è·å¾—ä¸€ä¸ªç›¸å¯¹é™¡å³­çš„ç»“æœã€‚
  - çŸ¥è¯†è’¸é¦æ˜¯ä¸ºäº†èŠ‚çœè®¡ç®—èµ„æºï¼Œå°†åŸæ¨¡å‹ä¸­æ¯”è¾ƒâ€œæ²¡ç”¨â€çš„å‚æ•°ç»™è’¸å‘æ‰ï¼Œæœ¬è´¨ä¸Šå°†åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„é‡é‡çº§æ¨¡å‹ä½œä¸º**æ•™å¸ˆ**ï¼Œç„¶åå–ç›¸å¯¹æ›´è½»é‡çš„æ¨¡å‹ä½œä¸º**å­¦ç”Ÿ**ï¼Œä»¤å­¦ç”Ÿè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒå°½å¯èƒ½çš„é€¼è¿‘æ•™å¸ˆè¾“å‡ºçš„åˆ†å¸ƒï¼Œä»è€Œè¾¾åˆ°çŸ¥è¯†æçº¯çš„ç›®çš„ã€‚
  - è’¸é¦æœ¬è´¨: è®©å­¦ç”Ÿç½‘ç»œå»å­¦ä¹ æ•™å¸ˆç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›. è®­ç»ƒå¥½çš„æ¨¡å‹æœ¬èº«ä¼šå‡ºç°è¿‡åº¦è‡ªä¿¡çš„é—®é¢˜ï¼Œsoftmaxè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒç†µå¾ˆå°ï¼Œtop kçš„ä¼˜åŠ¿è¿‡äºæ˜æ˜¾ã€‚å› æ­¤æ·»åŠ ä¸€ä¸ªå¤§çš„æ¸©åº¦ç³»æ•°ï¼Œæ¥ä»¤ç»“æœä¸é‚£ä¹ˆè‡ªä¿¡ï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬å¯¹å½“å‰ç»“æœä¸è‡ªä¿¡ï¼‰ã€‚å› æ­¤é€šè¿‡é™¤ä»¥ T>1 æ¥ä»¤åˆ†å¸ƒå˜å¾—å¹³æ»‘ï¼Œè¿›è€Œæ¥ä»¤å­¦ç”Ÿæ¨¡å‹å­¦åˆ°çš„ç»“æœæ›´åŠ å‡†ç¡®ã€‚
- æ¸©åº¦ç³»æ•°å°çš„æƒ…å†µï¼ˆæ›²çº¿å˜å¾—é™¡å³­ï¼ŒT<1ï¼‰: å¯¹äºå½“å‰æ¨¡å‹æ˜¯å¾ˆè‡ªä¿¡ -- `å¯¹æ¯”å­¦ä¹ `
  - æ¨¡å‹å˜å¾—æ›´åŠ æ•æ„Ÿ: softmax å¯¹ä¸Šä¸€æ­¥çš„è¾“å…¥å¾ˆ**æ•æ„Ÿ**ï¼Œç¨å¾®é™¡å³­çš„ç»“æœç»è¿‡æŸå¤±å‡½æ•°ä¹‹åå˜å¾—éå¸¸é™¡å³­ã€‚
  - æ·»åŠ å°çš„æ¸©åº¦ç³»æ•°æ¥çªå‡ºè®¡ç®—çš„ä¼˜åŠ¿ï¼Œå°±å¯ä»¥æœ‰æ•ˆåŠ å¿«æ¨¡å‹æ”¶æ•›é€Ÿåº¦ã€‚
  - å¯¹æ¯”å­¦ä¹ ä»æœ¬è´¨ä¸Šè®²ï¼Œä»¥è‡ªç›‘ç£å­¦ä¹ ä¸ºä¾‹ï¼Œæ›´å¤šçš„æ˜¯å¤„ç†æ ·æœ¬æ­£å¯¹ä¸è´Ÿå¯¹çš„é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯ä»¤æ­£å¯¹æ›´è¿‘ï¼Œè´Ÿå¯¹æ›´è¿œã€‚
  - å…¸å‹çš„NCEæŸå¤±, å­é¡¹éƒ½é™¤ä»¥ä¸€ä¸ªæ¸©åº¦ç³»æ•° T(å°äº1)ã€‚åŸå› ï¼šé¦–å…ˆå¯¹æ¯”å­¦ä¹ åº”ç”¨è¿™ç§æŸå¤±å½¢å¼æœ¬èº«å¯ä»¥æŒ–æ˜è´Ÿæ ·æœ¬ï¼Œç»è¿‡softmaxæ“ä½œåï¼Œä¼šç»™è·ç¦»æ›´è¿‘çš„è´Ÿæ ·æœ¬æ›´å¤šçš„æƒ©ç½šã€‚é‚£ä¹ˆä¸ºäº†æ§åˆ¶å¯¹å›°éš¾æ ·æœ¬çš„æƒ©ç½šç¨‹åº¦ï¼Œå½“ T è¶Šå°ï¼Œsoftmaxå°±ä¼šè¶Šé™¡å³­ï¼Œè¾“å‡ºå·®å¼‚å°±è¢«æ”¾å¾—è¶Šå¤§ï¼Œé‚£ä¹ˆå¯¹å›°éš¾è´Ÿæ ·æœ¬çš„æƒ©ç½šæ›´å¤§ï¼ˆlossæ›´å¤§ï¼‰[SLLç»¼è¿°](https://zhuanlan.zhihu.com/p/540791001)
  - ![](https://pic1.zhimg.com/80/v2-2f9832e66f9d53f2825abc0b1155ff88_1440w.webp)
- ![](https://pic3.zhimg.com/80/v2-ce0109d1d29668b06a4a2c18b74e5b12_1440w.webp)

æ³¨æ„: T ä¸èƒ½å¤ªå°! æ— ç›‘ç£å­¦ä¹ çš„å¯¹æ¯”å­¦ä¹ **å‡åŒ€æ€§-å®¹å¿æ€§**å›°å¢ƒï¼š
- â€œ**å‡åŒ€æ€§**â€ï¼šå°æ¸©åº¦ç³»æ•°æ›´å…³æ³¨äºå°†ä¸æœ¬æ ·æœ¬ç›¸ä¼¼çš„å›°éš¾æ ·æœ¬åˆ†å¼€ï¼Œå› æ­¤å¸Œæœ›å¾—åˆ°ä¸€ä¸ª**åˆ†å¸ƒå‡åŒ€**çš„è¡¨å¾ç©ºé—´ï¼Œä»è€Œä»¤è´Ÿå¯¹æ›´è¿œï¼ˆç»¼è¿°æåˆ°è¿™ç§è¡¨å¾æˆ–è®¸æ˜¯æˆåŠŸçš„å…³é”®ï¼‰ã€‚æ‰€ä»¥è¯´ <span style='color:blue'>T åº”å½“å°</span>ã€‚
- â€œ**å®¹å¿æ€§**â€ï¼šå›°éš¾æ ·æœ¬å¾€å¾€æ˜¯ä¸æœ¬æ ·æœ¬ç›¸ä¼¼ç¨‹åº¦è¾ƒé«˜çš„ï¼ŒåŒç±»åˆ«çš„ç‹—ï¼Œä½†æ˜¯è¨æ‘©è€¶å’Œå‰å¨ƒå¨ƒè¿™ä¸¤ç§ä¸åŒå®ä¾‹ã€‚å¾ˆå¤šå›°éš¾è´Ÿæ ·æœ¬å…¶å®æ˜¯æ½œåœ¨çš„æ­£æ ·æœ¬ï¼Œæ‰€ä»¥ä¸èƒ½è¿‡åº¦åœ°åˆ†å¼€â€œå›°éš¾æ ·æœ¬â€å¯¼è‡´ç ´åæ½œåœ¨è¯­ä¹‰ç»“æ„ã€‚æ‰€ä»¥è¯´ <span style='color:blue'>T ä¸èƒ½å¤ªå°</span>ã€‚

- é‡‡æ ·æ—¶æœ‰ä¸ªå¯æ§è¶…å‚æ•°ï¼Œç§°ä¸º**æ¸©åº¦**(temperature, T)ã€‚
  - æ¨¡å‹è’¸é¦é‡Œç”¨åˆ°
- è§£ç å™¨çš„è¾“å‡ºå±‚åé¢é€šå¸¸ä¼šè·Ÿä¸€ä¸ªsoftmaxå‡½æ•°æ¥å°†è¾“å‡ºæ¦‚ç‡å½’ä¸€åŒ–ï¼Œé€šè¿‡æ”¹å˜Tå¯ä»¥æ§åˆ¶æ¦‚ç‡çš„å½¢è²Œã€‚
- softmaxçš„å…¬å¼å¦‚ä¸‹
  - å½“Tå¤§çš„æ—¶å€™ï¼Œæ¦‚ç‡åˆ†å¸ƒè¶‹å‘å¹³å‡ï¼Œéšæœºæ€§å¢å¤§ï¼›
  - å½“Tå°çš„æ—¶å€™ï¼Œæ¦‚ç‡å¯†åº¦è¶‹å‘äºé›†ä¸­ï¼Œå³å¼ºè€…ä¿å¼ºï¼Œéšæœºæ€§é™ä½ï¼Œä¼šæ›´å¤šåœ°é‡‡æ ·å‡ºâ€œæ”¾ä¹‹å››æµ·è€Œçš†å‡†â€çš„è¯æ±‡ã€‚


å…¬å¼
- ![img](https://picx.zhimg.com/80/v2-85841701ef0074344a545b4ece6fc3e1_1440w.webp?source=1940ef5c)
- `|V|`è¡¨ç¤ºè¯æ±‡çš„cardinalityã€‚
- é€šè¿‡æ·»åŠ ä¸€ä¸ªæ¸©åº¦å‚æ•°Tæ¥è½»æ¾æ§åˆ¶è¾“å‡ºçš„**å¤šæ ·æ€§**ï¼Œè¯¥å‚æ•°åœ¨é‡‡å–softmaxä¹‹å‰é‡æ–°è°ƒæ•´å¯¹æ•°ï¼š
- ![img](https://picx.zhimg.com/80/v2-16883c3dda877b20a4b3269bccc37ffb_1440w.webp?source=1940ef5c)

é€šè¿‡è°ƒæ•´Tæ§åˆ¶æ¦‚ç‡åˆ†å¸ƒçš„å½¢çŠ¶ã€‚
- å½“ Tâ‰ª1 æ—¶ï¼Œåˆ†å¸ƒåœ¨åŸç‚¹å‘¨å›´å˜å¾—å°–é”ï¼Œç½•è§çš„æ ‡è®°è¢«å‹åˆ¶ã€‚
- å½“ Tâ‰«1 æ—¶ï¼Œåˆ†å¸ƒå˜å¾—å¹³ç¼“ï¼Œæ¯ä¸ªä»¤ç‰Œçš„å¯èƒ½æ€§ç›¸åŒã€‚

æ¸©åº¦å¯¹æ ‡è®°æ¦‚ç‡çš„å½±å“ã€‚
- å½“ temperatureâ†’0ï¼Œå°±å˜æˆ`greedy search`ï¼›
- å½“ temperatureâ†’âˆï¼Œå°±å˜æˆ`å‡åŒ€é‡‡æ ·`ï¼ˆuniform samplingï¼‰ã€‚
- ![img](https://picx.zhimg.com/80/v2-13462a3839b939f7a70ae0aaf80da28c_1440w.webp?source=1940ef5c)
- è¯¦è§è®ºæ–‡ï¼šThe Curious Case of Neural Text Degeneration

generate() å‡½æ•°ä¸­è®¾ç½®æ¸©åº¦å‚æ•°`temperature`,`top_k`ï¼Œä»¥T=2ä¸ºä¾‹è¿›è¡Œé‡‡æ ·

```py
import matplotlib.pyplot as plt
import numpy as np

def softmax(logits, T=1):
    e_x = np.exp(logits / T)
    return e_x / e_x.sum()

logits = np.exp(np.random.random(1000))
sorted_logits = np.sort(logits)[::-1]
x = np.arange(1000)

for T in [0.5, 1.0, 2.0]:
    plt.step(x, softmax(sorted_logits, T), label=f"T={T}")
plt.legend(loc="best")
plt.xlabel("Sorted token probabilities")
plt.ylabel("Probability")
plt.show()
```

è°ƒç”¨

```py
torch.manual_seed(42);
# é«˜æ¸©
output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)
# æ¸©åº¦é™ä¸‹æ¥
output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0)
print(tokenizer.decode(output_temp[0]))
```

é«˜æ¸©äº§ç”Ÿäº†å¤§éƒ¨åˆ†çš„èƒ¡è¨€ä¹±è¯­ï¼›
- é€šè¿‡è°ƒå¤§ç½•è§è¯æ±‡å‡ºç°çš„æ¦‚ç‡ï¼Œä½¿æ¨¡å‹äº§ç”Ÿäº†å¥‡æ€ªçš„è¯­æ³•å’Œç›¸å½“å¤šçš„ç”Ÿé€ è¯
- é™æ¸©åï¼Œæ›´æœ‰è¿è´¯æ€§

æ§åˆ¶æ ·æœ¬è´¨é‡(**ä¸€è‡´æ€§**å’Œ**å¤šæ ·æ€§**)çš„æ–¹æ³•, åœ¨**ä¸€è‡´æ€§**ï¼ˆä½æ¸©ï¼‰å’Œ**å¤šæ ·æ€§**ï¼ˆé«˜æ¸©ï¼‰ä¹‹é—´æ€»æœ‰ä¸€ä¸ªæƒè¡¡
- æ¸©åº¦
- æˆªæ–­è¯æ±‡çš„åˆ†å¸ƒ

éšç€æ¸©åº¦è‡ªç”±åœ°è°ƒæ•´å¤šæ ·æ€§ï¼Œåœ¨æ›´æœ‰é™çš„èŒƒå›´å†…ï¼Œæ’é™¤é‚£äº›åœ¨è¯­å¢ƒä¸­è¿‡äºå¥‡æ€ªçš„è¯ï¼ˆå³ä½æ¦‚ç‡è¯ï¼‰ã€‚æœ‰ä¸¤ç§ä¸»è¦çš„æ–¹æ³•ï¼š`top-k`å’Œ`nucleus`ï¼ˆæˆ–`top-p`ï¼‰é‡‡æ ·ã€‚

tempreature é€‰æ‹©å‘ˆç°å¦‚ä¸‹è§„å¾‹ï¼š
- å½“ temperature è®¾ç½®ä¸ºè¾ƒå°æˆ–è€…0çš„å€¼æ—¶ï¼Œ Temperature Sampling ç­‰åŒäº æ¯æ¬¡é€‰æ‹©æœ€å¤§æ¦‚ç‡çš„ Greedy Searchã€‚ 
- å°çš„temperature ä¼šå¼•å‘æå¤§çš„ repetitive å’Œpredictableæ–‡æœ¬ï¼Œä½†æ˜¯æ–‡æœ¬å†…å®¹å¾€å¾€æ›´è´´åˆè¯­æ–™(highly realistic)ï¼ŒåŸºæœ¬æ‰€æœ‰çš„è¯éƒ½æ¥è‡ªä¸è¯­æ–™åº“ã€‚ å½“temperaturesè¾ƒå¤§æ—¶, ç”Ÿæˆçš„æ–‡æœ¬æ›´å…·æœ‰éšæœºæ€§(random)ã€è¶£å‘³æ€§(interesting)ï¼Œç”šè‡³åˆ›é€ æ€§(creative); ç”šè‡³æœ‰äº›æ—¶å€™èƒ½å‘ç°ä¸€äº›æ–°è¯(misspelled words) ã€‚ 
- å½“ è®¾ç½®é«˜ temperatureæ—¶ï¼Œæ–‡æœ¬å±€éƒ¨ç»“æ„å¾€å¾€ä¼šè¢«ç ´åï¼Œå¤§å¤šæ•°è¯å¯èƒ½ä¼šæ—¶ semi-random strings çš„å½¢å¼ã€‚ 
- å®é™…åº”ç”¨ä¸­ï¼Œå¾€å¾€experiment with multiple temperature values! å½“ä¿æŒäº†ä¸€å®šçš„éšæœºæ€§åˆèƒ½ä¸ç ´åç»“æ„æ—¶ï¼Œå¾€å¾€ä¼šå¾—åˆ°æœ‰æ„æ€çš„ç”Ÿæˆæ–‡æœ¬ã€‚

`Top-k`å’Œ`nucleus`ï¼ˆ`top-p`ï¼‰æŠ½æ ·æ˜¯ä¸¤ç§æµè¡Œçš„æ›¿ä»£æ–¹æ³•/ä½¿ç”¨æ¸©åº¦çš„æ‰©å±•ã€‚
- åŸºæœ¬æ€æƒ³: é™åˆ¶æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸­å¯ä»¥å–æ ·çš„å¯èƒ½æ ‡è®°æ•°é‡ã€‚
- ![](https://picx.zhimg.com/80/v2-20a086d6f1c3250a28dd567b4ac144e3_1440w.webp?source=1940ef5c)
- ä¸Šå›¾æŒ‘é€‰æ¦‚ç‡æœ€é«˜çš„å­—ç¬¦ï¼ˆ10^-1å¤„çš„å­¤ç«‹æ¡ï¼‰çš„æ¦‚ç‡æ˜¯1/10ã€‚
- æŒ‰æ¦‚ç‡é™åºæ’åˆ—æ ‡è®°ï¼Œå¹¶è®¡ç®—å‰10,000ä¸ªæ ‡è®°çš„ç´¯ç§¯æ€»å’Œï¼ˆGPT-2çš„è¯æ±‡ä¸­æ€»å…±æœ‰50,257ä¸ªæ ‡è®°ï¼‰
- åœ¨æ¦‚ç‡æœ€é«˜çš„1,000ä¸ªæ ‡è®°ä¸­ï¼Œå¤§çº¦æœ‰96%çš„æœºä¼šæŒ‘é€‰ä»»ä½•ä¸€ä¸ªæ ‡è®°ã€‚è¯¥æ¦‚ç‡è¿…é€Ÿä¸Šå‡åˆ°90%ä»¥ä¸Šï¼Œä½†åœ¨å‡ åƒä¸ªæ ‡è®°ä¹‹åæ‰é¥±å’Œï¼Œæ¥è¿‘100%ã€‚è¯¥å›¾ æ˜¾ç¤ºï¼Œæœ‰1/100çš„æ¦‚ç‡æ²¡æœ‰é€‰åˆ°ä»»ä½•ç”šè‡³ä¸åœ¨å‰2000åçš„æ ‡è®°ã€‚

è¿™äº›æ•°å­—ä¹çœ‹å¾ˆå°ï¼Œä½†å¾ˆé‡è¦ï¼Œå› ä¸ºåœ¨ç”Ÿæˆæ–‡æœ¬æ—¶
- å¯¹æ¯ä¸ªæ ‡è®°å–æ ·ä¸€æ¬¡, åªæœ‰1/100æˆ–1/1000çš„æœºä¼š
- å¦‚æœå–æ ·æ•°ç™¾æ¬¡ï¼Œå°±æœ‰å¾ˆå¤§çš„æœºä¼šåœ¨æŸä¸€æ—¶åˆ»é€‰åˆ°ä¸€ä¸ªä¸å¯èƒ½çš„æ ‡è®°ï¼Œè€Œä¸”åœ¨å–æ ·æ—¶é€‰åˆ°è¿™æ ·çš„æ ‡è®°ä¼šä¸¥é‡å½±å“ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€‚

å› æ­¤, é€šå¸¸å¸Œæœ›é¿å…è¿™äº›éå¸¸ä¸å¯èƒ½çš„æ ‡è®°ã€‚top-kå’Œtop-pé‡‡æ ·å‘æŒ¥ä½œç”¨çš„åœ°æ–¹

top-kæŠ½æ ·
- åœ¨Top-K Samplingä¸­ï¼Œå°†æŒ‘é€‰å‡ºKä¸ªæœ€æœ‰å¯èƒ½çš„ä¸‹ä¸€ä¸ªå•è¯ï¼Œå¹¶ä¸”ä»…åœ¨è¿™Kä¸ªä¸‹ä¸€ä¸ªå•è¯ä¹‹é—´é‡æ–°ä¸ºå®ƒä»¬åˆ†é…æ¦‚ç‡ã€‚ 
- GPT2å°±æ˜¯é‡‡ç”¨äº†è¿™ç§é‡‡æ ·æ–¹æ¡ˆï¼Œè¿™ä¹Ÿæ˜¯å…¶ç”Ÿæˆæ•…äº‹æ•ˆæœä¸é”™çš„åŸå› ä¹‹ä¸€ã€‚
- ![](https://pic1.zhimg.com/80/v2-a165f4fbb64fcc76e8796bc3df82b4d9_1440w.webp?source=1940ef5c)
- K=6ï¼Œå°†é‡‡æ ·æœ€æœ‰å¯èƒ½çš„6ä¸ªå•è¯ï¼Œè®°ä¸ºV top-K  . åœ¨ç¬¬ä¸€æ­¥é‡‡æ ·ä¸­ï¼ŒV top-K åŒ…å«äº†æ•´ä½“çš„2/3ï¼Œç¬¬äºŒæ­¥é‡‡æ ·åˆ™åŒ…å«äº†å‡ ä¹å…¨éƒ¨ï¼Œä½†æ˜¯æœ‰æ•ˆåœ°å»é™¤äº†ä¸€äº›å¥‡å¥‡æ€ªæ€ªçš„å•è¯ã€‚

top-kæŠ½æ ·èƒŒåçš„æƒ³æ³•
- é€šè¿‡åªä»æ¦‚ç‡æœ€é«˜çš„kä¸ªæ ‡è®°ä¸­æŠ½æ ·æ¥é¿å…ä½æ¦‚ç‡çš„é€‰æ‹©ã€‚
- è¿™å°±åœ¨åˆ†å¸ƒçš„é•¿å°¾ä¸Šè®¾ç½®äº†ä¸€ä¸ªå›ºå®šçš„åˆ‡å£ï¼Œç¡®ä¿æˆ‘ä»¬åªä»å¯èƒ½çš„é€‰æ‹©ä¸­å–æ ·ã€‚
- top-kæŠ½æ ·ç›¸å½“äºå®šä¹‰ä¸€æ¡å‚ç›´çº¿å¹¶ä»å·¦è¾¹çš„æ ‡è®°ä¸­æŠ½æ ·ã€‚

åŒæ ·ï¼Œ`generate()` å‡½æ•°é€šè¿‡`top_k`å‚æ•°æä¾›äº†ä¸€ä¸ªç®€å•çš„æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹:

```py
output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50)
print(tokenizer.decode(output_topk[0]))
```

æœ€ç»ˆå¾—åˆ°æœ€åƒäººç±»çš„æ–‡æœ¬

å¦‚ä½•é€‰æ‹©kå‘¢ï¼Ÿ
- kçš„å€¼æ˜¯æ‰‹åŠ¨é€‰æ‹©çš„ï¼Œå¯¹åºåˆ—ä¸­çš„æ¯ä¸ªé€‰æ‹©éƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¸å®é™…çš„è¾“å‡ºåˆ†å¸ƒæ— å…³ã€‚
- é€šè¿‡æŸ¥çœ‹ä¸€äº›æ–‡æœ¬è´¨é‡æŒ‡æ ‡æ¥æ‰¾åˆ°ä¸€ä¸ªå¥½çš„kå€¼

åŠ¨æ€æˆªæ–­
- åœ¨æ ¸æŠ½æ ·æˆ–é¡¶æŠ½æ ·ä¸­ï¼Œä¸é€‰æ‹©ä¸€ä¸ªå›ºå®šçš„æˆªæ–­å€¼ï¼Œè€Œæ˜¯è®¾å®šä¸€ä¸ªæˆªæ–­çš„æ—¶é—´æ¡ä»¶ã€‚åœ¨é€‰æ‹©ä¸­è¾¾åˆ°ä¸€å®šçš„æ¦‚ç‡è´¨é‡æ—¶ã€‚

top-p é‡‡æ ·
- åœ¨ Top-p é‡‡æ ·ä¸­ï¼Œä¸æ˜¯ä»ä»…æœ€å¯èƒ½çš„Kä¸ªå•è¯ä¸­é‡‡æ ·ï¼Œè€Œæ˜¯ä»å…¶**ç´¯ç§¯æ¦‚ç‡**è¶…è¿‡ä¸€ä¸ªé˜ˆå€¼pçš„æœ€å°å¯èƒ½å•è¯é›†åˆä¸­è¿›è¡Œé€‰æ‹©ï¼Œç„¶åå°†è¿™ç»„å•è¯é‡æ–°åˆ†é…æ¦‚ç‡ã€‚ 
- è¿™æ ·ï¼Œå•è¯é›†åˆçš„å¤§å°ï¼ˆä¹Ÿå°±æ˜¯é›†åˆä¸­å•è¯çš„æ•°é‡ï¼‰å¯ä»¥æ ¹æ®ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒåŠ¨æ€åœ°å¢åŠ æˆ–å‡å°‘ã€‚
- ![](https://picx.zhimg.com/80/v2-0d091bc6c6d820a8715befa576fe3f42_1440w.webp?source=1940ef5c)
- è®¾ç½® p = 0.92ï¼Œå®šä¹‰ä¸º V top-p ï¼Œæ‰€æœ‰å•è¯ç´¯è®¡æ¦‚ç‡è¶…è¿‡0.92çš„æœ€å°å•è¯å­é›†ã€‚ åœ¨ç¬¬ä¸€æ­¥é‡‡æ ·ä¸­ï¼ŒåŒ…æ‹¬äº†9ä¸ªæœ€æœ‰å¯èƒ½çš„å•è¯ï¼Œè€Œåœ¨ç¬¬äºŒæ­¥é‡‡æ ·ä¸­ï¼Œåªéœ€é€‰æ‹©å‰3ä¸ªå•è¯å³å¯è¶…è¿‡92ï¼…ã€‚
- å½“ä¸‹ä¸€ä¸ªå•è¯çš„å¯é¢„æµ‹æ€§ä¸ç¡®å®šæ—¶ï¼Œä¿ç•™äº†è¾ƒå¤šçš„å•è¯

generate() å‡½æ•°ä¹Ÿæä¾›äº†ä¸€ä¸ªæ¿€æ´» top-p æŠ½æ ·çš„å‚æ•°

```py
torch.manual_seed(42)
output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.90)
print(tokenizer.decode(output_topp[0]))
```

Top-p é‡‡æ ·ä¹Ÿäº§ç”Ÿäº†ä¸€ä¸ªè¿è´¯çš„æ•…äº‹ã€‚æŠŠè¿™ä¸¤ç§æŠ½æ ·æ–¹æ³•ç»“åˆèµ·æ¥ä»¥è·å¾—æœ€ä½³æ•ˆæœã€‚
- è®¾ç½® top_k=50 å’Œ top_p=0.9ï¼Œç›¸å½“äºä»æœ€å¤š50ä¸ªæ ‡è®°çš„æ± å­é‡Œé€‰æ‹©æ¦‚ç‡è´¨é‡ä¸º90%çš„æ ‡è®°çš„è§„åˆ™ã€‚

ä½¿ç”¨æŠ½æ ·æ—¶ï¼Œä¹Ÿå¯ä»¥ç”¨æŸæœç´¢ã€‚ä¸å…¶è´ªå©ªåœ°é€‰æ‹©ä¸‹ä¸€æ‰¹å€™é€‰æ ‡è®°ï¼Œå¯ä»¥å¯¹å®ƒä»¬è¿›è¡ŒæŠ½æ ·ï¼Œå¹¶ä»¥åŒæ ·çš„æ–¹å¼å»ºç«‹èµ·æ³¢æŸã€‚

å‚è€ƒï¼š[å…³äºæ–‡æœ¬ç”Ÿæˆï¼ˆtext generationï¼‰ï¼Œæœ‰å“ªäº›æé«˜ç”Ÿæˆå¤šæ ·æ€§çš„æ–¹æ³•ï¼Ÿ](https://www.zhihu.com/question/415657741/answer/2430106609)



#### ç¤ºä¾‹

å¤§å°ä¸º`[1, 4]`çš„logitså¼ é‡ï¼Œä¾‹å­ä¸­å…¶å®æ˜¯`[1, 32000]`ï¼Œç„¶å, å°†logitsè¾“å…¥åˆ°softmaxå‡½æ•°ä¸­ï¼Œåˆ†åˆ«è®¡ç®—å¤šç§æƒ…å†µä¸‹çš„æ¦‚ç‡åˆ†å¸ƒ:
- æ²¡æœ‰temperature
- temperature=0.5
- temperature=2

```py
import torch

logits = torch.tensor([[0.5, 1.2, -1.0, 0.1]])
# æ— temperature
probs = torch.softmax(logits, dim=-1)
# temperature low 0.5
probs_low = torch.softmax(logits / 0.5, dim=-1)
# temperature high 2
probs_high = torch.softmax(logits / 2, dim=-1)

print(f"probs:{probs}")
print(f"probs_low:{probs_low}")
print(f"probs_high:{probs_high}")
# ç»“æœ
# probs: tensor([[0.2559, 0.5154, 0.0571, 0.1716]])
# probs_low: tensor([[0.1800, 0.7301, 0.0090, 0.0809]])
# probs_high: tensor([[0.2695, 0.3825, 0.1273, 0.2207]])
```

åˆ†æ
- temperature è¾ƒé«˜æ—¶ï¼Œä¼šæ›´å¹³å‡åœ°åˆ†é…æ¦‚ç‡ç»™å„ä¸ªtokenï¼Œè¿™å¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬æ›´å…·**éšæœºæ€§**å’Œ**å¤šæ ·æ€§**ï¼›
- temperature è¾ƒä½æ¥è¿‘0æ—¶ï¼Œä¼šå€¾å‘äºé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼Œä»è€Œä½¿ç”Ÿæˆçš„æ–‡æœ¬æ›´åŠ **ç¡®å®šå’Œé›†ä¸­**ã€‚
- temperature=1 æ—¶ï¼Œä¸ç”¨æ­¤æ–¹å¼ã€‚


#### pytorch å®ç°

Temperature é‡‡æ ·ä»£ç å®ç°ï¼š

```py
import torch
from torch.distributions import Categorical

from labml_nn.sampling import Sampler


class TemperatureSampler(Sampler):
    """
    ## Sampler with Temperature
    """
    def __init__(self, temperature: float = 1.0):
        """
        :param temperature: is the temperature to sample with
        """
        self.temperature = temperature

    def __call__(self, logits: torch.Tensor):
        """
        Sample from logits
        """

        # Create a categorical distribution with temperature adjusted logits
        dist = Categorical(logits=logits / self.temperature)

        # Sample
        return dist.sample()
```


### Top-k é‡‡æ ·

ç”Ÿæˆä¸‹ä¸€ä¸ªtokenæ—¶ï¼Œé™åˆ¶æ¨¡å‹åªèƒ½è€ƒè™‘å‰kä¸ªæ¦‚ç‡æœ€é«˜çš„token
- è¿™ä¸ªç­–ç•¥å¯ä»¥é™ä½æ¨¡å‹ç”Ÿæˆ**æ— æ„ä¹‰**æˆ–**é‡å¤**çš„è¾“å‡ºæ¦‚ç‡ï¼ŒåŒæ—¶æé«˜æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦å’Œæ•ˆç‡ã€‚

é‡‡æ ·å‰å°†è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒ**æˆªæ–­**ï¼Œå–å‡ºæ¦‚ç‡æœ€å¤§çš„kä¸ªè¯æ„æˆä¸€ä¸ªé›†åˆï¼Œç„¶åå°†è¿™ä¸ªå­é›†è¯çš„æ¦‚ç‡**å†å½’ä¸€åŒ–**ï¼Œæœ€åé‡æ–°çš„æ¦‚ç‡åˆ†å¸ƒä¸­é‡‡æ ·è¯æ±‡ã€‚
- æ®è¯´å¯ä»¥è·å¾—æ¯”Beam Searchå¥½å¾ˆå¤šçš„æ•ˆæœï¼Œä½†æœ‰ä¸ªé—®é¢˜ï¼Œå°±æ˜¯è¿™ä¸ª**kä¸å¤ªå¥½é€‰**ã€‚
  - æ¦‚ç‡åˆ†å¸ƒå˜åŒ–æ¯”è¾ƒå¤§ï¼Œæœ‰æ—¶å€™å¯èƒ½å¾ˆ**å‡åŒ€**(flat)ï¼Œæœ‰çš„æ—¶å€™æ¯”è¾ƒ**é›†ä¸­**(peaked)ã€‚
  - [å›¾](http://www.wuyuanhao.com/wp-content/uploads/2020/03/distribution.png) ![å›¾](http://www.wuyuanhao.com/wp-content/uploads/2020/03/distribution.png)
  - å¯¹äºé›†ä¸­çš„æƒ…å†µè¿˜å¥½è¯´ï¼Œå½“åˆ†å¸ƒå‡åŒ€æ—¶ï¼Œä¸€ä¸ªè¾ƒå°çš„kå®¹æ˜“ä¸¢æ‰å¾ˆå¤šä¼˜è´¨å€™é€‰è¯ã€‚
  - ä½†å¦‚æœkå®šçš„å¤ªå¤§ï¼Œè¿™ä¸ªæ–¹æ³•åˆä¼šé€€åŒ–å›æ™®é€šé‡‡æ ·ã€‚


```py
import torch

filter_value = -float("Inf")
top_k = 2
probs = torch.tensor([[0.2559, 0.5154, 0.0571, 0.1716]])
indices_to_remove = probs < torch.topk(probs, top_k)[0][..., -1, None]
new_probs = probs.masked_fill(indices_to_remove, filter_value)
print("new_probs:", new_probs)
# ç»“æœ
# new_probs: tensor([[0.2559, 0.5154,   -inf,   -inf]])
```

#### top k å®ç°

```py
def top_k_sampling(model, input, max_length, k):
    output = input
    for _ in range(max_length):
        predictions = model(output)
        # å–æœ€å¯èƒ½çš„kä¸ªå•è¯
        top_k_scores, top_k_words = torch.topk(predictions, k, dim=-1)
        # å¯¹æœ€å¯èƒ½çš„kä¸ªå•è¯è¿›è¡Œsoftmaxæ“ä½œä»¥å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
        probabilities = F.softmax(top_k_scores, dim=-1)
        # æ ¹æ®æ¦‚ç‡åˆ†å¸ƒæŠ½æ ·ä¸€ä¸ªå•è¯
        next_word = torch.multinomial(probabilities, 1)
        # å°†æŠ½æ ·çš„å•è¯æ·»åŠ åˆ°è¾“å‡ºä¸­
        output = torch.cat((output, next_word), dim=-1)
    # åœ¨ç”Ÿæˆå®Œæˆåè¿”å›è¾“å‡º
    return output
```

`Top-k é‡‡æ ·`: â€œè´ªå¿ƒç­–ç•¥â€çš„ä¼˜åŒ–
- ä»æ’åå‰ k çš„ token ä¸­æŠ½æ ·ï¼Œå…è®¸åˆ†æ•°/æ¦‚ç‡è¾ƒé«˜çš„token æœ‰æœºä¼šè¢«é€‰ä¸­ã€‚è¿™ç§æŠ½æ ·å¸¦æ¥çš„éšæœºæ€§æœ‰åŠ©äºæé«˜ç”Ÿæˆè´¨é‡ã€‚
- æ¯æ­¥åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ªå•è¯ä¸­è¿›è¡Œéšæœºé‡‡æ ·ï¼Œè€Œä¸è€ƒè™‘å…¶ä»–ä½æ¦‚ç‡çš„å•è¯ã€‚
- ä¾‹å¦‚ï¼Œå¦‚æœ k=2ï¼Œé‚£ä¹ˆåªä»å¥³å­©ã€é‹å­ä¸­é€‰æ‹©ä¸€ä¸ªå•è¯ï¼Œè€Œä¸è€ƒè™‘å¤§è±¡ã€è¥¿ç“œç­‰å…¶ä»–å•è¯ã€‚è¿™æ ·é¿å…é‡‡æ ·åˆ°ä¸€äº›ä¸åˆé€‚æˆ–ä¸ç›¸å…³çš„å•è¯ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ä¿ç•™ä¸€äº›æœ‰è¶£æˆ–æœ‰åˆ›æ„çš„å•è¯ã€‚
- ![](https://pic3.zhimg.com/80/v2-84999dc8b60cf679844f2a73b9c3d7e2_1440w.webp)
- é€šè¿‡è°ƒæ•´ k çš„å¤§å°ï¼Œå³å¯æ§åˆ¶é‡‡æ ·åˆ—è¡¨çš„å¤§å°ã€‚â€œ`è´ªå¿ƒç­–ç•¥`â€å…¶å®å°±æ˜¯ k = 1 çš„ `top-k é‡‡æ ·`ã€‚
- ![](https://pic1.zhimg.com/80/v2-1a7e2450809497727140e44ca8932edc_1440w.webp)

```py
import torch
from labml_nn.sampling import Sampler

# Top-k Sampler
class TopKSampler(Sampler):
    # k is the number of tokens to pick
    # sampler is the sampler to use for the top-k tokens
    # sampler can be any sampler that takes a logits tensor as input and returns a token tensor; e.g. `TemperatureSampler`.
    def __init__(self, k: int, sampler: Sampler):
        self.k = k
        self.sampler = sampler

    # Sample from logits
    def __call__(self, logits: torch.Tensor):
        # New logits filled with âˆ’âˆ; i.e. zero probability
        zeros = logits.new_ones(logits.shape) * float('-inf')
        # Pick the largest k logits and their indices
        values, indices = torch.topk(logits, self.k, dim=-1)
        # Set the values of the top-k selected indices to actual logits.
        # Logits of other tokens remain âˆ’âˆ
        zeros.scatter_(-1, indices, values)
        # Sample from the top-k logits with the specified sampler.
        return self.sampler(zeros)
```

#### top k ä¼˜ç¼ºç‚¹

top-k ä¼˜ç‚¹ï¼š
- æ ¹æ®ä¸åŒè¾“å…¥æ–‡æœ¬**åŠ¨æ€è°ƒæ•´**å€™é€‰å•è¯çš„æ•°é‡ï¼Œè€Œä¸æ˜¯å›ºå®šä¸º k ä¸ªã€‚è¿™æ˜¯å› ä¸ºä¸åŒçš„è¾“å…¥æ–‡æœ¬å¯èƒ½ä¼šå¯¼è‡´ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒï¼Œæœ‰äº›åˆ†å¸ƒå¯èƒ½æ¯”è¾ƒå¹³å¦ï¼Œæœ‰äº›åˆ†å¸ƒå¯èƒ½æ¯”è¾ƒå°–é”ã€‚å¦‚æœåˆ†å¸ƒæ¯”è¾ƒå¹³å¦ï¼Œé‚£ä¹ˆå‰ k ä¸ªå•è¯å¯èƒ½éƒ½æœ‰ç›¸è¿‘çš„æ¦‚ç‡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥ä»ä¸­è¿›è¡Œéšæœºé‡‡æ ·ï¼›å¦‚æœåˆ†å¸ƒæ¯”è¾ƒå°–é”ï¼Œé‚£ä¹ˆå‰ k ä¸ªå•è¯å¯èƒ½ä¼šå æ®ç»å¤§éƒ¨åˆ†æ¦‚ç‡ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥è¿‘ä¼¼åœ°è¿›è¡Œè´ªå¿ƒè§£ç ã€‚
- é€šè¿‡è°ƒæ•´ k çš„å¤§å°æ¥æ§åˆ¶ç”Ÿæˆçš„**å¤šæ ·æ€§å’Œè´¨é‡**ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œk è¶Šå¤§ï¼Œç”Ÿæˆçš„å¤šæ ·æ€§è¶Šé«˜ï¼Œä½†æ˜¯ç”Ÿæˆçš„è´¨é‡è¶Šä½ï¼›k è¶Šå°ï¼Œç”Ÿæˆçš„è´¨é‡è¶Šé«˜ï¼Œä½†æ˜¯ç”Ÿæˆçš„å¤šæ ·æ€§è¶Šä½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®ä¸åŒçš„ä»»åŠ¡å’Œåœºæ™¯æ¥é€‰æ‹©åˆé€‚çš„k å€¼ã€‚
- ä¸å…¶ä»–è§£ç ç­–ç•¥**ç»“åˆ**ä½¿ç”¨ï¼Œä¾‹å¦‚ æ¸©åº¦è°ƒèŠ‚ï¼ˆTemperature Scalingï¼‰ã€é‡å¤æƒ©ç½šï¼ˆRepetition Penaltyï¼‰ã€é•¿åº¦æƒ©ç½šï¼ˆLength Penaltyï¼‰ç­‰ï¼Œæ¥è¿›ä¸€æ­¥ä¼˜åŒ–ç”Ÿæˆçš„æ•ˆæœã€‚

ä½†æ˜¯ top-k ä¸€äº›ç¼ºç‚¹ï¼š
- ç”Ÿæˆæ–‡æœ¬**ä¸ç¬¦åˆå¸¸è¯†æˆ–é€»è¾‘**ã€‚
  - top-k é‡‡æ ·åªè€ƒè™‘äº†**å•è¯æ¦‚ç‡**ï¼Œè€Œæ²¡æœ‰è€ƒè™‘å•è¯ä¹‹é—´çš„**è¯­ä¹‰å’Œè¯­æ³•å…³ç³»**ã€‚
  - ä¾‹å¦‚ï¼Œå¦‚æœè¾“å…¥æ–‡æœ¬æ˜¯â€œæˆ‘å–œæ¬¢åƒâ€ï¼Œé‚£ä¹ˆå³ä½¿é¥ºå­çš„æ¦‚ç‡æœ€é«˜ï¼Œä¹Ÿä¸ä¸€å®šæ˜¯æœ€åˆé€‚çš„é€‰æ‹©ï¼Œå› ä¸ºå¯èƒ½ç”¨æˆ·æ›´å–œæ¬¢åƒå…¶ä»–é£Ÿç‰©ã€‚
- ç”Ÿæˆæ–‡æœ¬**è¿‡äºç®€å•æˆ–æ— èŠ**ã€‚
  - top-k é‡‡æ ·åªè€ƒè™‘äº†**æ¦‚ç‡æœ€é«˜çš„ k ä¸ªå•è¯**ï¼Œè€Œæ²¡æœ‰è€ƒè™‘å…¶ä»–ä½æ¦‚ç‡ä½†æœ‰æ„ä¹‰æˆ–æœ‰åˆ›æ„çš„å•è¯ã€‚
  - ä¾‹å¦‚ï¼Œå¦‚æœè¾“å…¥æ–‡æœ¬æ˜¯â€œæˆ‘å–œæ¬¢åƒâ€ï¼Œé‚£ä¹ˆå³ä½¿è‹¹æœã€é¥ºå­å’Œç«é”…éƒ½æ˜¯åˆç†çš„é€‰æ‹©ï¼Œä¹Ÿä¸ä¸€å®šæ˜¯æœ€æœ‰è¶£æˆ–æœ€æƒŠå–œçš„é€‰æ‹©ï¼Œå› ä¸ºå¯èƒ½ç”¨æˆ·æ›´å–œæ¬¢åƒä¸€äº›ç‰¹åˆ«æˆ–æ–°å¥‡çš„é£Ÿç‰©ã€‚

é€šå¸¸ä¼šè€ƒè™‘ top-k å’Œå…¶å®ƒç­–ç•¥ç»“åˆï¼Œæ¯”å¦‚ top-pã€‚

### Top-p é‡‡æ · 

åˆç§° `æ ¸é‡‡æ ·` Nucleus sampling

top-k æœ‰ä¸ªç¼ºé™·
- â€œk å€¼å–å¤šå°‘æ˜¯æœ€ä¼˜çš„ï¼Ÿâ€ éå¸¸éš¾ç¡®å®šã€‚

äºæ˜¯ï¼Œå‡ºç°äº†**åŠ¨æ€è®¾ç½® token å€™é€‰åˆ—è¡¨å¤§å°ç­–ç•¥**â€”â€”å³`æ ¸é‡‡æ ·`ï¼ˆNucleus Samplingï¼‰
- ä¸å†å–ä¸€ä¸ªå›ºå®šçš„kï¼Œè€Œæ˜¯å›ºå®šå€™é€‰é›†åˆçš„æ¦‚ç‡å¯†åº¦å’Œåœ¨æ•´ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­çš„æ¯”ä¾‹
- é€‰å‡ºæ¥è¿™ä¸ªé›†åˆä¹‹åä¹Ÿå’Œtop-ké‡‡æ ·ä¸€æ ·ï¼Œé‡æ–°å½’ä¸€åŒ–é›†åˆå†…è¯çš„æ¦‚ç‡ï¼Œå¹¶æŠŠé›†åˆå¤–è¯çš„æ¦‚ç‡è®¾ä¸º0ã€‚
- è¿™ç§æ–¹å¼ä¹Ÿç§°ä¸ºtop-pé‡‡æ ·ã€‚

top-p å…¨åæ˜¯"**top probability**"ï¼Œé€šå¸¸ç”¨ä¸€ä¸ªä»‹äº 0 ~ 1 ä¹‹é—´å€¼ï¼Œè¡¨ç¤ºç”Ÿæˆä¸‹ä¸€ä¸ªtokenæ—¶ï¼Œåœ¨æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©çš„æœ€é«˜æ¦‚ç‡çš„ç´¯ç§¯é˜ˆå€¼

top-p é‡‡æ ·æ€è·¯
- æ¯æ­¥åªä»ç´¯ç§¯æ¦‚ç‡è¶…è¿‡æŸä¸ªé˜ˆå€¼ p çš„**æœ€å°å•è¯é›†åˆ**ä¸­éšæœºé‡‡æ ·ï¼Œè€Œä¸è€ƒè™‘å…¶ä»–ä½æ¦‚ç‡çš„å•è¯ã€‚
- è¿™ç§æ–¹æ³•ä¹Ÿè¢«ç§°ä¸º**æ ¸é‡‡æ ·**ï¼ˆnucleus samplingï¼‰ï¼Œåªå…³æ³¨æ¦‚ç‡åˆ†å¸ƒçš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œè€Œå¿½ç•¥äº†å°¾éƒ¨éƒ¨åˆ†ã€‚
- ä¾‹å¦‚ï¼Œå¦‚æœ p=0.9ï¼Œåªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° 0.9 çš„æœ€å°å•è¯é›†åˆä¸­é€‰æ‹©ä¸€ä¸ªå•è¯ï¼Œè€Œä¸è€ƒè™‘å…¶ä»–ç´¯ç§¯æ¦‚ç‡å°äº 0.9 çš„å•è¯ã€‚è¿™æ ·é¿å…é‡‡æ ·åˆ°ä¸€äº›ä¸åˆé€‚æˆ–ä¸ç›¸å…³çš„å•è¯ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ä¿ç•™ä¸€äº›æœ‰è¶£æˆ–æœ‰åˆ›æ„çš„å•è¯ã€‚

ä¸‹å›¾å±•ç¤ºäº† top-p å€¼ä¸º 0.9 çš„ Top-p é‡‡æ ·æ•ˆæœï¼š
- ![](https://pic1.zhimg.com/80/v2-d543614ab60a1f52b0001f1e90d9f16c_1440w.webp)

top-p å€¼é€šå¸¸è®¾ç½®ä¸ºæ¯”è¾ƒé«˜çš„å€¼ï¼ˆå¦‚0.75ï¼‰ï¼Œç›®çš„æ˜¯é™åˆ¶ä½æ¦‚ç‡ token çš„é•¿å°¾ã€‚å¯åŒæ—¶ä½¿ç”¨ top-k å’Œ top-pã€‚å¦‚æœ k å’Œ p åŒæ—¶å¯ç”¨ï¼Œåˆ™ p åœ¨ k ä¹‹åèµ·ä½œç”¨ã€‚


#### ç¤ºä¾‹

åˆ†æ
- å½“top_pè¾ƒé«˜æ—¶ï¼Œæ¯”å¦‚ 0.9ï¼Œå‰ 90% çš„æ¦‚ç‡çš„tokenä¼šè¢«è€ƒè™‘åœ¨æŠ½æ ·ä¸­ï¼Œè¿™æ ·ä¼šå…è®¸æ›´å¤šçš„tokenå‚ä¸æŠ½æ ·ï¼Œå¢åŠ ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼›
- å½“top_pè¾ƒä½æ—¶ï¼Œæ¯”å¦‚æ¯”å¦‚ 0.1ï¼Œåªæœ‰å‰ 10% æœ€é«˜æ¦‚ç‡çš„tokenä¼šè¢«è€ƒè™‘åœ¨æŠ½æ ·ä¸­ï¼Œè¿™æ ·ä¼šé™åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¯èƒ½æ€§ï¼Œä½¿ç”Ÿæˆçš„æ–‡æœ¬æ›´åŠ ç¡®å®šå’Œé›†ä¸­ã€‚
- top_p=1æ—¶ï¼Œè¡¨ç¤ºä¸ä½¿ç”¨æ­¤æ–¹å¼ã€‚

ç–‘é—®
- å½“top-pè®¾ç½®çš„å¾ˆå°ï¼Œç´¯åŠ çš„æ¦‚ç‡æ²¡è¶…è¿‡æ€ä¹ˆåŠï¼Ÿä¸€èˆ¬ä»£ç ä¸­éƒ½ä¼šå¼ºåˆ¶è‡³å°‘é€‰å‡ºä¸€ä¸ªtokenã€‚

```py
import torch

# æ ·ä¾‹ï¼šprobs: tensor([[0.2559, 0.5154, 0.0571, 0.1716]])
probs = torch.tensor([[0.2559, 0.5154, 0.0571, 0.1716]])
# ç¬¬ä¸€æ­¥è¿›è¡Œæ’åº
probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
# ç»“æœ
# probs_sort: tensor([[0.5154, 0.2559, 0.1716, 0.0571]])
# probs_idx: tensor([[1, 0, 3, 2]])

# ç¬¬äºŒæ­¥æ¦‚ç‡çš„ç´¯ç§¯å’Œ
probs_sum = torch.cumsum(probs_sort, dim=-1)
# ç»“æœ
# probs_sum: tensor([[0.5154, 0.7713, 0.9429, 1.0000]])

# ç¬¬ä¸‰æ­¥æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¤§äºé˜ˆå€¼pçš„ä½ç½®ï¼Œå‡è®¾p=0.9ï¼Œå¹¶å°†åé¢çš„æ¦‚ç‡å€¼ç½®ä¸º0ï¼š
mask = probs_sum - probs_sort > p
probs_sort[mask] = 0.0
# ç»“æœ
# probs_sort: tensor([[0.5154, 0.2559, 0.1716, 0.0000]])

# ç¬¬å››æ­¥å¤åŸåŸåºåˆ—
new_probs = probs_sort.scatter(1, probs_idx, probs_sort)
# ç»“æœ
# new_probs: tensor([[0.2559, 0.5154, 0.0000, 0.1716]])

# æ³¨ï¼šåœ¨çœŸå®å®ç°ä¸­ä¸€èˆ¬ä¼šæŠŠèˆå¼ƒçš„æ¦‚ç‡ç½®ä¸º-infï¼Œå³
zero_indices = (new_probs == 0)
new_probs[zero_indices] = float('-inf')
# ç»“æœ
# new_probs: tensor([[0.2559, 0.5154, -inf, 0.1716]])

# å®Œæ•´ä»£ç 
def sample_top_p(probs, p):
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort[mask] = 0.0
    new_probs = probs_sort.scatter(1, probs_idx, probs_sort)
    zero_indices = (new_probs == 0)
    new_probs[zero_indices] = float('-inf')
    return new_probs
```


#### å®ç°


```py
def top_p_sampling(model, input, max_length, p):
    output = input
    for _ in range(max_length):
        predictions = model(output)
        # å¯¹é¢„æµ‹è¿›è¡Œæ’åºå¹¶è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        sorted_logits, sorted_indices = torch.sort(predictions, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        # åˆ é™¤ç´¯ç§¯æ¦‚ç‡å¤§äºpçš„å•è¯
        sorted_indices_to_remove = cumulative_probs > p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        predictions[indices_to_remove] = float('-inf')
        # å¯¹å‰©ä¸‹çš„å•è¯è¿›è¡ŒæŠ½æ ·
        next_word = torch.multinomial(F.softmax(predictions, dim=-1), 1)
        # å°†æŠ½æ ·çš„å•è¯æ·»åŠ åˆ°è¾“å‡ºä¸­
        output = torch.cat((output, next_word), dim=-1)
    # åœ¨ç”Ÿæˆå®Œæˆåè¿”å›è¾“å‡º
    return output
```

```py
import torch
from torch import nn

from labml_nn.sampling import Sampler


class NucleusSampler(Sampler):
    """
    ## Nucleus Sampler
    """
    def __init__(self, p: float, sampler: Sampler):
        """
        :param p: is the sum of probabilities of tokens to pick $p$
        :param sampler: is the sampler to use for the selected tokens
        """
        self.p = p
        self.sampler = sampler
        # Softmax to compute $P(x_i | x_{1:i-1})$ from the logits
        self.softmax = nn.Softmax(dim=-1)

    def __call__(self, logits: torch.Tensor):
        """
        Sample from logits with Nucleus Sampling
        """

        # Get probabilities $P(x_i | x_{1:i-1})$
        probs = self.softmax(logits)

        # Sort probabilities in descending order
        sorted_probs, indices = torch.sort(probs, dim=-1, descending=True)

        # Get the cumulative sum of probabilities in the sorted order
        cum_sum_probs = torch.cumsum(sorted_probs, dim=-1)

        # Find the cumulative sums less than $p$.
        nucleus = cum_sum_probs < self.p

        # Prepend ones so that we add one token after the minimum number
        # of tokens with cumulative probability less that $p$.
        nucleus = torch.cat([nucleus.new_ones(nucleus.shape[:-1] + (1,)), nucleus[..., :-1]], dim=-1)

        # Get log probabilities and mask out the non-nucleus
        sorted_log_probs = torch.log(sorted_probs)
        sorted_log_probs[~nucleus] = float('-inf')

        # Sample from the sampler
        sampled_sorted_indexes = self.sampler(sorted_log_probs)

        # Get the actual indexes
        res = indices.gather(-1, sampled_sorted_indexes.unsqueeze(-1))

        #
        return res.squeeze(-1)
```


### è”åˆé‡‡æ ·

é€šå¸¸å°† top-kã€top-pã€Temperature è”åˆä½¿ç”¨ã€‚

å…ˆåé¡ºåº: 
- `top-k` -> `top-p` -> `Temperature`

è®¾ç½® top-k = 3ï¼Œè¡¨ç¤ºä¿ç•™æ¦‚ç‡æœ€é«˜çš„3ä¸ª tokenã€‚

è¿™æ ·å°±ä¼šä¿ç•™å¥³å­©ã€é‹å­ã€å¤§è±¡è¿™3ä¸ª tokenã€‚
- å¥³å­©ï¼š0.664
- é‹å­ï¼š0.199
- å¤§è±¡ï¼š0.105

æ¥ä¸‹æ¥ä½¿ç”¨ top-p æ–¹æ³•ï¼Œä¿ç•™æ¦‚ç‡çš„ç´¯è®¡å’Œè¾¾åˆ° 0.8 çš„å•è¯
- é€‰å–å¥³å­©å’Œé‹å­è¿™ä¸¤ä¸ª tokenã€‚

æ¥ç€ä½¿ç”¨ Temperature = 0.7 è¿›è¡Œå½’ä¸€åŒ–ï¼Œå˜æˆï¼š
- å¥³å­©ï¼š0.660
- é‹å­ï¼š0.340

æ¥ç€ï¼Œä»ä¸Šè¿°åˆ†å¸ƒä¸­è¿›è¡Œéšæœºé‡‡æ ·ï¼Œé€‰å–ä¸€ä¸ªå•è¯ä½œä¸ºæœ€ç»ˆçš„ç”Ÿæˆç»“æœã€‚


#### ç¤ºä¾‹

top k å’Œ top p è”åˆ

```py
# ä»£ç è¾“å…¥çš„æ˜¯logitsï¼Œè€Œä¸”è€ƒè™‘å¾ˆå‘¨å…¨ï¼ˆæˆ‘æ„Ÿè§‰æ¼äº†è€ƒè™‘kå’Œpéƒ½ç»™äº†çš„æƒ…å†µï¼Œè¿™åº”è¯¥æ˜¯ä¸åˆé€‚çš„ï¼‰
# å·§å¦™åœ°ä½¿ç”¨äº†torch.cumsum
# é¿å…äº†ä¸€ä¸ªè¯éƒ½é€‰ä¸å‡ºæ¥çš„å°´å°¬æƒ…å†µ
def top_k_top_p_filtering(logits, top_k=0, top_p=1.0, filter_value=-float("Inf"), min_tokens_to_keep=1):
    """ Filter a distribution of logits using top-k and/or nucleus (top-p) filtering
        Args:
            logits: logits distribution shape (batch size, vocabulary size)
            if top_k > 0: keep only top k tokens with highest probability (top-k filtering).
            if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).
                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)
            Make sure we keep at least min_tokens_to_keep per batch example in the output
        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317
    """
    if top_k > 0:
        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check
        # Remove all tokens with a probability less than the last token of the top-k
        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
        logits[indices_to_remove] = filter_value

    if top_p < 1.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)
        sorted_indices_to_remove = cumulative_probs > top_p
        if min_tokens_to_keep > 1:
            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)
            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0
        # Shift the indices to the right to keep also the first token above the threshold
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0

        # scatter sorted tensors to original indexing
        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
        logits[indices_to_remove] = filter_value
    return logits
```



### æƒ©ç½šé‡å¤

ä¸ºäº†è§£å†³é‡å¤é—®é¢˜ï¼Œè¿˜æœ‰å¯ä»¥é€šè¿‡æƒ©ç½šå› å­å°†å‡ºç°è¿‡è¯çš„æ¦‚ç‡å˜å°æˆ–è€…å¼ºåˆ¶ä¸ä½¿ç”¨é‡å¤è¯æ¥è§£å†³ã€‚

repetition_penalty ç›®æ ‡
- å¯¹é‡å¤ç”Ÿæˆçš„tokenè¿›è¡Œæƒ©ç½šï¼ˆé™ä½æ¦‚ç‡ï¼‰ï¼Œä»¥å‡å°‘ç”Ÿæˆæ–‡æœ¬ä¸­çš„é‡å¤æ€§

æƒ©ç½šé‡å¤

æ–¹å¼ï¼š
- æ¯æ­¥å¯¹å‡ºç°è¿‡çš„è¯çš„æ¦‚ç‡åšå‡ºæƒ©ç½šï¼Œå³é™ä½å‡ºç°è¿‡çš„å­—çš„é‡‡æ ·æ¦‚ç‡ï¼Œè®©æ¨¡å‹è¶‹å‘äºè§£ç å‡ºæ²¡å‡ºç°è¿‡çš„è¯

#### repetition_penalty

å‚æ•°ï¼š
- `repetition_penalty`ï¼ˆfloatï¼Œå–å€¼èŒƒå›´>0ï¼‰ã€‚
- é»˜è®¤ä¸º1ï¼Œå³ä»£è¡¨ä¸è¿›è¡Œæƒ©ç½šã€‚
- å€¼è¶Šå¤§ï¼Œå³å¯¹é‡å¤çš„å­—åšå‡ºæ›´å¤§çš„æƒ©ç½š

ä»£ç å®ç°é€»è¾‘ï¼š
- å¦‚æœå­—çš„æ¦‚ç‡score<0ï¼Œåˆ™ score = score*penalty, æ¦‚ç‡ä¼šè¶Šä½ï¼› 
- å¦‚æœå­—çš„æ¦‚ç‡score>0, åˆ™åˆ™score = score/penalty,åŒæ ·æ¦‚ç‡ä¹Ÿä¼šå˜ä½ã€‚
- ![](https://pic4.zhimg.com/80/v2-628cb53483ffb707ab1f01f7944ead8b_1440w.webp)


```py
import numpy as np
def apply_repetition_penalty(probs, repetition_penalty, prev_tokens):
    adjusted_probs = np.copy(probs)
    for token in set(prev_tokens):
        adjusted_probs[token] *= (1/repetition_penalty)
    adjusted_probs /= np.sum(adjusted_probs)  
    return adjusted_probs
# ç¤ºä¾‹æ¦‚ç‡åˆ†å¸ƒï¼Œç´¢å¼•å¯¹åº”ä¸åŒè¯è¯­
original_probs = np.array([0.3, 0.1, 0.3, 0.1, 0.2])
# ç¤ºä¾‹å…ˆå‰ç”Ÿæˆçš„è¯è¯­
previous_tokens = [2, 4, 2]
# é‡å¤æƒ©ç½šç³»æ•°
repetition_penalty = 1.25
# åº”ç”¨é‡å¤æƒ©ç½šï¼Œå¾—åˆ°è°ƒæ•´åçš„æ¦‚ç‡åˆ†å¸ƒ
adjusted_probs = apply_repetition_penalty(original_probs, repetition_penalty, previous_tokens)

print("åŸå§‹æ¦‚ç‡åˆ†å¸ƒï¼š", original_probs)
print("è°ƒæ•´åçš„æ¦‚ç‡åˆ†å¸ƒï¼š", adjusted_probs)

# ç»“æœ
# åŸå§‹æ¦‚ç‡åˆ†å¸ƒï¼š [0.3 0.1 0.3 0.1 0.2]
# è°ƒæ•´åçš„æ¦‚ç‡åˆ†å¸ƒï¼š [0.33333333 0.11111111 0.26666667 0.11111111 0.17777778]
```

å‡ºç°è¿‡çš„ token æ¦‚ç‡å˜ä½äº†ï¼Œæœªå‡ºç°è¿‡çš„tokençš„æ¦‚ç‡å˜é«˜äº†ã€‚



### æƒ©ç½šn-gram

æ–¹å¼ï¼š
- é™åˆ¶n-gramåœ¨ç”Ÿæˆç»“æœä¸­å‡ºç°æ¬¡æ•°

å‚æ•°ï¼š
- no_repeat_ngram_sizeï¼Œé™åˆ¶n-gramä¸å‡ºç°2æ¬¡ã€‚ (no_repeat_ngram_size=6å³ä»£è¡¨:6-gramä¸å‡ºç°2æ¬¡)

#### no_repeat_ngram_size

no_repeat_ngram_size è¿™ä¸ªå‚æ•°
- å½“è®¾ä¸ºå¤§äº0çš„æ•´æ•°æ—¶ï¼Œç”Ÿæˆçš„æ–‡æœ¬ä¸ä¼šå‡ºç°æŒ‡å®šå¤§å°çš„é‡å¤n-gramï¼ˆnä¸ªè¿ç»­çš„tokenï¼‰

å¯ä»¥ä½¿ç”Ÿæˆçš„æ–‡æœ¬æ›´åŠ **å¤šæ ·åŒ–**ï¼Œé¿å…å‡ºç°é‡å¤çš„**çŸ­è¯­æˆ–å¥å­ç»“æ„**ã€‚

å®ç°åŸç†å’Œ `repetition_penalty` å·®ä¸å¤šï¼Œåªä¸è¿‡è¿™é‡Œæ˜¯nä¸ªè¿ç»­çš„tokenã€‚

æ³¨
- é»˜è®¤ä¸è®¾ç½®


### use_cache

è¯¥å‚æ•°å¦‚ä½•è®¾ç½®ä¸ºTrueæ—¶ï¼Œåˆ™æ¨¡å‹ä¼šåˆ©ç”¨ä¹‹å‰è®¡ç®—å¾—åˆ°çš„æ³¨æ„åŠ›æƒé‡ï¼ˆkey/values attentionsï¼‰çš„ç¼“å­˜ï¼Œè¿™äº›æ³¨æ„åŠ›æƒé‡æ˜¯åœ¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ä¸­ï¼Œæ ¹æ®è¾“å…¥ä¸Šä¸‹æ–‡å’Œå·²ç”Ÿæˆéƒ¨åˆ†æ–‡æœ¬ï¼Œè®¡ç®—å‡ºæ¥çš„ï¼Œå½“ä¸‹ä¸€ä¸ªtokenéœ€è¦è¢«ç”Ÿæˆæ—¶ï¼Œæ¨¡å‹å¯ä»¥é€šè¿‡ç¼“å­˜çš„æ³¨æ„åŠ›æƒé‡æ¥é‡ç”¨ä¹‹å‰è®¡ç®—çš„ä¿¡æ¯ï¼Œè€Œä¸éœ€è¦é‡æ–°è®¡ç®—ä¸€æ¬¡ï¼Œæœ‰æ•ˆåœ°è·³è¿‡é‡å¤è®¡ç®—çš„æ­¥éª¤ï¼Œä»è€Œå‡å°‘è®¡ç®—è´Ÿæ‹…ï¼Œæé«˜ç”Ÿæˆé€Ÿåº¦å’Œæ•ˆç‡


### MoE

é—®é¢˜
- è§£ç ä»»åŠ¡ä¸­å¸¸ç”¨çš„Beam Searchç”Ÿæˆåºåˆ—æ¡ä»¶æ¦‚ç‡**æœ€å¤§**çš„å¥å­ï¼Œå¾ˆå®¹æ˜“å¯¼è‡´ç”Ÿæˆå¥å­çš„å¤šæ ·æ€§ä¸è¶³

ç°æœ‰è§£æ³•
- Diverse Beam Searché€šè¿‡å¯¹ç”Ÿæˆç»“æœ**åˆ†ç»„**å¹¶åŠ å…¥**ç›¸ä¼¼æ€§æƒ©ç½š**æ¥æé«˜å¤šæ ·æ€§ï¼Œä¸€å®šç¨‹åº¦ç¼“è§£å•ä¸€æ€§é—®é¢˜ï¼Œä½†å¹¶ä¸å½»åº•ã€‚
- å¦å¤–ï¼ŒSamplingè§£ç ç®—æ³•ï¼ˆå¦‚toppã€topkï¼‰ç”Ÿæˆç»“æœçš„**éšæœºæ€§**æ›´å¤§ï¼Œå¤šæ ·æ€§æ›´å¥½ä¸€äº›ï¼Œä½†ç›¸åº”å‡†ç¡®ç‡ä¹Ÿæ›´å·®ã€‚

MoEä½¿ç”¨å¤šä¸ªæ¨¡å‹ç»„åˆè®­ç»ƒä¸åŒåˆ†å¸ƒçš„æ•°æ®ï¼Œå¾ˆé€‚åˆè§£å†³å¤šæ ·æ€§é—®é¢˜
- æœ‰[å·¥ä½œ](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007)ç”¨MoEç»“æ„æ¥å­¦ä¹ æ¨èä¸­å¤šä»»åŠ¡æ¨¡å‹å„ä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œè§£å†³ä¸ç›¸å…³çš„ä»»åŠ¡å¯¼è‡´æ¨¡å‹æ•ˆæœå·®çš„é—®é¢˜ã€‚

ã€2019-5-24ã€‘MoE in Text Generation -- ç”¨äº byte push
- [Mixture Models for Diverse Machine Translation: Tricks of the Trade](https://arxiv.org/pdf/1902.07816.pdf)
- ç”¨å¤šç§ä¸åŒMoEç­–ç•¥ï¼Œæå‡ºç”Ÿæˆå¤šæ ·æ€§çš„è¯„ä¼°æŒ‡æ ‡ï¼Œåœ¨ç”Ÿæˆè´¨é‡åŸºæœ¬ä¸é™çš„æƒ…å†µä¸‹ï¼Œå¤šæ ·æ€§å¾—åˆ°äº†å¾ˆå¤§æ”¹å–„ã€‚

MoE (Mixture of Experts)æ¨¡å‹çš„åŸºæœ¬æ€æƒ³
- è®­ç»ƒå¤šä¸ªç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªç½‘ç»œï¼ˆä½œä¸ºExpertï¼‰è®­ç»ƒæ—¶ä½¿ç”¨æ•°æ®é›†ä¸åŒéƒ¨åˆ†ã€‚
- æ•°æ®é›†å†…éƒ¨åˆ†å¸ƒå¯èƒ½ä¸åŒï¼Œå•æ¨¡å‹å¾€å¾€å–„äºå¤„ç†å…¶ä¸­ä¸€éƒ¨åˆ†æ•°æ®ï¼Œä¸å–„äºå¤„ç†å¦ä¸€éƒ¨åˆ†æ•°æ®ï¼Œè€Œ**å¤šä¸“å®¶ç³»ç»Ÿ**è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼šç³»ç»Ÿä¸­æ¯ä¸ªExpertéƒ½ä¼šæœ‰ä¸€ä¸ªæ“…é•¿çš„æ•°æ®åŒºåŸŸï¼Œåœ¨è¿™ç»„åŒºåŸŸä¸Šå…¶è¦æ¯”å…¶ä»–Expertè¡¨ç°å¾—å¥½ã€‚

å¸¸ç”¨çš„encoder-decoderç»“æ„ï¼Œé€šè¿‡encoderå¾—åˆ°hidden statesï¼Œå†è¾“å…¥è¿›decoderå¾—åˆ°ç”Ÿæˆç»“æœ
- é‰´äºè¯¸å¦‚Beam Searchè§£ç å¯¼è‡´çš„ç”Ÿæˆå¤šæ ·æ€§é—®é¢˜ï¼Œä½œè€…å¼•å…¥Multinomial Latent Variable å¹¶åˆ†è§£ç”Ÿæˆæ¨¡å‹çš„è¾¹é™…ä¼¼ç„¶å‡½æ•°ï¼ˆzè¡¨ç¤ºä¸€ä¸ªexpertï¼‰
- è®­ç»ƒé˜¶æ®µä½¿ç”¨EMç®—æ³•
- è§£ç æ—¶ç”¨ç”Ÿæˆæ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªExpertçš„ç»“æœä½œä¸ºæœ€ç»ˆç»“æœ

å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™ä¼šå¯¼è‡´ä¸¤ä¸ªä¸¥é‡çš„é—®é¢˜ï¼š
- åœ¨è®­ç»ƒä¸­å¯èƒ½åªæœ‰ä¸€ä¸ªExpertä¼šè¢«è¿­ä»£ï¼Œå¯¼è‡´rich gets richerçš„é—®é¢˜ï¼›
- Latent Variableå¤±æ•ˆï¼›

ä½œè€…æå‡ºäº†ä¸¤ä¸ªæ”¹è¿›æ–¹å¼ï¼š
- hard/soft selectionï¼Œå³å¦‚ä½•ä½¿ç”¨expertçš„æƒé‡è¿›è¡Œè¿­ä»£
- learned/uniform priorï¼Œpriorå³å‰é¢æåˆ°çš„p(z\|x; Î¸) 
  - learnedï¼šè®¾ç½®ä¸€ä¸ªgateç½‘ç»œï¼Œæ¨¡å‹è‡ªå·±å»å­¦ï¼›
  - uniformï¼šæ‰€æœ‰expertsæƒé‡ç»Ÿä¸€

ä½œè€…æå‡ºPairwise-BLEUï¼Œç”¨äºè¡¡é‡å¤šæ ·æ€§çš„æŒ‡æ ‡ï¼Œçºµè½´æ˜¯bleuã€‚
- Pairwise-BLEUæ˜¯å¯¹ç”Ÿæˆçš„å¤šä¸ªç»“æœä¸¤ä¸¤è®¡ç®—BLEUï¼Œå¤šæ ·æ€§è¶Šå¥½ï¼Œä¸¤ä¸¤å°±è¶Šä¸ç›¸ä¼¼ï¼Œåˆ†æ•°å°±è¶Šä½


### å°‘è§å‚æ•°

å°‘è§å‚æ•°
1. num_return_sequences æ¨¡å‹è¿”å›ä¸åŒçš„æ–‡æœ¬åºåˆ—çš„æ•°é‡ï¼Œè¦å’Œ beam search ä¸­çš„ num_beams ä¸€è‡´ï¼Œåœ¨è´ªå¿ƒè§£ç ç­–ç•¥ä¸­ï¼Œnum_return_sequencesåªèƒ½ä¸º1ï¼Œé»˜è®¤ä¹Ÿä¸º1ã€‚
2. max_length ç”Ÿæˆçš„tokençš„æœ€å¤§é•¿åº¦ã€‚å®ƒæ˜¯è¾“å…¥promptçš„é•¿åº¦åŠ ä¸Šmax_new_tokensçš„å€¼ã€‚å¦‚æœåŒæ—¶è®¾ç½®äº†max_new_tokensï¼Œåˆ™ä¼šè¦†ç›–æ­¤å‚æ•°ï¼Œé»˜è®¤ä¸º20ã€‚
3. max_new_tokens ç”Ÿæˆçš„æœ€å¤§tokençš„æ•°é‡ï¼Œä¸è€ƒè™‘è¾“å…¥promptä¸­çš„tokenæ•°ï¼Œé»˜è®¤æ— è®¾ç½®
4. min_length ç”Ÿæˆçš„tokençš„æœ€å°é•¿åº¦ã€‚å®ƒæ˜¯è¾“å…¥promptçš„é•¿åº¦åŠ ä¸Šmin_new_tokensçš„å€¼ã€‚å¦‚æœåŒæ—¶è®¾ç½®äº†min_new_tokensï¼Œåˆ™ä¼šè¦†ç›–æ­¤å‚æ•°ï¼Œé»˜è®¤ä¸º0ã€‚
5. min_new_tokens ç”Ÿæˆçš„æœ€å°tokençš„æ•°é‡ï¼Œä¸è€ƒè™‘è¾“å…¥promptä¸­çš„tokenæ•°ï¼Œé»˜è®¤æ— è®¾ç½®
6. early_stopping æ§åˆ¶åŸºäºæŸæœç´¢ï¼ˆbeam searchï¼‰ç­‰æ–¹æ³•çš„åœæ­¢æ¡ä»¶ï¼Œæ¥å—ä»¥ä¸‹å€¼ï¼š
  - Trueï¼šç”Ÿæˆä¼šåœ¨å‡ºç°num_beamsä¸ªå®Œæ•´å€™é€‰é¡¹æ—¶åœæ­¢ã€‚
  - Falseï¼šåº”ç”¨å¯å‘å¼æ–¹æ³•ï¼Œå½“å¾ˆä¸å¯èƒ½æ‰¾åˆ°æ›´å¥½çš„å€™é€‰é¡¹æ—¶åœæ­¢ç”Ÿæˆã€‚
  - neverï¼šåªæœ‰å½“ä¸èƒ½æ‰¾åˆ°æ›´å¥½çš„å€™é€‰é¡¹æ—¶ï¼ŒæŸæœç´¢è¿‡ç¨‹æ‰ä¼šåœæ­¢ï¼ˆç»å…¸çš„æŸæœç´¢ç®—æ³•ï¼‰ã€‚
  - é»˜è®¤ä¸ºFalse
7. bad_words_ids åŒ…å«è¯æ±‡idçš„åˆ—è¡¨ï¼Œè¿™ä¸ªå‚æ•°ç”¨äºæŒ‡å®šä¸å…è®¸åœ¨ç”Ÿæˆæ–‡æœ¬ä¸­å‡ºç°çš„è¯æ±‡,å¦‚æœç”Ÿæˆçš„æ–‡æœ¬åŒ…å«ä»»ä½•åœ¨è¿™ä¸ªåˆ—è¡¨ä¸­çš„è¯æ±‡ï¼Œå®ƒä»¬å°†è¢«è¢«æ›¿æ¢æˆ–æ’é™¤åœ¨æœ€ç»ˆç”Ÿæˆçš„æ–‡æœ¬ä¹‹å¤–ã€‚
8. force_words_ids åŒ…å«è¯æ±‡idçš„åˆ—è¡¨ï¼Œç”¨äºæŒ‡å®šå¿…é¡»åŒ…å«åœ¨ç”Ÿæˆæ–‡æœ¬ä¸­çš„è¯æ±‡ï¼Œå¦‚æœç»™å®šä¸€ä¸ªåˆ—è¡¨ï¼Œç”Ÿæˆçš„æ–‡æœ¬å°†åŒ…å«è¿™äº›è¯æ±‡ã€‚
9. constraints è‡ªå®šä¹‰çº¦æŸæ¡ä»¶ï¼Œå¯ä»¥æŒ‡å®šçº¦æŸæ¡ä»¶ï¼Œè¿™äº›çº¦æŸæ¡ä»¶å¯ä»¥æ˜¯å¿…é¡»å‡ºç°çš„å…³é”®è¯ã€çŸ­è¯­ã€ç‰¹å®šæœ¯è¯­æˆ–å…¶ä»–æ–‡æœ¬å…ƒç´ ï¼Œå…¶å®å’Œforce_words_idsæ˜¯å·®ä¸å¤šçš„æ„æ€ï¼Œåœ¨ä»£ç å®ç°ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚



## èµ„æ–™

- NLPç•Œè‘—åPythonåŒ…[Transformers](https://github.com/huggingface/transformers)
- è§£æè¿‡ç¨‹è§ï¼š[è§£è¯»Beam Search (1/2)](http://www.wuyuanhao.com/2020/03/20/%e8%a7%a3%e8%af%bbbeam-search-1-2/)





# ç»“æŸ
