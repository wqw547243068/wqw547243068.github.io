---
layout: post
title:   LLM基建 MCP+A2A
date:   2025-04-10 10:42:00
categories: 大模型
tags: mcp a2a
excerpt: LLM 基础设施架构：MCP+A2A
mathjax: true
permalink: /llm_infra
---

* content
{:toc}


# LLM 基础设施


## MCP

问题：
- 什么是 MCP？AI 应用的 `USB-C` 端口
- 为什么要 MCP？
- 如何使用/开发 MCP？

资料
- 【2025-3-9】[MCP (Model Context Protocol)](https://zhuanlan.zhihu.com/p/29001189476)
- 【2025-4-13】[模型上下文协议 (MCP) 可视化指南](https://zhuanlan.zhihu.com/p/1894743783121343410)
  - 原文 [Visual Guide to Model Context Protocol (MCP)](https://blog.dailydoseofds.com/p/visual-guide-to-model-context-protocol?source=queue)

![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4d25a162-5cc0-42cb-a7d9-a63a3210279a_1086x1280.gif)

### 什么是 MCP

`MCP`（模型上下文协议）：大模型飞速发，数据却常常被困在“孤岛”里，AI助手无法高效地获取和利用这些数据。

每新增一个数据源，都需要复杂的定制实现，不仅耗时还难以规模化扩展。

[微博](https://weibo.com/5648162302/5141628437136111)

 2024年11月25日, `Anthropic` 推出开放协议 `MCP`（Model Context Protocol）：
 - 官方文章 [Introducing the Model Context Protocol](https://www.anthropic.com/news/model-context-protocol)

像 AI 应用的 `USB-C` 接口，为 AI 模型与不同数据源和工具的整合提供了标准方式，使大语言模型（LLM）可以安全有序地访问本地或远程资源。

`MCP` （Model Context Protocol，模型上下文协议）定义了**应用程序**和 **AI 模型**之间交换**上下文信息**的方式。

开发者以一致方式将各种**数据源**、**工具**和**功能**连接到 AI 模型（中间协议层），像 `USB-C` 让不同设备能够通过相同的接口连接一样。

MCP 目标:
- 创建一个通用标准，使 AI 应用程序的开发和集成变得更加简单和统一
- ![](https://pic4.zhimg.com/v2-3a242914e1f4958e631dd158e043b7c3_1440w.jpg)

MCP 以更标准的方式让 LLM Chat 使用不同工具

Anthropic 实现 LLM Tool Call 标准
- ![](https://picx.zhimg.com/v2-9fe7fb51f264338a079a444eefa041b1_1440w.jpg)


### 为什么要用 MCP

以前，需要人工从数据库中筛选或使用工具检索可能信息，手动粘贴到 prompt 中。

问题: 场景复杂时，手工引入prompt 很困难。

许多 LLM 平台（如 OpenAI、Google）引入 `function call` 功能, 允许模型调用预定义函数获取数据或执行操作，提升自动化水平。
- 局限性: function call 平台依赖性强，不同 LLM 平台 function call API 实现差异较大。
- 例如，OpenAI 函数调用方式与 Google 不兼容，开发者在切换模型时需要重写代码，增加了适配成本。
- 除此之外，还有**安全性**，交互性等问题。

每个AI智能体各自为政，缺乏统一规则，互通有壁垒。

AI行业能否有一套大家都认可的协议，让智能体之间、智能体与工具之间互相对接更加顺畅。

Google 和 Anthropic 分别站了出来，各自抛出了一个方案：`A2A`协议和`MCP`协议。

数据与工具本身客观存在，如何将数据连接到模型的环节更智能、统一?
- Anthropic 设计了 MCP，充当 AI 模型的"万能转接头"，让 LLM 能轻松的获取数据或者调用工具。

MCP 优势：
- **生态** - MCP 提供很多插件， 直接使用。
- **统一性** - 不限制于特定的 AI 模型，任何支持 MCP 的模型都可以灵活切换。
- **数据安全** - 敏感数据留在自己的电脑上，不必全部上传。

### MCP 原理


Function Call 发展而来

- ![](https://pic3.zhimg.com/v2-2bcd98f6541da0b6f14dc9082ee2dcda_r.jpg)

分析
- MCP 本质：统一的协议标准，使 AI 模型以一致方式连接各种数据源和工具，类似于 AI 世界的"USB-C"接口。
- MCP 价值：解决了传统 function call 的**平台依赖**问题，提供更统一、开放、安全、灵活的工具调用机制，让用户和开发者都能从中受益。
- 使用与开发：
  - 对于普通用户，MCP 提供了丰富的现成工具，用户不了解任何技术细节的情况下使用；
  - 对于开发者，MCP 提供了清晰的架构和 SDK，使工具开发变得相对简单。


核心架构：
- MCP 采用 客户端-服务器 模型，包含 `MCP Host`（负责用户交互的平台）、`MCP Client`（与 Server 建立连接的桥梁）和 `MCP Server`（提供资源、工具和提示信息的轻量级程序）。

架构图
- ![](https://pica.zhimg.com/v2-9d3681630ed930a8dc74d3b452c0cc94_1440w.jpg)

MCP 三个核心组件：Host、Client 和 Server。
- 用 Claude Desktop (Host) 询问："我桌面上有哪些文档？"
- `Host`：Claude Desktop 作为 `Host`，负责接收提问并与 Claude 模型交互。
- `Client`：当 Claude 模型访问文件系统时，`Host` 中内置的 `MCP Client` 会被激活, 负责与适当的 `MCP Server` 建立连接。
- `Server`：文件系统 `MCP Server` 会被调用, 负责执行实际的文件扫描操作，访问桌面目录，并返回找到的文档列表。

MCP servers 提供三种主要类型的功能：
- `Resources`（资源）：类似文件的数据，可以被客户端读取（如 API 响应或文件内容）
- `Tools`（工具）：可以被 LLM 调用的函数（需要用户批准）
- `Prompts`（提示）：预先编写的模板，帮助用户完成特定任务


整个流程：
- 问题 → Claude Desktop(Host) → Claude 模型 → 需要文件信息 → MCP Client 连接 → 文件系统 MCP Server → 执行操作 → 返回结果 → Claude 生成回答 → 显示在 Claude Desktop 上。

![](https://pic3.zhimg.com/v2-3f7ceba80b16ef134b27119308a04472_1440w.jpg)

关键点：
1. 数据集成：无论是本地数据库、文件系统，还是远程的 API 服务，MCP 都能助力 AI 模型无缝对接。例如，在财务分析场景下，AI 模型可以通过 MCP 直接访问云端的财务数据 API，快速获取和处理海量的财务报表信息。
2. 工具调用：它为 AI 模型提供了丰富的预定义工具，包括执行脚本、浏览器自动化、金融数据查询等。就好比给 AI 模型配备了一整套“工具箱”，让它可以更灵活地完成各种任务。
3. 提示管理：通过标准化的提示模板指导 AI 模型完成任务。比如在内容创作场景中，AI 模型可以依据预设的提示模板，快速生成符合要求的文案、故事等内容。
4. 传输层多样：支持多种传输机制，像 tdio 传输、基于 HTTP+SSE 的传输等，保障了数据在不同网络环境下的高效传输。
5. 安全性：MCP 在基础设施内部保护数据，确保用户隐私和数据安全，让用户可以安心地使用 AI 应用。

MCP 这一技术突破了数据隔阂，使 AI 模型的数据获取更便捷、功能集成更灵活，推动我们迈向更智能、更互联的 AI 未来！


### MCP 生态

【2025-4-10】MCP 市场
- 国外 
  - [MCPMarket](https://mcpmarket.com/) Browse All MCP Servers
  - [MCP.so](mcp.so) Find Awesome MCP Servers and Clients The largest collection of MCP Servers.
    - 已经有 7966 个 MCP
- 国内 [MCPmarket](https://mcpmarket.cn/) 是中文首个聚焦MCP的工具市场

超过 6000个可直接调用的 MCP 工具 已上线，包括：
- 官方发布的MCP，如微软、字节、Perplexity
- 各类社区开发的MCP工具
- 涵盖社交、效率、数据、搜索、创作等各类工具
- 全中文文档、社区群支持
	
不仅能玩，还能学、以后还能赚！

| 类目 | 国外类目 | 国内类目 | 国内server数 | 国外servers数 |
| --- | --- | --- | --- | --- |
| Developer Tools | 开发者工具 | 暂无 | 暂无 | 3094 |
| API Development | API 开发 | 暂无 | 暂无 | 2397 |
| Data Science & ML | 数据科学与机器学习 | 暂无 | 暂无 | 1199 |
| Productivity & Workflow | 生产力与工作流程 | 暂无 | 暂无 | 886 |
| Web Scraping & Data Collection | 网络爬虫与数据收集 | 暂无 | 暂无 | 431 |
| Collaboration Tools | 协作工具 | 暂无 | 暂无 | 414 |
| Deployment & DevOps | 部署与开发运维 | 暂无 | 暂无 | 376 |
| Database Management | 数据库管理 | 暂无 | 暂无 | 366 |
| Learning & Documentation | 学习与文档 | 暂无 | 暂无 | 315 |
| Security & Testing | 安全与测试 | 暂无 | 暂无 | 285 |
| Cloud Infrastructure | 云基础设施 | 暂无 | 暂无 | 278 |
| Analytics & Monitoring | 分析与监控 | 暂无 | 暂无 | 247 |
| Design Tools | 设计工具 | 暂无 | 暂无 | 128 |
| Browser Automation | 浏览器自动化 | 暂无 | 暂无 | 101 |
| Social Media Management | 社交媒体管理 | 暂无 | 暂无 | 92 |
| Content Management | 内容管理 | 暂无 | 暂无 | 90 |
| Game Development | 游戏开发 | 暂无 | 暂无 | 84 |
| Official | 官方 | 暂无 | 暂无 | 79 |
| Marketing Automation | 营销自动化 | 暂无 | 暂无 | 59 |
| Other | 其他 | 暂无 | 暂无 | 1881 |


### MCP 实现


环境准备

```sh
# 安装 uv
curl -LsSf https://astral.sh/uv/install.sh | sh
# 创建项目目录
uv init txt_counter
cd txt_counter
# 设置 Python 3.10+ 环境
echo "3.11" > .python-version
# 创建虚拟环境并激活
uv venv
source .venv/bin/activate
# Install dependencies
uv add "mcp[cli]" httpx
# Create our server file
touch txt_counter.py
```

#### 自定义计数器

构造 prompt

打造一个 MCP 服务器，它能够：
- 功能：
  - 统计当前桌面上的 txt 文件数量
  - 获取对应文件的名字

要求：
- 不需要给出 prompt 和 resource 相关代码。
- 假设桌面路径为 /Users/{username}/Desktop

MCP 提供的功能包括：

（1）资源Resources

MCP服务中可提供的资源包括：文件内容、数据库记录、图像等。LLM可以通过MCP服务，读取文件、数据库等等。

（2）提示Prompt

包括可重复使用的提示模板和工作流程。提示使服务器能够定义可重用的提示模板和工作流程，客户端可以轻松地向用户和大型语言模型展示这些模板和工作流程

（3）工具Tools

LLM可用直接调用MCP中提供的工具，工具一般来说就是函数实现，MCP服务将会提供函数的描述和参数的描述给LLM，LLM将会判断应该执行哪个函数，并填写参数，最后在服务端执行函数。

（4）采样Sampling

让MCP 服务器请求 LLM的生成结果。 采样是MCP的一个强大功能，它允许 MCP 服务器通过客户端请求LLM的生成结果，从而实现复杂的智能行为，同时保持安全性和隐私性。


关键依赖
- mcp==0.1.0  # MCP 客户端库
- openai==1.0.0以上  # 支持 OpenAI 兼容API的客户端

MCP环境配置过程如下：

```sh
#（1）.创建一个uv项目
uv init mcp-test
cd mcp-test
# （2）.将 MCP 添加到项目依赖项中
uv add "mcp[cli]"
#
```


MCP Server 代码

```py
import os
from pathlib import Path
from mcp.server.fastmcp import FastMCP

# 创建 MCP Server
mcp = FastMCP("桌面 TXT 文件统计器")

@mcp.tool()
def count_desktop_txt_files() -> int:
    """Count the number of .txt files on the desktop."""
    # Get the desktop path
    username = os.getenv("USER") or os.getenv("USERNAME")
    desktop_path = Path(f"/Users/{username}/Desktop")

    # Count .txt files
    txt_files = list(desktop_path.glob("*.txt"))
    return len(txt_files)

@mcp.tool()
def list_desktop_txt_files() -> str:
    """Get a list of all .txt filenames on the desktop."""
    # Get the desktop path
    username = os.getenv("USER") or os.getenv("USERNAME")
    desktop_path = Path(f"/Users/{username}/Desktop")

    # Get all .txt files
    txt_files = list(desktop_path.glob("*.txt"))

    # Return the filenames
    if not txt_files:
        return "No .txt files found on desktop."

    # Format the list of filenames
    file_list = "\n".join([f"- {file.name}" for file in txt_files])
    return f"Found {len(txt_files)} .txt files on desktop:\n{file_list}"

if __name__ == "__main__":
    # Initialize and run the server
    mcp.run()
```


自定义工具注意：
- （1）要给每个函数工具**写好注释**
  - MCP服务将会**自动解析**这些注释，作为工具的描述。
- （2）定义好每个**参数**类型
  - 例如：`weight_kg: float`，MCP将自动解析参数的类型，作为参数的描述，并且会将LLM的输出结果自动转为相应的类型。
- （3）定义好**返回值**类型
  - 例如：`-> float`，这表示函数将返回一个float类型的值。

测试 server

```sh
mcp dev txt_counter.py
# Starting MCP inspector...
# Proxy server listening on port 3000

# MCP Inspector is up and running at http://localhost:5173
```

链接内容
- ![](https://pica.zhimg.com/v2-a5e671c689907229a1d86162597e2da4_1440w.jpg)

MCP 接入到 Claude Desktop 中。流程如下：

```
# 打开 claude_desktop_config.json (MacOS / Linux)
# 如果你用的是 cursor 或者 vim 请更换对应的命令
code ~/Library/Application\ Support/Claude/claude_desktop_config.json
```

在配置文件中添加以下内容，记得替换 /Users/{username} 为你的实际用户名，以及其他路径为你的实际路径。

```json
{
  "mcpServers": {
    "txt_counter": {
      "command": "/Users/{username}/.local/bin/uv",
      "args": [
        "--directory",
        "/Users/{username}/work/mcp-learn/code-example-txt", // 你的项目路径（这里是我的）
        "run",
        "txt_counter.py" // 你的 MCP Server 文件名
      ]
    }
  }
}
```

uv 最好是绝对路径，推荐使用 which uv 获取。
配置好后重启 Claude Desktop，如果没问题就能看到对应的 MCP Server 了

简单的 prompt 进行实际测试：
- 能推测我当前桌面上 txt 文件名的含义吗？

可能会请求使用权限，如图一所示，点击 Allow for This Chat
- ![](https://pic1.zhimg.com/v2-d99e12160a8ae3af75df8ddf7eddda24_1440w.jpg)


#### MCP 链接 DeepSeek


[如何用MCP实现DeepSeek的工具调用](https://zhuanlan.zhihu.com/p/1893771347332687580)

- 向 DeepSeek 提问：“我想知道我的身高和体重是否符合标准”
- DeepSeek 引导用户说出身高与体重，DeepSeek还能够根据用户说出的数字，自动将厘米单位转为为米，随后调用MCP服务中的calculate_bmi，实现BMI的计算
- 计算完成后，DeepSeek 将会根据计算结果，判断BMI是否为正常范围。

实现
- server.py: MCP服务器中定义了两个工具 ['get_time', 'calculate_bmi']
- client.py: DeepSeek将会直接调用MCP服务中的get_time工具，查询当前时间，并告诉用户当前的时间。

1. 用户输入
2. LLM响应，并输出
3. 判断LLM的响应是否为工具调用
4. 若结果为工具调用，执行工具，再将执行的结果发送给LLM，再次获得响应，并输出。

### MCP 调试

MCP Server Debug
- Official Tutorial: [Debugging](https://modelcontextprotocol.io/docs/tools/debugging)
- Official Tutorial: [Inspector](https://modelcontextprotocol.io/docs/tools/inspector)



## A2A



### 什么是 A2A

【2025-4-9】 [5000字长文带你看懂，Agent世界里的A2A、MCP协议到底是个啥]()

2025年4月10日，谷歌在 Google Cloud Next 2025大会上宣布开源首个标准**智能体交互协议** —— Agent2Agent Protocol（简称`A2A`），标志着智能体交互领域的一大突破。
- [Unlock Collaborative, agent to agent scenarios with a new open protocol](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)
- [A2A](https://github.com/google/A2A)
- [文档](https://google.github.io/A2A/#/)

![](https://google.github.io/A2A/images/a2a_main.png)


### MCP vs A2A

智能体通信的开放标准
- MCP 用于 Agent 与外部 Tool、API、资源之间的交互
- A2A 用于 Agent 与 Agent 之间的交互
- ![](https://google.github.io/A2A/images/a2a_mcp_readme.png)

- 用于工具和资源的`MCP`（模型上下文协议）
  - 将智能体与具有结构化输入/输出的工具、应用程序接口（APIs）和资源相连接。
  - 谷歌ADK支持MCP工具，使得大量MCP服务器可与智能体配合使用。
- 用于智能体间协作的`A2A`（智能体对智能体协议）
  - 不同智能体之间无需共享内存、资源和工具即可进行动态、多模态通信。 
  - 由社区推动的开放标准。
  - 使用谷歌ADK、LangGraph、Crew.AI 可获取相关示例。 

### A2A 生态

该协议将打破系统孤岛，对智能体的能力、跨平台、执行效率产生质的改变
- 支持 Atlassian、Box、Cohere、Intuit、Langchain、MongoDB、PayPal、Salesforce、SAP、ServiceNow、UKG和Workday 等主流企业应用平台。
- A2A协议的开源，类似于谷歌当年牵头80多家企业共同开发`安卓系统`，首批就有50多家著名企业加入。

随着更多企业的加入，A2A的商业价值将得到极大提升，并推动整个智能体生态的快速发展。

这一举措对于促进AI代理在企业间的通信、信息交换和行动协调具有重要意义。

谷歌还效仿 OpenAI 开源了Agent开发套件ADK、内部测试工具`AgentEngine`以及新的Agent市场等。

Google 发布关于Agent的新开放协议 Agent2Agent，简称A2A。

A2A（Agent-to-Agent）协议，让AI代理彼此直接对话、协同工作的协议。
- Salesforce、SAP、ServiceNow、MongoDB等在内的50多家科技公司的支持参与

### A2A 原理

设计初衷：
- 让不同来源、不同厂商的Agent 互相理解、协作。
- 就像 WTO 消减各国间关税壁垒一样。

![](https://google.github.io/A2A/images/a2a_mcp.png)

A2A 开放协议 为智能体提供了一种标准的协作方式，无论其底层框架或供应商如何。

在与合作伙伴设计该协议时，谷歌云遵循了五个关键原则：
- 拥抱**智能体能力**： A2A专注于使智能体能够以其自然、非结构化的方式进行协作，即使不共享内存、工具和上下文。谷歌云正在实现真正的多智能体场景，而不将智能体限制为“工具”。
- 基于现有标准构建： 该协议建立在现有的流行标准之上，包括HTTP、SSE、JSON-RPC，这意味着它更容易与企业日常使用的现有IT堆栈集成。
- 默认**安全**： A2A旨在支持企业级身份验证和授权，在发布时与OpenAPI的身份验证方案保持同等水平。
- 支持**长时间运行**的任务： 谷歌云将A2A设计得灵活，并支持各种场景，从快速完成任务到可能需要数小时甚至数天（当有人类参与时）的深度研究，它都能胜任。在此过程中，A2A可以向其用户提供实时反馈、通知和状态更新。
- **模态无关**： 智能体的世界不仅限于文本，这就是为什么谷歌云将A2A设计为支持各种模态，包括音频和视频流。

一旦采用A2A，不同供应商和框架的Agent(小国家)，加入了一个自由贸易区，能够用共同语言交流、无缝协作，联手完成单个Agent难以独立完成的复杂工作流程。

A2A促进了“客户端”智能体和“远程”智能体之间的通信。

客户端智能体负责制定和传达任务，而远程智能体负责对这些任务采取行动，以尝试提供正确的信息或采取正确的行动。这种交互涉及几个关键能力：
- 能力发现： 智能体可以使用JSON格式的“智能体卡片（Agent Card）”来宣传其能力，允许客户端智能体识别能够执行任务的最佳智能体，并利用A2A与远程智能体通信。
- 任务管理： 客户端和远程智能体之间的通信面向任务完成，智能体在其中努力满足最终用户的请求。这个“任务”对象由协议定义并具有生命周期。它可以立即完成，或者对于长时间运行的任务，每个智能体可以进行通信以就完成任务的最新状态保持同步。任务的输出被称为“工件（artifact）”。
- 协作： 智能体可以相互发送消息以传达上下文、回复、工件或用户指令。
- 用户体验协商： 每条消息包含“部分（parts）”，这是一个完全成型的内容片段，如生成的图像。每个部分都有指定的内容类型，允许客户端和远程智能体协商所需的正确格式，并明确包括用户UI能力的协商——例如，iframes、视频、Web表单等。


## 应用


### 爬虫


用 DeepSeek 和 MCP 实现一句话抓取网页
- 工具: Cursor 里的 FireCrawl Agent MCP


### 招聘


招聘经理通过 Agentspace 界面让代理寻找合适的候选人👀 
- [a2a-a-new-era-of-agent-interoperability](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)
1. 客户端代理与其他专门代理协作，搜集潜在候选人
2. 用户收到候选人建议后，指示代理安排面试
3. 面试结束后，另一个代理进行背景调查

演示视频
- [demo](https://x.com/i/status/1910117477859942582)

### 阿里云

【2025-4-9】阿里云百炼也官宣搞MCP

### AI代码管家

【2025-4-6】 [GitHub突然放大招！全球首个「AI代码管家」炸场，程序员集体沸腾](https://mp.weixin.qq.com/s/R3r6y68bXvWkVOy7iztN4g)

GitHub 官宣一款代号`MCP Server`的开源神器，短短2小时冲上Hacker News榜首

这款由GitHub联合AI独角兽Anthropic打造的「代码智能管家」，用Go语言彻底重构，号称能让AI替你打工写代码、修Bug、管仓库，甚至能用说人话指挥它干活

旧版服务器只能做“代码搬运工”，而 MCP Server 直接打通**AI大脑**和**代码仓库**，把GitHub变成「会思考的活系统」

5大逆天功能，打工人跪求内测
1. 🤖 AI审查官：PR合并速度飙升530%
2. 🔒 仓库「安全盾」：72小时漏洞→4小时灭火
3. 🛠️ 自定义武器库：代码扫描任你魔改
4. 🌐 跨仓库「量子纠缠」：依赖冲突减少30%
5. 🎙️ 说人话编程：「get_me」函数封神

过去程序员最怕什么？等PR等到天荒地老！

传统流程：人工检查TODO标记、License冲突、代码规范 → 平均耗时3.2天

MCP Server 骚操作：
- • 自动扫描PR，1秒生成漏洞地图（配图：高亮显示未处理TODO的代码截图）
- • 附赠AI修复方案，甚至能联动CI/CD自动打补丁
某电商大厂实测：6小时极速合码，半夜12点的咖啡终于省了！

2. 🔒 仓库「安全盾」：72小时漏洞→4小时灭火

硬编码密钥、SQL注入、敏感信息泄露…这些坑你踩过吗？

MCP Server每天定时扫描全仓库，连陈年老代码都不放过！

真实案例：某金融公司凌晨2点突现密钥泄露风险，AI秒级定位+自动生成掩码方案，修复速度从72小时→4小时，安全团队集体保住年终奖💰

3. 🌐 跨仓库「量子纠缠」：依赖冲突减少30%

微服务架构下最头疼的依赖地狱，MCP Server一招破解：
- • 自动对齐多项目版本（比如同时升级Spring Boot到2.7.5）
- • 智能推荐兼容方案（配图：依赖关系可视化图谱）
某开源社区维护者哭诉：“早用这个，我能少秃一半！”

1. 🎙️ 说人话编程：「get_me」函数封神

对着电脑喊一嗓子就能操控GitHub？

神奇语法：

```sh
get_me("显示我上周创建的私有仓库")  
get_me("把feature/login分支的TODO全改成FIXME")  
```

网友玩梗：“以后改代码是不是得先考普通话二甲？”

5. 🛠️ 自定义武器库：代码扫描任你魔改

厌倦了千篇一律的ESLint规则？MCP Server开放工具描述自定义接口：
- • 写个YAML文件就能接入自研扫描工具
- • 联动AI做动态测试（比如模拟万人并发压测）

极客玩法：有人已经做出了二次元风格代码审查插件，检测到Bug就播放“阿姨压一压”…


### 高德MCP


Al+高德MCP：10分钟自动制作一份旅行手卡
- 小红书[笔记](https://www.xiaohongshu.com/explore/67e8fd5a000000001d027d85?app_platform=android&ignoreEngage=true&app_version=8.77.0&share_from_user_hidden=true&xsec_source=app_share&type=video&xsec_token=CB3Vv4c5kyzT8aT-BZx9UUIfCirVpBEjLskUR9VeP3dck=&author_share=1&xhsshare=WeixinSession&shareRedId=OD06NUlINT42NzUyOTgwNjY7OTpIOT5B&apptime=1744081522&share_id=85632878adf0407c968e139f82f906c4&share_channel=wechat&wechatWid=da79273b5c86874c0a4ff1852e64df68&wechatOrigin=menu)

# 结束
