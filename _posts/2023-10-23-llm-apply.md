---
layout: post
title:   大模型落地方案 LLM Solution
date:   2023-10-23 16:52:00
categories: 大模型
tags: 微调 RAG 
excerpt: 大模型工业落地经验总结
mathjax: true
permalink: /llm_solution
---

* content
{:toc}

# LLM 应用实践

开箱即用的预训练LLM没有按预期或希望执行时，如何提高LLM应用的性能？
- 用检索增强生成（RAG）还是模型微调来改善结果？

## 如何选择优化方法


建议
- 微调之前先尝试RAG


## 方法分析

### RAG vs finetune

分析
- 微调在特定任务上训练模型，就像在问题解答数据集上微调 GPT-3.5 以提高其在特定数据集上的问题解答性能一样。

判断
- 如果数据集**足够大**而且**不会变**，那么采用**微调**。
- 如果数据集动态变化，需要不断重新训练模型，以跟上变化。
- 如果没有大型数据集，不建议微调。建议用 RAG 来提高 LLM 的性能。同样，也可用 RAG 来提高 LLM 在摘要、翻译等任务上的性能，因为这些任务可能无法进行微调。

这两种方法都获得类似结果，但在复杂性、成本和质量方面有所不同。
- ![](https://pic1.zhimg.com/80/v2-c2058b77b95bdb3fb533b7949a6258b8_1440w.webp)

RAG更简单、便宜，但质量可能不匹配。

但这两种方案不是实现相同结果的两个方案，而是正交，满足LLM应用的不同需求。

RAG 和 微调之间的细微差别跨越了模型架构、数据需求、计算复杂性等。忽略这些细节可能会破坏项目时间轴和预算。

如何选择？
- **访问外部数据源**？是 → RAG 更有效、容易扩展
  - 非常适合需要查询数据库、文档或其他结构化/非结构化数据存储库的应用
  - 微调需要大量标注数据集，数据更新时，模型更新不及时
  - 微调过程没有对查询外部知识的检索和推理步骤进行建模。
- **修改模型行为、风格、领域知识**？是 → 微调
  - 微调擅长将LLM行为适配到特定细微差别、语调或术语，如 医学专业人士、以诗意的风格写作，或者使用特定行业的行话
  - RAG虽然善于整合外部知识，但主要侧重信息检索，不会根据检索信息调整其语言风格或领域特异性
- 抑制幻觉重要吗？是 → RAG
  - RAG 相对 微调 不容易产生幻觉，检索机制相当于事实检查器
- 监督语料多吗？是 → 微调，否则 RAG
  - 微调依赖有标签数据的数量和质量，数据不足会过拟合
- 数据会变化吗？是 → RAG
  - 如果数据经常更新，模型容易过时，而重新训练耗时耗力，增加评估成本
  - RAG 检索机制不断查询外部资源，保持最新，知识库/数据源更新时，RAG无缝集成，保持相关性，不用频繁训练
- 要求可解释吗？如果要求较高的透明性+可解释性 → RAG
  - LLM 原理像黑盒，推理机制不明，难以解释为什么
  - RAG 透明性相对较高，检索+生成，用户可以洞察全过程

|维度|解释|`RAG`|`FineTune`|
|---|---|---|
|External knowledge read?|访问外部数据?|✅|❌|
|Changing model behaviour read?|改变模型行为?|❌|✅|
|Minimise hallucinations?|幻觉最小化?|✅|❌|
|Training data availiable?|较多训练数据?|❌|✅|
|Is data (mostly) dynamic?|数据动态变化?|✅|❌|
|Interpretability|要求可解释?|✅|❌|


建议：
- 从RAG开始，评估其性能，如果发现不足，则转向微调。
- 最佳选择: 自动化，混合方法
  - 微调确保聊天机器人符合公司的品牌、语调和一般知识，处理大多数典型的客户查询。
  - RAG可以作为一个补充系统，处理更动态或更具体的查询，确保聊天机器人能够从最新的公司文档或数据库中获取信息，从而最大限度地减少幻觉。
  - 整合这两种方法，公司可以提供全面、及时且与品牌一致的客户支持体验。
- ![](https://pic1.zhimg.com/80/v2-8a98c6db80f32f2fea6fa2503360fd38_1440w.webp)

### 四种方法对比

【2023-10-17】[如何选择最适合你的LLM优化方法：全面微调、PEFT、提示工程和RAG对比分析](https://zhuanlan.zhihu.com/p/661830285?utm_psn=1697685536221999105)
- [RAG vs Finetuning — Which Is the Best Tool to Boost Your LLM Application?](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)

四种主要的调优方法：
- **全面微调**：使用任务特定数据调整LLM的所有参数。
  - 一个较小、任务特定、带标签的数据集上进行微调，调整一些模型参数，优化其对特定任务或一组任务的性能
  - 全面微调： 所有模型参数都被更新，使其类似于预训练，只不过是在一个**带标签**且**规模较小**的数据集上进行。
  - ![](https://pic2.zhimg.com/80/v2-e8c7286930eb81b57aaf109fe92ac58d_1440w.webp)
  - 优点: 训练数据集更少、提高精度、增加鲁棒性
  - 缺点: 高计算成本、内存需求高、时间/专业知识密集
- **参数高效精细调整**（PEFT）：修改选定参数以实现更高效的适应。进一步调整预训练模型，只更新其总参数的一小部分
  - PEFT 方法可训练的部分不同。一些技术优先训练原始模型参数的**选定部分**。其他方法集成并训练较小的**附加组件**，如适配器层，而不修改原始结构
  - ![](https://pic2.zhimg.com/80/v2-1d62f9b57373a592407db8aedd90b681_1440w.webp)
  - LoRA是最常用的 PEFT 方法，使用重参数化，这种技术通过执行低秩近似来缩小可训练参数的集合。
  - LoRA优点：
    - 任务切换效率 - 创建模型的不同版本以适应特定任务变得更容易。你可以简单地存储预训练权重的单个副本，并构建许多小 LoRA 模块。当你从任务切换到任务时，你只替换矩阵 A 和 B，并保留 LLM。这显著减少了存储需求。
    - 需要更少的 GPU - LoRA 将 GPU 内存需求减少了最多 3 倍，因为我们不计算/重新训练大多数参数。
    - 高精度 - 在各种评估基准上，LoRA 的性能被证明几乎等同于全面微调 - 而且只需要一部分成本
  - PEFT 相比全面微调的优势
    - 更高效和更快的训练
    - 保留预训练的知识
- **提示工程**：改进模型输入以指导其输出。
  - 在新数据集和任务上训练模型参数，使用所有预训练权重（如全面微调）或一组独立权重（如 LoRA）。
  - 相比之下，提示工程根本不涉及训练网络权重
  - ![](https://pic3.zhimg.com/80/v2-4e5ddc95da8e4945cf30c65e1593050e_1440w.webp)
  - 基础提示: 零样本提示、少样本提示、链式思考引导
  - ![](https://pic4.zhimg.com/80/v2-857d925cf7adc11d94a2fbd9aca37213_1440w.webp)
- **RAG**（检索增强生成）：将提示工程与数据库查询结合，以获得丰富的上下文答案。
  - 将引导工程与从外部数据源检索上下文相结合，以提高语言模型的性能和相关性。通过在模型上附加额外信息，它允许更准确和上下文感知的响应。
  - RAG模型架构将用户查询的嵌入与知识库向量中的embedding进行比较，将来自知识库中相似文档的相关上下文附加到原始用户提示中。然后将这个增强的prompt给到LLMs，可以异步更新知识库及其相关的embedding
  - ![](https://pic3.zhimg.com/80/v2-db7c5fbf5f95c69846fc3805eb287086_1440w.webp)
  - RAG 本质上将信息检索机制与文本生成模型相结合。信息检索组件有助于从数据库中拉取相关的上下文信息，并且文本生成模型使用这个添加的上下文来产生更准确和“知识丰富”的响应。以下是它的工作方式：
    - 向量数据库：实施 RAG 包括嵌入内部数据集，从中创建向量，并将它们存储在向量数据库中。
    - 用户查询：RAG 从提示中获取用户查询，这是一个需要回答或完成的自然语言问题或陈述。
    - 检索组件：一旦接收到用户查询，检索组件扫描向量数据库以识别与查询语义相似的信息块。然后使用这些相关片段为 LLM 提供额外上下文，使其能够生成更准确和上下文感知的响应。
    - 串联：将检索到的文档与原始查询串联成一个提供生成响应所需额外上下文的提示。
    - 文本生成：将包含串联查询和检索文档的提示馈送到 LLM 以产生最终输出。
    - ![](https://pic1.zhimg.com/80/v2-63c902a479d54ff27917dd94d3c65174_1440w.webp)
    - 开源应用框架: 
      - OpenAI [chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin)
      - [langchain](https://github.com/langchain-ai/langchain)
      - [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/index.html)
  - [Creating a RAG Pipeline with LangChainPermalink](https://www.maartengrootendorst.com/blog/improving-llms/#creating-a-rag-pipeline-with-langchain), [中文版](https://zhuanlan.zhihu.com/p/661349721?utm_psn=1697558407270424576)
  - ![RAG方法的大致过程](https://www.maartengrootendorst.com/assets/images/posts/2023-12-09-improving-llms/rag.svg)
  - RAG 有许多明显的优点：
    - 最小化幻觉 - 当模型做出“最佳猜测”假设，本质上填补了它“不知道”的内容时，输出可能是错误的或纯粹的胡说八道。与简单的提示工程相比，RAG 产生的结果更准确，幻觉的机会更低。
    - 易于适应新数据 - RAG 可以在事实可能随时间演变的情况下进行适应，使其对生成需要最新信息的响应非常有用。
    - 可解释 - 使用 RAG，可以确定 LLM 答案的来源。对答案来源进行追溯对于内部监控、质量保证或处理客户纠纷可能是有益的。
    - 成本有效 - 与在特定任务数据集上对整个模型进行微调相比，你可以使用 RAG 获得相当的结果，这涉及到更少的标记数据和计算资源。
  - RAG 的潜在限制
    - RAG 旨在通过从外部文档中提取上下文来增强 LLM 的信息检索能力。然而，在某些使用案例中，额外的上下文还不够。如果一个预训练的 LLM 在总结财务数据或从患者的医疗文档中提取见解方面遇到困难，很难看出以单个文档形式提供额外上下文如何有所帮助。在这种情况下，微调更有可能产生期望的输出。

[improving-llms](https://www.maartengrootendorst.com/blog/improving-llms/), 3 of the most common methods for improving the performance of any LLM:
- Prompt Engineering
- Retrieval Augmented Generation (RAG)
- Parameter Efficient Fine-Tuning (PEFT)
- ![](https://www.maartengrootendorst.com/assets/images/posts/2023-12-09-improving-llms/common.svg)
- ![](https://www.maartengrootendorst.com/assets/images/posts/2023-12-09-improving-llms/overview.svg)

四个重要指标上进行比较：复杂性、成本、准确性和灵活性。
- **成本**： PE ＜ RAG ＜ PEFT ＜ Full Fine-tuning
- **复杂性**：PE ＜ RAG ＜ PEFT = Full Fine-tuning
- **准确性**：
  - 特定领域术语：PE ＜ RAG ＜ PEFT ＜ Full Fine-tuning
  - 时效性：PEFT = Full Fine-tuning < PE < RAG
  - 可解释性：PE = PEFT = Full Fine-tuning < RAG
  - 幻觉: PE < PEFT < Full Fine-tuning < RAG
    - 微调可以通过将 LLM 集中在特定领域数据上来减少这些幻觉。然而，不熟悉的查询仍然可能导致 LLM 编造出一个捏造出来的答案。
    - RAG 通过将 LLM 的响应锚定在检索到的文档中来减少幻觉。初始检索步骤本质上进行事实检查，而随后生成受限于检索数据的上下文。对于避免幻觉至关重要的任务，推荐使用 RAG。
  - 总结
    - 解释性、时效性和避免幻觉至关重要 → RAG
    - 要求特定领域风格 → 全面微调 和 PEFT
    - 两者都要 → 微调 和 RAG
- **灵活性**： Full Fine-tuning < PEFT < PE < RAG


## （1）PE 提示工程

**提示工程**：改进模型输入以指导其输出。
- 在新数据集和任务上训练模型参数，使用所有预训练权重（如全面微调）或一组独立权重（如 LoRA）。
- 相比之下，提示工程根本不涉及训练网络权重
- ![](https://pic3.zhimg.com/80/v2-4e5ddc95da8e4945cf30c65e1593050e_1440w.webp)
- 基础提示: 零样本提示、少样本提示、链式思考引导
- ![](https://pic4.zhimg.com/80/v2-857d925cf7adc11d94a2fbd9aca37213_1440w.webp)


## （2）RAG 检索增强生成

### 起因

LLM 通过大量数据训练，回答任何问题或完成任务，利用其参数化记忆。这些模型有一个知识**截止日期**，取决于上次训练的时间。
- 被问及超出其知识范围或知识截止日期后发生的事件时，模型会产生**幻觉**。

Meta 研究人员发现，通过提供与手头任务相关的信息，模型在完成任务时表现**显著改善**。

例如，询问模型关于截止日期之后发生的事件，则提供该事件作为背景信息并随后提问将帮助模型正确回答问题。

由于LLM具有有限的上下文窗口长度，在处理当前任务时**只能传递最相关的知识**。添加到上下文中数据质量影响着模型生成响应结果的质量。机器学习从业者在RAG流程不同阶段使用多种技术来改善LLM性能。

### 什么是 RAG

科里·祖
> “检索增强生成是用您（系统）从其他地方检索到的附加信息来补充用户输入到 ChatGPT 等大型语言模型 (LLM) 的过程。然后，法学硕士可以使用该信息来增强其生成的响应。” 

检索增强生成（简称 `RAG`）是 Meta 于 2020 年推广的一种架构，通过将**相关信息**与**问题/任务细节**一起传递给模型来提高 LLM 的性能。


【2023-9-27】[RAG 与 Finetuning，谁是提升 LLM 的最佳工具？](https://mp.weixin.qq.com/s/D-8r3FHKCyh4xk-yM7lMag)

### RAG 流程

将原始文件拆解后, 每个部分都会生成相应embedding 并且 存放到vector store 中. 当查询发送给 vector store 时, 查询也会转换为 embedding , 然后 vector store 返回与查询最相似 的 embeddings
- ![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jq9bEbitg1Pv4oASwEQwJg.png)

RAG 包含三个阶段：数据准备、检索和生成。
- **数据准备**阶段：确定数据源、从数据源中提取数据、清理数据并将其存储到数据库中。
  - 识别数据来源、从来源中提取数据、清洗数据并将其存储在数据库中
  - 向量存储器：存储文本、图像、音频等非结构化数据，并基于语义相似性搜索该类别下的内容。
- **检索**阶段：根据手任务从数据库中检索相关数据。
  - 关键词搜索：简单的检索数方法，数据根据关键词进行索引，并且搜索引擎返回包含这些关键的文档。
  - 关键词搜索适用于存储**结构化数据**（如表格、文档等）并使用关键词对数据进行搜索。
  - 图数据库以节点和边的形式存储数据。适用于存储结构化数据（如表格、文档等），并通过数据之间的关系进行搜索
  - 搜索引擎：从公共搜索引擎（如Google、Bing等）或内部擎（如Elasticsearch、Solr等）中检索RAG管道中的数据；搜索引擎适用于从网络上检索数据并使用关键字对其进行搜索。
  - 可将来自**搜索引擎**的数据与**其他数据库**（如向量存储、图数据库等）中获取到的数据相结合，以提高输出质量。推荐结合多种策略（如语义搜索 + 关键字匹配）的混合方法
  - 矢量数据库中对嵌入式数据进行相似性搜索
- **生成**阶段：利用检索到的数据和任务生成输出结果。
  - 检索到相关数据，就会连同用户的查询或任务一起传递给生成器（LLM）。LLM 使用检索到的数据和用户的查询或任务生成输出
  - 输出质量取决于数据的质量和检索策略。



### 提升性能

【2023-10-1】[提升RAG性能的 10 种方法](https://mp.weixin.qq.com/s/WDV31S3C7YQKekwJTIYt5Q)

使用 LangChain 或 LlamaIndex 等框架的快速入门指南，任何人都可以使用大约五行代码构建一个简单的 RAG 系统，例如文档的聊天机器人。

但是，用这五行代码构建的机器人不会很好地工作。RAG 很容易制作原型，但很难达到用户满意的地步。基本教程可能会让 RAG 以 80% 的速度运行。但要弥补接下来的 20%，通常需要进行一些认真的实验。

提高RAG性能的 10 种方法
- 清理数据：提升数据质量，优化数据分布
- 探索不同索引类型：索引是LlamaIndex和LangChain的核心
  - RAG 标准方法涉及嵌入和相似性搜索，将上下文数据分块，嵌入所有内容，当查询到来时，从上下文中找到相似的部分。
  - 这种方法效果很好，但并不是适合每个用例的最佳方法。
- 尝试多种分块方法
  - 将上下文数据分块是构建 RAG 系统的核心
  - 块大小很重要。较小的块通常可以改善检索，但可能会导致生成过程缺乏周围的上下文；
  - 小、中、大块大小循环浏览了每一组，发现小是最好的
- 覆盖基本提示: 使用时覆盖基础提示
  - 示例： 你是一名客户支持代理。您的目的是在仅提供事实信息的同时尽可能提供帮助。你应该友好，但不要过于健谈。`上下文信息如下。给定上下文信息而不是先验知识，回答查询`。
- 元数据过滤
  - 元数据（如日期）添加到块中，然后用它来帮助处理结果
  - 构建 RAG 时要记住的一般概念：<span style='color:red'>相似 ≠ 相关</span>
- 查询路由
  - 适用场景：有多个索引，如摘要、敏感问题识别、日期相关，优化成一个索引不一定好
- 重排名
  - 重新排名是解决**相似性**和**相关性**之间差异问题的一种解决方案
  - 如 Cohere Rereanker，LangChain 和 LlamaIndex 都有抽象，可以轻松设置。
- 查询转换（改写）
  - 将用户查询放入基本提示中来更改它
  - 重新措辞：如果系统找不到查询的相关上下文，让LLM重新措辞查询并重试
  - HyDE 是一种策略，接受查询，生成假设的响应，然后将两者用于嵌入查找。研究发现这可以显着提高性能。
  - 将一个查询分解为多个问题（子查询），LLM在分解复杂查询时往往会工作得更好
- 微调embedding模型
  - 基于嵌入的相似性是 RAG 的标准检索机制
  - 预训练模型（如 OpenAI的ada）关于嵌入空间中相似内容的概念可能与场景上下文中相似内容不一致
    - 处理法律文件：希望嵌入更多地基于您的领域特定术语（例如“知识产权”或“违反合同”）对相似性的判断，而不是基于“特此”和“协议”等一般术语。
  - 微调可以将检索指标提高 5-10%，LlamaIndex 可以生成训练集
- LLM 开发工具
  - LlamaIndex 或 LangChain 这两个框架都提供调试工具，允许定义回调、查看使用的上下文、检索来自哪个文档等等。
  - Arize AI 有一个笔记本内工具，可探索如何检索哪些上下文及其原因。
  - Rivet 是一个提供可视化界面的工具，可帮助您构建复杂的代理，由法律技术公司 Ironclad 开源。

其它：【2023-9-27】[检索增强生成 (RAG):What, Why and How?](https://mp.weixin.qq.com/s?__biz=MzkzNDQxNDU1Ng==&mid=2247484067&idx=1&sn=1eafa47e700526ecd9862f7c39587738&chksm=c2bcd310f5cb5a06db8832792c4d810e1a53a26b4435195cba6adf5848e8a253b6150f102834&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)
- 混合搜索：将语义搜索与关键词搜索结合起来，从向量存储中检索相关数据 —— 已被证明对大多数用例都能获得更好的结果
- 摘要：对块进行摘要并将摘要存储在向量存储中，而不是原始块
- 丢失问题：LLMs并不给予输入中所有标记**相同权重**。中间标记似乎比输入开头和结尾处的标记被赋予较低权重，中间丢失问题。
  - 可重新排列上下文片段，使最重要的片段位于输入**开头**和**结尾**，并将次要片段放置在中间位置。

### RAG-Fusion

【2023-10-7】[使用RAG-Fusion和RRF让RAG在意图搜索方面更进一步](https://mp.weixin.qq.com/s/N7HgjsqgCVf2i-xy05qZtA)
- 原文: [Forget RAG, the Future is RAG-Fusion](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1)
- [第三方来源](https://luxiangdong.com/2023/10/07/ragfusion/#/%E6%B7%B1%E5%85%A5%E7%A0%94%E7%A9%B6RAG-Fusion%E7%9A%84%E6%9C%BA%E5%88%B6)，包含图片

#### 起因

RAG有许多优点:
- 向量搜索融合： RAG通过将向量搜索功能与生成模型集成，引入了一种新的范例。这种融合能够从大型语言模型(大语言模型)生成更丰富、更具上下文感知的输出。
- 减少幻觉： RAG显著降低了LLM的幻觉倾向，使生成的文本更基于数据。
- 个人和专业实用程序：从个人应用程序如筛选笔记到更专业的集成，RAG展示了在提高生产力和内容质量方面的多功能性，同时基于可信赖的数据源。
然而，我发现越来越多的“限制”:

当前搜索技术限制： 
- RAG受到基于检索的**词法**和**向量搜索技术**的相同限制。
- 人工搜索效率低下：人类并不擅长在搜索系统中输入他们想要的东西，比如打字错误、模糊的查询或有限的词汇，这通常会导致错过明显的顶级搜索结果之外的大量信息。虽然RAG有所帮助，但它并没有完全解决这个问题。
- 搜索的**过度简化**：流行的搜索模式将查询线性地映射到答案，缺乏深度来理解人类查询的多维本质。这种线性模型通常无法捕捉更复杂的用户查询的细微差别和上下文，从而导致相关性较低的结果。


#### 为什么使用RAG-Fusion?

为什么使用RAG-Fusion
- 通过生成**多个**用户查询和**重新排序**结果来解决RAG固有的约束。
- 利用**倒数排序融合**（RRF）和自定义向量评分加权，生成全面准确的结果。

RAG-Fusion 弥合用户明确提出的问题和（原本的意图）打算提出的问题之间的差距，更接近于发现通常仍然隐藏的变革性知识。
- [RAG-Fusion代码](https://github.com/Raudaschl/rag-fusion)

#### RAG-Fusion 机制

RAG Fusion的基本三要素与RAG相似，并在于相同的三个关键技术:
- 通用编程语言，通常是Python。
- 专用的向量搜索数据库，如Elasticsearch或Pinecone，指导文档检索。
- 强大的大型语言模型，如ChatGPT，制作文本。

然而，与RAG不同，RAG-Fusion通过几个额外步骤来区分自己——查询生成和结果的重新排序。

RAG-Fusion’s 工作步骤:
- 查询语句的相关性复制(多查询生成)：通过LLM将用户的查询转换为**相似但不同**的查询。
  - 单个查询可能无法捕获用户感兴趣的全部范围，或者它可能太窄而无法产生全面的结果。这就是从不同角度生成多个查询的原因。
- 并发的向量搜索：对原始查询及其新生成的同级查询执行并发的向量搜索。
- 智能重新排名：聚合和细化所有结果使用倒数排序融合(RRF)。
- 最后优中选优：将精心挑选的结果与新查询配对，引导LLM进行有针对性的查询语句输出，考虑所有查询和重新排序的结果列表。
- ![](https://simg.baai.ac.cn/hub-detail/9d417a2e8e269db73c441281edd48cd31696839601937.webp)

#### 多查询生成

工作原理:
- 对语言模型的函数调用:函数调用语言模型(在本例中为chatGPT)。这种方法需要一个特定的指令集(通常被描述为“系统消息”)来指导模型。例如，这里的系统消息指示模型充当“AI助手”。
- 自然语言查询:然后模型根据原始查询生成多个查询。
- 多样性和覆盖范围:这些查询不仅仅是随机变化。它们是精心生成的，以提供对原始问题的不同观点。例如，如果最初的查询是关于“气候变化的影响”，生成的查询可能包括“气候变化的经济后果”、“气候变化和公共卫生”等角度。
- ![](https://luxiangdong.com/images/ragfusion/4.jpeg)

#### 倒数排序融合 (RRF)

Why RRF?
- 倒数排序融合(RRF) 是一种将具有不同相关性指标的多个结果集组合成单个结果集的方法，不同的相关性指标也不必相互关联即可获得高质量的结果。

该方法的优势在于不利用相关分数，而仅靠排名计算。相关分数存在的问题在于不同模型的分数范围差。RRF是与滑铁卢大学(CAN)和谷歌(Google)合作开发
- [cormacksigir09-rrf](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)
- “比任何单独的系统产生更好的结果，比标准的重新排名方法产生更好的结果”。

在elasticsearch的8.8版本，已经引入了RRF。

RRF 想象成那种坚持在做决定之前征求每个人意见的人，这种意见是有帮助的，兼听则明，多多益善
- ![](https://luxiangdong.com/images/ragfusion/6.jpeg)

#### 生成输出

用户意图保存
- 使用多个查询的挑战之一是可能会削弱用户的原始意图。为了缓解这种情况，我们指示模型在prompt工程中给予原始查询更多的权重。


#### RAG-Fusion优缺点

优势
- 1、优质的原材料质量
  - 使用RAG Fusion时，搜索深度不仅仅是“增强”，并且其实搜索范围已经被放大了。相关文档的重新排序意味着你不仅仅是在抓取信息的字面意思，而是在深入这个搜索的意图，所以会涉及到更多的优质文档和待搜索内容。
- 2、增强用户意图对齐
  - RAG Fusion的设计理念中包含了自动提示，很多时候我们在搜索的时候并不知道应该怎么描述，像Google、百度就会进行输入框的自动补全提示。RAG Fusion可以捕获用户信息需求的多个方面，从而提供整体输出并与对用户意图进行增强。
- 3、自动为用户查询输入纠错
  - 该系统不仅可以解释用户的查询，还可以精炼用户的查询。通过生成多个查询变体，RAG Fusion执行隐式拼写和语法检查，从而提高搜索结果的准确性。
- 4、导航复杂查询（自动分解长句的意图）
  - 人类的语言在表达复杂或专门的思想时往往会结结巴巴。该系统就像一个语言催化剂，生成各种变体，这些变体可能包含更集中、更相关的搜索结果所需的行话或术语。它还可以处理更长的、更复杂的查询，并将它们分解成更小的、可理解的块，用于向量搜索。
- 5、搜索中的意外发现（关联推荐）
  - 以前在亚马逊买书的时候，总能因为相关推荐发现我更想要的书，RAG Fusion允许这个偶然的发现。通过使用更广泛的查询范围，系统有可能挖掘到信息，而这些信息虽然没有明确搜索，但却成为用户的“啊哈”时刻。这使得RAG Fusion有别于其他传统的搜索模型。

挑战
- 1、过于啰嗦的风险
  - RAG-Fusion的深度有时会导致信息泛滥。输出可能会详细到令人难以承受的程度，把RAG-Fusion想象成一个过度解释事物的朋友。
- 2、可能成本会比较昂贵
  - 多查询输入是需要LLM来做处理的，这时候，很有可能会引起更多的tokens消耗。


### Self-RAG

【2023-10-17】华盛顿大学 [Self-RAG：通过自我反思实现检索增强生成](https://zhuanlan.zhihu.com/p/662969847)

检索增强生成（Retrieval-Augmented Generation，`RAG`）方法通过检索相关知识来减少这类问题，降低了LLMs在知识密集型任务中的事实错误率）。但是，会存在如下问题
- <span style='color:red'>不加区别地检索和合并一定数量的检索文段</span>，无论是否**需要检索**或文段**是否相关**，这会降低LLMs的**多功能性**或导致生成质量不佳（Shi等人，2023），因为不加区别地检索文段，无论事实支持是否有帮助。
- <span style='color:red'>生成结果与检索段落未必一致</span>（Gao等人，2023），因为这些模型没有明确训练以利用和遵循所提供文段的事实。
- ![](https://pic1.zhimg.com/80/v2-86a0ca09dc393444588990f487fdfa00_1440w.webp)

论文提出一种新框架：`自我反思检索增强生成`（[SELF-RAG](https://selfrag.github.io/)），通过**按需检索**和**自我反思**来提高LLM的生成质量，包括其事实准确性，而不损害其多功能性。
- [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511)
- [SELF-RAG](https://selfrag.github.io/) 主页包含代码、模型和数据

论文以端到端方式训练**任意LLM**来学习反思自身的生成过程，通过生成任务输出和间歇性的特殊token（即反思token）。
- 反思token分为**检索**和**评论**token，分别表示检索的需求和生成的质量（图中右侧）。

具体做法如下：
- 给定输入提示和先前的生成，[SELF-RAG](https://selfrag.github.io/)
- 首先，确定继续生成增加检索文段是否**有所帮助**。如果是,输出一个检索标记，以便按需调用一个检索模型（步骤1）。
- 随后，[SELF-RAG](https://selfrag.github.io/)同时处理**多个检索文段**，评估相关性，然后生成相应的任务输出（步骤2）。
- 然后，生成**评论标记**来评估输出，并选择在事实准确性和整体质量方面最好的生成（步骤3）。
  - 这个过程与传统RAG（图1左侧）不同，后者不管检索是否有必要（例如，底部示例不需要事实知识），都会一律检索固定数量的文档进行生成，并且从不第二次访问生成质量。
- 此外，[SELF-RAG](https://selfrag.github.io/)为每个部分提供**引文**，附带自我评估是否输出受文段支持，从而简化了事实验证。

实验证据表明
- SELF-RAG 在 6个任务上**明显优于**经过预训练或指令学习的LLMs，以及更高引用准确性的RAG方法，达到sota。
- SELF-RAG 在 4个任务上优于具有检索增强功能的`ChatGPT`，`Llama2-chat`和`Alpaca`, 在所有任务中的性能更好。

Self-RAG outperforms vanilla ChatGPT or LLama2-chat across six tasks, and outperforms those SOTA models with widely-used retrieval-augmentation methods in most tasks by large margin

论文分析证明了使用**反思标记**进行训练和推理对整体性能提升以及测试时模型自定义（例如，在引文预测和完整性之间的权衡）的有效性。

Connections to Prior Work
- v.s. **Retrieval-augmented Generation** 与传统RAG相比，`Self-RAG`针对多样性任务自适应检索,并评估相关性，更加灵活
  - `Standard RAG` only retrieves once or fixed number of time steps, while `Self-RAG` enables **Adaptive retrieval** for diverse task inputs, and can retrieve multiple times while generations, or completely skip retrieval, making it more suitable for diverse downstream queries (e.g., instruction-following).
  - `Self-RAG` carefully criticize retrieved passages or its own generations via reflection tokens and incorporate hard or soft constrained during decoding, while `standard RAG` does not assess relevance of passages or whether the output is indeed supported by the passages.
- v.s. **Learning from Critique** (Feedback) `Self-RAG`再多个参考结果中调整奖励权重，不需要训练
  - Reflection tokens are inserted offline by another Critic model trained on machine-generated feedback, making training much more memory efficient and stable than widely adopted RLHF methods (e.g., PPO).
  - `Self-RAG` enables tailored behaviors by simply adjusting reward weights across multiple preference aspects, while prior fine-grained feedback learning method requires training for different model behaviors.


## （3）PEFT 参数高效微调

- **参数高效精细调整**（PEFT）：修改选定参数以实现更高效的适应。进一步调整预训练模型，只更新其总参数的一小部分
  - PEFT 方法可训练的部分不同。一些技术优先训练原始模型参数的**选定部分**。其他方法集成并训练较小的**附加组件**，如适配器层，而不修改原始结构
  - ![](https://pic2.zhimg.com/80/v2-1d62f9b57373a592407db8aedd90b681_1440w.webp)
  - LoRA是最常用的 PEFT 方法，使用重参数化，这种技术通过执行低秩近似来缩小可训练参数的集合。
  - LoRA优点：
    - 任务切换效率 - 创建模型的不同版本以适应特定任务变得更容易。你可以简单地存储预训练权重的单个副本，并构建许多小 LoRA 模块。当你从任务切换到任务时，你只替换矩阵 A 和 B，并保留 LLM。这显著减少了存储需求。
    - 需要更少的 GPU - LoRA 将 GPU 内存需求减少了最多 3 倍，因为我们不计算/重新训练大多数参数。
    - 高精度 - 在各种评估基准上，LoRA 的性能被证明几乎等同于全面微调 - 而且只需要一部分成本
  - PEFT 相比全面微调的优势
    - 更高效和更快的训练
    - 保留预训练的知识


### 微调原理

FineTune 微调

预训练模型在小规模特定数据集上进一步训练，调整模型权重，适应特定任务或提高其性能。
- ![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*JSJBBnslBE9S5i77Rz9r_g.png)


## （4）全量微调

**全面微调**：使用任务特定数据调整LLM的所有参数。
- 一个较小、任务特定、带标签的数据集上进行微调，调整一些模型参数，优化其对特定任务或一组任务的性能
- 全面微调： 所有模型参数都被更新，使其类似于预训练，只不过是在一个**带标签**且**规模较小**的数据集上进行。
- ![](https://pic2.zhimg.com/80/v2-e8c7286930eb81b57aaf109fe92ac58d_1440w.webp)
- 优点: 训练数据集更少、提高精度、增加鲁棒性
- 缺点: 高计算成本、内存需求高、时间/专业知识密集

# 结束