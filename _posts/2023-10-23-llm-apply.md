---
layout: post
title:   大模型落地方案 LLM Solution
date:   2023-10-23 16:52:00
categories: 大模型
tags: 微调 RAG 
excerpt: 大模型工业落地经验总结
mathjax: true
permalink: /llm_solution
---

* content
{:toc}

# LLM 应用实践

开箱即用的预训练LLM没有按预期或希望执行时，如何提高LLM应用的性能？
- 用检索增强生成（RAG）还是模型微调来改善结果？

## 如何选择优化方法


建议
- 微调之前先尝试RAG


## 方法分析

### RAG vs finetune

分析
- 微调在特定任务上训练模型，就像在问题解答数据集上微调 GPT-3.5 以提高其在特定数据集上的问题解答性能一样。

判断
- 如果数据集**足够大**而且**不会变**，那么采用**微调**。
- 如果数据集动态变化，需要不断重新训练模型，以跟上变化。
- 如果没有大型数据集，不建议微调。建议用 RAG 来提高 LLM 的性能。同样，也可用 RAG 来提高 LLM 在摘要、翻译等任务上的性能，因为这些任务可能无法进行微调。

这两种方法都获得类似结果，但在复杂性、成本和质量方面有所不同。
- ![](https://pic1.zhimg.com/80/v2-c2058b77b95bdb3fb533b7949a6258b8_1440w.webp)

RAG更简单、便宜，但质量可能不匹配。

但这两种方案不是实现相同结果的两个方案，而是正交，满足LLM应用的不同需求。

RAG 和 微调之间的细微差别跨越了模型架构、数据需求、计算复杂性等。忽略这些细节可能会破坏项目时间轴和预算。

如何选择？
- **访问外部数据源**？是 → RAG 更有效、容易扩展
  - 非常适合需要查询数据库、文档或其他结构化/非结构化数据存储库的应用
  - 微调需要大量标注数据集，数据更新时，模型更新不及时
  - 微调过程没有对查询外部知识的检索和推理步骤进行建模。
- **修改模型行为、风格、领域知识**？是 → 微调
  - 微调擅长将LLM行为适配到特定细微差别、语调或术语，如 医学专业人士、以诗意的风格写作，或者使用特定行业的行话
  - RAG虽然善于整合外部知识，但主要侧重信息检索，不会根据检索信息调整其语言风格或领域特异性
- 抑制幻觉重要吗？是 → RAG
  - RAG 相对 微调 不容易产生幻觉，检索机制相当于事实检查器
- 监督语料多吗？是 → 微调，否则 RAG
  - 微调依赖有标签数据的数量和质量，数据不足会过拟合
- 数据会变化吗？是 → RAG
  - 如果数据经常更新，模型容易过时，而重新训练耗时耗力，增加评估成本
  - RAG 检索机制不断查询外部资源，保持最新，知识库/数据源更新时，RAG无缝集成，保持相关性，不用频繁训练
- 要求可解释吗？如果要求较高的透明性+可解释性 → RAG
  - LLM 原理像黑盒，推理机制不明，难以解释为什么
  - RAG 透明性相对较高，检索+生成，用户可以洞察全过程

|维度|解释|`RAG`|`FineTune`|
|---|---|---|
|External knowledge read?|访问外部数据?|✅|❌|
|Changing model behaviour read?|改变模型行为?|❌|✅|
|Minimise hallucinations?|幻觉最小化?|✅|❌|
|Training data availiable?|较多训练数据?|❌|✅|
|Is data (mostly) dynamic?|数据动态变化?|✅|❌|
|Interpretability|要求可解释?|✅|❌|


建议：
- 从RAG开始，评估其性能，如果发现不足，则转向微调。
- 最佳选择: 自动化，混合方法
  - 微调确保聊天机器人符合公司的品牌、语调和一般知识，处理大多数典型的客户查询。
  - RAG可以作为一个补充系统，处理更动态或更具体的查询，确保聊天机器人能够从最新的公司文档或数据库中获取信息，从而最大限度地减少幻觉。
  - 整合这两种方法，公司可以提供全面、及时且与品牌一致的客户支持体验。
- ![](https://pic1.zhimg.com/80/v2-8a98c6db80f32f2fea6fa2503360fd38_1440w.webp)
- ![](https://pic1.zhimg.com/80/v2-8d953a512d1ec078223cb5687ed0419c_1440w.webp)

### 四种方法对比

【2023-10-17】[如何选择最适合你的LLM优化方法：全面微调、PEFT、提示工程和RAG对比分析](https://zhuanlan.zhihu.com/p/661830285?utm_psn=1697685536221999105)
- [RAG vs Finetuning — Which Is the Best Tool to Boost Your LLM Application?](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)

四种主要的调优方法：
- **全面微调**：使用任务特定数据调整LLM的所有参数。
  - 一个较小、任务特定、带标签的数据集上进行微调，调整一些模型参数，优化其对特定任务或一组任务的性能
  - 全面微调： 所有模型参数都被更新，使其类似于预训练，只不过是在一个**带标签**且**规模较小**的数据集上进行。
  - ![](https://pic2.zhimg.com/80/v2-e8c7286930eb81b57aaf109fe92ac58d_1440w.webp)
  - 优点: 训练数据集更少、提高精度、增加鲁棒性
  - 缺点: 高计算成本、内存需求高、时间/专业知识密集
- **参数高效精细调整**（PEFT）：修改选定参数以实现更高效的适应。进一步调整预训练模型，只更新其总参数的一小部分
  - PEFT 方法可训练的部分不同。一些技术优先训练原始模型参数的**选定部分**。其他方法集成并训练较小的**附加组件**，如适配器层，而不修改原始结构
  - ![](https://pic2.zhimg.com/80/v2-1d62f9b57373a592407db8aedd90b681_1440w.webp)
  - LoRA是最常用的 PEFT 方法，使用重参数化，这种技术通过执行低秩近似来缩小可训练参数的集合。
  - LoRA优点：
    - 任务切换效率 - 创建模型的不同版本以适应特定任务变得更容易。你可以简单地存储预训练权重的单个副本，并构建许多小 LoRA 模块。当你从任务切换到任务时，你只替换矩阵 A 和 B，并保留 LLM。这显著减少了存储需求。
    - 需要更少的 GPU - LoRA 将 GPU 内存需求减少了最多 3 倍，因为我们不计算/重新训练大多数参数。
    - 高精度 - 在各种评估基准上，LoRA 的性能被证明几乎等同于全面微调 - 而且只需要一部分成本
  - PEFT 相比全面微调的优势
    - 更高效和更快的训练
    - 保留预训练的知识
- **提示工程**：改进模型输入以指导其输出。
  - 在新数据集和任务上训练模型参数，使用所有预训练权重（如全面微调）或一组独立权重（如 LoRA）。
  - 相比之下，提示工程根本不涉及训练网络权重
  - ![](https://pic3.zhimg.com/80/v2-4e5ddc95da8e4945cf30c65e1593050e_1440w.webp)
  - 基础提示: 零样本提示、少样本提示、链式思考引导
  - ![](https://pic4.zhimg.com/80/v2-857d925cf7adc11d94a2fbd9aca37213_1440w.webp)
- **RAG**（检索增强生成）：将提示工程与数据库查询结合，以获得丰富的上下文答案。
  - 将引导工程与从外部数据源检索上下文相结合，以提高语言模型的性能和相关性。通过在模型上附加额外信息，它允许更准确和上下文感知的响应。
  - RAG模型架构将用户查询的嵌入与知识库向量中的embedding进行比较，将来自知识库中相似文档的相关上下文附加到原始用户提示中。然后将这个增强的prompt给到LLMs，可以异步更新知识库及其相关的embedding
  - ![](https://pic3.zhimg.com/80/v2-db7c5fbf5f95c69846fc3805eb287086_1440w.webp)
  - RAG 本质上将信息检索机制与文本生成模型相结合。信息检索组件有助于从数据库中拉取相关的上下文信息，并且文本生成模型使用这个添加的上下文来产生更准确和“知识丰富”的响应。以下是它的工作方式：
    - 向量数据库：实施 RAG 包括嵌入内部数据集，从中创建向量，并将它们存储在向量数据库中。
    - 用户查询：RAG 从提示中获取用户查询，这是一个需要回答或完成的自然语言问题或陈述。
    - 检索组件：一旦接收到用户查询，检索组件扫描向量数据库以识别与查询语义相似的信息块。然后使用这些相关片段为 LLM 提供额外上下文，使其能够生成更准确和上下文感知的响应。
    - 串联：将检索到的文档与原始查询串联成一个提供生成响应所需额外上下文的提示。
    - 文本生成：将包含串联查询和检索文档的提示馈送到 LLM 以产生最终输出。
    - ![](https://pic1.zhimg.com/80/v2-63c902a479d54ff27917dd94d3c65174_1440w.webp)
    - 开源应用框架: 
      - OpenAI [chatgpt-retrieval-plugin](https://github.com/openai/chatgpt-retrieval-plugin)
      - [langchain](https://github.com/langchain-ai/langchain)
      - [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/index.html)
  - [Creating a RAG Pipeline with LangChainPermalink](https://www.maartengrootendorst.com/blog/improving-llms/#creating-a-rag-pipeline-with-langchain), [中文版](https://zhuanlan.zhihu.com/p/661349721?utm_psn=1697558407270424576)
  - ![RAG方法的大致过程](https://www.maartengrootendorst.com/assets/images/posts/2023-12-09-improving-llms/rag.svg)
  - RAG 有许多明显的优点：
    - 最小化幻觉 - 当模型做出“最佳猜测”假设，本质上填补了它“不知道”的内容时，输出可能是错误的或纯粹的胡说八道。与简单的提示工程相比，RAG 产生的结果更准确，幻觉的机会更低。
    - 易于适应新数据 - RAG 可以在事实可能随时间演变的情况下进行适应，使其对生成需要最新信息的响应非常有用。
    - 可解释 - 使用 RAG，可以确定 LLM 答案的来源。对答案来源进行追溯对于内部监控、质量保证或处理客户纠纷可能是有益的。
    - 成本有效 - 与在特定任务数据集上对整个模型进行微调相比，你可以使用 RAG 获得相当的结果，这涉及到更少的标记数据和计算资源。
  - RAG 的潜在限制
    - RAG 旨在通过从外部文档中提取上下文来增强 LLM 的信息检索能力。然而，在某些使用案例中，额外的上下文还不够。如果一个预训练的 LLM 在总结财务数据或从患者的医疗文档中提取见解方面遇到困难，很难看出以单个文档形式提供额外上下文如何有所帮助。在这种情况下，微调更有可能产生期望的输出。

[improving-llms](https://www.maartengrootendorst.com/blog/improving-llms/), 3 of the most common methods for improving the performance of any LLM:
- Prompt Engineering
- Retrieval Augmented Generation (RAG)
- Parameter Efficient Fine-Tuning (PEFT)
- ![](https://www.maartengrootendorst.com/assets/images/posts/2023-12-09-improving-llms/common.svg)
- ![](https://www.maartengrootendorst.com/assets/images/posts/2023-12-09-improving-llms/overview.svg)

四个重要指标上进行比较：复杂性、成本、准确性和灵活性。
- **成本**： PE ＜ RAG ＜ PEFT ＜ Full Fine-tuning
- **复杂性**：PE ＜ RAG ＜ PEFT = Full Fine-tuning
- **准确性**：
  - 特定领域术语：PE ＜ RAG ＜ PEFT ＜ Full Fine-tuning
  - 时效性：PEFT = Full Fine-tuning < PE < RAG
  - 可解释性：PE = PEFT = Full Fine-tuning < RAG
  - 幻觉: PE < PEFT < Full Fine-tuning < RAG
    - 微调可以通过将 LLM 集中在特定领域数据上来减少这些幻觉。然而，不熟悉的查询仍然可能导致 LLM 编造出一个捏造出来的答案。
    - RAG 通过将 LLM 的响应锚定在检索到的文档中来减少幻觉。初始检索步骤本质上进行事实检查，而随后生成受限于检索数据的上下文。对于避免幻觉至关重要的任务，推荐使用 RAG。
  - 总结
    - 解释性、时效性和避免幻觉至关重要 → RAG
    - 要求特定领域风格 → 全面微调 和 PEFT
    - 两者都要 → 微调 和 RAG
- **灵活性**： Full Fine-tuning < PEFT < PE < RAG


## FineTune 微调


### 微调原理

预训练模型在小规模特定数据集上进一步训练，调整模型权重，适应特定任务或提高其性能。
- ![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*JSJBBnslBE9S5i77Rz9r_g.png)


## RAG

### 起因

LLM 通过大量数据训练，回答任何问题或完成任务，利用其参数化记忆。这些模型有一个知识**截止日期**，取决于上次训练的时间。
- 被问及超出其知识范围或知识截止日期后发生的事件时，模型会产生**幻觉**。

Meta 研究人员发现，通过提供与手头任务相关的信息，模型在完成任务时表现**显著改善**。

例如，询问模型关于截止日期之后发生的事件，则提供该事件作为背景信息并随后提问将帮助模型正确回答问题。

由于LLM具有有限的上下文窗口长度，在处理当前任务时**只能传递最相关的知识**。添加到上下文中数据质量影响着模型生成响应结果的质量。机器学习从业者在RAG流程不同阶段使用多种技术来改善LLM性能。

### 什么是 RAG

科里·祖
> “检索增强生成是用您（系统）从其他地方检索到的附加信息来补充用户输入到 ChatGPT 等大型语言模型 (LLM) 的过程。然后，法学硕士可以使用该信息来增强其生成的响应。” 

检索增强生成（简称 `RAG`）是 Meta 于 2020 年推广的一种架构，通过将**相关信息**与**问题/任务细节**一起传递给模型来提高 LLM 的性能。


【2023-9-27】[RAG 与 Finetuning，谁是提升 LLM 的最佳工具？](https://mp.weixin.qq.com/s/D-8r3FHKCyh4xk-yM7lMag)

### RAG 流程

将原始文件拆解后, 每个部分都会生成相应embedding 并且 存放到vector store 中. 当查询发送给 vector store 时, 查询也会转换为 embedding , 然后 vector store 返回与查询最相似 的 embeddings
- ![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jq9bEbitg1Pv4oASwEQwJg.png)

RAG 包含三个阶段：数据准备、检索和生成。
- **数据准备**阶段：确定数据源、从数据源中提取数据、清理数据并将其存储到数据库中。
  - 识别数据来源、从来源中提取数据、清洗数据并将其存储在数据库中
  - 向量存储器：存储文本、图像、音频等非结构化数据，并基于语义相似性搜索该类别下的内容。
- **检索**阶段：根据手任务从数据库中检索相关数据。
  - 关键词搜索：简单的检索数方法，数据根据关键词进行索引，并且搜索引擎返回包含这些关键的文档。
  - 关键词搜索适用于存储**结构化数据**（如表格、文档等）并使用关键词对数据进行搜索。
  - 图数据库以节点和边的形式存储数据。适用于存储结构化数据（如表格、文档等），并通过数据之间的关系进行搜索
  - 搜索引擎：从公共搜索引擎（如Google、Bing等）或内部擎（如Elasticsearch、Solr等）中检索RAG管道中的数据；搜索引擎适用于从网络上检索数据并使用关键字对其进行搜索。
  - 可将来自**搜索引擎**的数据与**其他数据库**（如向量存储、图数据库等）中获取到的数据相结合，以提高输出质量。推荐结合多种策略（如语义搜索 + 关键字匹配）的混合方法
  - 矢量数据库中对嵌入式数据进行相似性搜索
- **生成**阶段：利用检索到的数据和任务生成输出结果。
  - 检索到相关数据，就会连同用户的查询或任务一起传递给生成器（LLM）。LLM 使用检索到的数据和用户的查询或任务生成输出
  - 输出质量取决于数据的质量和检索策略。



### 提升性能

【2023-10-1】[提升RAG性能的 10 种方法](https://mp.weixin.qq.com/s/WDV31S3C7YQKekwJTIYt5Q)

使用 LangChain 或 LlamaIndex 等框架的快速入门指南，任何人都可以使用大约五行代码构建一个简单的 RAG 系统，例如文档的聊天机器人。

但是，用这五行代码构建的机器人不会很好地工作。RAG 很容易制作原型，但很难达到用户满意的地步。基本教程可能会让 RAG 以 80% 的速度运行。但要弥补接下来的 20%，通常需要进行一些认真的实验。

提高RAG性能的 10 种方法
- 清理数据：提升数据质量，优化数据分布
- 探索不同索引类型：索引是LlamaIndex和LangChain的核心
  - RAG 标准方法涉及嵌入和相似性搜索，将上下文数据分块，嵌入所有内容，当查询到来时，从上下文中找到相似的部分。
  - 这种方法效果很好，但并不是适合每个用例的最佳方法。
- 尝试多种分块方法
  - 将上下文数据分块是构建 RAG 系统的核心
  - 块大小很重要。较小的块通常可以改善检索，但可能会导致生成过程缺乏周围的上下文；
  - 小、中、大块大小循环浏览了每一组，发现小是最好的
- 覆盖基本提示: 使用时覆盖基础提示
  - 示例： 你是一名客户支持代理。您的目的是在仅提供事实信息的同时尽可能提供帮助。你应该友好，但不要过于健谈。`上下文信息如下。给定上下文信息而不是先验知识，回答查询`。
- 元数据过滤
  - 元数据（如日期）添加到块中，然后用它来帮助处理结果
  - 构建 RAG 时要记住的一般概念：<span style='color:red'>相似 ≠ 相关</span>
- 查询路由
  - 适用场景：有多个索引，如摘要、敏感问题识别、日期相关，优化成一个索引不一定好
- 重排名
  - 重新排名是解决**相似性**和**相关性**之间差异问题的一种解决方案
  - 如 Cohere Rereanker，LangChain 和 LlamaIndex 都有抽象，可以轻松设置。
- 查询转换（改写）
  - 将用户查询放入基本提示中来更改它
  - 重新措辞：如果系统找不到查询的相关上下文，让LLM重新措辞查询并重试
  - HyDE 是一种策略，接受查询，生成假设的响应，然后将两者用于嵌入查找。研究发现这可以显着提高性能。
  - 将一个查询分解为多个问题（子查询），LLM在分解复杂查询时往往会工作得更好
- 微调embedding模型
  - 基于嵌入的相似性是 RAG 的标准检索机制
  - 预训练模型（如 OpenAI的ada）关于嵌入空间中相似内容的概念可能与场景上下文中相似内容不一致
    - 处理法律文件：希望嵌入更多地基于您的领域特定术语（例如“知识产权”或“违反合同”）对相似性的判断，而不是基于“特此”和“协议”等一般术语。
  - 微调可以将检索指标提高 5-10%，LlamaIndex 可以生成训练集
- LLM 开发工具
  - LlamaIndex 或 LangChain 这两个框架都提供调试工具，允许定义回调、查看使用的上下文、检索来自哪个文档等等。
  - Arize AI 有一个笔记本内工具，可探索如何检索哪些上下文及其原因。
  - Rivet 是一个提供可视化界面的工具，可帮助您构建复杂的代理，由法律技术公司 Ironclad 开源。

其它：【2023-9-27】[检索增强生成 (RAG):What, Why and How?](https://mp.weixin.qq.com/s?__biz=MzkzNDQxNDU1Ng==&mid=2247484067&idx=1&sn=1eafa47e700526ecd9862f7c39587738&chksm=c2bcd310f5cb5a06db8832792c4d810e1a53a26b4435195cba6adf5848e8a253b6150f102834&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)
- 混合搜索：将语义搜索与关键词搜索结合起来，从向量存储中检索相关数据 —— 已被证明对大多数用例都能获得更好的结果
- 摘要：对块进行摘要并将摘要存储在向量存储中，而不是原始块
- 丢失问题：LLMs并不给予输入中所有标记**相同权重**。中间标记似乎比输入开头和结尾处的标记被赋予较低权重，中间丢失问题。
  - 可重新排列上下文片段，使最重要的片段位于输入**开头**和**结尾**，并将次要片段放置在中间位置。


# 结束