---
layout: post
title:  åµŒå…¥/å‘é‡åŒ–æŠ€æœ¯ Embedding Tech
date:   2023-05-22 19:10:00
categories: è‡ªç„¶è¯­è¨€å¤„ç†
tags: å‘é‡åŒ– milvus vector embedding mistral
excerpt: åµŒå…¥ï¼ˆEmbeddingï¼‰æŠ€æœ¯åŸç†ã€æ¡ˆä¾‹
mathjax: true
permalink: /emb
---

* content
{:toc}


# å‘é‡åŒ–

## å‘é‡åŒ–ç”¨é€”

å‘é‡åŒ–ç”¨é€”å¹¿æ³›ï¼ŒLLMæ—¶ä»£çš„ä½œç”¨å¤§

[LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to comprehensively support the retrieval augmentation needs of large language models, including knowledge retrieval, memory retrieval, examplar retrieval, and tool retrieval. It is fine-tuned over 6 tasks:
-   çŸ¥è¯†é—®ç­” _Question Answering (qa)_
-   å¯¹è¯æ£€ç´¢ _Conversational Search (convsearch)_
-   èŠå¤© _Long Conversation (chat)_
-   é•¿ç¨‹è¯­è¨€æ¨¡å‹ _Long-Range Language Modeling (lrlm)_
-   æƒ…å¢ƒå­¦ä¹  _In-Context Learning (icl)_
-   å·¥å…·è°ƒç”¨ _Tool Learning (tool)_
- ![](https://github.com/FlagOpen/FlagEmbedding/raw/master/FlagEmbedding/llm_embedder/imgs/llm-embedder.png)

## æ–‡æœ¬å‘é‡åŒ–

`åµŒå…¥`ï¼ˆEmbeddingï¼‰æ˜¯ä¸€ç§å°†**æ–‡æœ¬æˆ–å¯¹è±¡**è½¬æ¢ä¸º**å‘é‡è¡¨ç¤º**çš„æŠ€æœ¯ï¼Œå°†è¯è¯­ã€å¥å­æˆ–å…¶ä»–æ–‡æœ¬å½¢å¼è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºã€‚
- åµŒå…¥å‘é‡æ˜¯ç”±ä¸€ç³»åˆ—æµ®ç‚¹æ•°æ„æˆçš„**å‘é‡**ã€‚
- é€šè¿‡è®¡ç®—ä¸¤ä¸ªåµŒå…¥å‘é‡ä¹‹é—´çš„è·ç¦»ï¼Œå¯ä»¥è¡¡é‡å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚è·ç¦»è¾ƒå°çš„åµŒå…¥å‘é‡è¡¨ç¤ºæ–‡æœ¬ä¹‹é—´å…·æœ‰è¾ƒé«˜çš„ç›¸å…³æ€§ï¼Œè€Œè·ç¦»è¾ƒå¤§çš„åµŒå…¥å‘é‡è¡¨ç¤ºæ–‡æœ¬ä¹‹é—´ç›¸å…³æ€§è¾ƒä½ã€‚

ä»¥ `Milvus` ä¸ºä»£è¡¨çš„`å‘é‡æ•°æ®åº“`åˆ©ç”¨è¯­ä¹‰æœç´¢ï¼ˆSemantic Searchï¼‰æ›´å¿«åœ°æ£€ç´¢åˆ°ç›¸å…³æ€§æ›´å¼ºçš„æ–‡æ¡£ã€‚

è¯¦è§ï¼šsklearnä¸“é¢˜é‡Œçš„[æ–‡æœ¬å‘é‡åŒ–](sklearn#%E5%90%91%E9%87%8F%E5%8C%96)

## æ–‡æ¡£å‘é‡åŒ–

è‡ªç ”æ¡†æ¶çš„é€‰æ‹©
- åŸºäºOpenAIEmbeddingsï¼Œå®˜æ–¹ç»™å‡ºäº†åŸºäºembeddingsæ£€ç´¢æ¥è§£å†³GPTæ— æ³•å¤„ç†é•¿æ–‡æœ¬å’Œæœ€æ–°æ•°æ®çš„é—®é¢˜çš„å®ç°æ–¹æ¡ˆã€‚[å‚è€ƒ](https://www.datalearner.com/blog/1051681543488862)
- ä¹Ÿå¯ä»¥ä½¿ç”¨ LangChain æ¡†æ¶ã€‚

å‚è€ƒ
- [ChatGPTæ€ä¹ˆå»ºç«‹ç§æœ‰çŸ¥è¯†åº“ï¼Ÿ](https://www.zhihu.com/question/596838257/answer/3004754396)
- [åˆ©ç”¨LangChainå’Œå›½äº§å¤§æ¨¡å‹ChatGLMå®ç°åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„è‡ªåŠ¨é—®ç­”](https://www.zhihu.com/zvideo/1630964532179812353)

é™¤äº†ä»æ–‡æ¡£ä¸­æŠ“å–æ•°æ®ï¼Œä»æŒ‡å®šç½‘ç«™URLæŠ“å–æ•°æ®ï¼Œå®ç°æ™ºèƒ½å®¢æœå¤–éƒ¨çŸ¥è¯†åº“ï¼Œå¯ä»¥å€ŸåŠ©ChatGPTå†™Pythonä»£ç ï¼ŒPythonBeautiful Soupåº“çš„å®ç°æ–¹å¼å¾ˆæˆç†Ÿ

å¤§å‚äº§å“æˆªå›¾ï¼šæ™ºèƒ½å®¢æœçŸ¥è¯†åº“å»ºè®¾
- ä¼ä¸šèµ„æ–™åº“ï¼Œå…³è”å¤§è¯­è¨€æ¨¡å‹è‡ªåŠ¨æœç´¢
- ![a](https://p3-sign.toutiaoimg.com/tos-cn-i-tjoges91tu/TdjoYAg6aFUKbz~noop.image?_iz=58558&from=article.pc_detail&x-expires=1684219771&x-signature=5kqgHWkZX6LPdrX2H9Zq59m%2BwO0%3D)
- å¤§æ¨¡å‹æ–‡æ¡£çŸ¥è¯†æŠ½å–å’Œâ€œå³æœå³é—®â€
- ![b](https://p9-sign.toutiaoimg.com/tos-cn-i-tjoges91tu/TdjoYkN7rAGx03~noop.image?_iz=58558&from=article.pc_detail&x-expires=1684219771&x-signature=gO%2FqHxavS7KVAfE08Mv0WayLjLQ%3D)
- ![b](https://p3-sign.toutiaoimg.com/tos-cn-i-tjoges91tu/TdjoYmX2ImA5oO~noop.image?_iz=58558&from=article.pc_detail&x-expires=1684219771&x-signature=rlim2GCZ8zsapnPyzA47LCHJkEo%3D)

éƒ½æœ‰ChatGPTäº†ï¼Œä¸ºä»€ä¹ˆè¿˜è¦äº†è§£Embeddingè¿™ç§ã€Œä½çº§è´§ã€æŠ€æœ¯ï¼Ÿä¸¤ä¸ªåŸå› ï¼š
- æœ‰äº›é—®é¢˜ä½¿ç”¨Embeddingè§£å†³ï¼ˆæˆ–å…¶ä»–éChatGPTçš„æ–¹å¼ï¼‰ä¼šæ›´åŠ åˆç†ã€‚é€šä¿—æ¥è¯´å°±æ˜¯ã€Œæ€é¸¡ç„‰ç”¨ç‰›åˆ€ã€ã€‚
- ChatGPT**æ€§èƒ½**æ–¹é¢ä¸æ˜¯ç‰¹åˆ«å‹å¥½ï¼Œæ¯•ç«Ÿæ˜¯é€å­—ç”Ÿæˆï¼ˆä¸€ä¸ªTokenä¸€ä¸ªTokenåå‡ºæ¥çš„ï¼‰ã€‚

æ›´å¤šï¼š[ChatGPTç›¸ä¼¼åº¦åŒ¹é…ç¬”è®°](https://github.com/datawhalechina/hugging-llm/blob/main/content/ChatGPT%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94%E7%9B%B8%E4%BC%BC%E5%8C%B9%E9%85%8D.ipynb)

### æ–‡æœ¬åˆ‡åˆ†

LangChain åˆ‡åˆ†å·¥å…·
- [Text Splittersæ–‡æ¡£](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html): é€‰æ‹©å¯¹åº”çš„æ–‡æœ¬åˆ‡åˆ†å™¨ï¼Œå¦‚æœæ˜¯é€šç”¨æ–‡æœ¬çš„è¯ï¼Œå»ºè®®é€‰æ‹© `RecursiveCharacterTextSplitter`

```py
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# å¯¼å…¥æ–‡æœ¬
loader = UnstructuredFileLoader("./data/news_test.txt")
# å°†æ–‡æœ¬è½¬æˆ Document å¯¹è±¡
data = loader.load()
print(f'documents:{len(data)}')

# åˆå§‹åŒ–åŠ è½½å™¨
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
# åˆ‡å‰²åŠ è½½çš„ document
split_docs = text_splitter.split_documents(data)
print("split_docs size:",len(split_docs))
```

### ç›¸ä¼¼åº¦

ç›¸ä¼¼åº¦ç´¢å¼•
- åªç”¨embeddingæ¥è®¡ç®—å¥å­çš„ç›¸ä¼¼åº¦

```py
# åˆå§‹åŒ– prompt å¯¹è±¡
question = "2022å¹´è…¾è®¯è¥æ”¶å¤šå°‘"
# æœ€å¤šè¿”å›åŒ¹é…çš„å‰4æ¡ç›¸ä¼¼åº¦æœ€é«˜çš„å¥å­
similarDocs = db.similarity_search(question, include_metadata=True,k=4)
# [print(x) for x in similarDocs]
```

æ¥å…¥ChatGLMæ¥å¸®å¿™åšæ€»ç»“å’Œæ±‡æ€»

```py
from langchain.chains import RetrievalQA
import IPython
# æ›´æ¢ LLM
qa = RetrievalQA.from_chain_type(llm=ChatGLM(temperature=0.1), chain_type="stuff", retriever=retriever)
# è¿›è¡Œé—®ç­”
query = "2022å¹´è…¾è®¯è¥æ”¶å¤šå°‘"
print(qa.run(query))
```

## å‘é‡åŒ–åŸç†

åŸºç¡€æ¨¡å‹å¤§å¤šåŸºäº Transformer Encoder é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹: `BERT`, `RoBERTa`ï¼Œ`Ernie`ç­‰



### å‘é‡åŒ–æ–¹å¼

æ–‡æœ¬é€šè¿‡æ¨¡å‹è¿›è¡Œå‘é‡åŒ–çš„æ–¹å¼ä¹Ÿæœ‰å¾ˆå¤šç§ä¸åŒçš„æ–¹å¼
- `cls`: å–æœ€åä¸€å±‚çš„**ç¬¬ä¸€ä¸ªtoken**(CLS)ä½œä¸ºå¥å­å‘é‡
- `last_mean`: å¯¹æœ€åä¸€å±‚çš„**æ‰€æœ‰tokenå–å¹³å‡**pooling
- `first_last_mean`: ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚åˆ†åˆ«**å¹³å‡æ± åŒ–**,å†å–å¹³å‡
- `embedding_last_mean`: embeddingå±‚å’Œæœ€åä¸€å±‚åˆ†åˆ«å¹³å‡æ± åŒ–,å†å–å¹³å‡
- `last_weighted`: æœ€åä¸€å±‚æŒ‰tokenä½ç½®**åŠ æƒå¹³å‡æ± åŒ–**


### æŸå¤±å‡½æ•°

è¯­ä¹‰ç›¸ä¼¼åº¦æŸå¤±å‡½æ•°
1. PairInBatchNegCoSentLoss: è®¡ç®—ä¸€ä¸ªbatchå†…æ¯ä¸ªå¥å­ä¸æ­£ä¾‹å¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦,ç„¶åå‡å»è¯¥å¥å­ä¸è‡ªèº«çš„ç›¸ä¼¼åº¦,å†å–log-sum-expä½œä¸ºlossã€‚
2. TripletInBatchNegCoSentLoss: åœ¨PairInBatchNegCoSentLossçš„åŸºç¡€ä¸Š,é¢å¤–åŠ å…¥è´Ÿä¾‹æ ·æœ¬,è®¡ç®—å¥å­ä¸æ­£è´Ÿä¾‹çš„ç›¸ä¼¼åº¦å·®å€¼ã€‚å¸Œæœ›æ­£ä¾‹ç›¸ä¼¼åº¦â€“è´Ÿä¾‹ç›¸ä¼¼åº¦çš„å·®å€¼è¶Šå¤§ã€‚
3. PairInBatchNegSoftmaxContrastLoss: å°†å¥å­ä¸¤ä¸¤ç›¸ä¼¼åº¦çŸ©é˜µè¿›è¡Œsoftmax,å¹¶ä¸å¥å­æœ¬èº«çš„labelè®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚
4. TripletInBatchNegSoftmaxContrastLoss: åœ¨3çš„åŸºç¡€ä¸ŠåŠ å…¥è´Ÿä¾‹æ ·æœ¬ã€‚
5. PairInBatchNegSigmoidContrastLoss: å°†å¥å­ä¸¤ä¸¤ç›¸ä¼¼åº¦çŸ©é˜µè¿›è¡Œsigmoid,é«˜äºå¯¹è§’çº¿çš„éƒ¨åˆ†å–è´Ÿå¯¹æ•°ä½œä¸ºlossã€‚
6. TripletInBatchNegSigmoidContrastLoss: åœ¨5çš„åŸºç¡€ä¸ŠåŠ å…¥è´Ÿä¾‹æ ·æœ¬ã€‚
7. CoSentLoss: è¾“å…¥é¢„æµ‹ç›¸ä¼¼åº¦çŸ©é˜µå’ŒçœŸå®ç›¸ä¼¼åº¦çŸ©é˜µ,è®¡ç®—ä¸¤è€…å·®å€¼è¿›è¡Œlog-sum-expä½œä¸ºlossã€‚è¿™ä¸ªæŸå¤±æ˜¯è‹ç¥æå‡ºæ¥çš„ï¼Œå°†æ’åºçš„é€»è¾‘å¼•å…¥åˆ°äº†å¯¹æ¯”æŸå¤±ä¸­ã€‚

è¯­ä¹‰ç›¸ä¼¼çš„æŸå¤±å‡½æ•°â€”â€”ä¸»è¦æ€è·¯æ˜¯æ„å»ºå¥å­ä¹‹é—´çš„**ç›¸ä¼¼åº¦çŸ©é˜µ**,ç„¶åé€šè¿‡æ¯”è¾ƒæ­£ä¾‹å’Œè´Ÿä¾‹çš„ç›¸ä¼¼åº¦,é‡‡ç”¨äº¤å‰ç†µã€log-sum-expç­‰æ–¹å¼è®¡ç®—loss,ä¼˜åŒ–æ¨¡å‹çš„å¥å­è¡¨ç¤º,ä½¿å¾—æ­£ä¾‹ç›¸ä¼¼åº¦æ›´é«˜ã€‚


### è®­ç»ƒç­–ç•¥

è®­ç»ƒç­–ç•¥
1. FullParametersTraining: **å…¨å‚æ•°**å¾®è°ƒ,ä¸è¿›è¡Œä»»ä½•ä¿®æ”¹,æ‰€æœ‰å‚æ•°éƒ½å‚ä¸è®­ç»ƒã€‚
2. BitFitTraining: åªè®­ç»ƒ**æŸäº›**å…³é”®è¯å‚æ•°,å…¶ä»–å‚æ•°å›ºå®šã€‚é€šè¿‡keywordsæŒ‡å®šè¦è®­ç»ƒçš„å‚æ•°åã€‚
3. PrefixTraining: åªè®­ç»ƒ**å‰ç¼€**tokenså¯¹åº”çš„embeddingå‚æ•°ã€‚é€šè¿‡additional_special_tokensæŒ‡å®šå‰ç¼€tokens,å¹¶åªè®­ç»ƒè¿™äº›tokensçš„embeddingã€‚

è¿™äº›ç­–ç•¥å¯ä»¥å¹³è¡¡è¯­æ–™å¤§å°ä¸æ¨¡å‹å¤§å°,é™ä½è¿‡æ‹Ÿåˆé£é™©,å¸®åŠ©æ¨¡å‹å¿«é€Ÿé€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚é€šå¸¸å…ˆåšå…¨å‚æ•°è®­ç»ƒé¢„çƒ­ä¸€ä¸‹,å†ä½¿ç”¨éƒ¨åˆ†å›ºå®šæˆ–å‰ç¼€è®­ç»ƒç­‰ç­–ç•¥å¾®è°ƒåˆ°ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ã€‚

## å‘é‡åŒ–æ–¹æ¡ˆ

å¯é€‰
- å•ç‹¬çš„embeddingæœåŠ¡
- LLMé‡Œçš„embedding


### Embedding è¯„æµ‹

ã€2023-12-14ã€‘è¯„æµ‹ç»“è®º

Embeddingå‘é‡åŒ–å®éªŒ
- Top 1å¬å› Accracy: `Ada v2`(84%) > `BGE`(81%) > `M3E`(?) > `AngIE`(67%)

| model | top1 | top3 | top5 | top10 | ç»´åº¦ | è¯­è¨€ |
| --- | --- | --- | --- | --- | --- | --- |
| Openai AdaEmbedding V2 | 0.8415 | 0.9471 | 0.9660 | 0.9849 | 1536 | ä¸­æ–‡ |
| m3e | 0.7811 | 0.9132 | 0.9472 | 0.9811 | 768 | ä¸­æ–‡ |
| bge | 0.8113| 0.9358| 0.9660| 0.9849| 768| ä¸­æ–‡ |
| angle | 0.6716 | 0.8604 | 0.9056 | 0.9283 | 768 | ä¸­æ–‡ |

çº¬åº¦ä¸åŒï¼Œè¿™ä¸ªæ¯”è¾ƒå¯¹m3eå’Œbgeå’Œangleä¸å¤ªå…¬å¹³

top3çš„bgeï¼Œm3eå’Œada002æ¯”å·®è·ä¸å¤§äº†ï¼Œtop5åŸºæœ¬å°±æŒå¹³

Embeddingæ¦œå• [MTEB](https://huggingface.co/spaces/mteb/leaderboard)

### LLM Embedding

ã€2023-8-1ã€‘[ä½¿ç”¨LLMsè¿›è¡Œå¥å­åµŒå…¥ä¸å¦‚ç›´æ¥ç”¨BERT](https://mp.weixin.qq.com/s/mdC8EJ2Ajs8a_DCaxAET3Q)
- [Scaling Sentence Embeddings with Large Language Models](https://arxiv.org/abs/2307.16645.pdf)
- ä»£ç  [scaling_sentemb](https://github.com/kongds/scaling_sentemb)

ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ä½¿LLMsç”Ÿæˆé«˜è´¨é‡çš„**å¥å­åµŒå…¥**ï¼Œæ— éœ€å¾®è°ƒï¼Œå…¶æ€§èƒ½å¯ä¸å½“å‰çš„**å¯¹æ¯”å­¦ä¹ **æ–¹æ³•ç›¸åª²ç¾ã€‚

æ¨¡å‹å‚æ•°è¶…è¿‡åäº¿è§„æ¨¡ä¼šå¯¹è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆSTSï¼‰ä»»åŠ¡çš„æ€§èƒ½é€ æˆå½±å“ã€‚ç„¶è€Œï¼Œæœ€å¤§è§„æ¨¡çš„æ¨¡å‹è¶…è¶Šäº†å…¶ä»–å¯¹æ¯”æ–¹æ³•ï¼Œå–å¾—äº†è¿ç§»ä»»åŠ¡ä¸Šçš„æœ€æ–°æœ€ä¼˜ç»“æœã€‚åŒæ—¶ï¼Œè¿˜é‡‡ç”¨å½“å‰çš„å¯¹æ¯”å­¦ä¹ æ–¹æ³•å¯¹LLMsè¿›è¡Œäº†å¾®è°ƒï¼Œå…¶ä¸­ç»“åˆäº†æœ¬æ–‡æå‡ºçš„åŸºäºæç¤ºçš„æ–¹æ³•çš„ 2.7B OPTæ¨¡å‹ï¼Œåœ¨STSä»»åŠ¡ä¸Šçš„ç»“æœè¶…è¿‡äº†4.8B ST5ï¼Œè¾¾åˆ°äº†æœ€æ–°æœ€ä¼˜çš„æ€§èƒ½ã€‚

é—®é¢˜æ¢è®¨
1. **PromptEOLæ–¹æ³•ä¸ä¹‹å‰çš„å¥å­è¡¨ç¤ºæ–¹æ³•æœ‰ä½•ä¸åŒ**ï¼ŸPromptEOL åœ¨å¥å­åµŒå…¥æ–¹é¢æœ‰ä½•ä¼˜åŠ¿ï¼Ÿ
  - PromptEOLæ–¹æ³•æ˜¯ä¸€ç§æ˜¾å¼å•è¯é™åˆ¶,åŸºäºæç¤ºçš„æ–¹æ³•ï¼Œç”¨äºåœ¨LLMsä¸­è¡¨ç¤ºå¥å­ã€‚
  - ç›¸æ¯”äºä¹‹å‰çš„æ–¹æ³•ï¼ŒPromptEOLæ–¹æ³•åœ¨ä½¿ç”¨LLMsæ—¶æ›´ä¸ºå…¼å®¹ã€‚é€šè¿‡å°†å¥å­åµŒå…¥ä¸ºæç¤º â€œ`This sentence: â€œ[text]â€ means`â€ ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªtokenï¼Œå¹¶æå–æœ€ç»ˆtokençš„**éšè—å‘é‡**ä½œä¸ºå¥å­åµŒå…¥ã€‚PromptEOLè¡¨ç¤ºèƒ½åŠ›æ›´å¼ºã€‚
2. æ¯”è¾ƒPromptEOLæ–¹æ³•ä¸å…¶ä»–ä¸¤ç§æ–¹æ³•ï¼ˆå¹³å‡å’Œä½¿ç”¨æœ€åä¸€ä¸ªtokenï¼‰æ—¶ï¼Œ**ä¸ºä»€ä¹ˆPromptEOLæ–¹æ³•èƒ½å¤Ÿè¡¨ç°å¾—æ›´å¥½**ï¼Ÿ
  - PromptEOLæ–¹æ³•åœ¨ä¸åŒå‚æ•°è®¾ç½®ä¸‹çš„LLMsä¸­éƒ½èƒ½æ˜¾è‘—æå‡å¥å­è¡¨ç¤ºæ€§èƒ½ã€‚ç›¸è¾ƒäºå¹³å‡å’Œä½¿ç”¨æœ€åä¸€ä¸ªtokençš„æ–¹æ³•ï¼Œåœ¨å¥å­åµŒå…¥è¿‡ç¨‹ä¸­ï¼ŒPromptEOLæ–¹æ³•é€šè¿‡ä½¿ç”¨æç¤ºæ¥å¼•å¯¼LLMsç”Ÿæˆä¸‹ä¸€ä¸ªtokenï¼Œå¹¶æå–æœ€ç»ˆtokençš„éšè—å‘é‡ã€‚
  - è¿™ç§æç¤º-basedçš„æ–¹æ³•æœ‰åŠ©äºåœ¨LLMsä¸­æ›´å¥½åœ°æ•æ‰åˆ°å¥å­çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæé«˜å¥å­è¡¨ç¤ºçš„æ•ˆæœã€‚
3. **ä¸ºä»€ä¹ˆå³ä½¿LLMsçš„å‚æ•°æ¯”BERTæ›´å¤šï¼Œä½†åœ¨å¥å­è¡¨ç¤ºä»»åŠ¡ä¸Šä»è¡¨ç°ä¸å¦‚BERT**ï¼Ÿ 
  - å°½ç®¡LLMs å‚æ•°æ•°é‡æ¯”BERTæ›´å¤šï¼Œä½†åœ¨ä½¿ç”¨åŒæ ·å¥å­è¡¨ç¤ºæ–¹æ³•ä¸‹ï¼ŒLLMsï¼ˆä¾‹å¦‚OPTï¼‰åœ¨å¥å­è¡¨ç¤ºä»»åŠ¡ä¸Šä»ç„¶æ— æ³•è¶…è¶Š BERTã€‚
  - å¯èƒ½å› ä¸ºBERTå…·æœ‰**åŒå‘æ³¨æ„åŠ›æœºåˆ¶**ï¼Œå¯ä»¥éšå«åœ°å°†æ•´ä¸ªè¯­ä¹‰ä¿¡æ¯å‹ç¼©åˆ°å•ä¸ªçš„ `[MASK]` tokenä¸­ã€‚
  - è€ŒLLMsé€šè¿‡æç¤ºï¼ˆPromptEOLæ–¹æ³•ï¼‰æ–¹å¼åœ¨å¥å­è¡¨ç¤ºä¸­å¼•å…¥äº†**å•å‘ç”Ÿæˆ**çš„é™åˆ¶ï¼Œå¯èƒ½æ— æ³•å……åˆ†åœ°æ•æ‰åˆ°å¥å­çš„å…¨å±€è¯­ä¹‰ä¿¡æ¯ï¼Œå¯¼è‡´åœ¨å¥å­è¡¨ç¤ºä»»åŠ¡ä¸Šçš„æ€§èƒ½ç›¸å¯¹è¾ƒä½ã€‚
4. **In-Context Learningçš„æ¦‚å¿µæ˜¯ä»€ä¹ˆ**ï¼Ÿå¦‚ä½•ä¸å¥å­åµŒå…¥ç›¸å…³è”ï¼Ÿ 
  - In-Context Learningæ˜¯ä¸€ç§åŸºäº**ä¸Šä¸‹æ–‡å­¦ä¹ **çš„å¥å­åµŒå…¥æ–¹æ³•ã€‚ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ æä¾›æ›´ä¸°å¯Œçš„å¥å­åµŒå…¥ç”Ÿæˆè¿‡ç¨‹ã€‚æ–‡ç« æå‡ºäº†ä¸¤ç§è‡ªåŠ¨ç”Ÿæˆæ¼”ç¤ºæ–‡æœ¬çš„æ–¹æ³•ï¼Œç”¨äºåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„å¥å­åµŒå…¥è¿‡ç¨‹ä¸­æä¾›æ–‡å­—è¾“å‡ºã€‚æ¼”ç¤ºæ–‡æœ¬ç”¨äºæŒ‡å¯¼æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆæƒ³è¦çš„å¥å­åµŒå…¥ã€‚é€šè¿‡ä½¿ç”¨è¿™ç§å¸¦æœ‰æ¼”ç¤ºçš„ä¸Šä¸‹æ–‡å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥å¢å¼ºå¥å­åµŒå…¥çš„è´¨é‡å’Œè¡¨ç°èƒ½åŠ›ã€‚
5. å¦‚ä½•è¿ç”¨Fine-tuningæ–¹æ³•æ¥æ”¹å–„LLMsåœ¨å¥å­è¡¨ç¤ºä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Ÿè¿™ç§æ–¹æ³•æœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ 
  - é€šè¿‡Fine-tuningæ–¹æ³•ç»“åˆ**å¯¹æ¯”å­¦ä¹ **æ¡†æ¶ï¼Œå¯æ”¹å–„LLMsåœ¨å¥å­è¡¨ç¤ºä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚ä¸ºäº†è§£å†³Fine-tuningè¿‡ç¨‹ä¸­å†…å­˜éœ€æ±‚å¤§çš„é—®é¢˜ï¼Œé‡‡ç”¨äº†é«˜æ•ˆè°ƒä¼˜çš„æ–¹æ³•ã€‚
  - é€šè¿‡ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¡†æ¶ï¼Œä»¥åŠç»“åˆé«˜æ•ˆè°ƒä¼˜æ–¹æ³•ï¼Œå¯ä»¥å‡å°‘Fine-tuningè¿‡ç¨‹ä¸­çš„å†…å­˜éœ€æ±‚ï¼Œä»è€Œæé«˜LLMsåœ¨å¥å­è¡¨ç¤ºä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜åŠ¿åœ¨äºå¯ä»¥åœ¨ä¿è¯å¥å­è¡¨ç¤ºè´¨é‡çš„åŒæ—¶ï¼Œé™ä½è®¡ç®—æˆæœ¬å’Œå†…å­˜æ¶ˆè€—ï¼Œæé«˜äº†æ–¹æ³•çš„å®ç”¨æ€§å’Œå¯æ‰©å±•æ€§ã€‚


```py
# (1) Loading base model
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Import our models. The package will take care of downloading the models automatically
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-2.7b")
model = AutoModelForCausalLM.from_pretrained("facebook/opt-2.7b")
tokenizer.pad_token_id = 0 
tokenizer.padding_side = "left"
texts = [
    "There's a kid on a skateboard.",
    "A kid is skateboarding.",
    "A kid is inside the house."
]
# (2) Use in-context learning to generate embeddings
# Directly using in-contex learning get embeddings
template = 'This_sentence_:_"A_jockey_riding_a_horse."_means_in_one_word:"Equestrian".This_sentence_:_"*sent_0*"_means_in_one_word:"'
inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True,  return_tensors="pt")
with torch.no_grad():
    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]

# (3) Use contrastive learning models to generate embeddings
# Using trained LoRA to get embeddings
from peft import PeftModel
peft_model = PeftModel.from_pretrained(model, "royokong/prompteol-opt-2.7b", torch_dtype=torch.float16)
template = 'This_sentence_:_"*sent_0*"_means_in_one_word:"'
inputs = tokenizer([template.replace('*sent_0*', i).replace('_', ' ') for i in texts], padding=True, return_tensors="pt")
with torch.no_grad():
    embeddings = peft_model(**inputs, output_hidden_states=True, return_dict=True).hidden_states[-1][:, -1, :]
Setup
```

### OpenAIEmbeddings

OpenAIå®˜æ–¹çš„embeddingæœåŠ¡

OpenAIEmbeddingsï¼š
- ä½¿ç”¨ç®€å•ï¼Œå¹¶ä¸”æ•ˆæœæ¯”è¾ƒå¥½ï¼›

2022å¹´12æœˆå‘å¸ƒçš„text-embedding-ada-002

Ada å‡ ä¸ªç‰ˆæœ¬

#### OpenAIçš„EmbeddingæœåŠ¡

ç›´æ¥ä½¿ç”¨openaiçš„embeddingæœåŠ¡

```py
import os
import openai

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
# OPENAI_API_KEY = "å¡«å…¥ä¸“å±çš„API key"
openai.api_key = OPENAI_API_KEY

text = "æˆ‘å–œæ¬¢ä½ "
model = "text-embedding-ada-002"
emb_req = openai.Embedding.create(input=[text], model=model)
emb = emb_req.data[0].embedding
len(emb), type(emb)
# ç›¸ä¼¼åº¦è®¡ç®—
from openai.embeddings_utils import get_embedding, cosine_similarity

# æ³¨æ„å®ƒé»˜è®¤çš„æ¨¡å‹æ˜¯text-similarity-davinci-001ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ¢æˆtext-embedding-ada-002
text1 = "æˆ‘å–œæ¬¢ä½ "
text2 = "æˆ‘é’Ÿæ„ä½ "
text3 = "æˆ‘ä¸å–œæ¬¢ä½ "
emb1 = get_embedding(text1)
emb1 = get_embedding(text1, "text-embedding-ada-002") # æŒ‡å®šæ¨¡å‹
emb2 = get_embedding(text2)
emb3 = get_embedding(text3)
cosine_similarity(emb1, emb2) # 0.9246855139297101
cosine_similarity(emb1, emb3) # 0.8578009661644189
```

é—®é¢˜
- ä¼šæ¶ˆè€—openaiçš„tokenï¼Œç‰¹åˆ«æ˜¯å¤§æ®µæ–‡æœ¬æ—¶ï¼Œ**æ¶ˆè€—çš„token**è¿˜ä¸å°‘ï¼Œå¦‚æœçŸ¥è¯†åº“æ˜¯æ¯”è¾ƒå›ºå®šçš„ï¼Œå¯ä»¥è€ƒè™‘å°†æ¯æ¬¡ç”Ÿæˆçš„embeddingåšæŒä¹…åŒ–ï¼Œè¿™æ ·å°±ä¸éœ€è¦å†è°ƒç”¨openaiäº†ï¼Œå¯ä»¥å¤§å¤§èŠ‚çº¦tokençš„æ¶ˆè€—ï¼›
- å¯èƒ½ä¼šæœ‰**æ•°æ®æ³„éœ²**çš„é£é™©ï¼Œå¦‚æœæ˜¯ä¸€äº›é«˜åº¦ç§å¯†çš„æ•°æ®ï¼Œä¸å»ºè®®ç›´æ¥è°ƒç”¨ã€‚

#### LangChainè°ƒç”¨OpenAI

```py
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain import VectorDBQA
from langchain.document_loaders import UnstructuredMarkdownLoader
from langchain.embeddings.openai import OpenAIEmbeddings
import IPython
import os
from dotenv import load_dotenv
load_dotenv()

os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_BASE"] = os.getenv("OPENAI_API_BASE")

embeddings = OpenAIEmbeddings()

# ---- ç®€æ´ç‰ˆ -------
from langchain.embeddings import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()
```

#### ã€2024-1-26ã€‘text-embedding-3-large

ã€2024-1-26ã€‘[OpenAIå‘å¸ƒæ–°ä¸€ä»£å‘é‡å¤§æ¨¡å‹](https://www.datalearner.com/blog/1051706229448685)ï¼Œæ¥å£å·²ç»æ›´æ–°åˆ°text-embedding-3-largeï¼Œembeddingé•¿åº¦å‡çº§ï¼Œä»·æ ¼æœ€é«˜ä¸‹é™5å€

å†³å®šå‘é‡æ£€ç´¢å‡†ç¡®æ€§çš„æ ¸å¿ƒæ˜¯å‘é‡å¤§æ¨¡å‹çš„èƒ½åŠ›ï¼Œå³æ–‡æœ¬è½¬æˆembeddingå‘é‡æ˜¯å¦å‡†ç¡®ã€‚

OpenAIå®£å¸ƒäº†ç¬¬ä¸‰ä»£å‘é‡å¤§æ¨¡å‹text-embedding
- å‘é‡å¤§æ¨¡å‹åŒ…æ‹¬2ä¸ªç‰ˆæœ¬ï¼Œåˆ†åˆ«æ˜¯: `text-embedding-3-small` å’Œ `text-embedding-3-large`
  - è§„æ¨¡è¾ƒå°ä½†æ˜¯æ•ˆç‡å¾ˆé«˜,åè€…æ˜¯è§„æ¨¡æ›´å¤§çš„ç‰ˆæœ¬ï¼Œæœ€é«˜æ”¯æŒ3072ç»´åº¦çš„å‘é‡
- æ¨¡å‹èƒ½åŠ›å¢å¼ºçš„åŒæ—¶ä»·æ ¼ä¸‹é™

å½“å‰OpenAIä¸åŒå‘é‡å¤§æ¨¡å‹çš„å¯¹æ¯”ï¼š

| æ¨¡å‹åç§° | å‘å¸ƒæ—¥æœŸ | è¾“å…¥ç»´åº¦ | è¾“å‡ºå‘é‡ç»´åº¦ | MIRACL å¹³å‡åˆ† | MTEBå¹³å‡åˆ† | ä»·æ ¼ |
| --- | --- | --- | --- | --- | --- | --- |
| `text-embedding-ada-002` | 2022å¹´12æœˆ | 8191 | 1536 | 31.4 | 61.0 | $0.0001 /1K tokens |
| `text-embedding-3-small` | 2023å¹´1æœˆ25æ—¥ | 8191 | 512å’Œ1536å¯é€‰ | 44 | 512å¾—åˆ†61.6<br>1536å¾—åˆ†62.3 | $0.00002 /1K tokens |
| `text-embedding-3-large` | 2023å¹´1æœˆ25æ—¥ | 8191 | 256/1024/3072å¯é€‰ | 54.9 | 256å¾—åˆ†62.0<br>1024å¾—åˆ†64.1<br>3072å¾—åˆ†64.6 | $0.00013 / 1k tokens |

æ–°å‘é‡å¤§æ¨¡å‹`text-embedding-3`æ”¯æŒdimensionså‚æ•°ï¼Œå¯é€‰æ‹©ç”Ÿæˆä¸åŒé•¿åº¦çš„å‘é‡ã€‚è€Œæ›´é•¿çš„å‘é‡æ•ˆæœæ›´å¥½ï¼Œä½†æ˜¯æˆæœ¬æ›´é«˜ï¼Œé€Ÿåº¦æ›´æ…¢ã€‚ä»ä»·æ ¼ä¸Šæ¥è¯´ï¼Œ`text-embedding-3-small`å’Œå‰ä¸€ä»£çš„å‘é‡å¤§æ¨¡å‹ç»´åº¦ä¸€è‡´ï¼Œæ•ˆæœç•¥å¼ºï¼Œä¸è¿‡ä»·æ ¼ä¸‹é™5å€

MTEBè¯„åˆ†ç»“æœ
- å°½ç®¡text-embedding-3-largeæœ€é«˜å·²ç»è¾¾åˆ°64.6åˆ†ï¼Œä½†æ˜¯MTEBæ’è¡Œæ¦œä¸Šä¾ç„¶åªèƒ½æ‹ç¬¬å››

| æ’å | æ¨¡å‹åç§° | æ¨¡å‹å¤§å°(GB) | è¾“å‡ºå‘é‡ç»´åº¦ | è¾“å…¥é•¿åº¦ | MTEBå¹³å‡åˆ† |
| --- | --- | --- | --- | --- | --- |
| 1 | `voyage-lite-02-instruct` | / | 1024 | 4000 | 67.13 |
| 2 | `e5-mistral-7b-instruct` | 14.22 | 4096 | 32768 | 66.63 |
| 3 | `UAE-Large-V1` | 1.34 | 1024 | 512 | 64.64 |

### HuggingFaceEmbeddings

HuggingFaceEmbeddingsï¼š

å¯ä»¥åœ¨HuggingFaceä¸Šé¢é€‰æ‹©å„ç§sentence-similarityæ¨¡å‹æ¥è¿›è¡Œå®éªŒï¼Œæ•°æ®éƒ½æ˜¯åœ¨æœ¬æœºä¸Šè¿›è¡Œè®¡ç®—
éœ€è¦ä¸€å®šçš„ç¡¬ä»¶æ”¯æŒï¼Œæœ€å¥½æ˜¯æœ‰GPUæ”¯æŒï¼Œä¸ç„¶ç”Ÿæˆæ•°æ®å¯èƒ½ä¼šéå¸¸æ…¢
ç”Ÿæˆçš„å‘é‡æ•ˆæœå¯èƒ½ä¸æ˜¯å¾ˆå¥½ï¼Œå¹¶ä¸”HuggingFaceä¸Šçš„ä¸­æ–‡å‘é‡æ¨¡å‹ä¸æ˜¯å¾ˆå¤šã€‚

```py
from langchain.vectorstores import Chroma
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
import IPython
import sentence_transformers

embedding_model_dict = {
    "ernie-tiny": "nghuyong/ernie-3.0-nano-zh",
    "ernie-base": "nghuyong/ernie-3.0-base-zh",
    "text2vec": "GanymedeNil/text2vec-large-chinese",
    "text2vec2":"uer/sbert-base-chinese-nli",
    "text2vec3":"shibing624/text2vec-base-chinese",
}

EMBEDDING_MODEL = "text2vec3"
# åˆå§‹åŒ– hugginFace çš„ embeddings å¯¹è±¡
embeddings = HuggingFaceEmbeddings(model_name=embedding_model_dict[EMBEDDING_MODEL], )
embeddings.client = sentence_transformers.SentenceTransformer(
        embeddings.model_name, device='mps')
```

### ã€2023-7-14ã€‘M3E

ã€2023-7-14ã€‘[ç ”ç©¶äººå‘˜å¼€æºä¸­æ–‡æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œå¡«è¡¥ä¸­æ–‡å‘é‡æ–‡æœ¬æ£€ç´¢é¢†åŸŸçš„ç©ºç™½](https://www.toutiao.com/article/7254900867097625127)
- [M3Eï¼Œå¼€æºä¸­æ–‡ Embedding æ¨¡å‹æ–° SOTA](https://blog.csdn.net/sinat_30045277/article/details/131208109)

ç”±äº GPT ä½¿ç”¨çš„ Transformer æ¨¡å‹çš„è‡ªèº«ç‰¹æ€§ï¼Œå¯¼è‡´æ¨¡å‹åªèƒ½ä»å›ºå®šé•¿åº¦çš„ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆæ–‡æœ¬ã€‚é‚£ä¹ˆï¼Œå½“è¦æ¨¡å‹æ„ŸçŸ¥æ›´å¹¿é˜”çš„ä¸Šä¸‹æ–‡æ—¶ï¼Œè¯¥æ€ä¹ˆåšå‘¢ï¼Ÿ

é¢†åŸŸå†…é€šç”¨çš„è§£å†³æ–¹æ¡ˆ: å°†å†å²å¯¹è¯æˆ–è€…é¢†åŸŸè¯­æ–™ä¸­çš„ç›¸å…³çŸ¥è¯†é€šè¿‡å‘é‡æ£€ç´¢ï¼Œå†è¡¥å……åˆ° GPT æ¨¡å‹çš„ä¸Šä¸‹æ–‡ä¸­ã€‚

è¿™æ ·ï¼ŒGPT æ¨¡å‹å°±ä¸éœ€è¦æ„ŸçŸ¥å…¨éƒ¨æ–‡æœ¬ï¼Œè€Œæ˜¯æœ‰é‡ç‚¹ã€æœ‰ç›®çš„åœ°åªå…³å¿ƒé‚£äº›ç›¸å…³çš„éƒ¨åˆ†ï¼Œè¿™å’Œ Transformer å†…éƒ¨çš„ Attention æœºåˆ¶åŸç†ç›¸ä¼¼ï¼Œä½¿å¾—æ–‡æœ¬åµŒå…¥æ¨¡å‹å˜æˆäº† GPT æ¨¡å‹çš„è®°å¿†æ£€ç´¢æ¨¡å—ã€‚

ä½†æ˜¯é•¿æœŸä»¥æ¥ï¼Œé¢†åŸŸå†…ä¸€ç›´ç¼ºå°‘å¼€æºã€å¯ç”¨çš„**ä¸­æ–‡æ–‡æœ¬åµŒå…¥æ¨¡å‹**ä½œä¸ºæ–‡æœ¬æ£€ç´¢ã€‚
- ä¸­æ–‡å¼€æºæ–‡æœ¬åµŒå…¥æ¨¡å‹ä¸­æœ€è¢«å¹¿æ³›ä½¿ç”¨çš„ `text2vec` ä¸»è¦æ˜¯åœ¨ä¸­æ–‡è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚
- OpenAI å‡ºå“çš„ `text-embedding-ada-002` æ¨¡å‹è¢«å¹¿æ³›ä½¿ç”¨ ï¼Œè™½ç„¶è¯¥æ¨¡å‹æ•ˆæœè¾ƒå¥½ï¼Œä½†æ­¤æ¨¡å‹ä¸å¼€æºã€ä¹Ÿä¸å…è´¹ï¼ŒåŒæ—¶è¿˜æœ‰æ•°æ®**éšç§**å’Œæ•°æ®**å‡ºå¢ƒ**ç­‰é—®é¢˜ã€‚

#### MokaHR å¼€æº M3E

æœ€è¿‘ï¼Œ`MokaHR` å›¢é˜Ÿå¼€å‘äº†ä¸€ç§åä¸º `M3E` çš„æ¨¡å‹ï¼Œè¿™ä¸€æ¨¡å‹å¼¥è¡¥äº†ä¸­æ–‡å‘é‡æ–‡æœ¬æ£€ç´¢é¢†åŸŸçš„ç©ºç™½ï¼Œ `M3E` æ¨¡å‹åœ¨ä¸­æ–‡åŒè´¨æ–‡æœ¬ S2S ä»»åŠ¡ä¸Šåœ¨ 6 ä¸ªæ•°æ®é›†çš„å¹³å‡è¡¨ç°å¥½äº `text2vec` å’Œ `text-embedding-ada-002` ï¼Œåœ¨ä¸­æ–‡æ£€ç´¢ä»»åŠ¡ä¸Šä¹Ÿä¼˜äºäºŒè€…ã€‚
- huggingface: [m3e-base](https://huggingface.co/moka-ai/m3e-base)

- 2023.06.08ï¼Œæ·»åŠ **æ£€ç´¢ä»»åŠ¡**çš„è¯„æµ‹ç»“æœï¼Œåœ¨ T2Ranking 1W ä¸­æ–‡æ•°æ®é›†ä¸Šï¼Œ`m3e-base` åœ¨ ndcg@10 ä¸Šè¾¾åˆ°äº† **0.8004**ï¼Œè¶…è¿‡äº† `openai-ada-002` çš„ **0.7786**
- 2023.06.07ï¼Œæ·»åŠ **æ–‡æœ¬åˆ†ç±»**ä»»åŠ¡çš„è¯„æµ‹ç»“æœï¼Œåœ¨ 6 ç§æ–‡æœ¬åˆ†ç±»æ•°æ®é›†ä¸Šï¼Œm3e-base åœ¨ accuracy ä¸Šè¾¾åˆ°äº† 0.6157ï¼Œè¶…è¿‡äº† openai-ada-002 çš„ 0.5956

M3E æ˜¯ Moka Massive Mixed Embedding çš„ç¼©å†™
- Mokaï¼Œæ­¤æ¨¡å‹ç”± MokaAI è®­ç»ƒï¼Œå¼€æºå’Œè¯„æµ‹ï¼Œè®­ç»ƒè„šæœ¬ä½¿ç”¨ uniem ï¼Œè¯„æµ‹ BenchMark ä½¿ç”¨ MTEB-zh
- Massiveï¼Œæ­¤æ¨¡å‹é€šè¿‡åƒä¸‡çº§ (2200w+) çš„ä¸­æ–‡å¥å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
- Mixedï¼Œæ­¤æ¨¡å‹æ”¯æŒä¸­è‹±åŒè¯­çš„åŒè´¨æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¼‚è´¨æ–‡æœ¬æ£€ç´¢ç­‰åŠŸèƒ½ï¼Œæœªæ¥è¿˜ä¼šæ”¯æŒä»£ç æ£€ç´¢
- Embeddingï¼Œæ­¤æ¨¡å‹æ˜¯æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œå¯ä»¥å°†è‡ªç„¶è¯­è¨€è½¬æ¢æˆç¨ å¯†çš„å‘é‡

Tips:
- ä½¿ç”¨åœºæ™¯ä¸»è¦æ˜¯ä¸­æ–‡ï¼Œå°‘é‡è‹±æ–‡çš„æƒ…å†µï¼Œå»ºè®®ä½¿ç”¨ m3e ç³»åˆ—çš„æ¨¡å‹
- å¤šè¯­è¨€ä½¿ç”¨åœºæ™¯ï¼Œå¹¶ä¸”ä¸ä»‹æ„æ•°æ®éšç§çš„è¯ï¼Œæˆ‘å»ºè®®ä½¿ç”¨ openai text-embedding-ada-002
- ä»£ç æ£€ç´¢åœºæ™¯ï¼Œæ¨èä½¿ç”¨ openai text-embedding-ada-002
- æ–‡æœ¬æ£€ç´¢åœºæ™¯ï¼Œè¯·ä½¿ç”¨å…·å¤‡æ–‡æœ¬æ£€ç´¢èƒ½åŠ›çš„æ¨¡å‹ï¼Œåªåœ¨ S2S ä¸Šè®­ç»ƒçš„æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼Œæ²¡æœ‰åŠæ³•å®Œæˆæ–‡æœ¬æ£€ç´¢ä»»åŠ¡

M3E æ¨¡å‹ä¸­ä½¿ç”¨çš„æ•°æ®é›†ã€è®­ç»ƒè„šæœ¬ã€è®­ç»ƒå¥½çš„æ¨¡å‹ã€è¯„æµ‹æ•°æ®é›†ä»¥åŠè¯„æµ‹è„šæœ¬éƒ½å·²å¼€æºï¼Œç”¨æˆ·å¯ä»¥è‡ªç”±åœ°è®¿é—®å’Œä½¿ç”¨ç›¸å…³èµ„æºã€‚è¯¥é¡¹ç›®ä¸»è¦ä½œè€…ã€MokaHR è‡ªç„¶è¯­è¨€å¤„ç†å·¥ç¨‹å¸ˆç‹å®‡æ˜•è¡¨ç¤ºï¼š
> â€œæˆ‘ç›¸ä¿¡ M3E æ¨¡å‹å°†æˆä¸ºä¸­æ–‡æ–‡æœ¬å‘é‡æ£€ç´¢ä¸­ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘ï¼Œæœªæ¥ç›¸å…³é¢†åŸŸçš„å·¥ä½œï¼Œéƒ½å¯èƒ½ä»è¿™äº›å¼€æºçš„èµ„æºä¸­æ”¶ç›Šã€‚â€

M3E ä½¿ç”¨ in-batch è´Ÿé‡‡æ ·çš„å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼åœ¨å¥å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä¸ºäº†ä¿è¯ in-batch è´Ÿé‡‡æ ·çš„æ•ˆæœï¼Œæˆ‘ä»¬ä½¿ç”¨ A100 80G æ¥æœ€å¤§åŒ– batch-sizeï¼Œå¹¶åœ¨å…±è®¡ 2200W+ çš„å¥å¯¹æ•°æ®é›†ä¸Šè®­ç»ƒäº† 1 epochã€‚è®­ç»ƒè„šæœ¬ä½¿ç”¨ [uniem](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/train_m3e.py)


#### ç›´æ¥ä½¿ç”¨

```py
# pip install -U sentence-transformers
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('moka-ai/m3e-base')

#Our sentences we like to encode
sentences = [
    '* Moka æ­¤æ–‡æœ¬åµŒå…¥æ¨¡å‹ç”± MokaAI è®­ç»ƒå¹¶å¼€æºï¼Œè®­ç»ƒè„šæœ¬ä½¿ç”¨ uniem',
    '* Massive æ­¤æ–‡æœ¬åµŒå…¥æ¨¡å‹é€šè¿‡**åƒä¸‡çº§**çš„ä¸­æ–‡å¥å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒ',
    '* Mixed æ­¤æ–‡æœ¬åµŒå…¥æ¨¡å‹æ”¯æŒä¸­è‹±åŒè¯­çš„åŒè´¨æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¼‚è´¨æ–‡æœ¬æ£€ç´¢ç­‰åŠŸèƒ½ï¼Œæœªæ¥è¿˜ä¼šæ”¯æŒä»£ç æ£€ç´¢ï¼ŒALL in one'
]

#Sentences are encoded by calling model.encode()
embeddings = model.encode(sentences)

#Print the embeddings
for sentence, embedding in zip(sentences, embeddings):
    print("Sentence:", sentence)
    print("Embedding:", embedding)
    print("")

```

#### M3E å¾®è°ƒ

M3Eå¾®è°ƒå®æˆ˜ï¼š[uniem](https://github.com/wangyuxinwhy/uniem/tree/main)
- uniem é¡¹ç›®ç›®æ ‡: åˆ›å»º**ä¸­æ–‡**æœ€å¥½çš„é€šç”¨æ–‡æœ¬åµŒå…¥æ¨¡å‹ã€‚

uniem æä¾›äº†éå¸¸æ˜“ç”¨çš„ finetune æ¥å£ï¼Œå‡ è¡Œä»£ç ï¼Œå³åˆ»é€‚é…ï¼

```py
from datasets import load_dataset

from uniem.finetuner import FineTuner

dataset = load_dataset('shibing624/nli_zh', 'STS-B')
# æŒ‡å®šè®­ç»ƒçš„æ¨¡å‹ä¸º m3e-small
finetuner = FineTuner.from_pretrained('moka-ai/m3e-small', dataset=dataset)
finetuner.run(epochs=1)
```

è¯¦è§ [uniem å¾®è°ƒæ•™ç¨‹](https://github.com/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb)


### MUSE å¤šè¯­ç§

2019å¹´ META æ¨å‡ºçš„[MUSE](https://github.com/facebookresearch/MUSE), åŒ…å«å¾ˆå¤šå°è¯­ç§

A library for Multilingual Unsupervised or Supervised word Embeddings, whose goal is to provide the community with:
- state-of-the-art multilingual word embeddings (`fastText` embeddings aligned in a common space)
- large-scale high-quality bilingual dictionaries for training and evaluation

![](https://github.com/facebookresearch/MUSE/raw/main/outline_all.png)


### S-BERT

ã€2023-9-5ã€‘[s-bert](https://www.sbert.net/docs/pretrained_models.html)

claudeæ¨èç”¨ s-bert embedding


### ã€2023-9-21ã€‘C-Pack - FlagEmbedding

#### ä»‹ç»

ã€2023-9-21ã€‘æ™ºæºæ¨å‡º [è¯­ä¹‰æ¨¡å‹ï¼šFlagEmbedding](https://zhuanlan.zhihu.com/p/657056257)
- è®ºæ–‡: [C-Pack: Packaged Resources To Advance General Chinese Embedding](https://arxiv.org/pdf/2309.07597.pdf)
- ä»£ç ï¼š[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)ï¼Œ [ä¸­æ–‡ä»‹ç»](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)

æ•°æ®é›†
- ç”±äºembeddingä¸æ˜¯ç¡®å®šæ€§ä»»åŠ¡ï¼Œæ‰€ä»¥å¯¹æ•°æ®çš„è§„æ¨¡ï¼Œå¤šæ ·æ€§ä»¥åŠè´¨é‡æœ‰å¾ˆé«˜çš„è¦æ±‚
  - åŒ…æ‹¬ä½†ä¸é™äºRetrievalï¼ŒClusteringï¼ŒPair classificationï¼ŒRerankingï¼ŒSTSï¼ŒSummarizationï¼ŒClassificationï¼‰
- ä¸ºäº†èƒ½è¾¾åˆ°Embeddingçš„é«˜åŒºåˆ†åº¦ï¼Œè‡³å°‘éœ€è¦ä¸Šäº¿çº§åˆ«çš„è®­ç»ƒå®ä¾‹ã€‚
- åŒæ—¶ï¼Œæ•°æ®é›†çš„éœ€è¦æ¥æºå°½é‡å¹¿æ³›æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–æ€§ï¼ˆgeneralityï¼‰ã€‚
- æ•°æ®å¢å¼ºï¼ˆData Argumentationï¼‰å¯¹äºåŸå§‹æ•°æ®çš„è´¨é‡æœ‰å¾ˆé«˜çš„è¦æ±‚ï¼Œæ‰€ä»¥éœ€è¦è¿›è¡Œæ•°æ®æ¸…æ´—ï¼ˆData cleanï¼‰ï¼Œå¦åˆ™å®¹æ˜“å¼•å…¥å™ªå£°ã€‚

#### åŸç†

Training è®­ç»ƒä¸€ä¸ª general-purpose çš„text embeddingsæœ‰ä¸¤ä¸ªé‡è¦å› ç´ ï¼š
- 1ï¼‰ä¸€ä¸ªå¥½çš„ç¼–ç å™¨ã€‚
- 2ï¼‰ä¸€ä¸ªå¥½çš„è®­ç»ƒæ–¹æ¡ˆã€‚

è™½ç„¶è¯´èƒ½å¤Ÿä½¿ç”¨ unlabeled data å¹¶åŸºäº Bert æˆ–è€…æ˜¯T5èƒ½æå¤§åœ°æé«˜é¢„è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½ï¼Œä½†æ˜¯å…‰è¿™æ ·è¿˜æ˜¯ä¸å¤Ÿçš„ï¼Œéœ€è¦é€šè¿‡å¤åˆçš„è®­ç»ƒæ–¹æ¡ˆã€‚
- ä»¥é¢å‘åµŒå…¥ï¼ˆembeddingï¼‰çš„é¢„è®­ç»ƒæ¥å‡†å¤‡æ–‡æœ¬ç¼–ç å™¨
- å¯¹æ¯”å­¦ä¹ éœ€è¦ä»å¤æ‚çš„è´Ÿé‡‡æ ·ä¸­æé«˜åµŒå…¥çš„å¯è¾¨åˆ«æ€§
- åŸºäºæŒ‡ä»¤å¾®è°ƒæ¥é›†æˆæ–‡æœ¬åµŒå…¥çš„ä¸åŒè¡¨ç¤ºèƒ½åŠ›

(1) Pre-training
- ä½¿ç”¨ä¸ºembeddingä»»åŠ¡é‡èº«å®šåšçš„ç®—æ³•æ¨¡å‹ï¼Œwudaoæ•°æ®é›†ï¼Œé«˜è´¨é‡ï¼Œå¤šæ ·æ€§çš„æ•°æ®é›†æ¥è¿›è¡Œä¸­æ–‡è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒã€‚
- æ¡†æ¶ä½¿ç”¨çš„æ˜¯`RetroMAE`ä¸­çš„`MAE-style`ï¼Œç®€å•ä½†æ˜¯é«˜æ•ˆã€‚Maskedçš„æ–‡æœ¬è¢«ç¼–ç åˆ°å…¶åµŒå…¥ä¸­ï¼Œç„¶ååœ¨è½»é‡çº§è§£ç å™¨ä¸Šæ¢å¤å¹²å‡€çš„æ–‡æœ¬ï¼š
- ![](https://pic1.zhimg.com/80/v2-0371ca9ad3f1db22a492ead11d5d5460_1440w.webp)
- å…¶ä¸­ Encï¼ŒDecåˆ†åˆ«è¡¨ç¤ºencoderå’Œdecoderï¼ŒXï¼Œ X'è¡¨ç¤ºåˆå§‹æ–‡æœ¬å’Œmasked æ–‡æœ¬ã€‚

(2) General purpose fine-tuning
- é¢„è®­ç»ƒæ¨¡å‹ç”¨C-MTPçš„unlabeled data è¿›è¡Œå¯¹æ¯”å­¦ä¹ ï¼Œä»negatives æ¥å­¦ä¹ ä¸åŒæ–‡æœ¬ä¹‹é—´çš„å·®å¼‚
- ![](https://pic3.zhimg.com/80/v2-755911ce893ac29b57c54ad4aa3c3332_1440w.webp)
- på’Œqéƒ½æ˜¯æ–‡æœ¬å¯¹ï¼ŒQ' æ˜¯negatives q'çš„é›†åˆï¼Œtæ˜¯æ¸©åº¦ã€‚å¯¹æ¯”å­¦ä¹ çš„å…³é”®å› ç´ æ˜¯negativesï¼Œé™¤äº†negatives miningä¹‹å¤–ï¼Œè¿˜ä½¿ç”¨äº†`in-batch negative samples` æŠ€æœ¯ï¼ŒåŒæ—¶ä½¿ç”¨å¤§batch sizeï¼ˆ19200ï¼‰ï¼Œgithubä¸Šçš„ä»£ç è¿˜æœ‰`cross device negatives`è·¨è®¾å¤‡è´Ÿæ ·æœ¬ã€‚

(3) Task-speific fine-tuning
- é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒã€‚ä½¿ç”¨ C-MTPï¼ˆlabeled dataï¼‰è¿›ä¸€æ­¥å¾®è°ƒåµŒå…¥æ¨¡å‹ã€‚å¸¦æ ‡ç­¾çš„æ•°æ®é›†è¾ƒå°ä½†è´¨é‡è¾ƒé«˜ã€‚ç„¶è€Œï¼Œæ‰€åŒ…å«çš„ä»»åŠ¡ç±»å‹ä¸åŒï¼Œå…¶å½±å“å¯èƒ½ç›¸äº’çŸ›ç›¾ã€‚è¿™é‡Œä½œè€…ç”¨äº†ä¸¤ä¸ªæ–¹å¼å»è§£å†³è¿™ä¸ªé—®é¢˜ï¼š
- 1ï¼‰æŒ‡ä»¤å¾®è°ƒæ¥å¸®åŠ©æ¨¡å‹æ¥åŒºåˆ†ä¸åŒçš„ä»»åŠ¡ã€‚å¯¹äºæ¯ä¸€ä¸ªæ–‡æœ¬å¯¹(p,q), ç‰¹å®šä»»åŠ¡çš„æŒ‡ä»¤ $I_t$ ä¼šè¢«é™„åŠ åˆ°queryè¿™ä¸€ç«¯ï¼Œæ”¹å†™ä¹‹åçš„queryï¼š $q' \leftarrow q + I_t$ . è¿™ç§æŒ‡ä»¤æ˜¯åå‘äºå£è¯­è¯çš„ï¼Œä¾‹å¦‚â€œsearch relevant passages for the queryâ€. åŒæ—¶ä½¿ç”¨ANN-styleç­–ç•¥è¿›è¡Œnegative samplingã€‚

è®ºæ–‡æ²¡æœ‰æå‡ºå¾ˆæ–°çš„è®­ç»ƒæ–¹æ³•ã€å¾®è°ƒæŠ€å·§ï¼Œä»åŸºæœ¬å†…å®¹ä¸Š
- å…ˆè¿›è¡ŒBertç±»å‹çš„æ¨¡å‹é¢„è®­ç»ƒï¼ŒRetroMAEæ–¹æ³•
- ç„¶åè¿›è¡Œæ— ç›‘ç£å­¦ä¹ ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰
- æœ€åæ˜¯å¤šä»»åŠ¡çš„æ— ç›‘ç£å­¦ä¹ ï¼ˆåŠ å…¥æŒ‡ä»¤æ¥åŒºåˆ†ä¸åŒçš„ä»»åŠ¡ï¼‰ã€‚

#### æ•ˆæœè¯„æµ‹

æ¨¡å‹çš„æ•ˆæœè¡¨ç°å‡ºè‰²ï¼Œæ•°æ®é›†çš„æ„å»ºç»™ä¹‹åçš„ç ”ç©¶è€…æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„Benchmarkã€‚
- ![](https://pic2.zhimg.com/80/v2-a1ec17e47b55792ab42188a7ebfa5ba1_1440w.webp)

baai-general-embedding æ¨¡å‹åœ¨ MTEB å’Œ C-MTEB æ’è¡Œæ¦œä¸Šéƒ½å®ç°äº†æœ€å…ˆè¿›çš„æ€§èƒ½
- è¶…è¿‡ OpenAI `text-embedding-ada-002` å’Œ `m3e-large`

[è¯­ä¹‰æ¨¡å‹ FlagEmbedding å®è·µ](https://zhuanlan.zhihu.com/p/657722124)

BGEåœ¨è¯­ä¹‰ç†è§£é—®é¢˜ä¸Šæ”¯æŒ`s2s`ï¼ˆçŸ­æ–‡æœ¬åŒ¹é…çŸ­æ–‡æœ¬ï¼‰ï¼Œå’Œ`s2p`ï¼ˆçŸ­æ–‡æœ¬åŒ¹é…é•¿æ–‡æœ¬ï¼‰

ä¸‰ç»„å®éªŒ
- çŸ­æ–‡æœ¬åŒ¹é…é•¿æ–‡æœ¬
- çŸ­æ–‡æœ¬åŒ¹é…é•¿æ–‡æœ¬
- ï¼ˆinstruction+ï¼‰çŸ­æ–‡æœ¬åŒ¹é…é•¿æ–‡æœ¬

æ•°æ®é›†: ä¸‹è½½äº†1wæ¡è®­ç»ƒæ•°æ®
- [SimCLUE](https://github.com/CLUEbenchmark/SimCLUE)

å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹ï¼š`BGE`å’Œ`M3E`

ç»“è®º
- æ•´ä½“ä¸ŠBGEæ¨¡å‹çš„åˆ†æ•°è¦é«˜ä¸€äº›

```py
def semantic_retrieval(s1, s2, model):
    s1_logit = model.encode(s1, show_progress_bar=True, normalize_embeddings=True)
    s2_logit = model.encode(s2, show_progress_bar=True, normalize_embeddings=True)
    score = np.dot(s1_logit, s2_logit.T)
    
    print(score)
    
    for i in range(len(s1)):
        for j in range(len(s2)):
            print(f"\n{s1[i]}\n{s2[j]}\nç›¸ä¼¼åº¦ä¸º: {score[i][j]}")
from sentence_transformers import SentenceTransformer
import numpy as np

# ç›´æ¥ä½¿ç”¨hugging face
# model = SentenceTransformer('moka-ai/m3e-base')

# ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œæ”¾åœ¨modelsç›®å½•ä¸‹é¢
model = SentenceTransformer('models/bge')

# this is a s2s task
s1 = ["ä»Šå¤©å¤©æ°”è¿™ä¹ˆæ ·", "ä»Šå¤©ä¸­åˆåƒä»€ä¹ˆ"]
s2 = ["å¤©æ°”å¦‚ä½•", "ä¸­åˆæƒ³åƒä»€ä¹ˆ"]
semantic_retrieval(s1, s2, model)

# this is a s2p task without instruction
s1 = ["é…’ç²¾çš„å±å®³", "å¦‚ä½•æˆ’çƒŸ", "å­¦ä¹ çš„é‡è¦æ€§"]
s2 = ["å¤©æ°”å¦‚ä½•ä¸€æ¬¡é¥®é…’è¿‡é‡å¯å¼•èµ·æ€¥æ€§é…’ç²¾ä¸­æ¯’ã€‚\nè¡¨ç°åˆ†ä¸‰æœŸï¼š\nï¼ˆ1ï¼‰æ—©æœŸ(å…´å¥‹æœŸ)ã€‚è¡€ä¸­é…’ç²¾æµ“åº¦è¾¾50mg/dlè¡¨ç°è¯­æ— ä¼¦æ¬¡ï¼Œæƒ…æ„Ÿçˆ†å‘ï¼Œå“­ç¬‘æ— å¸¸ç­‰ã€‚\nï¼ˆ2ï¼‰ä¸­æœŸ(å…±æµå¤±è°ƒæœŸ)ã€‚è¡€ä¸­é…’ç²¾æµ“åº¦150mg/dlã€‚è¡¨ç°è¯­è¨€ä¸æ¸…ï¼Œæ„è¯†æ¨¡ç³Šï¼Œæ­¥æ€è¹’è·šç­‰ã€‚\nï¼ˆ3ï¼‰åæœŸ(æ˜è¿·æœŸ)ã€‚è¡€ä¸­é…’ç²¾æµ“åº¦250mg/dlä»¥ä¸Šã€‚è¡¨ç°æ˜è¿·ï¼Œç³å­”æ•£å¤§ï¼Œå¤§å°ä¾¿å¤±ç¦ï¼Œé¢è‰²è‹ç™½ã€‚ä¸€èˆ¬äººçš„é…’ç²¾è‡´æ­»é‡ä¸º5ï½8g/kgã€‚",
      "é¥®èŒ¶å…»ç”Ÿï¼Œå½“ç„¶ï¼Œä¹Ÿèƒ½å¸®ä½ æ‘†è„±å¸çƒŸçš„æ¯›ç—…ã€‚å½“ä½ æƒ³è¦å¸çƒŸæ—¶ï¼Œæ¨èç»™è‡ªå·±ä¸€æ¯æµ“èŒ¶ï¼Œå–ä¸‹å»ï¼Œä¼šæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚",
      "åªæœ‰å­¦ä¹ ï¼Œæ‰èƒ½è®©æˆ‘ä»¬ä¸æ–­æˆé•¿ï¼Œå½“ä½ ä½“ä¼šåˆ°è‡ªå·±ä¸æ–­æˆé•¿çš„æ—¶å€™ï¼Œä½ å°±ä¼šå‘ç°ï¼Œé‚£ç§å‘è‡ªå†…å¿ƒçš„å¿«ä¹æ˜¯å…¶ä»–ç‰©è´¨æ— æ³•å¸¦ç»™æˆ‘ä»¬çš„ã€‚ åªæœ‰å­¦ä¹ ï¼Œæ‰èƒ½è·å¾—æ–°çŸ¥ï¼Œå¢é•¿æ‰å¹²ï¼Œæ‰èƒ½å®ç°æˆ‘ä»¬çš„æ¢¦æƒ³ã€‚"
     ]
semantic_retrieval(s1, s2, model)

# this is a s2p task with instruction
instruction = "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š" 
s1 = ["é…’ç²¾çš„å±å®³", "å¦‚ä½•æˆ’çƒŸ", "å­¦ä¹ çš„é‡è¦æ€§"]
s1 = [instruction + s for s in s1]
s2 = ["é¥®é…’è¿‡é‡å¯å¼•èµ·æ€¥æ€§é…’ç²¾ä¸­æ¯’ã€‚\nè¡¨ç°åˆ†ä¸‰æœŸï¼š\nï¼ˆ1ï¼‰æ—©æœŸ(å…´å¥‹æœŸ)ã€‚è¡€ä¸­é…’ç²¾æµ“åº¦è¾¾50mg/dlè¡¨ç°è¯­æ— ä¼¦æ¬¡ï¼Œæƒ…æ„Ÿçˆ†å‘ï¼Œå“­ç¬‘æ— å¸¸ç­‰ã€‚\nï¼ˆ2ï¼‰ä¸­æœŸ(å…±æµå¤±è°ƒæœŸ)ã€‚è¡€ä¸­é…’ç²¾æµ“åº¦150mg/dlã€‚è¡¨ç°è¯­è¨€ä¸æ¸…ï¼Œæ„è¯†æ¨¡ç³Šï¼Œæ­¥æ€è¹’è·šç­‰ã€‚\nï¼ˆ3ï¼‰åæœŸ(æ˜è¿·æœŸ)ã€‚è¡€ä¸­é…’ç²¾æµ“åº¦250mg/dlä»¥ä¸Šã€‚è¡¨ç°æ˜è¿·ï¼Œç³å­”æ•£å¤§ï¼Œå¤§å°ä¾¿å¤±ç¦ï¼Œé¢è‰²è‹ç™½ã€‚ä¸€èˆ¬äººçš„é…’ç²¾è‡´æ­»é‡ä¸º5ï½8g/kgã€‚",
      "é¥®èŒ¶å…»ç”Ÿï¼Œå½“ç„¶ï¼Œä¹Ÿèƒ½å¸®ä½ æ‘†è„±å¸çƒŸçš„æ¯›ç—…ã€‚å½“ä½ æƒ³è¦å¸çƒŸæ—¶ï¼Œæ¨èç»™è‡ªå·±ä¸€æ¯æµ“èŒ¶ï¼Œå–ä¸‹å»ï¼Œä¼šæœ‰å¾ˆå¥½çš„æ•ˆæœã€‚",
      "åªæœ‰å­¦ä¹ ï¼Œæ‰èƒ½è®©æˆ‘ä»¬ä¸æ–­æˆé•¿ï¼Œå½“ä½ ä½“ä¼šåˆ°è‡ªå·±ä¸æ–­æˆé•¿çš„æ—¶å€™ï¼Œä½ å°±ä¼šå‘ç°ï¼Œé‚£ç§å‘è‡ªå†…å¿ƒçš„å¿«ä¹æ˜¯å…¶ä»–ç‰©è´¨æ— æ³•å¸¦ç»™æˆ‘ä»¬çš„ã€‚ åªæœ‰å­¦ä¹ ï¼Œæ‰èƒ½è·å¾—æ–°çŸ¥ï¼Œå¢é•¿æ‰å¹²ï¼Œæ‰èƒ½å®ç°æˆ‘ä»¬çš„æ¢¦æƒ³ã€‚"
     ]
semantic_retrieval(s1, s2, model)

```


#### å®‰è£…

```sh
pip install -U FlagEmbedding
```

#### FlagEmbedding ä½¿ç”¨

```py
from FlagEmbedding import FlagModel
sentences = ["æ ·ä¾‹æ•°æ®-1", "æ ·ä¾‹æ•°æ®-2"]
model = FlagModel('BAAI/bge-large-zh-v1.5', 
                  query_instruction_for_retrieval="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š",
                  use_fp16=True) # è®¾ç½®use_fp16ä¸ºTrueå¯ä»¥åŠ å¿«è®¡ç®—ï¼Œæ•ˆæœä¼šç¨æœ‰ä¸‹é™
embeddings_1 = model.encode(sentences)
embeddings_2 = model.encode(sentences)
similarity = embeddings_1 @ embeddings_2.T
print(similarity)

# å¯¹äºçŸ­æŸ¥è¯¢åˆ°é•¿æ–‡æ¡£çš„æ£€ç´¢ä»»åŠ¡ï¼Œè¯·å¯¹æŸ¥è¯¢ä½¿ç”¨ encode_queries() å‡½æ•°ï¼Œå…¶ä¼šè‡ªåŠ¨ä¸ºæ¯ä¸ªæŸ¥è¯¢åŠ ä¸ŠæŒ‡ä»¤
# ç”±äºå€™é€‰æ–‡æœ¬ä¸éœ€è¦æ·»åŠ æŒ‡ä»¤ï¼Œæ£€ç´¢ä¸­çš„å€™é€‰é›†ä¾ç„¶ä½¿ç”¨ encode() æˆ– encode_corpus() å‡½æ•°
queries = ['query_1', 'query_2']
passages = ["æ ·ä¾‹æ–‡æ¡£-1", "æ ·ä¾‹æ–‡æ¡£-2"]
q_embeddings = model.encode_queries(queries)
p_embeddings = model.encode(passages)
scores = q_embeddings @ p_embeddings.T
# ------ è®¡ç®—ç›¸ä¼¼åº¦ --------
from FlagEmbedding import FlagReranker
reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) #è®¾ç½® fp16 ä¸ºTrueå¯ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œæ•ˆæœä¼šæœ‰å¯ä»¥å¿½ç•¥çš„ä¸‹é™

score = reranker.compute_score(['query', 'passage']) # è®¡ç®— query å’Œ passageçš„ç›¸ä¼¼åº¦
print(score)
scores = reranker.compute_score([['query 1', 'passage 1'], ['query 2', 'passage 2']])
print(scores)
```

Instructionå‚æ•° query_instruction_for_retrieval è¯·å‚ç…§ï¼š [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). å½“åŠ è½½å¾®è°ƒåçš„æ¨¡å‹æ—¶
- å¦‚æœæ²¡æœ‰åœ¨è®­ç»ƒçš„jsonæ–‡ä»¶ä¸­ä¸ºqueryæ·»åŠ æŒ‡ä»¤ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²""; 
- å¦‚æœè®­ç»ƒæ•°æ®ä¸­ä¸ºqueryæ·»åŠ äº†æŒ‡ä»¤ï¼Œæ›´æ”¹ä¸ºæ–°è®¾ç½®çš„æŒ‡ä»¤ã€‚

FlagModelæ”¯æŒ`GPU`ä¹Ÿæ”¯æŒ`CPU`æ¨ç†ã€‚
- å¦‚æœGPUå¯ç”¨ï¼Œå…¶é»˜è®¤ä¼˜å…ˆä½¿ç”¨GPUã€‚
- å¦‚æœæƒ³ç¦æ­¢å…¶ä½¿ç”¨GPUï¼Œè®¾ç½® `os.environ["CUDA_VISIBLE_DEVICES"]=""` 
- ä¸ºæé«˜æ•ˆç‡ï¼ŒFlagModelé»˜è®¤ä¼šä½¿ç”¨æ‰€æœ‰çš„GPUè¿›è¡Œæ¨ç†ã€‚å¦‚æœæƒ³è¦ä½¿ç”¨å…·ä½“çš„GPUï¼Œè¯·è®¾ç½® `os.environ["CUDA_VISIBLE_DEVICES"]`ã€‚

#### Langchian ä¸­ä½¿ç”¨

Langchianä¸­ä½¿ç”¨bgeæ¨¡å‹ï¼š

```py
from langchain.embeddings import HuggingFaceBgeEmbeddings
model_name = "BAAI/bge-large-en-v1.5"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity
model = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
```

#### transformers ä¸­ä½¿ç”¨

transformers ä¸­ä½¿ç”¨

```py
from transformers import AutoTokenizer, AutoModel
import torch
# Sentences we want sentence embeddings for
sentences = ["æ ·ä¾‹æ•°æ®-1", "æ ·ä¾‹æ•°æ®-2"]

# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')
model = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')

# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')
# å¯¹äºçŸ­æŸ¥è¯¢åˆ°é•¿æ–‡æ¡£çš„æ£€ç´¢ä»»åŠ¡, ä¸ºæŸ¥è¯¢åŠ ä¸ŠæŒ‡ä»¤
# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')

# Compute embeddings
with torch.no_grad():
    model_output = model(**encoded_input)
    # Perform pooling. In this case, cls pooling.
    sentence_embeddings = model_output[0][:, 0]
# normalize embeddings
sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
print("Sentence embeddings:", sentence_embeddings)
# ------ è®¡ç®—ç›¸ä¼¼åº¦ --------
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')
model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')
model.eval()

pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]
with torch.no_grad():
    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)
    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()
    print(scores)
```


### ã€2023-10-24ã€‘AnglE 

#### AnglE ä»‹ç»

ã€2023-10-24ã€‘Embedding SOTA æ˜¯ `AnglE`, åœ¨ STS13ï¼ŒSTS14ï¼ŒSTS15ï¼ŒSTS16 ä»¥åŠ Sick-R ä¸Šéƒ½è¾¾åˆ°äº† SOTAã€‚
- Arxiv: é¦™æ¸¯ç†å·¥ [AnglE-optimized Text Embeddings](https://arxiv.org/pdf/2309.12871.pdf)
- Github: [AnglE](github.com/SeanLee97/AnglE)
- Huggingface: SeanLee97/angle-llama-7b-nli-20231027
- [Compare with M3E](https://github.com/SeanLee97/AnglE/issues/3): è®ºæ–‡ä¸»è¦å¯¹æ¯”è‹±è¯­embeddingæ•ˆæœï¼Œè€Œm3eä¸»è¦æ˜¯ä¸­æ–‡embeddingï¼Œæ‰€ä»¥æš‚æœªå¯¹æ¯”ï¼Œ11æœˆå‘å¸ƒä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹, [è¯¦è§](https://github.com/SeanLee97/AnglE/blob/main/README_zh.md)


#### AnglE åŸç†

é«˜è´¨é‡æ–‡æœ¬åµŒå…¥åœ¨æé«˜è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆSTSï¼‰ä»»åŠ¡ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œè¿™æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨ä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç„¶è€Œï¼Œç°æœ‰æ–‡æœ¬åµŒå…¥æ¨¡å‹é¢ä¸´çš„ä¸€ä¸ªæ™®éæŒ‘æˆ˜æ˜¯**æ¢¯åº¦æ¶ˆå¤±**é—®é¢˜ï¼Œä¸»è¦æ˜¯ä¼˜åŒ–ç›®æ ‡ä¸­ä¾èµ–**ä½™å¼¦å‡½æ•°**ï¼Œè€Œä½™å¼¦å‡½æ•°å…·æœ‰**é¥±å’ŒåŒºåŸŸ**ã€‚

æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„**è§’åº¦ä¼˜åŒ–**æ–‡æœ¬åµŒå…¥æ¨¡å‹â€”â€”`AnglE`ã€‚ æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨å¤æ‚ç©ºé—´ä¸­å¼•å…¥**è§’åº¦ä¼˜åŒ–**ã€‚è¿™ç§æ–¹æ³•æœ‰æ•ˆåœ°ç¼“è§£äº†ä½™å¼¦å‡½æ•°é¥±å’ŒåŒºåŸŸçš„ä¸è‰¯å½±å“ï¼Œè¿™å¯èƒ½ä¼šé˜»ç¢æ¢¯åº¦å¹¶é˜»ç¢ä¼˜åŒ–è¿‡ç¨‹ã€‚

åŸºäº AnglE å¼€ç®±å³ç”¨çš„æ–‡æœ¬å‘é‡åº“ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œå¯ç”¨äºæ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ã€æ£€ç´¢å¬å›ã€åŒ¹é…ç­‰åœºæ™¯ã€‚ä»£ç åŸºäº ğŸ¤—transformers æ„å»ºï¼Œæä¾›æ˜“ç”¨çš„å¾®è°ƒæ¥å£ï¼Œå¯åœ¨ 3090Tiã€ 4090 ç­‰æ¶ˆè´¹çº§ GPU ä¸Šå¾®è°ƒ LLaMA-7B æ¨¡å‹ï¼Œæ”¯æŒå¤šå¡åˆ†å¸ƒå¼è®­ç»ƒã€‚

#### AnglE æ•ˆæœ

åœ¨ç°æœ‰çš„çŸ­æ–‡æœ¬STSæ•°æ®é›†å’Œä»GitHub Issuesæ”¶é›†çš„æ–°çš„é•¿æ–‡æœ¬STSæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒã€‚æ­¤å¤–ï¼Œè¿˜ç ”ç©¶äº†å…·æœ‰æœ‰é™æ ‡è®°æ•°æ®çš„ç‰¹å®šé¢†åŸŸSTSåœºæ™¯ï¼Œå¹¶æ¢è®¨äº†AnglEå¦‚ä½•ä¸LLMæ³¨é‡Šæ•°æ®é…åˆä½¿ç”¨ã€‚

å„ç§ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬çŸ­æ–‡æœ¬STSã€é•¿æ–‡æœ¬STSå’Œç‰¹å®šé¢†åŸŸçš„STSä»»åŠ¡ã€‚
- AnglEä¼˜äºå¿½ç•¥ä½™å¼¦é¥±å’ŒåŒºåŸŸçš„æœ€å…ˆè¿›çš„STSæ¨¡å‹ã€‚
- è¯æ˜äº†AnglEç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬åµŒå…¥çš„èƒ½åŠ›ä»¥åŠè§’åº¦ä¼˜åŒ–åœ¨STSä¸­çš„æœ‰ç”¨æ€§ã€‚

AnglE-roberta-wwm-ext æ•ˆæœæœ€ä½³

å„æ•°æ®é›†çš„å¾®è°ƒåŠè¯„ä¼°ä»£ç å¦‚ä¸‹ï¼š
-   ATEC: [examples/Angle-ATEC.ipynb](https://github.com/SeanLee97/AnglE/blob/main/examples/Angle-ATEC.ipynb)
-   BQ: [examples/Angle-BQ.ipynb](https://github.com/SeanLee97/AnglE/blob/main/examples/Angle-BQ.ipynb)
-   LCQMC: [examples/Angle-LCQMC.ipynb](https://github.com/SeanLee97/AnglE/blob/main/examples/Angle-LCQMC.ipynb)
-   PAWSX: [examples/Angle-PAWSX.ipynb](https://github.com/SeanLee97/AnglE/blob/main/examples/Angle-PAWSX.ipynb)
-   SST-B: [![Open In Colab](https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667)](https://colab.research.google.com/drive/1HzuaZjdkKqL_JasQnSGZ3g2H3H2aR6yG?usp=sharing)

#### AnglE ä½¿ç”¨

```py
# python -m pip install -U angle-emb
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

peft_model_id = 'SeanLee97/angle-llama-7b-nli-v2'
config = PeftConfig.from_pretrained(peft_model_id)
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path).bfloat16().cuda()
model = PeftModel.from_pretrained(model, peft_model_id).cuda()

def decorate_text(text: str):
    return f'Summarize sentence "{text}" in one word:"'

inputs = 'hello world!'
tok = tokenizer([decorate_text(inputs)], return_tensors='pt')
for k, v in tok.items():
    tok[k] = v.cuda()
vec = model(output_hidden_states=True, **tok).hidden_states[-1][:, -1].float().detach().cpu().numpy()
print(vec)
```

#### å¾®è°ƒæ¨¡å‹

åªéœ€è¦å‡†å¤‡å¥½æ•°æ®å³å¯å¿«é€Ÿå¾®è°ƒã€‚æ•°æ®æ ¼å¼å¿…é¡»è¦è½¬æˆ datasets.Dataset ï¼ˆä½¿ç”¨æ–¹å¼è¯·å‚ç…§å®˜æ–¹æ–‡æ¡£ [Datasets](https://huggingface.co/docs/datasets/index)ï¼‰ä¸”å¿…é¡»è¦æä¾› text1, text2, label ä¸‰åˆ—ã€‚

```py
from datasets import load_dataset
from angle_emb import AnglE, AngleDataTokenizer

# 1. åŠ è½½æ¨¡å‹
angle = AnglE.from_pretrained('hfl/chinese-roberta-wwm-ext', max_length=128, pooling_strategy='cls').cuda()

# 2. åŠ è½½æ•°æ®å¹¶è½¬æ¢æ•°æ®
ds = load_dataset('shibing624/nli_zh', 'STS-B')
ds = ds.rename_column('sentence1', 'text1')
ds = ds.rename_column('sentence2', 'text2')
ds = ds.select_columns(["text1", "text2", "label"])
train_ds = ds['train'].shuffle().map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)
valid_ds = ds['validation'].map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)
test_ds = ds['test'].map(AngleDataTokenizer(angle.tokenizer, angle.max_length), num_proc=8)

# 3. è®­ç»ƒ
angle.fit(
    train_ds=train_ds,
    valid_ds=valid_ds,
    output_dir='ckpts/sts-b',
    batch_size=64,
    epochs=5,
    learning_rate=3e-5,
    save_steps=100,
    eval_steps=1000,
    warmup_steps=0,
    gradient_accumulation_steps=1,
    loss_kwargs={
        'w1': 1.0,
        'w2': 1.0,
        'w3': 1.0,
        'cosine_tau': 20,
        'ibn_tau': 20,
        'angle_tau': 1.0
    },
    fp16=True,
    logging_steps=100
)

# 4. åŠ è½½æœ€ä¼˜æ¨¡å‹è¯„ä¼°æ•ˆæœ
angle = AnglE.from_pretrained('hfl/chinese-roberta-wwm-ext', pretrained_model_path='ckpts/sts-b/best-checkpoint').cuda()
corrcoef, accuracy = angle.evaluate(test_ds, device=angle.device)
print('corrcoef:', corrcoef)
```

### ã€2024-1-7ã€‘E5-mistral-7b-instruct æ–°sota

ã€2024-1-7ã€‘[å¾®è½¯E5-mistral-7b-instruct: ç«™åœ¨LLMè‚©è†€ä¸Šçš„text embedding](https://zhuanlan.zhihu.com/p/676366430)
- è®ºæ–‡ [Improving Text Embeddings with Large Language Models](https://arxiv.org/pdf/2401.00368.pdf)

å¾®è½¯å‘å¸ƒçš„text embeddingæ¨¡å‹E5-mistral-7b-instructç™»é¡¶[MTEB](https://huggingface.co/spaces/mteb/leaderboard)ï¼Œå¹¶ä¸”ç”©å‡ºäº†ç¬¬äºŒåä¸€æ®µæ˜æ˜¾è·ç¦»ã€‚
- é¦–æ¬¡é‡‡ç”¨LLMæ¥åšå‘é‡åŒ–æ¨¡å‹
- ç”¨LLMæ¥ç”Ÿæˆå‘é‡åŒ–ä»»åŠ¡çš„äººé€ æ•°æ®ï¼Œç„¶åç”¨`å¯¹æ¯”å­¦ä¹ `çš„lossï¼Œå¾®è°ƒ`mistral-7b`ï¼Œä»…ä»…ä½¿ç”¨äººé€ æ•°æ®ï¼Œè¾¾åˆ°å’Œå…¶ä»–æ¨¡å‹å¯æ¯”çš„ç»“æœï¼Œå½“ä½¿ç”¨äººé€ æ•°æ®å’Œå¼€æºçš„æ ‡æ³¨æ•°æ®å¾®è°ƒæ—¶ï¼Œè¾¾åˆ°äº†MTEBçš„**sota**ï¼Œæ¯”ç¬¬äºŒåé«˜äº†2%ã€‚
- ç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹å’Œå‘é‡åŒ–ä»»åŠ¡æ˜¯ä¸€æšç¡¬å¸çš„æ­£åé¢ã€‚éƒ½è¦æ±‚æ¨¡å‹å¯¹è‡ªç„¶è¯­è¨€æœ‰æ·±åˆ»çš„ç†è§£ã€‚ï¼ˆä»è€Œå¯ä»¥æ›´å¥½çš„è¡¨å¾å¥å­ï¼‰ã€‚è€Œç”Ÿæˆå¼å¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨**è‡ªå›å½’**çš„é¢„è®­ç»ƒæ–¹å¼ï¼Œåœ¨æ›´å¤šçš„æ•°æ®ä¸Šå¾®è°ƒè¿‡ï¼Œå¯ä»¥æ›´å¥½çš„è¡¨ç¤ºå¥å­ã€‚åªéœ€è¦å°‘é‡çš„å¾®è°ƒï¼Œå°±å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¥½çš„å‘é‡åŒ–æ¨¡å‹ã€‚

å‘é‡åŒ–ä»»åŠ¡åˆ†ä¸ºä¸¤ç§ï¼š**ä¸å¯¹ç§°**ä»»åŠ¡, **å¯¹ç§°**ä»»åŠ¡ã€‚
1. ä¸å¯¹ç§°ä»»åŠ¡ï¼šqueryå’Œdocè¯­ä¹‰ç›¸å…³ï¼Œä½†ä¸æ˜¯å½¼æ­¤å¦å¤–çš„é‡Šä¹‰ã€‚ï¼ˆä¸ä»…ä»…æ˜¯è¡¨è¾¾æ–¹å¼ä¸åŒï¼‰
  - æ ¹æ®é•¿åº¦ï¼Œåˆåˆ’åˆ†äº†å››ä¸ªç»†ç²’åº¦ï¼šçŸ­-é•¿ï¼›çŸ­-çŸ­ï¼›é•¿-çŸ­ï¼›é•¿-é•¿åŒ¹é…ã€‚
  - ä¸ºæ¯ä¸ªç»†ç²’åº¦ï¼Œè®¾è®¡äº†ä¸¤é˜¶æ®µçš„promptæ¨¡æ¿ã€‚
    1. æä¾›å‡ ä¸ªå€™é€‰ï¼Œè®©å¤§æ¨¡å¤´è„‘é£æš´ä¸€ä¸ªç±»ä¼¼å€™é€‰ä»»åŠ¡çš„æ± å­ã€‚ï¼ˆä¹¦ç±æœç´¢ï¼Œç§‘å­¦æ–‡æ¡£æœç´¢ï¼‰
    2. æ ¹æ®å…·ä½“çš„ä»»åŠ¡å®šä¹‰ï¼Œç”Ÿæˆæ•°æ®ã€‚
  - ä¸å¯¹ç§°ä»»åŠ¡ï¼Œç›´æ¥è®©LLMä¸€ä¸ªé˜¶æ®µç”Ÿæˆæ•°æ®çš„è¯ï¼Œæ²¡æœ‰ä¸¤ä¸ªé˜¶æ®µåšçš„å¤šæ ·æ€§å¥½ã€‚
2. å¯¹ç§°ä»»åŠ¡ï¼šqueryå’Œdocè¯­ä¹‰ç›¸å…³ï¼Œä¸è¿‡è¡¨è¾¾æ–¹å¼ä¸åŒã€‚
  - å•è¯­åŒ¹é…(STS)å’Œå¤šè¯­åŒ¹é…ã€‚
  - ä¸¤ä¸ªå®šä¹‰äº†ä¸åŒçš„promptæ¨¡æ¿ï¼Œå› ä¸ºä»»åŠ¡ç®€å•ï¼Œç›´æ¥ä¸€ä¸ªé˜¶æ®µåšçš„ã€‚

æ•ˆæœä¸ºä»€ä¹ˆå¥½ï¼Ÿ[å‚è€ƒ](https://www.zhihu.com/question/637789621/answer/3361687482?utm_psn=1731097826090815489)
- E5-mistral-7b-instructåœ¨queryå‰é¢åŠ ä¸€ä¸ªç»†ç²’åº¦çš„ä»»åŠ¡æè¿°ï¼Œåˆ©ç”¨LLMåœ¨è®­ç»ƒé˜¶æ®µçš„èƒ½åŠ›ï¼Œä¸ºå¥å­å¾—åˆ°æ›´å¥½çš„å‘é‡è¡¨ç¤ºï¼Œä¹‹å‰ç¡®å®æ¨¡å‹åšçš„æ²¡æœ‰è¿™ä¹ˆç»†ç²’åº¦ã€‚è€Œä¸”åŒä¸€ä¸ªqueryï¼Œä»»åŠ¡ä¸åŒï¼Œå¯ä»¥ç”Ÿæˆä¸åŒçš„å‘é‡ã€‚è¿™ä¸ªå‘é‡é’ˆå¯¹å…·ä½“ä»»åŠ¡ï¼Œæœ‰ä¸€å®šçš„è¾¨è¯†åº¦ã€‚
  - é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡ï¼Œç»™queryä¾§åŠ ä¸Šä»»åŠ¡å®šä¹‰çš„æŒ‡ä»¤ã€‚docä¾§ä¸åŠ ã€‚
- ä¸ç®¡æ˜¯LLMç”Ÿæˆçš„äººé€ æ•°æ®è¿˜æ˜¯å¼€æºæ•°æ®é›†ï¼ŒE5-mistral-7b-instructéƒ½ä¸ºæ­£ä¾‹æŒ–æ˜äº†éš¾è´Ÿä¾‹ã€‚å¯¹äºLLMç”Ÿæˆçš„äººé€ æ•°æ®ï¼Œè®©å¤§æ¨¡å‹è‡ªå·±ç”Ÿæˆéš¾è´Ÿä¾‹ã€‚å¯¹äºå¼€æºæ•°æ®é›†ï¼Œç”¨e5-baseå»æŒ–æ˜äº†top100çš„éš¾è´Ÿä¾‹ã€‚è¿™ç›¸å½“äºè®©å¦å¤–ä¸€ä¸ªæ¨¡å‹ï¼Œå»æ„é€ éš¾è´Ÿä¾‹ï¼Œå¢åŠ å­¦ä¹ éš¾åº¦ã€‚ç›¸å½“äºè®©encoderæ¨¡å‹å»ä¸ºdecoderæ„é€ äº†éš¾è´Ÿä¾‹ã€‚
- æ¨¡å‹æ‰©å¤§äº†10å€ï¼Œæ¯”MTEBæ¦œä¸Šçš„encoderæ¨¡å‹ï¼Œå‘é‡ç»´åº¦ä¹Ÿæ˜¯4096ï¼Œå¦‚æœencodeæ¨¡å‹è¿™ä¹ˆå¤§äº†ï¼Œä¼šä¸ä¼šæœ‰æ›´å¥½çš„æ•ˆæœå‘¢ï¼Ÿ

E5-mistral-7b-instructåˆ©ç”¨LLMäº§ç”Ÿäº†æ¥è¿‘**100ç§**è¯­è¨€çš„é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„è®­ç»ƒæ•°æ®ï¼Œåˆ©ç”¨**çº¯decoder**çš„LLMåœ¨åˆæˆæ•°æ®ä¸Šè¿›ä¸€æ­¥finetuneã€‚

ä»…ä¾é åˆæˆæ•°æ®è®­ç»ƒå¾—åˆ°çš„text embeddingå¯ä»¥åª²ç¾ç›®å‰ä¸»æµçš„sotaæ¨¡å‹ï¼Œè€Œæ··åˆåˆæˆæ•°æ®è·ŸçœŸå®æ ‡æ³¨æ•°æ®è®­ç»ƒå®Œæˆçš„text embeddingæ¨¡å‹åœ¨BEIRè·ŸMTEBä¸Šéƒ½è¾¾åˆ°æ–°çš„sotaæ•ˆæœã€‚

**ç»™queryåŠ æŒ‡ä»¤**ï¼š

é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡ï¼Œç»™queryä¾§åŠ ä¸Šä»»åŠ¡å®šä¹‰çš„æŒ‡ä»¤ã€‚docä¾§ä¸åŠ ã€‚ï¼ˆç±»ä¼¼ç»™æ¯ä¸ªä»»åŠ¡ç»†ç²’åº¦çš„ä»»åŠ¡å®šä¹‰ã€‚ï¼‰
1. ç”Ÿæˆçš„æ•°æ®ï¼Œå°±ç›´æ¥ç”¨å¤§æ¨¡å‹ç»™çš„ã€‚
2. å…¶ä»–å·²æœ‰çš„æ•°æ®é›†ï¼Œå°±äººå·¥æ„é€ ä¸€ä¸ªä»»åŠ¡å®šä¹‰ï¼ŒåŠ ä¸Šã€‚

æ¨¡å‹è®­ç»ƒï¼š
- å¯¹äºqueryï¼Œè¾“å…¥æŒ‡ä»¤+query+EOSï¼›å¯¹äºdocï¼Œè¾“å…¥doc+EOSã€‚
- è¾“å…¥åˆ°llmä¸­ï¼Œæ‹¿EOS tokençš„å‘é‡ï¼Œä½œä¸ºæ•´ä¸ªå¥å­çš„è¡¨ç¤ºã€‚
- ç„¶åç”¨å…¸å‹çš„å¯¹æ¯”å­¦ä¹ ï¼ŒinfoNCE lossã€‚batchå†…éšæœºè´Ÿä¾‹+éš¾è´Ÿä¾‹ã€‚

æ•°æ®é›†
- å€ŸåŠ©GPT3.5-Turboï¼ŒGPT4å»ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæ„å»ºå¤šç§è¯­è¨€è·Ÿä»»åŠ¡ç±»å‹çš„æ•°æ®æ¥å¢å¼ºè®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§ã€‚ä»å¤§ç±»æ¥çœ‹å¯ä»¥å°†åˆæˆæ•°æ®åˆ†ä¸ºä¸¤å¤§ç±»ï¼Œå³éå¯¹ç§°ä»»åŠ¡è·Ÿå¯¹ç§°ä»»åŠ¡ï¼Œæœ€ç»ˆæ„å»ºå¾—åˆ°è¶…è¿‡15ä¸‡ä¸ªtask definitionçš„åŒ…æ‹¬93ç§è¯­è¨€çš„50ä¸‡ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ ¼å¼ä¸ºï¼ˆtask definition, user query, positive document, hard negative documentï¼‰ã€‚

æ¨¡å‹è®­ç»ƒ
- mistral-7b-instructçš„è®­ç»ƒæ–¹å¼è·Ÿä¹‹å‰ä»‹ç»çš„instructorç›¸ä¼¼ï¼Œåœ¨queryä¾§å°†task definitionè·Ÿuser queryæ‹¼æ¥åˆ°ä¸€èµ·ä½œä¸ºä¸€ä¸ªæ•´ä½“å»ç”Ÿæˆqueryçš„å‘é‡è¡¨å¾ï¼Œè€Œdocumentä¾§åˆ™ä¸æ·»åŠ ä»»ä½•å‰ç¼€ã€‚ç”±äºé‡‡ç”¨çš„çº¯decoderçš„è¯­è¨€æ¨¡å‹Mistral-7bï¼Œä¼šåœ¨queryæˆ–è€…documentåæ’å…¥ä¸€ä¸ª`[EOS]`ï¼Œç„¶åä¸€åŒè¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ï¼Œå°†`[EOS]`ä½ç½®ä¸Šæœ€åä¸€å±‚çš„éšå±‚è¡¨ç¤ºä½œä¸ºå¥å‘é‡ã€‚è®­ç»ƒæŸå¤±é‡‡ç”¨çš„æ˜¯å¸¸è§„çš„å¯¹æ¯”æŸå¤±ï¼Œå¸Œæœ›task definition+user queryè·Ÿpositive documentè¶³å¤Ÿé è¿‘ï¼ŒåŒæ—¶è·Ÿhard negative documentæˆ–è€…å…¶ä»–batchçš„è´Ÿæ ·æœ¬è¶³å¤Ÿç–è¿œã€‚

å®éªŒç»“è®º
- ä»MTEBä¸Šçœ‹ï¼Œä»…ç”¨LLMç”Ÿæˆæ•°æ®è®­ç»ƒå¾—åˆ°çš„text embeddingæ•ˆæœå°±å¾ˆä¸é”™äº†ï¼Œæ··åˆäº†åˆæˆæ•°æ®è·ŸçœŸå®ç›‘ç£æ•°æ®è®­ç»ƒå¾—åˆ°çš„text embeddingæ›´æ˜¯å–å¾—äº†æ–°çš„sotaæ•ˆæœã€‚åœ¨å¤šè¯­è¨€èƒ½åŠ›ä¸Šï¼Œä¹Ÿæœ‰ä¸ä¿—çš„è¡¨ç°ï¼Œæ–‡ä¸­è®¤ä¸ºåœ¨ä½èµ„æºè¯­è¨€ä¸Šçš„è¡¨ç°ç¨å·®ä¸€ç­¹åœ¨äºåŸºåº•æ¨¡å‹Mistral-7bé¢„è®­ç»ƒè¯­æ–™ä¸»è¦æ˜¯è‹±è¯­ã€‚
- å¼±ç›‘ç£å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒæ˜¯ä¸»æµtext embeddingæ¨¡å‹æˆåŠŸçš„ä¸€ä¸ªå…³é”®å› ç´ ï¼Œç ”ç©¶äººå‘˜å¯¹æ¯”å¼±ç›‘ç£å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒå¯¹äºçº¯encoderçš„XLMè·Ÿçº¯decoderçš„Mistral-7bçš„å½±å“ï¼Œå‘ç°ä¸åšé¢„è®­ç»ƒå¯¹äºMistral-7bå‡ ä¹æ²¡æœ‰å½±å“ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºè‡ªå›å½’é¢„è®­ç»ƒä»»åŠ¡å·²ç»è®©çº¯decoderçš„Mistral-7bå…·å¤‡è·å–é«˜è´¨é‡æ–‡æœ¬è¡¨å¾çš„èƒ½åŠ›ï¼Œæ‰€ä»¥åªè¦ç»è¿‡finetuneå°±å¯ä»¥ç§°ä¸ºå¼ºå¤§çš„text embeddingæ¨¡å‹äº†ã€‚
- ![](https://pic2.zhimg.com/80/v2-a46537a220eebe5d1fc6f37eee218f1d_1440w.webp)

#### E5-mistral-7b-instruct ä»£ç 

[E5-mistral-7b-instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct) ä»£ç 

Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.

```py
import torch
import torch.nn.functional as F

from torch import Tensor
from transformers import AutoTokenizer, AutoModel


def last_token_pool(last_hidden_states: Tensor,
                 attention_mask: Tensor) -> Tensor:
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]


def get_detailed_instruct(task_description: str, query: str) -> str:
    return f'Instruct: {task_description}\nQuery: {query}'


# Each query must come with a one-sentence instruction that describes the task
task = 'Given a web search query, retrieve relevant passages that answer the query'
queries = [
    get_detailed_instruct(task, 'how much protein should a female eat'),
    get_detailed_instruct(task, 'summit define')
]
# No need to add instruction for retrieval documents
documents = [
    "As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.",
    "Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments."
]
input_texts = queries + documents

tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-mistral-7b-instruct')
model = AutoModel.from_pretrained('intfloat/e5-mistral-7b-instruct')

max_length = 4096
# Tokenize the input texts
batch_dict = tokenizer(input_texts, max_length=max_length - 1, return_attention_mask=False, padding=False, truncation=True)
# append eos_token_id to every input_ids
batch_dict['input_ids'] = [input_ids + [tokenizer.eos_token_id] for input_ids in batch_dict['input_ids']]
batch_dict = tokenizer.pad(batch_dict, padding=True, return_attention_mask=True, return_tensors='pt')

outputs = model(**batch_dict)
embeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])

# normalize embeddings
embeddings = F.normalize(embeddings, p=2, dim=1)
scores = (embeddings[:2] @ embeddings[2:].T) * 100
print(scores.tolist())

```


## å‘é‡è¯„ä¼°

ChatGPTè®°å¿†æ¨¡å—æœç´¢ä¼˜åŒ–â€”â€”æ–‡æœ¬è¯­ä¹‰å‘é‡ç›¸ä¼¼M3Eæ¨¡å‹å¾®è°ƒå®æˆ˜

`è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡`(Semantic Textual Similarity)æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ä¸€ä¸ªåŸºç¡€ä»»åŠ¡,ç›®çš„æ˜¯è¯„ä¼°ä¸¤ä¸ªæ–‡æœ¬ç‰‡æ®µåœ¨è¯­ä¹‰ä¸Šçš„ç›¸ä¼¼ç¨‹åº¦ã€‚

ä¸»è¦æ€è·¯:
- å°†æ–‡æœ¬æ˜ å°„åˆ°è¯­ä¹‰å‘é‡ç©ºé—´,ä¹Ÿå°±æ˜¯å°†æ–‡æœ¬è½¬åŒ–ä¸ºå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºã€‚
- è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦,ä¾‹å¦‚ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
- ç›¸ä¼¼åº¦é«˜åˆ™è¡¨ç¤ºä¸¤ä¸ªæ–‡æœ¬åœ¨è¯­ä¹‰ä¸Šç›¸ä¼¼,ç›¸ä¼¼åº¦ä½åˆ™è¡¨ç¤ºè¯­ä¹‰ä¸åŒã€‚

éš¾ç‚¹åœ¨äºè·å¾—åˆé€‚çš„æ–‡æœ¬å‘é‡è¡¨ç¤º,éœ€è¦æ¨¡å‹èƒ½å¤Ÿæ•æ‰æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯,å¿½ç•¥è¯æ±‡è¡¨é¢çš„å·®å¼‚,æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­è¯­ä¹‰æ˜¯å¦ç›¸è¿‘ã€‚

æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¾ƒçš„ä¸»è¦ä¸‰ç§ç±»å‹ã€‚
- `s2s`, å³ sentence to **sentence** ï¼Œä»£è¡¨äº†åŒè´¨æ–‡æœ¬ä¹‹é—´çš„åµŒå…¥èƒ½åŠ›ï¼Œé€‚ç”¨ä»»åŠ¡ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œé‡å¤é—®é¢˜æ£€æµ‹ï¼Œæ–‡æœ¬åˆ†ç±»ç­‰
- `s2p`, å³ sentence to **passage** ï¼Œä»£è¡¨äº†å¼‚è´¨æ–‡æœ¬ä¹‹é—´çš„åµŒå…¥èƒ½åŠ›ï¼Œé€‚ç”¨ä»»åŠ¡ï¼šæ–‡æœ¬æ£€ç´¢ï¼ŒGPT è®°å¿†æ¨¡å—ç­‰
- `s2c`, å³ sentence to **code** ï¼Œä»£è¡¨äº†è‡ªç„¶è¯­è¨€å’Œç¨‹åºè¯­è¨€ä¹‹é—´çš„åµŒå…¥èƒ½åŠ›ï¼Œé€‚ç”¨ä»»åŠ¡ï¼šä»£ç æ£€ç´¢

è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹å‡ ä¸ªå…³é”®é—®é¢˜,åŒ…æ‹¬: æ•°æ®æ ¼å¼ã€åŸºç¡€æ¨¡å‹é€‰æ‹©ã€æ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºã€æŸå¤±å‡½æ•°è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ç­‰éƒ½æœ‰æ‰€ä¸åŒã€‚

### è¯„æµ‹æ•°æ®é›†

è¯­ä¹‰ç›¸ä¼¼åº¦ä»»åŠ¡æ•°æ®é›†åŒ…æ‹¬`STS-B`, `SICK` ç­‰ã€‚æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­éœ€è¦å¤§é‡è¯­ä¹‰ç›¸å…³çš„æ–‡æœ¬å¯¹æ„æˆ**ç›‘ç£æ•°æ®**,æŸå¤±å‡½æ•°åˆ™å¸¸é‡‡ç”¨`ä½™å¼¦ç›¸ä¼¼åº¦`ä¸æ ‡æ³¨ç›¸ä¼¼åº¦çš„å·®å¼‚ä½œä¸ºä¼˜åŒ–ç›®æ ‡ã€‚

è¯­ä¹‰ç›¸ä¼¼åº¦æ˜¯NLPç³»ç»Ÿä¸­é‡è¦æ¨¡å—,åº”ç”¨åŒ…æ‹¬: é—®ç­”åŒ¹é…ã€çŸ­æ–‡æœ¬èšç±»ã€è¯­ä¹‰æœç´¢ç­‰ç­‰ã€‚æé«˜è¯­ä¹‰ç›¸ä¼¼åº¦çš„å‡†ç¡®æ€§æ˜¯è‡ªç„¶è¯­è¨€ç†è§£çš„å…³é”®æ­¥éª¤ã€‚


#### æ•°æ®æ ¼å¼

ä¸‰ç§è®­ç»ƒæ ·æœ¬æ ¼å¼

ç¤ºä¾‹å¦‚ä¸‹:

1. `Pair` (å¥å­å¯¹)ï¼šæ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªæ­£ä¾‹å¥å­å¯¹ã€‚ç”¨äºå­¦ä¹ åŒºåˆ†æ­£ä¾‹å’Œè´Ÿä¾‹å¥å­ã€‚

```json
[{
    'text': 'I love apples',
    'positive_text': 'Apples are my favorite fruit'
},
{
    'text': 'I play football', 
    'positive_text': 'Football is an exciting sport'
}]
```

2. `Triplet` (ä¸‰å…ƒç»„) ï¼šæ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªæ­£ä¾‹å¥å­å’Œä¸€ä¸ªè´Ÿä¾‹å¥å­ã€‚å¯ä»¥æ›´æ˜ç¡®åœ°å­¦ä¹ åŒºåˆ†æ­£è´Ÿæ ·æœ¬ã€‚

```json
[{
    'text': 'I love apples',
    'positive_text': 'Apples are my favorite fruit',
    'negative_text': 'Bananas are too soft'
},
{
    'text': 'I play football',
    'positive_text': 'Football is an exciting sport',
    'negative_text': 'Basketball is also great'
}]
```

3. `Scored Pair` (æ‰“åˆ†å¥å­å¯¹)ï¼šæ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€å¯¹å¥å­å’Œå®ƒä»¬çš„ç›¸ä¼¼åº¦åˆ†æ•°ã€‚å¯ä»¥ç›´æ¥å­¦ä¹ è¯„ä¼°è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚

```json
[{
    'text': 'I love apples',
    'scored_text': 'Apples are my favorite fruit',
    'score': 0.9 
},
{
    'text': 'I play football',
    'scored_text': 'Football is an exciting sport',
    'score': 0.8
}]
```

ä¸‰è€…å¯¹æ¯”:
- Pair: åŒºåˆ†**æ­£è´Ÿ**æ ·æœ¬
- Triplet: æ›´**æ˜ç¡®**çš„æ­£è´Ÿæ ·æœ¬
- Scored Pair: ç›´æ¥å­¦ä¹ **è¯­ä¹‰ç›¸ä¼¼åº¦**

é€‚ç”¨äºä¸åŒçš„è®­ç»ƒç›®æ ‡ã€‚


### è¯„æµ‹æ¦œå•

å¦‚ä½•è¯„ä¼°ä¸€ä¸ªæ¨¡å‹çš„å¥½åï¼š[MTEB Leaderboard - a Hugging Face Space by mteb](https://link.zhihu.com/?target=https%3A//huggingface.co/spaces/mteb/leaderboard) æ˜¯é’ˆå¯¹**å¤§è§„æ¨¡æ–‡æœ¬è¡¨ç¤º**å­¦ä¹ æ–¹æ³•çš„ä¸€ä¸ª**è¯„æµ‹æ’è¡Œæ¦œ**ã€‚
- å°†æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹åœ¨å¤§é‡çš„è¯„æµ‹æ•°æ®é›†: æ–‡æœ¬**åˆ†ç±»**ï¼Œ**èšç±»**ï¼Œæ–‡æœ¬**æ’åº**ï¼Œæ–‡æœ¬**å¬å›**ç­‰å¤§é‡æ•°æ®é›†ä¸Šè¿›è¡Œè¯„æµ‹ï¼Œå¹¶ç»™å‡ºä¸€ä¸ªå¹³å‡çš„åˆ†æ•°ï¼Œæ¥è¯„ä¼°è¿™ä¸ªæ¨¡å‹æ–‡æœ¬embedingçš„èƒ½åŠ›ã€‚

- ![](https://pic3.zhimg.com/80/v2-ada5462440f11d5cebc4bb1cb4c99322_1440w.webp)
- BGEæ¨¡å‹æ•ˆæœç›®å‰æœ€å¼ºï¼Œè€Œ M3E ç´§éšå…¶åï¼Œä¸åŒæ¨¡å‹é‡‡å–äº†ä¸åŒè®­ç»ƒæ•°æ®ï¼Œè®­ç»ƒæ¨¡å‹å’Œè®­ç»ƒç­–ç•¥ã€‚largeçš„æ¨¡å‹æ•ˆæœæ›´å¥½

é™¤äº†ä»¥ä¸Šç»¼åˆæ¦œå•ï¼Œä¸åŒæ¨¡å‹åœ¨ä¸åŒç±»å‹ä»»åŠ¡ä¸‹çš„è¡¨ç°æœ‰åŒºåˆ«ã€‚





## å‘é‡æ£€ç´¢

å¤§è§„æ¨¡æƒ…å†µä¸‹ï¼Œå¦‚ä½•ä½å»¶è¿Ÿæ£€ç´¢æ–‡æ¡£ï¼Ÿ
- **è¿‘ä¼¼æœ€è¿‘é‚»**ï¼ˆANNï¼‰ç®—æ³•ã€‚

`è¯­ä¹‰åŒ¹é…`ä¼šç›´æ¥å†³å®šæ˜¯å¦èƒ½å¤Ÿæ£€ç´¢åˆ°æ­£ç¡®çš„çŸ¥è¯†

### å‘é‡æ£€ç´¢æŠ€æœ¯

ä¼˜åŒ–äº†æ£€ç´¢é€Ÿåº¦ï¼Œå¹¶è¿”å›è¿‘ä¼¼ï¼ˆè€Œéç²¾ç¡®ï¼‰çš„å‰ k ä¸ªæœ€ç›¸ä¼¼çš„é‚»å±…ï¼Œä»¥å°‘è®¸å‡†ç¡®åº¦æŸå¤±æ¢å–å¤§å¹…åŠ é€Ÿã€‚

**ANNåµŒå…¥ç´¢å¼•**æ˜¯ä¸€ç§æ•°æ®ç»“æ„ï¼Œé«˜æ•ˆåœ°è¿›è¡ŒANNæœç´¢ã€‚åœ¨åµŒå…¥ç©ºé—´ä¸Šæ„å»ºåˆ†åŒºï¼Œä»¥ä¾¿å¿«é€Ÿå®šä½æŸ¥è¯¢å‘é‡æ‰€åœ¨çš„ç‰¹å®šç©ºé—´ã€‚ä¸€äº›æŠ€æœ¯åŒ…æ‹¬ï¼š
-   `å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ`ï¼ˆ`LSH`ï¼‰ï¼šæ ¸å¿ƒæ€æƒ³æ˜¯åˆ›å»ºå“ˆå¸Œå‡½æ•°ï¼Œä½¿å¾—ç›¸ä¼¼çš„é¡¹æœ‰å¯èƒ½è½å…¥åŒä¸€ä¸ªå“ˆå¸Œæ¡¶ä¸­ã€‚é€šè¿‡åªéœ€è¦æ£€æŸ¥ç›¸å…³çš„æ¡¶ï¼Œæˆ‘ä»¬å¯ä»¥é«˜æ•ˆåœ°æ‰§è¡Œæœ€è¿‘é‚»æŸ¥è¯¢ã€‚
-   Facebook AIç›¸ä¼¼æ€§æœç´¢ï¼ˆ`FAISS`ï¼‰ï¼šå®ƒä½¿ç”¨é‡åŒ–å’Œç´¢å¼•çš„ç»„åˆæ¥å®ç°é«˜æ•ˆçš„æ£€ç´¢ï¼Œæ”¯æŒCPUå’ŒGPUï¼Œå¹¶ä¸”ç”±äºå…¶å¯¹å†…å­˜çš„é«˜æ•ˆåˆ©ç”¨ï¼Œå¯ä»¥å¤„ç†æ•°åäº¿ä¸ªå‘é‡ã€‚
-   **åˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œ**ï¼ˆ`HNSW`ï¼‰ï¼šå—åˆ°â€œå…­åº¦åˆ†éš”â€çš„å¯å‘ï¼Œå®ƒæ„å»ºäº†ä¸€ä¸ªä½“ç°å°ä¸–ç•Œç°è±¡çš„åˆ†å±‚å›¾ç»“æ„ã€‚åœ¨è¿™é‡Œï¼Œå¤§å¤šæ•°èŠ‚ç‚¹å¯ä»¥é€šè¿‡æœ€å°‘çš„è·³æ•°ä»ä»»ä½•å…¶ä»–èŠ‚ç‚¹åˆ°è¾¾ã€‚è¿™ç§ç»“æ„å…è®¸HNSWä»æ›´å¹¿æ³›ã€æ›´ç²—ç³™çš„è¿‘ä¼¼å¼€å§‹æŸ¥è¯¢ï¼Œå¹¶é€æ¸åœ¨è¾ƒä½å±‚æ¬¡ä¸Šç¼©å°æœç´¢èŒƒå›´ã€‚
-   **å¯æ‰©å±•æœ€è¿‘é‚»å±…**ï¼ˆ`ScaNN`ï¼‰ï¼šANNé€šè¿‡ä¸¤ä¸ªæ­¥éª¤å®Œæˆã€‚
  - é¦–å…ˆï¼Œç²—ç²’åº¦é‡åŒ–å‡å°‘äº†æœç´¢ç©ºé—´ã€‚
  - ç„¶åï¼Œåœ¨å‡å°‘çš„é›†åˆå†…è¿›è¡Œç»†ç²’åº¦æœç´¢ã€‚è¿™æ˜¯æˆ‘è§è¿‡çš„æœ€ä½³å¬å›ç‡/å»¶è¿Ÿæƒè¡¡ã€‚

å±€éƒ¨æ•æ„Ÿæ•£åˆ—ï¼ˆLSHï¼‰æ˜¯ä¸€ç»„æ–¹æ³•ï¼Œç”¨äºå°†æ•°æ®å‘é‡è½¬æ¢æˆæ•£åˆ—å€¼ï¼ŒåŒæ—¶ä¿ç•™å…¶ç›¸ä¼¼æ€§ä¿¡æ¯ï¼Œä»è€Œç¼©å°æœç´¢èŒƒå›´ã€‚

ä¼ ç»Ÿæ–¹æ³•åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼š
- çŸ¢é‡åŒ–ï¼šå°†åŸå§‹æ–‡æœ¬ç¼–ç æˆçŸ¢é‡ã€‚
  - K-gram æ˜¯ç”± k ä¸ªè¿ç»­çš„æ ‡è®°è¯ç»„æˆçš„è¯ç»„ã€‚æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œæ ‡è®°å¯ä»¥æ˜¯å•è¯æˆ–ç¬¦å·ã€‚åˆ‡åˆ†çš„æœ€ç»ˆç›®çš„æ˜¯ä½¿ç”¨æ”¶é›†åˆ°çš„ k-gram å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œç¼–ç ã€‚
  - å¥å­ "learning datascience is fascinating "æ”¶é›†é•¿åº¦ä¸º k = 3 çš„å”¯ä¸€å­—ç¬¦ä¸²
- MinHashingï¼šå°†å‘é‡è½¬æ¢æˆä¸€ç§ç§°ä¸ºç­¾åçš„ç‰¹æ®Šè¡¨ç¤ºï¼Œå¯ç”¨äºæ¯”è¾ƒå®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
  - å‘é‡çš„ç›¸ä¼¼æ€§å¯ä»¥é€šè¿‡`é›…å¡æŒ‡æ•°`(ä¸¤ä¸ªé›†åˆçš„äº¤é›†)è¿›è¡Œæ¯”è¾ƒã€‚è¯·è®°ä½ï¼Œä¸¤ä¸ªé›†åˆçš„é›…å¡æŒ‡æ•°å®šä¹‰ä¸ºä¸¤ä¸ªé›†åˆä¸­å…±åŒå…ƒç´ çš„æ•°é‡é™¤ä»¥æ‰€æœ‰å…ƒç´ çš„é•¿åº¦ã€‚
  - ç¼–ç å‘é‡çš„ç¨€ç–æ€§: è®¡ç®—ä¸¤ä¸ªå•å‡»ç¼–ç å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦å¾—åˆ†å°†è€—è´¹å¤§é‡æ—¶é—´ã€‚å¦‚æœè½¬æ¢ä¸ºå¯†é›†æ ¼å¼ä¼šæ›´æœ‰æ•ˆç‡ã€‚æ˜¯è®¾è®¡è¿™æ ·ä¸€ä¸ªå‡½æ•°ï¼Œå°†è¿™äº›å‘é‡è½¬æ¢åˆ°ä¸€ä¸ªè¾ƒå°çš„ç»´åº¦ï¼Œå¹¶ä¿ç•™ç›¸ä¼¼æ€§ä¿¡æ¯ã€‚æ„å»ºè¿™ç§å‡½æ•°çš„æ–¹æ³•å«åš MinHashingã€‚
  - MinHashing æ˜¯ä¸€ä¸ªå“ˆå¸Œå‡½æ•°ï¼Œå¯¹è¾“å…¥å‘é‡çš„åˆ†é‡è¿›è¡Œæ’åˆ—ï¼Œç„¶åè¿”å›æ’åˆ—åçš„å‘é‡åˆ†é‡ç­‰äº 1 çš„ç¬¬ä¸€ä¸ªç´¢å¼•, [img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/83691a41b01b4055878f12ebed739141~tplv-tt-origin-asy1:5aS05p2hQOmXu-aVsOi1t-iIng==.image?_iz=58558&from=article.pc_detail&x-expires=1693980150&x-signature=KWF4IRvOK7clmV7XXfm9tVI0sn8%3D)
- LSH åŠŸèƒ½ï¼šå°†ç­¾åå—æ•£åˆ—åˆ°ä¸åŒçš„æ¡¶ä¸­ã€‚å¦‚æœä¸€å¯¹å‘é‡çš„ç­¾åè‡³å°‘æœ‰ä¸€æ¬¡è½åœ¨åŒä¸€ä¸ªæ¡¶ä¸­ï¼Œå®ƒä»¬å°±ä¼šè¢«è§†ä¸ºå€™é€‰è€…ã€‚
  - åŸå§‹æ–‡æœ¬è½¬åŒ–ä¸ºä¿ç•™ç›¸ä¼¼æ€§ä¿¡æ¯çš„ç­‰é•¿å¯†é›†ç­¾åã€‚ç„¶è€Œï¼Œè¿™ç§å¯†é›†ç­¾åé€šå¸¸ä»ç„¶å…·æœ‰å¾ˆé«˜çš„ç»´åº¦ï¼Œç›´æ¥æ¯”è¾ƒ,æ•ˆç‡å¾ˆä½ã€‚å»ºç«‹ä¸€ä¸ªå“ˆå¸Œè¡¨æ¥åŠ é€Ÿæœç´¢æ€§èƒ½ï¼Œä½†å³ä½¿ä¸¤ä¸ªç­¾åéå¸¸ç›¸ä¼¼ï¼Œåªæœ‰ä¸€ä¸ªä½ç½®ä¸åŒï¼Œå“ˆå¸Œå€¼ä»æœ‰å¯èƒ½ä¸åŒï¼ˆå› ä¸ºå‘é‡ä½™æ•°å¯èƒ½ä¸åŒï¼‰ã€‚ä¸è¿‡é€šå¸¸å¸Œæœ›å®ƒä»¬å½’å…¥åŒä¸€ä¸ªæ¡¶ä¸­ã€‚
  - LSH æœºåˆ¶ä¼šå»ºç«‹ä¸€ä¸ªç”±å¤šä¸ªéƒ¨åˆ†ç»„æˆçš„å“ˆå¸Œè¡¨ï¼Œå¦‚æœä¸€å¯¹ç­¾åè‡³å°‘æœ‰ä¸€ä¸ªç›¸åº”çš„éƒ¨åˆ†ï¼Œå°±ä¼šè¢«æ”¾å…¥åŒä¸€ä¸ªæ¡¶ä¸­ã€‚
- [å‚è€ƒ](https://www.toutiao.com/article/7264973689832096313/)

åœ¨è¯„ä¼°ANNæŒ‡æ•°æ—¶ï¼Œéœ€è¦è€ƒè™‘ä¸€äº›å› ç´ ï¼ŒåŒ…æ‹¬ï¼š
- **å›å¿†**ï¼šå®ƒåœ¨ä¸ç²¾ç¡®æœ€è¿‘é‚»çš„æ¯”è¾ƒä¸­è¡¨ç°å¦‚ä½•ï¼Ÿ    
- å»¶è¿Ÿ/ååé‡ï¼šæ¯ç§’å¯ä»¥å¤„ç†å¤šå°‘ä¸ªæŸ¥è¯¢ï¼Ÿ    
- **å†…å­˜**å ç”¨ï¼šä¸ºäº†æä¾›ç´¢å¼•éœ€è¦å¤šå°‘RAMï¼Ÿ    
- æ·»åŠ æ–°é¡¹ç›®çš„ä¾¿åˆ©æ€§ï¼šæ˜¯å¦å¯ä»¥åœ¨ä¸é‡æ–°ç´¢å¼•æ‰€æœ‰æ–‡æ¡£ï¼ˆLSHï¼‰æˆ–éœ€è¦é‡å»ºç´¢å¼•ï¼ˆScaNNï¼‰çš„æƒ…å†µä¸‹æ·»åŠ æ–°é¡¹ç›®ï¼Ÿ    

æ²¡æœ‰æœ€å¥½ï¼Œåªæœ‰æ›´åˆé€‚ã€‚è¿›è¡ŒåŸºå‡†æµ‹è¯•ä¹‹å‰ï¼Œé¦–å…ˆå®šä¹‰åŠŸèƒ½å’ŒéåŠŸèƒ½éœ€æ±‚ã€‚
- ScaNNåœ¨å¬å›ç‡å’Œå»¶è¿Ÿä¹‹é—´çš„æƒè¡¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼ˆè¯·å‚è§æ­¤å¤„çš„åŸºå‡†æµ‹è¯•å›¾è¡¨ï¼‰ã€‚

è¯¦è§
- [Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/)
- [è¯‘æ–‡](https://mp.weixin.qq.com/s/XVH5sCSyGccKt9K8nvkzdA)

### ç´¢å¼•ç»“æ„åˆ’åˆ†

ã€2023-8-25ã€‘æŒ‰ç´¢å¼•ç»“æ„åˆ’åˆ†
- æ ‘ç»“æ„ç´¢å¼•: 
- å›¾ç»“æ„ç´¢å¼•: 
- åˆ†å±‚èšç±»ç´¢å¼•: 
- å‘é‡é‡åŒ–ç´¢å¼•: 

| ç´¢å¼•ç»“æ„ç±»å‹ | åŸç† | é€‚ç”¨åœºæ™¯ | ç¤ºä¾‹ |
| --- | --- | --- | ---  |  
| **æ ‘ç»“æ„**ç´¢å¼•   |   -  | ä½ç»´ç©ºé—´ç›¸ä¼¼æ€§æœç´¢<br>ä¸é€‚åˆé«˜ç»´ç©ºé—´,å†…å­˜å ç”¨å¤§,æœç´¢æ…¢ | KD-Tree, BallTree  |   
| **å›¾ç»“æ„**ç´¢å¼•   |   -  | é«˜ç»´ç©ºé—´å®æ—¶ç›¸ä¼¼æ€§æœç´¢,å†…å­˜å ç”¨å°ï¼Œé€Ÿåº¦å¿« | HNSW  |   
| **åˆ†å±‚èšç±»**ç´¢å¼• |   -  | é«˜ç»´ç©ºé—´ç›¸ä¼¼æ€§æœç´¢,æ€§èƒ½å’Œå†…å­˜å¹³è¡¡ | Annoy  |    
| **å‘é‡é‡åŒ–**ç´¢å¼• |   -  | å¤§è§„æ¨¡é«˜ç»´ç©ºé—´ç›¸ä¼¼æ€§æœç´¢ï¼Œapiä¸°å¯Œ | Faiss (IVF,IVFPQ)  |    


### ç›¸ä¼¼æ€§æœç´¢

ç›¸ä¼¼æ€§æœç´¢ç»å¸¸å‡ºç°åœ¨ NLP é¢†åŸŸã€æœç´¢å¼•æ“æˆ–æ¨èç³»ç»Ÿä¸­ï¼Œåœ¨è¿™äº›é¢†åŸŸä¸­ï¼Œéœ€è¦ä¸ºæŸ¥è¯¢æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£æˆ–é¡¹ç›®ã€‚

## å‘é‡æ•°æ®åº“

å‘é‡æ•°æ®åº“ï¼ˆVectorstoresï¼‰ç”¨äºå­˜å‚¨æ–‡æœ¬ã€å›¾åƒç­‰ç¼–ç åçš„**ç‰¹å¾å‘é‡**ï¼Œæ”¯æŒå‘é‡ç›¸ä¼¼åº¦æŸ¥è¯¢ä¸åˆ†æã€‚
- æ–‡æœ¬è¯­ä¹‰æ£€ç´¢ï¼Œé€šè¿‡æ¯”è¾ƒè¾“å…¥æ–‡æœ¬çš„ç‰¹å¾å‘é‡ä¸åº•åº“æ–‡æœ¬ç‰¹å¾å‘é‡çš„ç›¸ä¼¼æ€§ï¼Œä»è€Œæ£€ç´¢ç›®æ ‡æ–‡æœ¬ï¼Œå³åˆ©ç”¨äº†å‘é‡æ•°æ®åº“ä¸­çš„ç›¸ä¼¼åº¦æŸ¥è¯¢ï¼ˆä½™å¼¦è·ç¦»ã€æ¬§å¼è·ç¦»ç­‰ï¼‰ã€‚

å› ä¸ºæ•°æ®ç›¸å…³æ€§æœç´¢å…¶å®æ˜¯å‘é‡è¿ç®—ã€‚æ‰€ä»¥ï¼Œä¸ç®¡ä½¿ç”¨ openai api embedding åŠŸèƒ½è¿˜æ˜¯ç›´æ¥é€šè¿‡ å‘é‡æ•°æ®åº“ ç›´æ¥æŸ¥è¯¢ï¼Œéƒ½éœ€è¦å°†åŠ è½½è¿›æ¥çš„æ•°æ® Document è¿›è¡Œ**å‘é‡åŒ–**ï¼Œæ‰èƒ½è¿›è¡Œå‘é‡è¿ç®—æœç´¢ã€‚

è½¬æ¢æˆå‘é‡ä¹Ÿå¾ˆç®€å•ï¼Œåªéœ€è¦æŠŠæ•°æ®å­˜å‚¨åˆ°å¯¹åº”çš„å‘é‡æ•°æ®åº“ä¸­å³å¯å®Œæˆå‘é‡çš„è½¬æ¢ã€‚

å®˜æ–¹ä¹Ÿæä¾›äº†å¾ˆå¤šçš„å‘é‡æ•°æ®åº“ä¾›æˆ‘ä»¬ä½¿ç”¨ï¼ŒåŒ…æ‹¬ï¼š
- Annoy
- Chroma
- ElasticSearch
- FAISS
- Milvus
- PGVector
- Pinecone
- Redis

ä»£è¡¨æ€§æ•°æ®åº“
- Chromaã€Pineconeã€Qdrand

æ›´å¤šæ”¯æŒçš„å‘é‡æ•°æ®åº“ä½¿ç”¨æ–¹æ³•ï¼Œå¯è½¬è‡³é“¾æ¥ã€‚

ã€2023-12-7ã€‘[ä¸»æµæ•°æ®åº“ä¸€è§ˆ](https://zhuanlan.zhihu.com/p/628148081)

å‡ ä¸ªä¸»æµçš„å‘é‡æ•°æ®åº“ä¸­
- Milvus åœ¨å¤§è§„æ¨¡ã€æ£€ç´¢æ€§èƒ½ã€ç¤¾åŒºå½±å“åŠ›ç­‰æ–¹é¢éƒ½å…·å¤‡**ç»å¯¹ä¼˜åŠ¿**ï¼Œå…¶åˆ†å¸ƒå¼æ¶æ„ä¹Ÿæ›´Matchä¸‹ä¸€ä»£å­˜å‚¨çš„ç†å¿µã€‚
- Weaviate åœ¨ä½¿ç”¨æ¡ˆä¾‹ä¸Šï¼Œæœ‰å¾ˆå¤šç°æˆçš„ä¾‹å­ï¼Œè·Ÿå½“å‰GPTå‰æ²¿çƒ­é—¨é¡¹ç›®è´´åˆæ¯”è¾ƒç´§ç§˜ï¼Œä½†åœ¨å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä½¿ç”¨åœºæ™¯ä¸­ï¼Œè¿˜éœ€è¦æ¥å—è€ƒéªŒã€‚
- Chroma æ˜¯ä¸€ä¸ªå¾ˆè½»é‡çº§çš„æ•°æ®åº“ï¼Œåº•å±‚ä½¿ç”¨äº†clickhouseã€duckdbç­‰å­˜å‚¨å¼•æ“

| æ•°æ®åº“åç§° | æ˜¯å¦å¼€æº	| ç¤¾åŒºå½±å“åŠ›  | ç¼–ç¨‹è¯­è¨€	| æ ¸å¿ƒç‰¹æ€§	| é€‚ç”¨åœºæ™¯ |
| --- | ---	| ---  | --- | ---	| --- |
| `Pinecone`	| å¦	|	| æœªçŸ¥	| å‘é‡å­˜å‚¨ä¸æ£€ç´¢ã€å…¨æ‰˜ç®¡ | Saasç±»ä¸šåŠ¡åœºæ™¯ |
| `weaviate`	| æ˜¯	| 5.3k star |	Go |  åŒæ—¶æ”¯æŒå‘é‡ä¸å¯¹è±¡çš„å­˜å‚¨ã€æ”¯æŒå‘é‡æ£€ç´¢ä¸ç»“æ„åŒ–è¿‡æ»¤ã€å…·å¤‡ä¸»æµæ¨¡å¼æˆç†Ÿçš„ä½¿ç”¨æ¡ˆä¾‹ã€‚é«˜é€Ÿã€çµæ´»ï¼Œä¸ä»…ä»…å…·å¤‡å‘é‡æ£€ç´¢ï¼Œè¿˜ä¼šæ”¯æŒæ¨èã€æ€»ç»“ç­‰èƒ½åŠ›	||
| `qdrant`	| æ˜¯	| 6.3k star	| Rust	| å‘é‡å­˜å‚¨ä¸æ£€ç´¢ã€äº‘åŸç”Ÿã€åˆ†å¸ƒå¼ã€æ”¯æŒè¿‡æ»¤ã€ä¸°å¯Œçš„æ•°æ®ç±»å‹ã€WALæ—¥å¿—å†™å…¥	||
| `milvus`	| æ˜¯	| 17.7k star |	Go	| æé«˜çš„æ£€ç´¢æ€§èƒ½: ä¸‡äº¿çŸ¢é‡æ•°æ®é›†çš„æ¯«ç§’çº§æœç´¢éç»“æ„åŒ–æ•°æ®çš„æç®€ç®¡ç†ä¸°å¯Œçš„APIè·¨å¹³å°å®æ—¶æœç´¢å’Œåˆ†æå¯é ï¼šå…·æœ‰å¾ˆé«˜çš„å®¹ç¾ä¸æ•…éšœè½¬ç§»èƒ½åŠ›é«˜åº¦å¯æ‹“å±•ä¸å¼¹æ€§æ”¯æŒæ··åˆæ£€ç´¢ç»Ÿä¸€çš„Lambdaæ¶æ„ç¤¾åŒºæ”¯æŒã€è¡Œä¸šè®¤å¯	||
| `Chroma`	| æ˜¯	| 4.1k star	| python	| è½»é‡ã€å†…å­˜çº§	 ||


### Redis

Redis é€šè¿‡ RedisSearch æ¨¡å—ï¼Œä¹ŸåŸç”Ÿæ”¯æŒå‘é‡æ£€ç´¢ã€‚ 

RedisSearch æ˜¯ä¸€ä¸ªRedisæ¨¡å—ï¼Œæä¾›äº†æŸ¥è¯¢ã€äºŒçº§ç´¢å¼•ï¼Œå…¨æ–‡æ£€ç´¢ä»¥åŠå‘é‡æ£€ç´¢ç­‰èƒ½åŠ›ã€‚

å¦‚æœè¦ä½¿ç”¨RedisSearchï¼Œéœ€è¦é¦–å…ˆåœ¨Redisæ•°æ®ä¸Šå£°æ˜ç´¢å¼•


### Faiss


```py
from langchain.vectorstores import FAISS

db = FAISS.from_documents(split_docs, embeddings)
db.save_local("./faiss/news_test")
# åŠ è½½æŒä¹…åŒ–å‘é‡
db = FAISS.load_local("./faiss/news_test",embeddings=embeddings)
# ====== æˆ– ========
vs_path = "./vector_store"
if vs_path and os.path.isdir(vs_path):
    vector_store = FAISS.load_local(vs_path, embeddings)
    vector_store.add_documents(docs)
else:
    if not vs_path:
        vs_path = f"""{VS_ROOT_PATH}{os.path.splitext(file)[0]}_FAISS_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}"""
    vector_store = FAISS.from_documents(docs, embeddings)

vector_store.save_local(vs_path)
docs = vector_store.similarity_search(query)
docs_and_scores = vector_store.similarity_search_with_score(query)
```


### Milvus

Milvus: é¢å‘ä¸‹ä¸€ä»£çš„ç”Ÿæˆå¼AIå‘é‡æ•°æ®åº“ï¼Œæ”¯æŒäº‘åŸç”Ÿ

Zilliz, æ„å»ºåœ¨Milvusä¹‹ä¸Šçš„æ•°æ®åŸºç¡€è®¾æ–½

```py
vector_db = Milvus.from_documents(
    docs,
    embeddings,
    connection_args={"host": "127.0.0.1", "port": "19530"},
)
docs = vector_db.similarity_search(query)
docs[0]
```

### Qdrant

Qdrant æ˜¯é¢å‘ä¸‹ä¸€ä»£çš„ç”Ÿæˆå¼AIå‘é‡æ•°æ®åº“ï¼ŒåŒæ—¶ä¹Ÿå…·å¤‡äº‘åŸç”Ÿçš„ç‰¹æ€§


### Chroma

Chromaæ˜¯ä¸€ä¸ªå¾ˆè½»é‡çº§çš„æ•°æ®åº“ï¼Œåº•å±‚ä½¿ç”¨äº†clickhouseã€duckdbç­‰å­˜å‚¨å¼•æ“

Chroma: ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“ã€‚ å¯ä»¥å¿«é€ŸåŸºäºPythonå’ŒJavaScriptæ„å»ºå†…å­˜çº§LLMåº”ç”¨

Chroma æ¯”è¾ƒè½»é‡ï¼Œç›´æ¥å®‰è£…åº“

```py
from langchain.vectorstores import Chroma
# åˆå§‹åŒ–åŠ è½½å™¨
db = Chroma.from_documents(split_docs, embeddings,persist_directory="./chroma/openai/news_test")
# æŒä¹…åŒ–
db.persist()
# æŒä¹…åŒ–åï¼Œå¯ä»¥ç›´æ¥é€‰æ‹©ä»æŒä¹…åŒ–æ–‡ä»¶ä¸­åŠ è½½ï¼Œä¸éœ€è¦å†é‡æ–°å°±å¯ä½¿ç”¨äº†
db = Chroma(persist_directory="./chroma/news_test", embedding_function=embeddings)
```

```py
# pip install chromadb
# åŠ è½½ç´¢å¼•
from langchain.vectorstores import Chroma
vectordb = Chroma(persist_directory="./vector_store", embedding_function=embeddings)
# å‘é‡ç›¸ä¼¼åº¦è®¡ç®—
query = "æœªå…¥èŒåŒäº‹å¯ä»¥å‡ºå·®å—"
docs = vectordb.similarity_search(query)
docs2 = vectordb.similarity_search_with_score(query)
print(docs[0].page_content)
# åœ¨æ£€ç´¢å™¨æ¥å£ä¸­å…¬å¼€è¯¥ç´¢å¼•
retriever = vectordb.as_retriever(search_type="mmr")
docs = retriever.get_relevant_documents(query)[0]
print(docs.page_content)
```

### Pinecone

[Pinecone](https://www.pinecone.io/)æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå·¥ç¨‹å¸ˆä¸å¼€å‘è€…è®¾è®¡çš„å‘é‡æ•°æ®åº“ã€‚ 

ä½œä¸ºä¸€ä¸ªå…¨æ‰˜ç®¡çš„æœåŠ¡ï¼Œå®ƒå‡è½»äº†å·¥ç¨‹å¸ˆä»¥åŠè¿ç»´äººå‘˜çš„è´Ÿæ‹…ï¼Œä½¿å¾—å®¢æˆ·å¯ä»¥èšç„¦äºæ•°æ®å†…åœ¨ä»·å€¼çš„æŠ½å–ã€‚ 
- å…è´¹ç‰ˆå¯ä»¥æ”¯æŒ500wçš„å‘é‡å­˜å‚¨ï¼Œå…¶ç”¨æ³•ç®€å•ï¼Œä»·æ ¼ä½å»‰ï¼Œå¯ä»¥å¿«é€Ÿæ”¯æŒå‘é‡æ£€ç´¢ä¸šåŠ¡çš„éªŒè¯ä¸å°è¯•ã€‚
- åœ¨ç‰¹æ€§ä¸ŠPineconeä¹Ÿå…·æœ‰é«˜é€Ÿã€å‡†ç¡®ä»¥åŠå¯æ‹“å±•ç­‰ç‰¹æ€§ï¼Œæ­¤å¤–ä¹Ÿå…·å¤‡å¯¹å•çº§å…ƒæ•°æ®è¿‡æ»¤å’Œå°–ç«¯ç¨€ç–-å¯†é›†ç´¢å¼•ç­‰é«˜çº§åŠŸèƒ½ã€‚

ä½¿ç”¨æ¡ˆä¾‹

| åç§°	| æè¿° |
|---|---|
| GPT-4 Retrieval Augmentation	| å¦‚ä½•é€šè¿‡æ£€ç´¢å¢å¼ºæ¥å¢å¼ºGPT4çš„èƒ½åŠ› |
| Generative Question-Answering	| ç”Ÿæˆå¼é—®ç­” |
| Semantic Search	| è¯­ä¹‰æœç´¢ï¼Œæ„å»ºä¸€ä¸ªç®€å•çš„è¯­ä¹‰æœç´¢ |


```py
import pinecone 

# Connecting to Pinecone
pinecone.deinit()
pinecone.init(
    api_key="YOUR_API_KEY",  # find at app.pinecone.io
    environment="YOUR_ENV"  # next to api key in console
)

# similarity_search
docsearch = Pinecone.from_documents(docs, embeddings, index_name="langchain-demo")
docs = docsearch.similarity_search(query)
print(docs[0].page_content)

# Create a Pinecone Service
pinecone_service = pinecone.Service()

# Create an Embedding Model
from langchain.embeddings.openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()

# Create a Vectorstore
from langchain.vectorstores import Chroma
vectorstore = Chroma(embeddings, pinecone_service)

# Upload documents to Pinecone Vectorstore
from langchain.vectorstores import Chroma
docsearch = Chroma.from_documents(texts, embeddings, collection_name="collection_name")
```

### PGVector

```py
from langchain.vectorstores.pgvector import PGVector
import os
CONNECTION_STRING = PGVector.connection_string_from_db_params(
    driver=os.environ.get("PGVECTOR_DRIVER", "psycopg2"),
    host=os.environ.get("PGVECTOR_HOST", "localhost"),
    port=int(os.environ.get("PGVECTOR_PORT", "5432")),
    database=os.environ.get("PGVECTOR_DATABASE", "postgres"),
    user=os.environ.get("PGVECTOR_USER", "postgres"),
    password=os.environ.get("PGVECTOR_PASSWORD", "postgres"),
)
db = PGVector.from_documents(
    embedding=embeddings,
    documents=docs,
    collection_name="state_of_the_union",
    connection_string=CONNECTION_STRING,
)

query = "What did the president say about Ketanji Brown Jackson"
docs_with_score: List[Tuple[Document, float]] = db.similarity_search_with_score(query)
for doc, score in docs_with_score:
    print("-" * 80)
    print("Score: ", score)
    print(doc.page_content)
    print("-" * 80)
```


### Weaviate

[Weaviate](https://weaviate.io/) æ˜¯ä¸€ä¸ªå¼€æºçš„çŸ¢é‡æ•°æ®åº“ï¼Œå…·æœ‰å¥å£®ã€å¯æ‹“å±•ã€äº‘åŸç”Ÿä»¥åŠå¿«é€Ÿç­‰ç‰¹æ€§

Weaviate æ˜¯ä¸€ä¸ªå¼€æºçš„å‘é‡æ•°æ®åº“ï¼Œå¯ä»¥å­˜å‚¨å¯¹è±¡ã€å‘é‡ï¼Œæ”¯æŒå°†çŸ¢é‡æœç´¢ä¸ç»“æ„åŒ–è¿‡æ»¤ä¸äº‘åŸç”Ÿæ•°æ®åº“å®¹é”™å’Œå¯æ‹“å±•æ€§ç­‰èƒ½åŠ›ç›¸ç»“åˆã€‚ 
- æ”¯æŒGraphQLã€RESTå’Œå„ç§è¯­è¨€çš„å®¢æˆ·ç«¯è®¿é—®ã€‚ 
- github[åœ°å€](https://github.com/weaviate/weaviate)

Weaviateåœ¨ä½¿ç”¨æ¡ˆä¾‹ä¸Šï¼Œæœ‰å¾ˆå¤šç°æˆçš„ä¾‹å­ï¼Œè·Ÿå½“å‰GPTå‰æ²¿çƒ­é—¨é¡¹ç›®è´´åˆæ¯”è¾ƒç´§ç§˜ï¼Œä½†åœ¨å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒä½¿ç”¨åœºæ™¯ä¸­ï¼Œè¿˜éœ€è¦æ¥å—è€ƒéªŒã€‚

# ç»“æŸ