---
layout: post
title:  全模态大模型专题
date:   2025-08-08 16:52:00
categories: 大模型
tags: qwen 多模态 阿里 蚂蚁
excerpt: 全模态大模型原理及各种实现
mathjax: true
permalink: /all_modality
---

* content
{:toc}


# 全模态大模型 

多模态大模型专题见站内：[多模态大模型案例](multimodal_case)

## 背景

现有技术
- 传统语音理解大模型的人机交互场景里，一般用 ASR（Automatic Speech Recognition，自动语音识别）把语音转文本，随后交给大语言模型处理，最终借助 TTS（Text-to-Speech，语音合成）转语音反馈给用户。
- 而视频理解模型是基于图片、视频进行大模型理解，并以文字形式输出反馈。

这两种模型均属于**相互独立**的单链路模型。


## 案例

### Google Gemini 1.5 Pro

【2024-3-11】 Google Gemini 团队开发多模态混合专家模型Gemini 1.5 Pro，标志着人工智能领域的一次重大进步。

注意
> MoE 架构下多模态理解能力，不含音频生成

该模型能够回忆和推理数百万个令牌（tokens）的上下文中的细粒度信息，包括多个长文档、数小时的视频和音频内容。它在跨模态的长上下文检索任务中实现了接近完美的召回率，在长文档问答、长视频问答和长上下文自动语音识别（ASR）等方面提高了现有的最佳性能，达到或超越了 Gemini 1.0 Ultra 在广泛基准测试中的领先性能。

技术细节

- 稀疏激活: 通过学习路由功能，MoE模型只激活（即使用）对于给定输入最相关的一部分参数，从而在大规模模型中保持高效计算。
- 参数规模: Gemini 1.5 Pro 的总参数数量极大，达到了多亿至数十亿的规模，但由于其稀疏激活特性，每次前向传播过程中只有一小部分参数被激活，这使得模型即便在参数规模巨大的情况下仍保持高效运行。
- 多模态输入处理: 该模型能够处理来自不同模态（文本、图像、视频和音频）的输入数据，并能够在这些不同类型的数据之间建立联系，进行综合理解和推理。

能力:
- 长上下文处理能力: Gemini 1.5 Pro 能够处理高达至少1000万个令牌的极长上下文，这是现有大型语言模型所不具备的。这使得模型可以处理整个文档集合、多小时的视频和近五天长的音频。
- 跨模态理解: 该模型不仅能处理文本，还能理解和处理视频与音频信息，实现跨模态的信息融合和推理。
- 近乎完美的信息检索: 在各种模态上，Gemini 1.5 Pro 都能实现超过99%的信息检索召回率，即使是在包含1000万令牌的海量信息中也能准确找到所需数据。
- 学习新语言的能力: 给定语法手册，Gemini 1.5 Pro 能够学习翻译拥有不到200名说话者的罕见语言，表现出与通过同样材料学习的人类相似的翻译能力。
- 优化的模型架构: Gemini 1.5 Pro 采用稀疏混合专家（MoE）的 Transformer 基础模型，实现了在大幅降低训练计算资源需求的同时，保持或超越前代模型的性能。
- 自适应学习和推理: 模型能够基于输入数据的特性动态调整其内部路由和激活的“专家”网络，从而针对不同的任务和数据类型自适应地优化其性能。Gemini 1.5 Pro 在保持高性能的同时，显著降低了资源消耗

### 阿里云

【2025-8-29】阿里云（小云）实时交互形态
- AI电话通话：电话呼出、电话呼入
  - 电话呼出: 填入自己的号码, 默认选择智能打断、多种音色可选
  - 电话呼入: 拨打号码 02566040232 即可
- 智能体通话：消息对话、语音通话、视觉理解通话、数字人通话、视频通话

体验
- [APP](https://alivc-demo-cms.alicdn.com/versionProduct/installPackage/AUI_Kits/AITalk/installPackage-AITalk.html)
- [Web](https://video.aliyuncs.com/aicall/desktop)


### 【2025-3-27】阿里 Qwen2.5-Omni


【2025-3-27】阿里巴巴发布 Qwen2.5-Omni，全球首个**端到端全模态**大模型，为多模态信息流**实时交互**提供了新技术框架。

Qwen2.5-Omni 整合了文本、图像、音频和视频的跨模态理解能力，实现**流式**文本与自然语音的双向同步生成。
- ![](https://picx.zhimg.com/v2-7df9b0935ac5619cf348f1939ce9f04d_1440w.jpg)

Qwen2.5-Omni 在保持全能的同时，并没有牺牲在各个垂直领域的能力

资料：
- [体验 Qwen Chat 新功能](https://chat.qwenlm.ai)
- [Qwen2.5-Omni技术报告](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/assets/Qwen2.5_Omni.pdf)
- 代码 Code: [Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni)
- 中文介绍: [Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/README_CN.md)
- 视频介绍: [Video](https://www.youtube.com/watch?v=UF55yM67EH0)


Qwen2.5-Omni 和 VL 区别
- 🔸Qwen2.5-Omni：能听懂、看懂、读懂你，还能实时语音回应你的全能选手
- 🔸Qwen2.5-VL：专注于图像解析、内容识别、视觉逻辑推导的视觉语言专家



#### 特点

特点
- Omni 和 架构：Thinker-Talker 架构，端到端的多模态模型，感知不同的模态，包括文本、图像、音频和视频，同时以**流式**方式生成文本和自然语音响应。提出了一种名为 `TMRoPE` （Time-aligned Multimodal RoPE） 的新型位置嵌入，以将视频输入的时间戳与音频同步。
- **实时语音和视频**聊天 ：专为完全实时交互而设计的架构，支持分块输入和即时输出。
- 自然而稳健的**语音生成** ：超越许多现有的流媒体和非流媒体替代方案，在语音生成方面表现出卓越的稳健性和自然性。
- **跨模态**的强劲性能 ：与类似大小的单模态模型进行基准测试时，在所有模态中都表现出卓越的性能。
  - Qwen2.5-Omni 在音频功能上优于同等尺寸的 Qwen2-Audio，并实现了与 Qwen2.5-VL-7B 相当的性能。
- 出色的端到端语音教学： Qwen2.5-Omni 在端到端语音教学跟踪方面的性能可与文本输入的有效性相媲美，MMLU 和 GSM8K 等基准测试证明了这一点。

Qwen2.5-Omni-7B 特点：原生支持视频、图片、语音、文字等多模态输入，并能原生生成语音及文字等多模态输出。
- 一个模型就能通过“看”、“听”、“阅读”等多种方式来综合思考。


#### 原理

Qwen2.5-Omni 采用 `Thinker-Talker` 双核架构。
- `Thinker` 模块如同**大脑**，负责处理文本、音频、视频等多模态输入，生成高层语义表征及对应文本内容；
- `Talker` 模块则类似发声器官(嘴)，以流式方式接收 Thinker 实时输出的**语义表征**与**文本**，流畅合成离散**语音单元**。

模型架构
- Thinker 基于 Transformer **解码器**架构，融合音频/图像编码器进行特征提取；
- Talker 则采用**双轨自回归** Transformer **解码器**设计，在训练和推理过程中直接接收来自 Thinker 的高维表征，并共享全部历史上下文信息，形成端到端的统一模型架构。

关键技术：
- 1）采用分块处理策略解耦长序列多模态数据，由多模态编码器负责感知、语言模型承担序列建模，通过共享注意力机制强化模态融合；
- 2）提出时间对齐的位置编码方法TMRoPE，通过音视频交错排列实现时间戳同步；
- 3）首创`Thinker-Talker`架构，分离文本生成（Thinker语言模型）与语音合成（基于隐藏表征的双轨自回归Talker模型），避免模态间干扰；
- 4）引入滑动窗口DiT解码器降低音频流初始延迟。

![](https://pic4.zhimg.com/v2-c2de76bf71d503c577ada48a3b9f4bf9_1440w.jpg)


效果分析：
- Omni-Bench 等多模态基准上达到SOTA，语音指令跟随能力与纯文本输入（MMLU/GSM8K）表现相当，流式语音生成在鲁棒性和自然度上超越主流流式/非流式方案。

评测
- 多模态任务 OmniBench，Qwen2.5-Omni 达到了SOTA的表现。（超过 Gemini 1.5-Pro）
- 单模态任务中，Qwen2.5-Omni 在多个领域中表现优异，包括：语音识别（Common Voice）、翻译（CoVoST2）、音频理解（MMAU）、图像推理（MMMU、MMStar）、视频理解（MVBench）以及语音生成（Seed-tts-eval和主观自然听感）。

「Thinker-Talker」（思考者-说话者） 架构。这个设计非常巧妙，让模型能 同时思考和说话：
1. `Thinker` (思考者): 扮演大脑的角色。它负责处理来自文本、音频、视频等多种模态的输入，通过专门的音视频编码器提取信息，再利用一个 Transformer 解码器进行理解和处理，最终生成高层语义表示和相应的文本内容
2. `Talker` (说话者): 担当嘴巴功能。它以流式（streaming）方式接收 Thinker 生成的高层表示和文本，并采用一种双轨自回归 Transformer 解码器架构，流畅地合成并输出离散的语音单元（tokens）。

关键点: Talker 并非独立工作，直接获取 Thinker 产生的**高维表示**，并且 共享 Thinker 全部历史上下文信息。这使得 Thinker 和 Talker 构成了一个紧密协作的单一整体模型，可以进行端到端的训练和推理。这种设计是实现低延迟、高流畅度语音交互的核心



#### 模型


##### Qwen2.5-Omni-7B

Qwen2.5-Omni-7B 模型是 Omni（全能）模型。
- 一个模型能同时理解 文本、音频、图像、视频 多种输入，并且能输出 文本和音频


##### Qwen2.5-Omni-3B

全模态 Qwen2.5-Omni-7B 模型推出后，开发者反馈更小尺寸的Qwen2.5-Omni，以便更方便地适配

2025年4月30日，开源 Qwen2.5-Omni-3B 版本，较之前 7B 相比，代码运行时的推理时间减少，响应开发者轻量级GPU适配需求的新模型。
- 🔹 与Qwen2.5-Omni-7B相比，3B版本在长上下文序列处理（约25k tokens）中显存消耗减少超50% 🚀，并可在普通24GB的消费级GPU上支持长达30秒的音视频交互 。
- 🔹 3B版本模型保留7B模型90%以上的多模态理解能力 ，语音输出自然度与稳定性与7B版本性能一致 💪🏻。

新的Omni模型已在魔搭社区和HuggingFace上开源



#### 效果

Qwen2.5-Omni全面评估：
- **跨模态能力** SOTA: 在需要整合多种模态信息的任务上（如 OmniBench 基准测试），Qwen2.5-Omni 达到了当前最佳水平（State-of-the-Art）
- **单模态**能力不俗: 与同等规模的单模态模型（如 Qwen2.5-VL-7B、Qwen2-Audio）以及一些强大的闭源模型（如 Gemini-1.5-pro）相比，Qwen2.5-Omni 在各项单模态任务上也展现出强大的竞争力。具体包括：
  - * 语音识别:Common Voice
  - * 语音翻译:CoVoST2
  - * 音频理解:MMAU
  - * 图像推理:MMMU, MMStar
  - * 视频理解:MVBench
  - * 语音生成: Seed-tts-eval 及主观自然度评估

评测
- 多模态任务 OmniBench，Qwen2.5-Omni 达到了SOTA的表现。（超过 Gemini 1.5-Pro）
- 单模态任务中，Qwen2.5-Omni 在多个领域中表现优异，包括：语音识别（Common Voice）、翻译（CoVoST2）、音频理解（MMAU）、图像推理（MMMU、MMStar）、视频理解（MVBench）以及语音生成（Seed-tts-eval和主观自然听感）。

#### 实践

消费级显卡也能运行 Qwen2.5-Omni 本地部署
- conda 创建虚拟环境，并激活
- 安装第三方库：transformers、accelerate、qwen-omni-utils-decord、modelscope
- 使用 modelscope 下载 qwen-2.5-omni 代码
- 创建 python 脚本，写脚本

#### 问题

Qwen-2.5-Omni-7B 问题——目前还没有更普适的**量化版本**
- 当前量化版本只有 `GPTQ`，没有 `gguf`/`mlx`. 导致大部分使用 `ollama`, `llama.cpp`, `mlx` 的用户根本没办法用。
- 而原版 7B 大小达到了**20GB+**，使用小显存显卡的用户完全没办法单卡部署。
	
而 `GPTQ` 量化理论上能用在 `vLLM`/`SGLang` 上。

但是这俩框架目前也不支持, 为纯本文模型准备的。


### 【2025-9-21】阿里 Qwen3 Omni

【2025-9-21】Qwen3-Omni 全模态正式登场，当前还看不到太多的相关信息。

仅从代码层面分析来看，相比 Qwen2.5-Omni，Qwen3-Omni 的主要升级点在于采用了**MoE 架构**，而Qwen2.5-Omni 的 Talker`模块在其标准实现中采用的是dense结构。

主要组成部分

1. Thinker 模块：
- 功能：负责理解和处理多模态输入（文本、图像、视频、音频），并生成相应的文本输出。
- 子模块：
  - Audio Encoder：将音频信号编码为连续的嵌入向量。
  - Vision Encoder：将图像或视频帧编码为视觉嵌入向量。
  - Text Model：基于 MoE 架构的文本模型，处理文本和视觉/音频嵌入，并生成文本输出。
	
2. Talker 模块：
- 功能：负责将文本输出转换为语音输出。
- 子模块：
  - Code Predictor：预测语音的残差码（residual codes）。
  - Text Model：生成语音的文本表示，这个模型是基于 MoE 结构的
  - Codec Head：将文本表示转换为语音码。
  - Hidden Projection 和 Text Projection：用于将 Thinker 模块的隐藏状态投影到 Talker 模块的维度。
	
3. Code2Wav 模块：
- 功能：将 Talker 模块生成的语音码转换为最终的音频波形。
	
关键特性
- 多模态支持：能够同时处理文本、图像、视频和音频输入。
- MoE 架构：通过稀疏激活专家网络，提高模型效率和性能。
- MRoPE (Multi-Dimensional Rotary Position Embedding)：支持处理不同维度的输入（如时间、高度、宽度），用于视觉和音频任务。
- DeepStack：在视觉编码器中使用 DeepStack 技术，以增强视觉特征的表示能力。

Qwen3-Omni 是原生端到端多语言全模态基础模型。它处理文本、图像、音频和视频，并以文本和自然语音形式提供实时流式响应。我们引入了多项架构升级，以提高性能和效率。
- modelscope 地址 [Qwen3-Omni-30B-A3B-Instruct](https://www.modelscope.cn/models/Qwen/Qwen3-Omni-30B-A3B-Instruct)

主要特点：
- 跨模态的最先进技术：早期以文本为主的预训练和混合多模态训练提供了原生多模态支持。在实现强大的音频和音视频结果的同时，单模态文本和图像性能没有退化。在36个音频/视频基准测试中的22个上达到SOTA，在36个中的32个上达到开源SOTA；ASR、音频理解和语音对话性能与Gemini 2.5 Pro相当。
- 多语言：支持119种文本语言，19种语音输入语言和10种语音输出语言。
  - 语音输入：英语、中文、韩语、日语、德语、俄语、意大利语、法语、西班牙语、葡萄牙语、马来语、荷兰语、印度尼西亚语、土耳其语、越南语、粤语、阿拉伯语、乌尔都语。
  - 语音输出：英语、中文、法语、德语、俄语、意大利语、西班牙语、葡萄牙语、日语、韩语。
- 新颖架构：基于MoE的思考者-说话者设计，通过AuT预训练获得强大的通用表示，加上多码本设计，将延迟降至最低。
- 实时音频/视频交互：低延迟流媒体，具有自然的轮流发言和即时文本或语音响应。
- 灵活控制：通过系统提示自定义行为，实现细粒度控制和轻松适应。
- 详细的音频字幕生成器：Qwen3-Omni-30B-A3B-Captioner现已开源：这是一个通用的、高度详细的、低幻觉的音频字幕生成模型，填补了开源社区的一个关键空白。

模型
- `Qwen3-Omni-30B-A3B-Instruct`：Qwen3-Omni-30B-A3B 的**指令**模型，包含`思考者`和`说话者`组件，支持音频、视频和文本输入，输出音频和文本。更多信息请阅读 Qwen3-Omni 技术报告。
- `Qwen3-Omni-30B-A3B-Thinking`：Qwen3-Omni-30B-A3B 的**思考**模型，包含`思考者`组件，具备链式思维推理能力，支持音频、视频和文本输入，输出文本。更多信息请阅读 Qwen3-Omni 技术报告。
- `Qwen3-Omni-30B-A3B-Captioner`：从 `Qwen3-Omni-30B-A3B-Instruct` 微调而来的下游音频细粒度字幕模型，为任意音频输入生成详细且低幻觉的字幕。它包含思考者组件，支持音频输入和文本输出。更多信息可以参考该模型的 cookbook。



### 【2025-6-11】蚂蚁 Ming-Omni


【2025-6-11】蚂蚁百灵团队 2.8B 参数就能媲美GPT-4o

开源 Ming-Omni：支持统一感知与生成的多模态模型，在**端到端**语音理解和指令执行方面表现优异，超越了 `Qwen2.5-Omni` 和 `Kimi-Audio`
- 论文 [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/pdf/2506.09344)
- 【2025-5-21】Code: [Ming](https://github.com/inclusionAI/Ming/tree/main)

Hugging Face 宝藏项目——[Ming-Omni](https://huggingface.co/inclusionAI/Ming-Lite-Omni)。

Ming-Omni 实现了真正的**多模态统一**：同时输入文字、图片、音频和视频，不仅能理解，还能生成高质量的语音和图像。

最震撼的是，只用**2.8B**的活跃参数就达到了GPT-4o级别的效果。

`Ming-Omni` 是蚂蚁与 inclusionAI 共同开发的首个开源多模态模型，旨在与 GPT-4o 竞争。
- 该模型支持多种输入形式，包括文本、语音、图片和视频，同时也可以生成文本、语音和图片输出。
- 这一创新的开源项目为开发者提供了灵活的应用选择，具有广泛的潜力和应用场景。

`Ming-lite-omni` 是统一的多模态模型，是 Ming-omni 的轻量版，源自 Ling-lite，拥有 28 亿激活参数。
- 该模型能够处理图像、文本、音频和视频，同时在语音和图像生成方面表现出强大的能力。
- Ming-lite-omni 采用专用编码器从不同模态中提取 token，随后由 Ling 处理，Ling 是一种配备了新提出的模态专用路由器的 MoE 架构。
- 该设计使单一模型能够在统一框架内高效处理和融合多模态输入，从而支持多样化任务，无需单独模型、任务特定微调或结构重设计。

Ming-lite-omni 超越了传统多模态模型，支持音频和图像生成。这通过集成先进的音频解码器实现自然语音生成，以及 Ming-Lite-Uni 实现高质量图像生成，使模型能够进行上下文感知聊天、文本转语音转换和多功能图像编辑。

技术突破：MoE架构设计。
- 传统模型要么参数量巨大，要么能力单一
- Ming-Omni 通过模态专用路由器，让每个任务都能调用最合适的专家网络。这意味着更高的效率，更低的成本。

![](https://pic4.zhimg.com/v2-f5bb04b0e3764d81d519677a854766b9_1440w.jpg)


无论是上下文对话、文本转语音，还是图像编辑，流畅度和准确性都超出预期。

关键是**完全开源**，代码和权重全部公开，这对整个AI社区是巨大的贡献。

意义
- 对于开发者来说，这是真正部署到生产环境的方案。
- 对于普通用户，这意味着不用再为AI能力付费就能获得顶级体验。


### 【2025-6-16】中科院 Stream-Omni

【2025-6-16】中科院发布 Stream-Omni 全模态实时交互
- 集成了语言、视觉和语音三种模态
- 论文 [Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model](https://arxiv.org/pdf/2506.13642)
- [解读](https://zhuanlan.zhihu.com/p/1925569029306840227)

技术亮点
- 采用了视觉-语音双路径对齐策略

![](https://pic2.zhimg.com/v2-609b2fa05168a1493d7de867e3c01765_1440w.jpg)


这是什么概念呢？
- 业界现有的解决方案，要么在视觉和语言之间勉强拼凑，要么在语音和文本之间生硬转换
- 但Stream-Omni通过序列维度拼接和CTC层维度映射，实现了视觉、语音和文本的无缝融合，让机器真正做到了“看懂”“听懂”和“理解”。

通过对各模态间的关系进行更有针对性的建模，Stream-Omni实现了更加高效和灵活的文本-视觉-语音模态对齐。
- ![](https://pic3.zhimg.com/v2-5a6dd3db2613af1ba16ff47d216dd3b4_r.jpg)

仅依赖包含**2.3万**小时语音的多模态数据，Stream-Omni即可具备文本交互、语音交互、基于视觉的语音交互等各种模态上的交互能力。

与此同时，依赖于创新的语音建模方式，Stream-Omni能在语音交互过程中像GPT-4o一样同步输出中间文本转录结果，为用户提供全方位的多模态交互体验。

Stream-Omni 针对视觉模态的语义互补特性，采用序列维度拼接，让图像和文本的语义融合得更加紧密；

而对于语音模态，创新性地引入了CTC层维度映射，直接将语音特征映射到文本特征空间。

这种双路径对齐机制不仅大幅降低了模态融合的数据需求，还让模型在处理多模态信息时更加高效。这就好比给机器安装了一双“眼睛”和“耳朵”，让它能够同时“看”和“听”，并实时做出反应。

框架
- ![](https://pic1.zhimg.com/v2-5e8f07f3c997c95a3240f794e142a402_1440w.jpg)

Stream-Omni以大语言模型作为主干，逐步将视觉和语音与文本对齐，高效地构建了一个支持文本、视觉和语音的多模态大模型。在视觉-文本对齐方面，Stream-Omni采用视觉编码器和投影模块提取视觉表示，并将其与文本表示进行拼接。在语音-文本对齐方面，Stream-Omni在 LLM 主干的底部和顶部分别引入若干语音层，用于将语音映射到文本以及基于文本生成语音。



视觉模态

基于视觉模态与文本模态之间具有语义互补性，Stream-Omni 采用LLaVA架构中的序列维度拼接的方式进行视觉-文本对齐。



语音模态
- （1）语音离散化：Stream-Omni采用CosyVoice Tokenizer对语音输入进行离散化，编码为若干离散的语音单元。
- （2）语音到文本映射：为了充分利用LLM的能力，Stream-Omni在LLM的底部引入语音层，用于学习语音与文本之间的映射关系，从而将 LLM 中的文本能力迁移到语音模态中。Stream-Omni利用在ASR任务上的CTC损失直接监督底部语音层语音表示，将其与文本模态对齐。
- （3）文本生成：LLM基于输入的视觉表示和语音表示，生成文本回复。
- （4）文本到语音生成：Stream-Omni通过顶部语音层来完成文本到语音生成。为了在生成文本的同时生成语音单元，Stream-Omni在顶部语音层中引入了alignment-based fusion模块。Alignment-based fusion沿用了StreamSpeech等实时生成研究中的同步生成策略，利用CTC对齐来指导同步生成过程。

任意模态组合下的多模态交互

Stream-Omni 通过灵活组合视觉编码器、底部语音层、LLM、顶部语音层来实现任意模态组合下的交互。同时，由于层级维度语音文本映射，Stream-Omni能够在语音到语音生成过程中提供中间的文本结果。


性能炸裂

Stream-Omni
- MSCOCO图像描述生成任务中，达到了128.5的CIDEr分数，吊打一众传统模型。
- 而在语音交互场景中，它的词错误率（WER）较基线降低了18.7%。
- 实时交互能力——能够在语音输入的过程中同步输出自动语音识别（ASR）转录文本和响应内容，真正实现了多模态信息的并行处理。好比在和它聊天的时候，不仅能立刻理解你说的话，还能同时“看”到周围的场景，并给出最合适的回答。
	
应用场景
- 车载交互领域，可以作为智能驾驶的“超级大脑”，不仅能听懂你的语音指令，还能实时分析车辆周围的视觉场景，提供最精准的导航和辅助驾驶建议


### 字节 VeOmni 框架


大模型从单一语言向文本 + 图像 + 视频的多模态进化时，训练流程却陷入了 “碎片化困境”：
- 当业务要同时迭代 DiT、LLM 与 VLM时，很难在一套代码里顺畅切换；
- 而当模型形态一旦变化，底层并行组合和显存调度往往需要大量手工改写，耗时耗力；
- DIT 模型蒸馏需要大量的资源消耗，但是缺少高效的训练 infra 支持来提升效率……

【2025-8-4】字节跳动 Seed 团队开源全模态 PyTorch 原生训练框架—— VeOmni 。
- 论文 [VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo](arXiv：https://arxiv.org/pdf/2508.02317)
- GitHub：[VeOmni](https://github.com/ByteDance-Seed/VeOmni)
- 官方 [解锁任意模态模型训练，字节跳动Seed开源VeOmni框架](https://mp.weixin.qq.com/s/A1CdiEiSaGrh_aH_ggBINg)
- 解读 [字节跳动 VeOmni 框架开源：统一多模态训练效率飞跃](https://zhuanlan.zhihu.com/p/1939325697580601916)

VeOmni 采用以模型为中心的分布式训练方案，将复杂的分布式并行逻辑与模型计算解耦，让研究员像搭积木一样，为全模态模型组合设置高效的并行训练方案。

这一方式可大幅降低工程开销，提升训练效率和扩展性，将数周的工程开发时间缩短至几天。

此前，用 Megatron-LM 等以系统为中心的分布式训练框架训练全新架构的**视觉-语言**模型，往往需要**一周以上**进行工程研发，以及更长时间推进分布式优化和精度对齐，且耗时高度依赖于 Infra 工程团队的经验积累。

而使用 VeOmni 只需一天即可完成模型代码构建，开启训练任务，工程耗时可压缩 90% 以上。

#### VeOmni 介绍

VeOmni 是什么？一套框架搞定所有多模态训练

字节 Seed 团队与火山机器学习平台、IaaS 异构计算团队联合研发的统一多模态模型训练框架，核心定位是三个统一：“统一**多模态**、统一**并行**策略、统一**算力底座**”。
- 通过统一的 API 将 LoRA 轻量微调、FSDP、Ulysses 和 Expert Parallel 等多种混合并行策略以及自动并行搜索能力内置于框架内部。无论是百亿级语言模型、跨模态视觉语言模型，还是 480P/720P、长序列的文本到视频（T2V）或图像到视频（I2V）生成模型，开发者都能够基于统一的训练流程快速启动训练。
- 框架支持在千卡级 GPU 集群上自动完成权重张量的切分、通信拓扑的优化、动态显存回收和异步 checkpoint。在开源的 Wan 2.1 等模型上实测显示，相较于同类开源方案，VeOmni 能够将训练吞吐提高超过 40%，同时显著降低显存使用与跨节点通信带宽压力。
- 借助 VeOmni，字节跳动成功实现了“支持最快落地的新模型形态、最大化超大规模算力利用率、最小化业务改动成本”三大目标，有效弥补了开源社区训练框架在扩展性和抽象层面上的不足，为包括 LLM 和 VLM 在内的多模态生成场景提供了一条统一且高效的训练路径。



#### 效果

实验结果

基于 VeOmni 框架，一个 300 亿参数的全模态 MoE 模型（支持文本、语音、图片、视频的理解和生成）， 在 128 张卡上训练吞吐量可超过 2800 tokens/sec/GPU，并能轻松扩展至 160K 超长上下文序列。

目前，VeOmni 的相关论文和代码仓库均已对外公开，GitHub Star 数超过 500。





# 结束
