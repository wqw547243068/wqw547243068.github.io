---
layout: post
title:  "扩散语言模型"
date:   2025-08-01 16:22:00
categories: 大模型
tags: 扩散模型 语言模型 自回归 深度搜索
excerpt: 新兴方向：扩散语言模型
author: 鹤啸九天
mathjax: true
permalink: /diffusion_lm
---

* content
{:toc}


# 扩散语言模型


## ARM 问题


问题：“难道只有自回归范式才能实现LLM的智能吗？”

实践表明，自回归模型虽然有效，但并非完美无缺。

自回归结构的一些局限性：
- 计算效率低：推理时无法并行生成
- 长程依赖于逆向推理难题：缺乏从右向左的推理能力，导致无法反向推理，利用后文
- 上下文限制：未来信息智能纳入隐式表示，无法条件影响后续词；BERT双向，但非生成模型

ARM存在两大固有缺陷： 
1. 顺序生成瓶颈：逐token生成导致计算效率低下 
1. 推理方向受限：左到右的单向建模难以处理反向任务（如“反转诅咒”现象）—— LLaDA因双向建模特性实现突破



## DLM 介绍

扩散语言模型（Large Language Diffusion Models, LLaDA）是一种新兴的语言生成模型，通过掩码扩散方法挑战传统自回归模型的局限性，展现出强大的生成能力和上下文学习能力。

别称
- LLaDA：Large Language Diffusion Models
- DLM（Diffusion Large Language Model ）

![](https://pic4.zhimg.com/v2-dd50be992cc740e6ef167b996a49c80d_b.webp)

【2025-5-29】[DLM（扩散语言模型）会成为2025年的Mamba吗？](https://www.zhihu.com/question/1910751738811638609/answer/1911481698606122125)
- 【2025-2-18】论文 [Large Language Diffusion Models](https://arxiv.org/pdf/2502.09992)
- Demo [LLaDA-demo](https://ml-gsai.github.io/LLaDA-demo/)

DLM 工作原理
- 根据问题直接生成一个回答草稿
- 然后一次次的修改和润色草稿，最终输出回答。
- ![](https://picx.zhimg.com/50/v2-dd50be992cc740e6ef167b996a49c80d_720w.webp?source=2c26e567)

DLM（Diffusion Large language model）与ARM差异很大，但更符合人类直觉。

高考作文题要求写一篇不少于800字的议论文： “AI的出现给人类带来了什么改变？”
- 传统LLM: 一个字一个字的往外蹦，即线性生成过程。每个时刻只生成一个字
  - 模型: DeepSeek，ChatGPT，Qwen，Gemini等等
  - ![](https://picx.zhimg.com/80/v2-0b1dcbb000780efe43010d2f97d0421f_720w.webp?source=2c26e567)
- DLM 一轮轮迭代更新，当前时刻可能只确定部分词汇（红色），但下一时刻，可能有更多的内容被确定出来（绿色）。
  - 就像油画创作
  - 图像、视频生成领域主流，OpenAI的Sora，阿里的Wan通义万相等。
  - 文本领域：
    - LLaDA，同等模型大小的前提下，在大多数任务上表现并不比其他的模型差多少
    - Google Gemini 2.5 推出 扩散语言模型
  - ![](https://picx.zhimg.com/80/v2-b4da3a641d34f970b9d39bdd0887052f_720w.webp?source=2c26e567)

|模式|原理|分析|图解|
|---|---|---|---|
|`ARM`|逐字输出|简单，但慢|![](https://picx.zhimg.com/80/v2-0b1dcbb000780efe43010d2f97d0421f_720w.webp?source=2c26e567)|
|`DLM`|更符合人类直觉|复杂，但快，更容易扩展|![](https://picx.zhimg.com/80/v2-b4da3a641d34f970b9d39bdd0887052f_720w.webp?source=2c26e567)|


DLM 只不过是把脱胎于图像生成的技术应用到了文字生成
- ![](https://pic1.zhimg.com/50/v2-55f71e9e697d794f053748643ad67dc6_720w.webp?source=2c26e567)

DLM 核心技术原理很直观，不同于LLM next token prediction，DLM 做 mask predictor。

作者：[平凡](https://www.zhihu.com/question/1910751738811638609/answer/1911481698606122125)


语言模型（自回归） autoregressive modeling (ARM)

核心公式: 根据前面的所有内容预测下个字符。
- ![](https://picx.zhimg.com/80/v2-f657f3fe9c7db7187c5152e0790e1ab5_720w.webp)


## DM 启示

语言模型（自回归）

ARM 自回归 vs DLM 扩散模型

在图像、音频等连续信号领域，近年兴起的`扩散模型` (Diffusion Models)为生成建模提供了新的可能。

扩散模型通过逐步添加噪声再去噪声的过程来生成数据，在图像生成任务上取得了革命性突破。
- 例如，Ho等人提出的DDPM模型, 在图像生成质量上媲美甚至超越GAN；
- 谷歌的Imagen模型和稳定扩散模型(Stable Diffusion)能够生成高保真、逼真的图像和艺术画作。
- 这些模型的多模态扩展（如文本到图像生成）也非常成功，如OpenAI的DALLE-2和StabilityAI的稳定扩散均采用了扩散技术。

扩散模型已经成为视觉生成AI的主流方法之一

扩散模型成功的关键：将生成过程划分为多步，每一步进行小幅修改，而非一次性生成全部内容。
- 扩散模型包含**正向**过程（逐步向数据添加噪声，使其趋于随机分布）和**反向**过程（逐步去除噪声，从纯噪声恢复数据分布）。
- 图像领域，这种逐步生成方式带来了稳定训练和高样本质量的优点。

详见站内专题：[扩散模型](ddpm)


例如，基于Transformer的扩散模型也在视觉任务中展现出良好扩展性和性能​。

研究表明，用Transformer实现的扩散模型（Diffusion Transformers）在大规模视觉数据上同样可行，并验证了Transformer+扩散的可扩展性

扩散模型的潜力对NLP领域具有启发意义：
- 文本虽然是**离散符号**序列，但理论上也可以设计类似的逐步“噪声扰动-还原”生成过程。

如果这种思路应用得当，可能在以下方面优于自回归模型：
- **并行生成**： 扩散的反向生成可在每个步骤中同时更新多个位置的词元，从而有希望并行化部分生成过程，减少生成时延。
- **双向依赖**： 反向去噪过程可以利用全局上下文（部分词元已填充，部分待生成），模型在预测时能够同时参考已生成内容的左侧和右侧。这天然缓解了自回归模型不能利用未来词的缺陷，使模型对顺序倒置的任务更为健壮。
- **新的优化目标**： 扩散模型训练基于对数似然的下界（ELBO）优化​，这与直接的交叉熵训练不同，可能提供不同的正则效果。理论上，任何符合生成建模基本原理（最大似然）的模型都应具备LLM的通用能力。

因此，用不同于AR的方式来逼近语言分布，也有望学习到相似的能力


### 语言模型首次击败扩散模型

【2023-10-13】[资讯](https://www.toutiao.com/article/7288583411143459363), 图像、视频生成上，**语言模型**首次击败**扩散模型**，tokenizer是关键

大型语言模型（LLM 或 LM）一开始是用来生成**语言**的，但随着时间的推移，已经能够生成**多模态**的内容，并在音频、语音、代码生成、医疗应用、机器人学等领域开始占据主导地位。

当然，LM 也能生成**图像**和**视频**。
- 图像像素会被视觉 tokenizer 映射为一系列**离散 token**。
- 这些 token 被送入 LM transformer，就像词汇一样被用于生成建模。

尽管 LM 在视觉生成方面取得了显著进步，但 LM 的表现仍然不如扩散模型。
- 例如，在图像生成的金标基准 — ImageNet 数据集上进行评估时，最佳语言模型的表现比扩散模型差了 48% 之多（以 256ˆ256 分辨率生成图像时，FID 为 3.41 对 1.79）。

为什么语言模型在视觉生成方面落后于扩散模型？
- 谷歌、CMU 的研究表明: <span style='color:red'>tokenizer 是关键</span>。缺乏一个良好的视觉表示，类似自然语言系统，以有效地建模视觉世界。
- 论文链接:[paper](https://arxiv.org/pdf/2310.05737.pdf)

在相同训练数据、可比模型大小和训练预算条件下，利用良好的视觉 tokenizer，掩码语言模型在图像和视频基准的生成保真度和效率方面都超过了 SOTA 扩散模型。这是语言模型在标志性的 ImageNet 基准上击败扩散模型的首个证据。

目的不是断言语言模型是否优于其他模型，而是促进 LLM 视觉 tokenization 方法的探索。
- LLM 与其他模型（如扩散模型）的根本区别: LLM 使用**离散**潜在格式，即从可视化 tokenizer 获得的 token。

这项研究表明，这些离散的视觉 token 的价值不应该被忽视，因为存在以下优势:
- 1、**与 LLM 的兼容性**。token 表示的主要优点是与语言 token **共享**相同的形式，可直接利用社区多年来为开发 LLM 所做的优化，包括更快的训练和推理速度、模型基础设施的进步、扩展模型的方法以及 GPU/TPU 优化等创新。通过相同的 token 空间统一视觉和语言可以为真正的多模态 LLM 奠定基础，后者可以在我们的视觉环境中理解、生成和推理。
- 2、**压缩表示**。离散 token 可以为视频压缩提供一个新视角。可视化 token 可以作为一种新的视频压缩格式，以减少数据在互联网传输过程中占用的磁盘存储和带宽。与压缩的 RGB 像素不同，这些 token 可以直接输入生成模型，绕过传统的解压缩和潜在编码步骤。这可以加快生成视频应用的处理速度，在边缘计算情况下尤其有益。
- 3、**视觉理解优势**。研究表明，离散 token 在自监督表示学习中作为预训练目标是有价值的，如 BEiT 和 BEVT 中所讨论的那样。此外，研究发现，使用 token 作为模型输入提高了鲁棒性和泛化性。

研究者提出了一个名为 `MAGVIT-v2` 的视频 tokenizer，旨在将视频（和图像）映射为紧凑的离散 token。

该模型建立在 VQ-VAE 框架内的 SOTA 视频 tokenizer——MAGVIT 基础上。基于此，研究者提出了两种新技术:
- 1）一种新颖的无查找（lookup-free）量化方法，使得大量词汇的学习成为可能，以提高语言模型的生成质量；
- 2）通过广泛的实证分析，他们确定了对 MAGVIT 的修改方案，不仅提高了生成质量，而且还允许使用共享词汇表对图像和视频进行 token 化。

实验结果表明，新模型在三个关键领域优于先前表现最好的视频 tokenizer——MAGVIT。
- 首先，新模型显著提高了 MAGVIT 的生成质量，在常见的图像和视频基准上刷新了 SOTA。
- 其次，用户研究表明，其压缩质量超过了 MAGVIT 和当前的视频压缩标准 HEVC。
- 此外，它与下一代视频编解码器 VVC 相当。
- 最后，研究者表明，与 MAGVIT 相比，他们的新 token 在两个设置和三个数据集的视频理解任务中表现更强。


## DLM 适用场景

【2025-7-30】[在数据受限场景下，Diffusion优于自回归模型](https://mp.weixin.qq.com/s/4GK1fKcGcgeDTW_ohb3uiQ)

大语言模型的发展长期依赖`自回归`（AR）模型，但这类模型在数据重复使用时容易**快速饱和**甚至过拟合。

随着高质量数据逐渐稀缺（预计2028年耗尽公开人类生成数据），如何高效利用有限数据成为关键。

而扩散模型作为替代方案，虽被认为计算需求高，但在数据受限场景中的优势尚未被充分挖掘。

CMU 团队在数据重复使用的受限场景中，对比了AR模型和掩码扩散模型。
- 论文标题：[Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/pdf/2507.15857)


两者共享相同架构和训练参数，核心差异在于：
- AR模型固定从左到右预测序列，扩散模型则通过随机掩码让模型学习多种token排序任务。
- 同时，团队拟合了数据受限下的缩放定律，重点分析了两者对重复数据的利用效率（如有效重复epoch的半衰期）。

核心结论：
- 计算量充足时，扩散模型表现远超AR模型：能从重复数据中持续获益（有效重复epoch达约500次，AR仅约30次），且无过拟合迹象；
- 存在临界计算点，超过该点后扩散模型的验证损失更低，下游任务（如问答、推理）表现更优；
- 临界计算量与数据集大小成幂律关系，为数据稀缺场景提供了明确的模型选择依据。


## DLM 案例


### 【2025-2-18】 LLaDA


【2025-2-14】LLaDA 提出掩码扩散模型（MDM） 架构，通过双向建模打破ARM的序列依赖
- 论文解读：《[Large Language Diffusion Models](https://arxiv.org/pdf/2502.09992)》（Shen Nie等，2024）
- 项目地址：[LLaDA](https://ml-gsai.github.io/LLaDA)

核心突破：首次证明扩散模型可在8B规模媲美LLaMA3，颠覆“自回归是LLM唯一路径”的固有认知

SFT后，LLaDA展现出多轮对话、多语言翻译、复杂推理能力

效果

同等计算量（2.3T token预训练 + 4.5M pair SFT）下，LLaDA 8B展现出惊人性能，媲美 LLama3


### 【2025-5-22】Google Gemini Diffusion

【2025-5-22】谷歌在 Google I/O 2025，重磅提出 文本扩散模型 [Gemini Diffusion](https://deepmind.google/models/gemini-diffusion/)，将扩散模型应用于文本生成的模型。

优势
- 1）更快的响应速度：生成内容的速度甚至比我们迄今为止最快的模型要快得多。
- 2）更连贯的文本：一次生成整个标记块，这意味着它比自回归模型更连贯地响应用户的提示。
- 3）迭代优化：纠正生成过程中的错误以获得更一致的输出。

该模型在保持与 Gemini 2.0 Flash-Lite 相当性能表现的同时，处理速度提升了惊人的5倍。这一创新或将重塑自然语言处理领域的技术路线，扩散模型通过渐进式噪声去除的生成方式，可能为长文本连贯性、多模态融合等Transformer的固有痛点提供新的解决方案。


### 【2025-7-21】TTD-DR 测试时扩展

【2025-7-21】谷歌：测试时扩散「Deep research agent」
- 论文：[Deep Researcher with Test-Time Diffusion](https://arxiv.org/pdf/2507.16075v1)

由大语言模型（LLM）驱动的深度研究智能体（Deep research agent）正在迅速发展；

然而，当使用通用**测试时扩展**算法生成复杂的长篇研究报告时，其性能往往会达到瓶颈。
	
受人类研究的迭代性（涉及搜索、推理和修订的循环）的启发，谷歌团队提出了“**测试时扩散深度研究者**”（Test-Time Diffusion Deep Researcher，TTD-DR）。

这一创新框架将研究报告的生成视为一个扩散过程。

[TTD-DR]() Test-Time Diffusion Deep Researcher
- 通过生成可更新的初始草稿启动该过程，该草稿作为不断演进的基础，引导研究方向。
- 随后，草稿通过一个“去噪”过程进行迭代精炼，该过程在每个步骤中动态整合外部信息，由检索机制驱动。
- 核心过程进一步通过应用于 agentic 工作流每个组件的自进化算法得到增强，确保扩散过程生成高质量的上下文。

这种基于草稿的设计使报告撰写过程更加及时和连贯，同时减少了迭代搜索过程中的信息丢失。
	
TTD-DR 在需要密集搜索和多跳推理的广泛基准测试中实现了 SOTA，显著优于现有 Deep research agent。

### 【2025-7-28】Seed Diffusion Preview

【2025-7-28】字节 Seed 团队发布实验性扩散语言模型 `Seed Diffusion Preview`, 标志着在语言模型领域的一次重大技术突破，尤其是在**代码生成**和**推理速度**方面的显著提升。
- 项目页面: [seed_diffusion](https://seed.bytedance.com/seed_diffusion) 
- 体验链接: [seed_diffusion](https://studio.seed.ai/exp/seed_diffusion)
- 技术报告：[sdiff_updated.pdf](https://lf3-static.bytednsdoc.com/obj/eden-cn/hyvsmeh7uhobf/sdiff_updated.pdf)

Seed Diffusion Preview 核心目标: 通过结构化的代码生成实验，验证离散扩散技术作为下一代语言模型基础框架的可行性。

与传统的自回归（AR）模型相比，Seed Diffusion Preview 在推理速度上实现了令人瞩目的进步，达到了每秒2146个tokens，提升幅度高达**5.4倍**，同时在多个代码生成基准测试中，性能与自回归模型不相上下，甚至在某些任务上表现得更加出色。

自回归模型在推理速度和全局控制方面的局限性。
- 扩散模型在图像和视频合成等连续数据领域取得了显著成功，但将其应用于自然语言等离散领域面临着不少挑战。
- 标准的扩散过程与离散状态空间的不兼容性，曾让这一技术的应用前景蒙上阴影。


将扩散模型迁移到离散的自然语言领域时，会面临重大挑战。主要困难在于标准扩散过程本质上是在连续状态空间中定义的，因此无法直接应用于离散领域如自然语言。

为弥合这一差距，现有方法主要有两类：
- 将离散token投影到连续潜空间，在潜空间应用扩散过程。
- 通过定义显式状态转移矩阵在离散状态空间上直接构建扩散过程。
- 近期的离散状态空间方法已通过先进的架构和训练方案展现出可扩展性和有效性。

但离散扩散模型在语言领域的仍面临两大挑战：
- **顺序建模的归纳偏差**：虽然离散扩散模型在理论层面具有强大的优势，以任意顺序建模和生成token，但自然语言本质上遵**循顺序性**特征。若采用完全随机顺序的学习信号，不仅效率低下，甚至可能对语言建模产生负面影响，导致模型性能下降。
- **推理效率不足**：离散扩散模型多步迭代去噪机制会产生严重延迟，这削弱了其相较于自回归模型的速度优势。

#### 原理

Seed Diffusion Preview 通过四项关键技术创新，展示了离散扩散模型在可扩展性和效果上的巨大潜力。

这四项创新包括：两阶段课程学习、约束顺序扩散、同策略学习以及块级并行扩散采样方案。
- **两阶段课程学习**策略通过基于掩码和编辑的扩散训练，旨在提升模型的局部上下文补全能力和全局代码合理性评估能力。
- **约束顺序扩散**引入代码的结构化先验，引导模型掌握正确的依赖关系
- 而**同策略学习**则通过优化生成步数来提升推理速度。
- **块级并行扩散采样**方案则在保持因果顺序的同时，实现了高效的块级推理。


#### 效果

实验结果表明，Seed Diffusion Preview 在代码推理速度上达到了2146tokens/s的突破性进展，这一速度提升并未以牺牲质量为代价，模型在多个业界基准上的性能与优秀的自回归模型相当，甚至在一些复杂推理任务中表现得更加优异。

推理速度方面，该模型在英伟达H20显卡上实现了2146 tokens/s，相比同等规模自回归模型的提升了5.4倍。

这不仅证明了离散扩散模型在推理加速方面的潜力，更展示了其在复杂推理任务中的应用前景。

这一发布无疑是字节跳动在AI领域的一次大胆尝试，标志着未来语言模型的发展方向。随着技术的不断进步，Seed Diffusion Preview 将为开发者和用户带来更多创新和便利，让我们拭目以待这一技术如何改变我们的编程和交互方式。




# 结束

