---
layout: post
title:  "扩散语言模型"
date:   2025-08-01 16:22:00
categories: 大模型
tags: 扩散模型 语言模型 自回归 深度搜索
excerpt: 新兴方向：扩散语言模型
author: 鹤啸九天
mathjax: true
permalink: /diffusion_lm
---

* content
{:toc}


# 扩散语言模型


## ARM 问题


问题：“难道只有自回归范式才能实现LLM的智能吗？”

实践表明，自回归模型虽然有效，但并非完美无缺。

自回归结构的一些局限性：
- 计算效率低：推理时无法并行生成
- 长程依赖于逆向推理难题：缺乏从右向左的推理能力，导致无法反向推理，利用后文
- 上下文限制：未来信息智能纳入隐式表示，无法条件影响后续词；BERT双向，但非生成模型

ARM存在两大固有缺陷： 
1. 顺序生成瓶颈：逐token生成导致计算效率低下 
1. 推理方向受限：左到右的单向建模难以处理反向任务（如“反转诅咒”现象）—— LLaDA因双向建模特性实现突破



## DLM 介绍

扩散语言模型（Large Language Diffusion Models, LLaDA）是一种新兴的语言生成模型，通过掩码扩散方法挑战传统自回归模型的局限性，展现出强大的生成能力和上下文学习能力。

别称
- LLaDA：Large Language Diffusion Models
- DLM（Diffusion Large Language Model ）

![](https://pic4.zhimg.com/v2-dd50be992cc740e6ef167b996a49c80d_b.webp)

【2025-5-29】[DLM（扩散语言模型）会成为2025年的Mamba吗？](https://www.zhihu.com/question/1910751738811638609/answer/1911481698606122125)
- 【2025-2-18】论文 [Large Language Diffusion Models](https://arxiv.org/pdf/2502.09992)
- Demo [LLaDA-demo](https://ml-gsai.github.io/LLaDA-demo/)
- 【2025-4-14】[What are Diffusion Language Models?](https://spacehunterinf.github.io/blog/2025/diffusion-language-models/)

两种范式对比
- ![](https://spacehunterinf.github.io/assets/img/diffusionlm_blog/diffusion_vs_ar-1400.webp)

【2025-8-27】人大、清华提出 DPT 对偶伪训练器, 基于扩散模型+半监督
- [Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels](https://proceedings.neurips.cc/paper_files/paper/2023/file/8735753cc18f6baa92d1f069fd8b14a0-Paper-Conference.pdf)
- 步骤：标注数据训练分类器 → 伪标签训练条件生成模型 → 真假数据再次训练分类器
- 代码 [DPT](https://github.com/ML-GSAI/DPT)
- 【2025-7-21】[Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/pdf/2507.15857v1)
- Github [Diffusion Language Models are Super Data Learners](https://github.com/JinjieNi/dlms-are-super-data-learners)

### DLM 原理

DLM 工作原理
- 根据问题直接生成一个回答草稿
- 然后一次次的修改和润色草稿，最终输出回答。
- ![](https://picx.zhimg.com/50/v2-dd50be992cc740e6ef167b996a49c80d_720w.webp?source=2c26e567)

DLM（Diffusion Large language model）与ARM差异很大，但更符合人类直觉。

高考作文题要求写一篇不少于800字的议论文： “AI的出现给人类带来了什么改变？”
- 传统LLM: 一个字一个字的往外蹦，即线性生成过程。每个时刻只生成一个字
  - 模型: DeepSeek，ChatGPT，Qwen，Gemini等等
  - ![](https://picx.zhimg.com/80/v2-0b1dcbb000780efe43010d2f97d0421f_720w.webp?source=2c26e567)
- DLM 一轮轮迭代更新，当前时刻可能只确定部分词汇（红色），但下一时刻，可能有更多的内容被确定出来（绿色）。
  - 就像油画创作
  - 图像、视频生成领域主流，OpenAI的Sora，阿里的Wan通义万相等。
  - 文本领域：
    - LLaDA，同等模型大小的前提下，在大多数任务上表现并不比其他的模型差多少
    - Google Gemini 2.5 推出 扩散语言模型
  - ![](https://picx.zhimg.com/80/v2-b4da3a641d34f970b9d39bdd0887052f_720w.webp?source=2c26e567)

|模式|原理|分析|图解|
|---|---|---|---|
|`ARM`|逐字输出|简单，但慢|![](https://picx.zhimg.com/80/v2-0b1dcbb000780efe43010d2f97d0421f_720w.webp?source=2c26e567)|
|`DLM`|更符合人类直觉|复杂，但快，更容易扩展|![](https://picx.zhimg.com/80/v2-b4da3a641d34f970b9d39bdd0887052f_720w.webp?source=2c26e567)|


DLM 只不过是把脱胎于图像生成的技术应用到了文字生成
- ![](https://pic1.zhimg.com/50/v2-55f71e9e697d794f053748643ad67dc6_720w.webp?source=2c26e567)

DLM 核心技术原理很直观，不同于LLM next token prediction，DLM 做 mask predictor。

作者：[平凡](https://www.zhihu.com/question/1910751738811638609/answer/1911481698606122125)


语言模型（自回归） autoregressive modeling (ARM)

核心公式: 根据前面的所有内容预测下个字符。
- ![](https://picx.zhimg.com/80/v2-f657f3fe9c7db7187c5152e0790e1ab5_720w.webp)


## DM 启示

语言模型（自回归）

ARM 自回归 vs DLM 扩散模型

在图像、音频等连续信号领域，近年兴起的`扩散模型` (Diffusion Models)为生成建模提供了新的可能。

扩散模型通过逐步添加噪声再去噪声的过程来生成数据，在图像生成任务上取得了革命性突破。
- 例如，Ho等人提出的DDPM模型, 在图像生成质量上媲美甚至超越GAN；
- 谷歌的Imagen模型和稳定扩散模型(Stable Diffusion)能够生成高保真、逼真的图像和艺术画作。
- 这些模型的多模态扩展（如文本到图像生成）也非常成功，如OpenAI的DALLE-2和StabilityAI的稳定扩散均采用了扩散技术。

扩散模型已经成为视觉生成AI的主流方法之一

扩散模型成功的关键：将生成过程划分为多步，每一步进行小幅修改，而非一次性生成全部内容。
- 扩散模型包含**正向**过程（逐步向数据添加噪声，使其趋于随机分布）和**反向**过程（逐步去除噪声，从纯噪声恢复数据分布）。
- 图像领域，这种逐步生成方式带来了稳定训练和高样本质量的优点。

详见站内专题：[扩散模型](ddpm)


例如，基于Transformer的扩散模型也在视觉任务中展现出良好扩展性和性能​。

研究表明，用Transformer实现的扩散模型（Diffusion Transformers）在大规模视觉数据上同样可行，并验证了Transformer+扩散的可扩展性

扩散模型的潜力对NLP领域具有启发意义：
- 文本虽然是**离散符号**序列，但理论上也可以设计类似的逐步“噪声扰动-还原”生成过程。

如果这种思路应用得当，可能在以下方面优于自回归模型：
- **并行生成**： 扩散的反向生成可在每个步骤中同时更新多个位置的词元，从而有希望并行化部分生成过程，减少生成时延。
- **双向依赖**： 反向去噪过程可以利用全局上下文（部分词元已填充，部分待生成），模型在预测时能够同时参考已生成内容的左侧和右侧。这天然缓解了自回归模型不能利用未来词的缺陷，使模型对顺序倒置的任务更为健壮。
- **新的优化目标**： 扩散模型训练基于对数似然的下界（ELBO）优化​，这与直接的交叉熵训练不同，可能提供不同的正则效果。理论上，任何符合生成建模基本原理（最大似然）的模型都应具备LLM的通用能力。

因此，用不同于AR的方式来逼近语言分布，也有望学习到相似的能力


### 语言模型首次击败扩散模型

【2023-10-13】[资讯](https://www.toutiao.com/article/7288583411143459363), 图像、视频生成上，**语言模型**首次击败**扩散模型**，tokenizer是关键

大型语言模型（LLM 或 LM）一开始是用来生成**语言**的，但随着时间的推移，已经能够生成**多模态**的内容，并在音频、语音、代码生成、医疗应用、机器人学等领域开始占据主导地位。

当然，LM 也能生成**图像**和**视频**。
- 图像像素会被视觉 tokenizer 映射为一系列**离散 token**。
- 这些 token 被送入 LM transformer，就像词汇一样被用于生成建模。

尽管 LM 在视觉生成方面取得了显著进步，但 LM 的表现仍然不如扩散模型。
- 例如，在图像生成的金标基准 — ImageNet 数据集上进行评估时，最佳语言模型的表现比扩散模型差了 48% 之多（以 256ˆ256 分辨率生成图像时，FID 为 3.41 对 1.79）。

为什么语言模型在视觉生成方面落后于扩散模型？
- 谷歌、CMU 的研究表明: <span style='color:red'>tokenizer 是关键</span>。缺乏一个良好的视觉表示，类似自然语言系统，以有效地建模视觉世界。
- 论文链接:[paper](https://arxiv.org/pdf/2310.05737.pdf)

在相同训练数据、可比模型大小和训练预算条件下，利用良好的视觉 tokenizer，掩码语言模型在图像和视频基准的生成保真度和效率方面都超过了 SOTA 扩散模型。这是语言模型在标志性的 ImageNet 基准上击败扩散模型的首个证据。

目的不是断言语言模型是否优于其他模型，而是促进 LLM 视觉 tokenization 方法的探索。
- LLM 与其他模型（如扩散模型）的根本区别: LLM 使用**离散**潜在格式，即从可视化 tokenizer 获得的 token。

这项研究表明，这些离散的视觉 token 的价值不应该被忽视，因为存在以下优势:
- 1、**与 LLM 的兼容性**。token 表示的主要优点是与语言 token **共享**相同的形式，可直接利用社区多年来为开发 LLM 所做的优化，包括更快的训练和推理速度、模型基础设施的进步、扩展模型的方法以及 GPU/TPU 优化等创新。通过相同的 token 空间统一视觉和语言可以为真正的多模态 LLM 奠定基础，后者可以在我们的视觉环境中理解、生成和推理。
- 2、**压缩表示**。离散 token 可以为视频压缩提供一个新视角。可视化 token 可以作为一种新的视频压缩格式，以减少数据在互联网传输过程中占用的磁盘存储和带宽。与压缩的 RGB 像素不同，这些 token 可以直接输入生成模型，绕过传统的解压缩和潜在编码步骤。这可以加快生成视频应用的处理速度，在边缘计算情况下尤其有益。
- 3、**视觉理解优势**。研究表明，离散 token 在自监督表示学习中作为预训练目标是有价值的，如 BEiT 和 BEVT 中所讨论的那样。此外，研究发现，使用 token 作为模型输入提高了鲁棒性和泛化性。

研究者提出了一个名为 `MAGVIT-v2` 的视频 tokenizer，旨在将视频（和图像）映射为紧凑的离散 token。

该模型建立在 VQ-VAE 框架内的 SOTA 视频 tokenizer——MAGVIT 基础上。基于此，研究者提出了两种新技术:
- 1）一种新颖的无查找（lookup-free）量化方法，使得大量词汇的学习成为可能，以提高语言模型的生成质量；
- 2）通过广泛的实证分析，他们确定了对 MAGVIT 的修改方案，不仅提高了生成质量，而且还允许使用共享词汇表对图像和视频进行 token 化。

实验结果表明，新模型在三个关键领域优于先前表现最好的视频 tokenizer——MAGVIT。
- 首先，新模型显著提高了 MAGVIT 的生成质量，在常见的图像和视频基准上刷新了 SOTA。
- 其次，用户研究表明，其压缩质量超过了 MAGVIT 和当前的视频压缩标准 HEVC。
- 此外，它与下一代视频编解码器 VVC 相当。
- 最后，研究者表明，与 MAGVIT 相比，他们的新 token 在两个设置和三个数据集的视频理解任务中表现更强。


## DLM 适用场景

【2025-7-30】[在数据受限场景下，Diffusion优于自回归模型](https://mp.weixin.qq.com/s/4GK1fKcGcgeDTW_ohb3uiQ)

大语言模型的发展长期依赖`自回归`（AR）模型，但这类模型在数据重复使用时容易**快速饱和**甚至过拟合。

随着高质量数据逐渐稀缺（预计2028年耗尽公开人类生成数据），如何高效利用有限数据成为关键。

而扩散模型作为替代方案，虽被认为计算需求高，但在数据受限场景中的优势尚未被充分挖掘。

CMU 团队在数据重复使用的受限场景中，对比了AR模型和掩码扩散模型。
- 论文标题：[Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/pdf/2507.15857)


两者共享相同架构和训练参数，核心差异在于：
- AR模型固定从左到右预测序列，扩散模型则通过随机掩码让模型学习多种token排序任务。
- 同时，团队拟合了数据受限下的缩放定律，重点分析了两者对重复数据的利用效率（如有效重复epoch的半衰期）。

核心结论：
- 计算量充足时，扩散模型表现远超AR模型：能从重复数据中持续获益（有效重复epoch达约500次，AR仅约30次），且无过拟合迹象；
- 存在临界计算点，超过该点后扩散模型的验证损失更低，下游任务（如问答、推理）表现更优；
- 临界计算量与数据集大小成幂律关系，为数据稀缺场景提供了明确的模型选择依据。


## DLM 实现


### 综述

#### 【2025-7-5】新加坡国立

【2025-7-5】新加坡国立大学 离散扩散语言模型综述
- 论文：[Discrete Diffusion in Large Language and Multimodal Models: A Survey](https://arxiv.org/pdf/2506.13759)
- GitHub 仓库：[DLLM-Survey](https://github.com/LiQiiiii/DLLM-Survey)
- 解读 [舍弃自回归，离散扩散语言模型如何演化？NUS综述解构技术图谱与应用前沿]()

自 GPT 引爆大语言模型热潮以来，**自回归**大语言模型（LLMs）与**多模态模型**（MLLMs）已成为智能系统的基石。

然而，当人们着眼于更快、更可控、更智能的生成范式时，一条新兴路径悄然浮现：`离散扩散`（Discrete Diffusion）。

传统大模型采用`自回归`（Autoregressive, AR）架构，从左至右逐词生成方式虽然自然，但存在显著的性能瓶颈：
- 无法并行解码、难以精确控制输出、局限于对输入的静态感知、对补全和逆向推理的建模能力差。
- 这使其在需要结构化控制与动态感知的复杂场景中表现受限。

离散扩散模型打破了这一范式, 不再逐词预测，而是将生成视为一个「**掩码 - 去噪**」迭代过程，并行处理所有 Token，并借助全局注意力机制实现动态感知。

这种设计带来了三大核心优势：
- 推理**并行**性（Parallel Decoding）: 并行推理是离散扩散模型最大的特点和优势。并行推理使得离散扩散每次迭代都可以解码出多个 Token，从而带来解码速度上的提升。
- 输出**可控性**（Controllability）与补全能力（Infilling）: 掩码 - 去噪的解码机制，使得每一次回答都可以预设回答的长度、格式、结构，为回答设定一个模板。
- **动态感知能力**（Dynamic Perception）: 全局注意力机制下模型对左侧 Token 的处理受到右侧 Token 的影响；多轮迭代的解码机制使得对所有 Token 的处理都可以反复多次进行。这使得 dLLM 和 dMLLM 可以对长语料和多模态输入进行多轮、有条件的动态感知，而不是如单向注意力一样仅仅能够感知一次。

自回归模型与典型离散扩散模型的对比

![](https://inews.gtimg.com/om_bt/OWl6RQcgY7tVYRwnhPxSq_aGm-Ibr4GqS68Egy0pt8kQ8AA/641)

本综述系统梳理了离散扩散方向的研究图谱，呈现了离散扩散语言模型（dLLMs）与离散扩散多模态语言模型（dMLLMs）的理论基础、代表模型、训练与推理技术，以及在推理、视觉、生物等多个领域的应用进展。

离散扩散语言模型（dLLMs）生态，分四类：
1. **轻量级**模型：早期的离散扩散模型参数量往往不超过 1B，代表作包括 D3PM、DiffusionBERT、RDM、Diffusion-NAT、TESS、SEDD、MDLM、MD4 等。这些模型重点在于探索基础的建模机制与去噪策略，验证离散扩散在文本和多模态生成任务上的可行性。
2. **大规模** dLLM：随着技术成熟，多个工作开始将扩散架构拓展至 10 亿以上参数量，构建具备完整语言理解与生成能力的「非自回归大模型」，代表模型包括：LLaDA 系列、DiffuGPT / DiffuLLaMA 和 DREAM 等。这些工作从规模上拓展了扩散语言模型的边界，系统性地探索了其工程可行性。
3. **多模态**扩展（dMLLM）：在语言能力日趋完善之后，研究者开始探索 dLLMs 在多模态任务中的适应性，典型代表有：Dimple、LaViDa 和 LLaDA-V。
4. **统一生成**模型：离散扩散在图片生成中的可行性很早就被验证了，随着语言生成能力的完善，MMaDA、FUDOKI 和 Muddit 等模型给出了一种统一的架构，使用离散扩散模型在一个神经网络中同时建模文本和视觉的生成。
- ![](https://inews.gtimg.com/om_bt/Ovn9Wg0wriJz8_WqkACC5TVCzgojc9i3ucGg62m4nyo8cAA/641)

模型结构
- ![](https://inews.gtimg.com/om_bt/O5jYxQ2vEeVSpJm2Tju6sO1kHnI_-3o5NTX0cLe6Be-f8AA/641)




### 【2025-2-18】 LLaDA


【2025-2-14】LLaDA 提出掩码扩散模型（MDM） 架构，通过双向建模打破ARM的序列依赖
- 论文解读：《[Large Language Diffusion Models](https://arxiv.org/pdf/2502.09992)》（Shen Nie等，2024）
- 项目地址：[LLaDA](https://ml-gsai.github.io/LLaDA)

核心突破：首次证明扩散模型可在8B规模媲美LLaMA3，颠覆“自回归是LLM唯一路径”的固有认知

SFT后，LLaDA展现出多轮对话、多语言翻译、复杂推理能力

效果

同等计算量（2.3T token预训练 + 4.5M pair SFT）下，LLaDA 8B展现出惊人性能，媲美 LLama3


### 【2025-5-22】Google Gemini Diffusion

【2025-5-22】谷歌在 Google I/O 2025，重磅提出 文本扩散模型 [Gemini Diffusion](https://deepmind.google/models/gemini-diffusion/)，将扩散模型应用于文本生成的模型。

优势
- 1）更快的响应速度：生成内容的速度甚至比我们迄今为止最快的模型要快得多。
- 2）更连贯的文本：一次生成整个标记块，这意味着它比自回归模型更连贯地响应用户的提示。
- 3）迭代优化：纠正生成过程中的错误以获得更一致的输出。

该模型在保持与 Gemini 2.0 Flash-Lite 相当性能表现的同时，处理速度提升了惊人的5倍。这一创新或将重塑自然语言处理领域的技术路线，扩散模型通过渐进式噪声去除的生成方式，可能为长文本连贯性、多模态融合等Transformer的固有痛点提供新的解决方案。


### 【2025-7-21】TTD-DR 测试时扩展

【2025-7-21】谷歌：测试时扩散「Deep research agent」
- 论文：[Deep Researcher with Test-Time Diffusion](https://arxiv.org/pdf/2507.16075v1)

由大语言模型（LLM）驱动的深度研究智能体（Deep research agent）正在迅速发展；

然而，当使用通用**测试时扩展**算法生成复杂的长篇研究报告时，其性能往往会达到瓶颈。
	
受人类研究的迭代性（涉及搜索、推理和修订的循环）的启发，谷歌团队提出了“**测试时扩散深度研究者**”（Test-Time Diffusion Deep Researcher，TTD-DR）。

这一创新框架将研究报告的生成视为一个扩散过程。

[TTD-DR]() Test-Time Diffusion Deep Researcher
- 通过生成可更新的初始草稿启动该过程，该草稿作为不断演进的基础，引导研究方向。
- 随后，草稿通过一个“去噪”过程进行迭代精炼，该过程在每个步骤中动态整合外部信息，由检索机制驱动。
- 核心过程进一步通过应用于 agentic 工作流每个组件的自进化算法得到增强，确保扩散过程生成高质量的上下文。

这种基于草稿的设计使报告撰写过程更加及时和连贯，同时减少了迭代搜索过程中的信息丢失。
	
TTD-DR 在需要密集搜索和多跳推理的广泛基准测试中实现了 SOTA，显著优于现有 Deep research agent。

### 【2025-7-28】Seed Diffusion Preview

【2025-7-28】字节 Seed 团队发布实验性扩散语言模型 `Seed Diffusion Preview`, 标志着在语言模型领域的一次重大技术突破，尤其是在**代码生成**和**推理速度**方面的显著提升。
- 项目页面: [seed_diffusion](https://seed.bytedance.com/seed_diffusion) 
- 体验链接: [seed_diffusion](https://studio.seed.ai/exp/seed_diffusion)
- 技术报告：[sdiff_updated.pdf](https://lf3-static.bytednsdoc.com/obj/eden-cn/hyvsmeh7uhobf/sdiff_updated.pdf)

Seed Diffusion Preview 核心目标: 通过结构化的代码生成实验，验证离散扩散技术作为下一代语言模型基础框架的可行性。

与传统的自回归（AR）模型相比，Seed Diffusion Preview 在推理速度上实现了令人瞩目的进步，达到了每秒2146个tokens，提升幅度高达**5.4倍**，同时在多个代码生成基准测试中，性能与自回归模型不相上下，甚至在某些任务上表现得更加出色。

自回归模型在推理速度和全局控制方面的局限性。
- 扩散模型在图像和视频合成等连续数据领域取得了显著成功，但将其应用于自然语言等离散领域面临着不少挑战。
- 标准的扩散过程与离散状态空间的不兼容性，曾让这一技术的应用前景蒙上阴影。


将扩散模型迁移到离散的自然语言领域时，会面临重大挑战。主要困难在于标准扩散过程本质上是在连续状态空间中定义的，因此无法直接应用于离散领域如自然语言。

为弥合这一差距，现有方法主要有两类：
- 将离散token投影到连续潜空间，在潜空间应用扩散过程。
- 通过定义显式状态转移矩阵在离散状态空间上直接构建扩散过程。
- 近期的离散状态空间方法已通过先进的架构和训练方案展现出可扩展性和有效性。

但离散扩散模型在语言领域的仍面临两大挑战：
- **顺序建模的归纳偏差**：虽然离散扩散模型在理论层面具有强大的优势，以任意顺序建模和生成token，但自然语言本质上遵**循顺序性**特征。若采用完全随机顺序的学习信号，不仅效率低下，甚至可能对语言建模产生负面影响，导致模型性能下降。
- **推理效率不足**：离散扩散模型多步迭代去噪机制会产生严重延迟，这削弱了其相较于自回归模型的速度优势。

#### 原理

Seed Diffusion Preview 通过四项关键技术创新，展示了离散扩散模型在可扩展性和效果上的巨大潜力。

这四项创新包括：两阶段课程学习、约束顺序扩散、同策略学习以及块级并行扩散采样方案。
- **两阶段课程学习**策略通过基于掩码和编辑的扩散训练，旨在提升模型的局部上下文补全能力和全局代码合理性评估能力。
- **约束顺序扩散**引入代码的结构化先验，引导模型掌握正确的依赖关系
- 而**同策略学习**则通过优化生成步数来提升推理速度。
- **块级并行扩散采样**方案则在保持因果顺序的同时，实现了高效的块级推理。


#### 效果

实验结果表明，Seed Diffusion Preview 在代码推理速度上达到了2146tokens/s的突破性进展，这一速度提升并未以牺牲质量为代价，模型在多个业界基准上的性能与优秀的自回归模型相当，甚至在一些复杂推理任务中表现得更加优异。

推理速度方面，该模型在英伟达H20显卡上实现了2146 tokens/s，相比同等规模自回归模型的提升了5.4倍。

这不仅证明了离散扩散模型在推理加速方面的潜力，更展示了其在复杂推理任务中的应用前景。

这一发布无疑是字节跳动在AI领域的一次大胆尝试，标志着未来语言模型的发展方向。随着技术的不断进步，Seed Diffusion Preview 将为开发者和用户带来更多创新和便利，让我们拭目以待这一技术如何改变我们的编程和交互方式。

### 【2025-8-1】港中文 DAEDAL

随着 Gemini-Diffusion，Seed-Diffusion 等扩散大语言模型（DLLM）的发布，这一领域成为了工业界和学术界的热门方向。

但是，当前 DLLM 存在问题：
- 推理时必须采用预设**固定长度**的限制，对于不同任务都需要专门调整才能达到最优效果。


【2025-8-1】香港中文大学 MMLab，上海 AI 实验室等提出 `DAEDAL`，赋予 DLLM 根据问题情况**自主调整回答长度**的能力，弥补了 DLLM 与自回归 LLM 的关键差距，为更灵活、高效、强大的扩散大语言模型打下了基石。
- 论文标题：[Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
- 代码地址：[DAEDAL](https://github.com/Li-Jinsong/DAEDAL)
- 论文解读：[扩散LLM推理新范式：打破生成长度限制，实现动态自适应调节](https://news.qq.com/rain/a/20250808A07LV700)

`DAEDAL` 作为 Training Free 去噪策略，从统一且短的初始长度开始，让模型根据需求在生成中调节长度，动态扩展，达到现有去噪策略在每个评测基准上精心调整生成长度得到的最佳性能相当的表现，有时甚至更胜一筹。
- DAEDAL 使用统一且很短的初始长度，在多个基准上取得了与精心调优的固定长度基线相当甚至更优的性能。
- DAEDAL 能够根据每个问题，在生成过程中自适应地动态调整长度，相比之下，现有方法则对所有问题都只能采用单一的固定长度。

#### DAEDAL 介绍

扩散大语言模型（DLLM）潜力巨大，但其现有推理流程存在关键问题：需要预定义的，固定的生成长度。

与能够边思考边决定 “说” 多少的人类和自回归模型不同，现有 DLLM 需要预先设定确切的输出长度。

导致了一个两难的困境：
- 设置太短，模型在复杂问题上难以发挥全部实力，可能导致做错；
- 设置太长，则会浪费大量的计算资源，同时，实验中还发现过长的生成长度可能导致性能下降。

这一问题的解决方案就蕴藏在模型自身之中。DLLM 在生成时会不断地全局规划其整体输出，而它的预测置信度正是其内部状态的强大信号。

两种关键信号：
- DLLM 在序列末端生成序列结束符 (EOS) 的意愿直接反映了其对全局预算的规划。当预设长度充足时，模型会自信地在末尾规划出结束区域，从而高置信度地预测 EOS。反之，当长度不足时，模型会试图利用所有可用空间来完成核心内容，因而抑制了在末尾生成 EOS 的置信度。
- 去噪过程中，对某个特定词元的极低预测置信度，则可作为一种局部信号，这不仅代表了模型对该词元的不确定性，更深层次地，它表明当前的局部上下文过于受限，不足以支撑一个复杂的逻辑步骤或细节的展开，或是需要插入空间对过去生成的内容进行补充和修正。

该热力图展示了在对一个长度为 128 的全掩码输入进行首次预测后，于序列末端测得的平均 EOS 词元置信度之差。该差值的计算方式为：用 “长度充足” 问题（在少于 128 长度的 setting 下被正确回答）的平均置信度减去 “长度不足” 问题（仅在长度更长的 setting 才能被正确回答）的平均置信度。图中大面积的绿色（差值 > 0）表明，对于长度充足的问题，结尾 EOS 序列置信度更高，验证了文中的核心发现。
- ![](https://inews.gtimg.com/om_bt/OwV30nTdCtHeEl9G-Ap__H0ClFEF7Nas9TZUrEs2NiwEkAA/641)

作者提出 DAEDAL，一种无需训练的两阶段推理策略，利用这些内部信号，赋予 DLLM 根据每个问题的具体情况，动态自主调整回答长度的能力。
1. 初始长度调整 (Initial Length Adjustment): 在去噪流程开始前，DAEDAL 从一个统一的很短初始长度出发。它会通过检测序列末端的 EOS 序列平均置信度来衡量：“对于这个任务，当前分配的长度是否充足？”。如果置信度很低，即模型规划充分利用全部长度，就表明模型认为长度预算不足。此时，DAEDAL 会通过增加 [MASK] 词元来扩展序列长度，并重复此过程，直到模型确信长度预算充足。这为任务设定了一个合理的全局规划长度。
2. 迭代式掩码插入 (Iterative Mask Insertion): 在逐步去噪的过程中，DAEDAL 会持续监控模型的置信度。如果它发现模型对某个 [MASK] 位置极不确定，便会将其标记为 “扩展点”。DAEDAL 通过将这个单个 [MASK] 替换为由多个 [MASK]，动态且精准地在模型最需要的地方为序列注入 “思考空间”，以便其在回复中进行补充修补，或是有足够空间去进行更复杂的思考。

通过结合这两个阶段，不需要进行任何训练，DAEDAL 使得 DLLM 能够根据每个问题的具体情况自主调整其回答的长度，展现了强大的效果。

#### 效果

实验结果

DAEDAL 使用统一的短初始长度即可取得强大性能。 实验结果清晰地展示了 DAEDAL 的优越性能。尽管 DAEDAL 默认从一个较短的初始长度开始，但其两阶段的长度调整与扩展机制，不仅使其性能显著优于使用相同短初始长度的基线方法，更能达到与基线方法在所有固定长度中精心调优后的峰值性能相当、甚至在某些情况下超越后者的水平。

这一发现凸显了 DAEDAL 的有效性，并揭示了固定长度范式的内在不便之处，因为基线方法的最佳长度因不同基准而异，这更强调了动态长度适应的必要性。为了直观展示这种动态适应性，图 3 对比了 DAEDAL 所用总生成长度（N_token）的分布与基线方法所用的单一最佳长度。

DAEDAL 能自适应地找到最佳生成长度。 进一步的分析表明，DAEDAL 能智能地预估并生成恰当长度的回答。在多数情况下，DAEDAL 产生的有效词元数（E_token）与基线方法在最佳性能配置下的有效词元数相当。这表明 DAEDAL 能自适应地找到模型内在的、针对特定任务所需词元长度的 “舒适点”。

基线方法的行为也印证了这一点：当设置的长度过长时，即使有效词元数可能继续增加，性能反而可能会下降。DAEDAL 的自适应特性有效避免了这种因过度扩展导致的性能下降。
- ![](https://inews.gtimg.com/om_bt/OS6VjmyI_Wt-ghobIYGx67uFByIGJPlWhWDTlXiVgoJ2QAA/641)

DAEDAL 能够提升计算资源利用率。在取得优越准确率的同时，DAEDAL 生成的总词元数（N_token）通常低于基线方法在最佳性能 setting 下的总词元数。相近的有效词元数和更低的总词元数带来了更高的有效词元利用率（E_ratio）。这大大地提升了计算资源的利用率。

总结

DAEDAL 通过其初始长度调整（Initial Length Adjustment）和迭代式掩码插入（Iterative Mask Insertion）机制，不仅在多个基准上取得了与精心调优的固定长度基线相当甚至更优的性能，还能为每个任务自适应地分配合适的长度。这使得模型在性能和计算效率上都取得了实质性的提升。DAEDA 弥补了扩散大语言模型与自回归大语言模型在核心能力上的一个关键差距，为更灵活、高效、强大的扩散大语言模型打下了基石。


# 结束

