---
layout: post
title:  "NER(命名实体识别)专题"
date:   2020-05-03 21:50:00
categories: 自然语言处理
tags: 分词 实体识别 ner 关系抽取 正则表达式
excerpt: NER识别方法汇总
author: 鹤啸九天
mathjax: true
permalink: /ner
---

* content
{:toc}

# NER 命名实体识别

## NER 定义

`命名实体识别`（`NER`, Named Entity Recognition），是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。

- 目标：识别序列中的人名、地名、组织机构名等实体。可以看做序列标注问题。

常见NER方法
- 【2021-11-7】[命名实体识别（NER）的过去和现在](https://zhuanlan.zhihu.com/p/425268651)
- ![](https://pic2.zhimg.com/v2-cd5152c5d16880c227e7f59295f2f249_r.jpg)
- [命名实体识别(Name Entity Recognition)综述](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/)


## NER 任务

NER 研究重点从常规的**扁平**命名实体识别（`Flat NER`）逐渐转向**重叠**命名实体识别（`Nested/Overlapped NER`）与**非连续**命名实体识别（`Discontinuous NER`）。这三类 NER 分别为：
- `Flat NER`：抽取**连续**实体片段（或者包含对应的实体语义类型）；
- `Nested/Overlapped NER`：抽取两个或多个实体片段之间有一部分的**文字重叠**；
- `Discontinuous NER`：所抽取的多个实体间存在多个片段，且片段之间不相连。

示例
- “aching in legs” 是一个扁平实体
- “aching in shoulders”是一个非连续实体，两者在“aching in”上重叠。

三种 NER 类型可概括为**统一**命名实体识别（Unified Named Entity Recognition，`UNER`）

总结
- ![](https://pic1.zhimg.com/70/v2-c78bc1a113594e7189729d5d43c1fc40_1440w.avis?source=172ae18b&biz_tag=Post)
- [W2NER：FLAT+嵌套+非连续NER之统一建模](https://zhuanlan.zhihu.com/p/559115919)

## NER 标注

### NER tag 标注集

NE识别模块的标注结果采用O-S-B-I-E标注形式，其含义为

| 标记 | 含义 |
| --- | --- |
| O | 这个词不是NE |
| S | 这个词单独构成一个NE |
| B | 这个词为一个NE的开始 |
| I | 这个词为一个NE的中间 |
| E | 这个词位一个NE的结尾 |

LTP中的NE 模块识别三种NE，分别如下：

| 标记 | 含义 |
| --- | --- |
| Nh | 人名 |
| Ni | 机构名 |
| Ns | 地名 |


- n/名词 np/人名 ns/地名 nt/机构名 nz/其它专名
- m/数词 q/量词 mq/数量词 t/时间词 f/方位词 s/处所词
- v/动词 a/形容词 d/副词 h/前接成分 k/后接成分
- i/习语 j/简称 r/代词 c/连词 p/介词 u/助词 y/语气助词
- e/叹词 o/拟声词 g/语素 w/标点 x/其它

完整版见：北大pkuseg [tags.txt](https://github.com/lancopku/pkuseg-python/blob/master/tags.txt)

### 序列标注模式

- 在序列标注中，我们想对一个序列的每一个元素(token)标注一个标签。一般来说，一个序列指的是一个句子，而一个元素(token)指的是句子中的一个词语或者一个字。比如信息提取问题可以认为是一个序列标注问题，如提取出会议时间、地点等。
- 不同的序列标注任务就是将目标句中的字或者词按照需求的方式标记，不同的结果取决于对样本数据的标注，一般序列的标注是要符合一定的标注标准的如([PKU数据标注规范](http://sighan.cs.uchicago.edu/bakeoff2005/data/pku_spec.pdf))。
- 另外, 词性标注、分词都属于同一类问题，他们的区别主要是对序列中的token的标签标注的方式不同。

下面以命名实体识别来举例说明. 我们在进行命名实体识别时，通常对每个字进行标注。中文为单个字，英文为单词，空格分割。

标签类型的定义一般如下：

|定义|	全称|	备注|
|---|---|---|
|B	|Begin	|实体片段的开始|
|I	|Intermediate|	实体片段的中间|
|E	|End	|实体片段的结束|
|S	|Single	|单个字的实体|
|O	|Other/Outside	|其他不属于任何实体的字符(包括标点等)|

### 常见的标签方案

常用的较为流行的标签方案有如下几种：
- IOB1: 标签I用于文本块中的字符，标签O用于文本块之外的字符，标签B用于在该文本块前面接续则一个同类型的文本块情况下的第一个字符。
- IOB2: 每个文本块都以标签B开始，除此之外，跟IOB1一样。
- IOE1: 标签I用于独立文本块中，标签E仅用于同类型文本块连续的情况，假如有两个同类型的文本块，那么标签E会被打在第一个文本块的最后一个字符。
- IOE2: 每个文本块都以标签E结尾，无论该文本块有多少个字符，除此之外，跟IOE1一样。
- START/END （也叫SBEIO、IOBES）: 包含了全部的5种标签，文本块由单个字符组成的时候，使用S标签来表示，由一个以上的字符组成时，首字符总是使用B标签，尾字符总是使用E标签，中间的字符使用I标签。
- IO: 只使用I和O标签，显然，如果文本中有连续的同种类型实体的文本块，使用该标签方案不能够区分这种情况。

其中最常用的是`BIO`, `BIOES`, `BMES`


#### BIO标注模式

将每个元素标注为“B-X”、“I-X”或者“O”。其中，“B-X”表示此元素所在的片段属于X类型并且此元素在此片段的开头，“I-X”表示此元素所在的片段属于X类型并且此元素在此片段的中间位置，“O”表示不属于任何类型。

命名实体识别中每个token对应的标签集合如下:
>LabelSet = {O, B-**PER**, I-**PER**, B-**LOC**, I-**LOC**, B-**ORG**, I-**ORG**}

其中，PER代表人名， LOC代表位置， ORG代表组织. B-PER、I-PER代表人名首字、人名非首字，B-LOC、I-LOC代表地名(位置)首字、地名(位置)非首字，B-ORG、I-ORG代表组织机构名首字、组织机构名非首字，O代表该字不属于命名实体的一部分。

![](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/BIO.jpg)

对于词性标注, 则可以用{B-NP, I-NP}给序列中的名词token打标签

#### BIOES标注模式

BIOES标注模式就是在BIO的基础上增加了单字符实体和字符实体的结束标识, 即

>LabelSet = {O, B-PER, I-PER, E-PER, S-PER, B-LOC, I-LOC, E-LOC, S-LOC, B-ORG, I-ORG, E-ORG, S-ORG}

![](https://sthsf.github.io/2020/02/18/NLP--%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/BIOES.jpg)

根据标注的复杂度, 还有会在其中添加其他的比如MISC之类的实体, 如

>LabelSet = {O ,B-MISC, I-MISC, B-ORG ,I-ORG, B-PER ,I-PER, B-LOC ,I-LOC]。
其中，一般一共分为四大类：PER（人名），LOC（位置[地名]），ORG（组织）以及MISC(杂项)，而且B表示开始，I表示中间，O表示不是实体。

其他类似的标注方式:

标注方式1:
>LabelSet = {BA, MA, EA, BO, MO, EO, BP, MP, EP, O}
其中，
- BA代表这个汉字是地址首字，MA代表这个汉字是地址中间字，EA代表这个汉字是地址的尾字；
- BO代表这个汉字是机构名的首字，MO代表这个汉字是机构名称的中间字，EO代表这个汉字是机构名的尾字；
- BP代表这个汉字是人名首字，MP代表这个汉字是人名中间字，EP代表这个汉字是人名尾字，而O代表这个汉字不属于命名实体。

标注方式2:
> LabelSet = {NA, SC, CC, SL, CL, SP, CP}
其中 NA = No entity, SC = Start Company, CC = Continue Company, SL = Start Location, CL = Continue Location, SP = Start Person, CP = Continue Person

上面两种标注方式与BIO和BIEOS类似, 只是使用不同的标签字符来标识而已.

### BMES标注模式

### 评估方法

序列标注算法一般用conlleval.pl脚本实现，但这是用perl语言实现的。在Python中，也有相应的序列标注算法的模型效果评估的第三方模块，那就是**seqeval**，其[官网网址](https://pypi.org/project/seqeval/0.0.3/), pip install seqeval==0.0.3

seqeval支持BIO，IOBES标注模式，可用于命名实体识别，词性标注，语义角色标注等任务的评估。

```python
# -*- coding: utf-8 -*-
from seqeval.metrics import f1_score
from seqeval.metrics import precision_score
from seqeval.metrics import accuracy_score
from seqeval.metrics import recall_score
from seqeval.metrics import classification_report

y_true = ['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER']
y_pred = ['O', 'O', 'B-MISC', 'I-MISC', 'B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER']

print("accuary: ", accuracy_score(y_true, y_pred))
print("p: ", precision_score(y_true, y_pred))
print("r: ", recall_score(y_true, y_pred))
print("f1: ", f1_score(y_true, y_pred))
print("classification report: ")
print(classification_report(y_true, y_pred))
```




## NER 算法


### NER 算法总结

主流方法大致可分类为四类：
- 基于**序列标注**的方法；
  - Flat NER 的基准模型
- 基于**超图**的方法；
  - 模型结构相对复杂，关注相对较少
- 基于**序列到序列**的方法；
- 基于**片段**的方法。

基于**序列到序列**方法和基于**片段**方法获得了当前最好的效果，社区最为流行的方法。技术思路
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/a2cd1f1947fc45409d09f96ff0f47968~tplv-tt-origin-asy2:5aS05p2hQOaVsOaNrua0vlRIVQ==.image?_iz=58558&from=article.pc_detail&x-expires=1695263354&x-signature=JsuqbNZoTUqyjPSK7qRp%2BBkUvKY%3D)

### 基于词典和规则的方法

- 利用**词典**，通过词典的先验信息，匹配出句子中的**潜在实体**，通过一些规则进行筛选。
- 或者利用**句式模板**，抽取实体，例如模板“**播放歌曲${song}**”，就可以将query=“播放歌曲**七里香**”中的 song= 七里香 抽取出来。
具体匹配方法：
- **正向**最大匹配：从前往后依次匹配子句是否是词语，以最长的优先。
- **反向**最大匹配：从后往前依次匹配子句是否是词语，以最长的优先。
- **双向**最大匹配
  - 覆盖 token **最多**的匹配。
  - 句子包含实体和切分后的片段，这种片段+实体个数最少的。

原理比较简单，直接看[代码](https://github.com/InsaneLife/MyPicture/blob/master/NER/ner_rule.py)

### 机器学习方法 CRF

CRF，原理可以参考：Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data


### 引入深度学习语义编码器 Bi-LSTM-CRF

（1）BI-LSTM-CRF模型可以有效地利用过去和未来的输入特征。借助 CRF 层, 它还可以使用句子级别的标记信息。BI-LSTM-CRF 模型在 `POS`（词性标注），chunking（语义组块标注）和 `NER`（命名实体识别）数据集上取得了当时的 SOTA 效果。同时 BI-LSTM-CRF 模型是健壮的，相比之前模型对词嵌入依赖更小。

论文[Bidirectional LSTM-CRF Models for Sequence Tagging]()中对比了 5 种模型：LSTM、BI-LSTM、CRF、LSTM-CRF、BI-LSTM-CRF，LSTM：通过输入门，遗忘门和输出门实现记忆单元，能够有效利用上文的输入特征。BI-LSTM：可以获取时间步的上下文输入特征。CRF：使用功能句子级标签信息，精度高。

比较经典的模型，BERT 之前很长一段时间的范式，小数据集仍然可以使用

（2）stack-LSTM & char-embedding

Neural Architectures for Named Entity Recognition
stack-LSTM ：stack-LSTM 直接构建多词的命名实体。Stack-LSTM 在 LSTM 中加入一个栈指针。模型包含 chunking 和 NER（命名实体识别）。

（3）CNN + BI-LSTM + CRF

通过 CNN 获取字符级的词表示。CNN 是一个非常有效的方式去抽取词的形态信息（例如词的前缀和后缀）进行编码的方法

（4）IDCNN

针对 Bi-LSTM 解码速度较慢的问题，本文提出 ID-CNNs 网络来代替 Bi-LSTM，在保证和 Bi-LSTM-CRF 相当的正确率，且带来了 14-20 倍的提速。句子级别的解码提速 8 倍相比于 Bi- LSTM-CRF。

（5）胶囊网络

Joint Slot Filling and Intent Detection via Capsule Neural Networks [7]

Git: https://github.com/czhang99/Capsule-NLU


NLU 中两个重要的任务，Intent detection 和 slot filling，当前的无论 pipline 或者联合训练的方法，没有显示地对字、槽位、意图三者之间的层次关系建模。本文提出将胶囊网络和 dynamic routing-by-agreement 应用于 slot filling 和 intent detection 联合任务。
1. 使用层次话的胶囊网络来封装字、槽位、意图之间的层次关系。
2. 提出 rerouting 的动态路由方案建模 slot filling。

（6）Transformer

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [10]

直说吧，就是 BERT，bert 之前万年 bilstm+crf，bert 之后，基本没它什么事儿了，bert 原理不多赘述，应用在 NER 任务上也很简单，直接看图，每个 token 的输出直接分类即可

### 深度学习-语义特征

（1）char-embedding

Neural Architectures for Named Entity Recognition，将英文字符拆解为字母，将词语的每个字母作为一个序列编码，编码器可以使用 rnn，cnn 等。

（2）Attending to Characters in Neural Sequence Labeling Models

Attending to Characters in Neural Sequence Labeling Models，使用了单词或字符级别 embedding 组合，并在两种 embedding 之间使用 attention 机制“灵活地选取信息”，而之前模型是直接将两种 embedding concat。char-embedding 学习的是所有词语之间更通用的表示，而 word-embedding 学习的是特特定词语信息。对于频繁出现的单词，可以直接学习出单词表示，二者也会更相似。

（3）Radical-Level Features（中文部首）

Character-Based LSTM-CRF with Radical-LevelFeatures for Chinese Named Entity Recognition 也是一种 char embedding 方法，将每个中文字拆分为各个部首，例如“朝”会被拆分为字符：十、日、十、月。后面结构都类似。

（4） n-gram prefixes and suffixes

Named Entity Recognition with Character-Level Models 提取每个词语的前缀和后缀作为词语的特征，例如：“aspirin” 提取出 3-gram 的前后缀：{"asp", "rin"}. 包含两个参数：n、T。n 表示 n-gram size，T 是阈值，表示该后缀或者前缀至少在语料库中出现过 T 次。

###  多任务联合学习

（1）联合分词学习

Improving Named Entity Recognition for Chinese Social Mediawith Word Segmentation Representation Learning [15]

将中文分词 和 NER 任务联合起来。使用预测的分割标签作为特征作为 NER 的输入之一，为 NER 系统提供更丰富的边界信息。分词语料目前是很丰富的。如果目标域数据量比较小，不妨用分词的语料作为源域，来预训练一个底层编码器，然后再在目标域数据上联合分词任务 fine-tuning。

（2）联合意图学习

slot-gated

Slot-Gated Modeling for Joint Slot Filling and Intent Prediction，slot-gated 这篇文章提出了 slot-gate 将槽位和意图的关系建模，同时使用了 attention 方法，所以介绍这篇文章直接一起介绍 attention，之前 attention 相关的就不介绍了。

（3）BERT for Joint Intent Classification and Slot Filling

BERT for Joint Intent Classification and Slot Filling 原理如图，底层编码器使用了 BERT，token 的输出向量接 softmax 预测序列标签，cls 向量预测意图。

bert 之后，似乎之前的一些优化都变成了奇技淫巧，那么就没有新的方法了吗？bert 之前实体识别都是以序列标注（sequence labeling）来识别，没有其他的解码方式吗？



## NER 优化

[工业界如何解决NER问题？12个trick](https://zhuanlan.zhihu.com/p/152463745)
- Q1、如何快速有效地提升NER性能（非模型迭代）？
- Q2、如何在模型层面提升NER性能？
- Q3、如何构建引入词汇信息（词向量）的NER？
- Q4、如何解决NER实体span过长的问题？
- Q5、如何客观看待BERT在NER中的作用？
- Q6、如何冷启动NER任务？
- Q7、如何有效解决低资源NER问题？
- Q8、如何缓解NER标注数据的噪声问题？
- Q9、如何克服NER中的类别不平衡问题？
- Q10、如何对NER任务进行领域迁移？
- Q11、如何让NER系统变得“透明”且健壮？
- Q12、如何解决低耗时场景下的NER任务？

### OOV问题

一般将 NER 视为`分类`/`预测` 问题，那如何处理不在训练语料库中的名称实体？
- “詹姆斯出生在英国”  --> James 被标记为 `PERSON`，England 被标记为 `LOCATION`。

NER Tagger 的目标是在给定一组预先指定的类的情况下，学习可用于对单词（或更一般地，标记）进行分类的语言模式。这些类是在训练之前定义的并且保持不变。类，例如：PERSON, DATETIME, ORGANIZATION, ... 你可以命名它。

但是输入了另一个完全奇怪的句子
- “Fyonair 来自 Fuabalada”。 ---> 人类可以理解 Fyonair 是一个人（或者也许是童话中的公主），而 Fuabalada 是她来自的土地。

一个好的 NER 标注器将学习一种**语言结构**，并识别出 "Fyonair is from Fuabalada land." 遵循一些语言规则和规律，并且从这些规律（在训练期间自主学习）中，分类器可以将 Fyonair class PERSON 和 Fuabalada class 归类 LOCATION。

如果它不包含在数十亿的语料库和令牌中，模型将如何识别它？

- 【2021-9-8】[ACL2021 一种巧妙解决NER覆盖和不连续问题的方法](https://mp.weixin.qq.com/s?__biz=MzI3ODgwODA2MA==&mid=2247505432&idx=2&sn=eb7ad917eaa560a444b9efb51f909295&chksm=eb53c28bdc244b9d2b3e765cafff4a7b20f5fafa6e5b0c8cbccc7223138486470d821a27d085&mpshare=1&scene=1&srcid=0908OsHWLx486VoYI2djrIGu&sharer_sharetime=1631096211455&sharer_shareid=b8d409494a5439418f4a89712efcd92a&version=3.1.0.6189&platform=mac#rd)
- 稍微复杂些问题：一种带有**覆盖**和**不连续**（Overlapped and Discontinuous）的命名实体识别任务。前人只是要么解决**覆盖**问题，要么解决**不连续**问题，但是本文提出一种联合解决这两种问题的**span-based**方法。
- 两个步骤构建模型：
  - 通过列举所有可能的text span来识别出实体片段（entity fragments）；
  - 在这些entity fragments上预测是两种关系overlapping or succession。
- 这样，我们不仅可以识别Discontinuous的实体，同时也可以对Overlapped的实体进行双重检查。
- 通过上述方法轻松将NER装换成RE（Relation Extraction）任务。最终实验在很多数据集上比如CLEF, GENIA and ACE05上展现除了很强劲的性能。

### 无监督 NER

深度学习模型比其他具有非常大数据集（所谓的“大数据”）的模型更好地工作。
- 在小型数据集上，它们并不是非常有用。

无监督学习能完成这个任务吗？

NER 标记是一项有监督的任务。
- 需要一组标记示例的训练集来为此训练模型。

但是，可以做一些无监督的工作来稍微提高模型的性能。

从 Geron 的书中摘录了一段有用的段落：
> 假设要处理一项没有太多标记训练数据的复杂任务
>- 如果您可以收集大量未标记的训练数据，则可以尝试使用它来训练**无监督模型**，例如自动编码器或生成对抗网络, 然后您可以重用`自动编码器`的较低层或 `GAN` 鉴别器的较低层，在顶部添加任务的输出层，并使用监督学习微调最终网络（即标签训练示例）。

Geoffrey Hinton 和他的团队在 2006 年使用的正是这种技术，并导致了神经网络的复兴和深度学习的成功。

（有史以来最好的机器学习书籍，恕我直言。）

这种无监督预训练是我能想到的将无监督模型用于 NER 的唯一方法。

【2022-10-14】[BERT的无监督NER](https://zhuanlan.zhihu.com/p/113758095)
- 采用BERT(bert-large-cased)无监督NER的标记句子样本，没有进行微调。 这些例子中突出了采用这种方法（BERT）标记的几个实体类型。 标记500个句子可以生成大约1000个独特的实体类型——其中一些映射到如上所示的合成标签。Bert-large-cased模型无法区分GENE和PROTE IN，因为这些实体的描述符与屏蔽词的预测分布落在同一尾部区域（无法将它们与基本词汇表中的词汇区分开来）。 区分这些密切相关的实体可能需要对特定领域的语料库进行MLM微调，或者使用scratch中自定义词汇进行预训练

如果要问术语（术语指文章中的单词和短语）的实体类型到底是什么，即使我们以前从未见过这个术语，但是也可以通过这个术语的发音和/或从这个术语的句子结构中猜出它来。 即，

（1） 术语的子词结构为它的实体类型提供了线索。
- Nonenbury is a _____
- Nonenbury是_____

这是一个杜撰的城市名称，但从它的后缀“bury”可以猜出这可能是一个地点。此时，即便没有任何其上下文，术语的后缀也给出了实体类型的线索。

（2）句子结构提供了术语的实体类型的线索。
- He flew from _____ to Chester
- 他从_____飞到切斯特

在这里，句子上下文给出了实体类型的线索，未知的术语是一个地点。即便以前从未见过它(例如：Nonenbury) ，但也可以猜测出句子中的空白处是一个地点（如：Nonenbury）。

详见原文：[BERT的无监督NER](https://zhuanlan.zhihu.com/p/113758095)

[ACL2021 NER 小量强标签和大量弱标签结合的命名实体识别](https://bbs.huaweicloud.com/blogs/288350)
- 弱监督在NLP里认证是有用的
- 现有工作：深度NER模型 + 只有弱监督
- 实际场景：既有小量强标签数据，又有大量弱标签数据
- 实际问题：两者简单/加权组合，未能提升性能（弱标签有大量噪声）
- 提出一个新的三阶段计算框架：针 NEEDLE (Noise-aware wEakly supErviseD continuaL prEtraining)
  - 弱标签补全
  - 噪声感知损失函数
  - 最终在强标签数据上微调
- 三份生物医学数据集上达到SOTA

### W2NER -- 2022 sota

【2023-9-8】[NER统一模型：刷爆14个中英文数据SOTA](https://zhuanlan.zhihu.com/p/476746322)

AAAI 2022 ，武汉大学论文刷新了14个中英文数据集的SOTA
- paper: [Unified Named Entity Recognition as Word-Word Relation Classification](https://arxiv.org/pdf/2112.10070.pdf)，基于词对关系建模的统一命名实体识别系统
- code: 
  - 官方 [W2NER](https://github.com/ljynlp/W2NER)
  - 第三方
    - 整合成一个文件 [task_sequence_labeling_ner_W2NER](https://github.com/Tongjilibo/bert4torch/blob/master/examples/sequence_labeling/task_sequence_labeling_ner_W2NER.py)
    - 新增 predict 文件: [W2NER_predict](https://github.com/taishan1994/W2NER_predict)

创新点
- 利用统一的`Word-Pair`标记方式建模不同类型的NER任务，并将NER统一模型称之为`W2NER`。
- 采用一种模型框架同时将三种不同类型的 NER 同时建模，即**端到端**抽取出所有的类型的实体。四种 NER 方法均可以被设计为支持统一命名实体识别的模型。

> I am having aching/eiking/ in legs and shoulder

NER任务可分为3种类型：
- 1）扁平实体；aching in legs
- 2）嵌套实体；
- 3）非连续实体；aching in shoulders

NER的相关建模方式主要包括：
1.  **序列标注**：对实体span内的每一个token进行标注，比如BIO或BIESO；  
2.  **Span标注**：对实体span的start和end进行标注，比如可采取指针网络、Token-pair矩阵建模、片段枚举组合预测等方式。

![](https://pic3.zhimg.com/80/v2-cf9f5ab3ff78ff4af50ea266540e9fc6_1440w.webp)

1.  **序列生成**：以`Seq2Seq`方式进行，序列输出的文本除了label信息，Span必须出现在原文中
  - 生成式统一建模时对解码进行限制（受限解码）。
  - `邱锡鹏`课题组就曾对NER进行统一建模，直接生成输入文本中word的索引。 [A Unified Generative Framework for Various NER Subtasks](https://aclanthology.org/2021.acl-long.451.pdf)
  - ![](https://pic4.zhimg.com/80/v2-2972568698454487c55c13b22b0cce13_1440w.webp)

本文采取的Word-Pair标记方式，不难看出，这种方式可以看作是Token-Pair一种拓展：即建模Word和Word之间的关系，主要有两种Tag标记：
-   **NNW(Next-Neighboring-Word)**: 表示当前Word下一个连接接的Word；
-   **THW(Tail-Head-Word-)**: 实体的tail-Word到head-Word的连接，并附带实体的label信息。

将 UNER 任务转化成一种**词对关系**分类任务，提出了一种新的 UNER 架构（NER as Word-Word Relation Classification），名为 W²NER。

W2NER模型将NER任务转换预测word-word(备注：中文是**字-字**)的**关系类别**，统一处理**扁平**实体、**重叠**实体和**非连续**实体三种NER任务，即一招通吃。

假设输入的句子 X 由 N 个token 或 word组成，即 `X={x1, x2, ..., xn}`, 模型对每个word pair `(xi, xj)` 中的两个word的关系类别R进行预测，其中 `R∈{None, NNW, THW-*}`。具体说明如下：
- ①、`None`：表示两个word之间没有关系，不属于同一个实体
- ②、`NNW`：即 Next-Neighboring-Word **邻接关系**，表示这两个word是在同一个实体中相邻的位置；
- ③、`THW-*`：即 Tail-Head-Word-* **头尾关系**，表示这两个word是在同一实体中，且分别是实体的结尾和开始。用THW-*来判断实体的类别和边界。

![](https://pic2.zhimg.com/80/v2-3dfcddfd68151a8551c16c3efc2e44c5_1440w.webp)

为了得到(b)中的关系，将句子按word维度构建二维矩阵，通过W2NER模型，预测word-word的关系

![](https://pic2.zhimg.com/80/v2-b7f79fce2129750c297dd4eed47e2235_1440w.webp)

通过上述的两种Tag标记方式连接任意两个Word，就可以解决如上图中各种复杂的实体抽取：（ABCD分别是一个Word）
-   a): AB和CD代表两个扁平实体；
-   b): 实体BC嵌套在实体ABC中；
-   c): 实体ABC嵌套在非连续实体ABD；
-   d): 两个非连续实体ACD和BCE；

![](https://pic1.zhimg.com/80/v2-9173e1c541ad15fd5b10a643c969b5b4_1440w.webp)

上图更清晰的展示了`扁平实体`(aching in legs)和`非连续实体`(aching in shoulders)的连接方式。

模型结构
- ![](https://pic3.zhimg.com/80/v2-42429d1a50913e637282bbfe1efda766_1440w.webp)

具体解释如下：
1.  输入的sentence经过EncodeLayer（BERT+BiLSTM）得到word\_reps，shape为\[batch\_size, cur\_batch\_max\_sentence\_length, lstm\_hidden\_size\]；
2.  将word\_reps经过Conditional Layer Normalization（简称CLN）层，得到cln；cln的shape为\[batch\_size, cur\_batch\_max\_sentence\_length, cur\_batch\_max\_sentence\_length, lstm\_hidden\_size\]，表示word pair的embedding；
3.  将word pair的distance\_embedding、所在三角区域的region\_embedding和word\_embedding按最后一个维度拼接起来，得到的conv\_inputs，shape为\[batch\_size, cur\_batch\_max\_sentence\_length, cur\_batch\_max\_sentence\_length, dist\_emb\_size + type\_emb\_size + lstm\_hidden\_size\];
4.  将conv\_inputs经过卷积层(核为1\*1的常规二维卷积 + 核为3\*3的多层空洞卷积)，得到conv\_outputs，shape为\[batch\_size, output\_height = cur\_batch\_max\_sentence\_length, output\_width = cur\_batch\_max\_sentence\_length, conv\_hidden\_size \* 3\]，这里的3表示空洞卷积的层数；
5.  将卷积层的输出conv\_outputs经过CoPredictor层(由Biaffine + MLP组成)，得到output，output的shape为\[batch\_size, cur\_batch\_max\_sentence\_length, cur\_batch\_max\_sentence\_length, label\_num\]，label\_num表示word-word关系类别的个数；

源码解释 

从输入句子到预测句子的word pair中两个word的关系类别，整个过程如下图所示：
- ![](https://pic3.zhimg.com/80/v2-5e20041f31a218ba364ccea97f93fbc2_1440w.webp)

1、从输入句子得到BertModel所需的bert\_inputs/input\_ids和attention\_mask

整个过程如下图所示：
- ![](https://pic2.zhimg.com/80/v2-1853a124b64c38f526977c116da231f9_1440w.webp)

在data\_loader.load\_data\_bert的process\_bert中，将输入的句子，经过一系列切分处理后，得到piece\_list，再将piece\_list转换为对应id组成\_bert\_inputs。然后在\_bert\_inputs前后分别加上CLS\_token\_id和SEP\_token\_id，得到新的\_bert\_inputs。再执行bert\_inputs.append(\_bert\_inputs)得到bert\_inputs。

在data\_loader.collate\_fn中，对bert\_inputs进行padding，得到bert模型所需的输入bert\_inputs/input\_dis。

再根据padding后的bert\_inputs，得到attention\_mask，具体方案是通过比较bert\_inputs中的token\_id是否不等于0。即padding 0的位置，token\_id为0，则attention\_mask中对应的值为0；非padding 0的位置，attention\_mask的值为1；

2、Conditional Layer Normalization

通过conditional layer normalization获取word pair中word-word的embedding。
- ![](https://pic1.zhimg.com/80/v2-13ffaebf0ff17e5ee51879d5b8e5726c_1440w.webp)

更多解读见原文
- [2022 统一NER SOTA模型【W2NER】详解](https://zhuanlan.zhihu.com/p/546602235)

SOTA结果
- **1、中文NER取得了SOTA**
  - ![](https://pic1.zhimg.com/80/v2-6a0444f2d2d553e73ca329cb1800b0b0_1440w.webp)

- **2、英文NER取得了SOTA**
  - ![](https://pic3.zhimg.com/80/v2-ae565e7303587b81588994bfdbcab91e_1440w.webp)
  - ![](https://pic4.zhimg.com/80/v2-4ca4e5864d0696f6ef5c0239ebb65187_1440w.webp)

### Google NER 本地

Google [ml-kit](https://developers.google.com/ml-kit?hl=zh-cn) 工具包提供多种识别能力：OCR、人脸检测、图片加标签、目标检测跟踪、姿势检测以及图片分类
- [中文文档](https://mlkit.cn/#/?id=ml-kit-%e4%b8%ad%e6%96%87%e6%96%87%e6%a1%a3)

NLP类API功能： 语种识别、翻译、智能回复、实体提取
- [NER](https://developers.google.com/ml-kit/language/entity-extraction?hl=zh-cn), 支持功能如下：

| 实体 | 示例 |
| --- | --- |
| **地址** | 马萨诸塞州剑桥市第三大街 350 号 |
| **日期**-时间 | 2019 年 9 月 29 日，明天明天下午 6 点见面 |
| 电子**邮件地址** | entity-extraction@google.com |
| **航班号**（仅限 IATA 航班代码） | LX37 |
| IBAN | CH52 0483 0000 0000 0000 9 |
| ISBN（仅限版本 13） | 978-1101904190 |
| **货币/货币**（仅限阿拉伯数字） | 12 美元、25 美元 |
| 付款 / **信用卡** | 4111 1111 1111 1111 |
| **电话号码** | (555) 225-3556  
12345 |
| **跟踪编号**（标准化国际格式） | 1Z204E380338943508 |
| **网址** | www.google.com<br>https://zh.wikipedia.org/wiki/Platypus |

API 侧重于精确率而非识别。为了确保检测准确性，特定实体的某些实例可能漏掉。大多数实体都可以**跨语言**和语言区域进行检测

## 工程实现

实体识别NER
- 3-1. [Bert-MRC](https://github.com/jasoncao11/nlp-notebook/tree/master/3-1.Bert-MRC)
- 3-2. [Bert-CRF](https://github.com/jasoncao11/nlp-notebook/tree/master/3-2.Bert-CRF)
- 【2023-9-14】NER 序列标注方法实现总结：[sequence_labeling](https://github.com/Tongjilibo/bert4torch/blob/master/examples/sequence_labeling), 包含 [task_sequence_labeling_ner_W2NER](https://github.com/Tongjilibo/bert4torch/blob/master/examples/sequence_labeling/task_sequence_labeling_ner_W2NER.py)


### 实体正则表达式

手机号码

```py
# 精确匹配11位
/^(13[0-9]|14[01456879]|15[0-35-9]|16[2567]|17[0-8]|18[0-9]|19[0-35-9])\d{8}$/
# 模糊匹配11位
/^1[3456789]\d{9}$/
```

座机号码

```sh
# 座机号码
/^(0\d{2,3})-?(\d{7,8})$/
# 国内电话号码(0511-4405222、021-87888822)：
\d{3}-\d{8}|\d{4}-\d{7}
```

人名
- 其中，{2,4}控制可输入字符长度

```sh
# 人名--中文
/^[\u4e00-\u9fa5]{2,4}$/  
# 中文字符的正则表达式：
[\u4e00-\u9fa5]
```

邮箱/邮编

```sh
/^\w+@[a-z0-9]+\.[a-z]{2,4}$/
/^\w+([-+.]\w+)*@\w+([-.]\w+)*\.\w+([-.]\w+)*$/
/^([a-zA-Z\d])(\w|\-)+@[a-zA-Z\d]+\.[a-zA-Z]{2,4}$/
# 腾讯QQ号：(腾讯QQ号从10000开始)
[1-9][0-9]{4,} 
# 中国邮政编码：
[1-9]\d{5}(?!\d) (中国邮政编码为6位数字)
```

身份证号码

```sh
# 简单校验
/(^\d{15}$)|(^\d{18}$)|(^\d{17}(\d|X|x)$)/
# 身份证号(15位、18位数字)，最后一位是校验位，可能为数字或字符X

/^[1-9]\d{7}((0\d)|(1[0-2]))(([0|1|2]\d)|3[0-1])\d{3}$|^[1-9]\d{5}[1-9]\d{3}((0\d)|(1[0-2]))(([0|1|2]\d)|3[0-1])\d{3}([0-9]|X)$/

# 港澳身份证
/^([A-Z]\d{6,10}(\w1)?)$/
```

网址

```sh
/^([hH][tT]{2}[pP]:\/\/|[hH][tT]{2}[pP][sS]:\/\/|www\.)(([A-Za-z0-9-~]+)\.)+([A-Za-z0-9-~\/])+$/

# 域名
[a-zA-Z0-9][-a-zA-Z0-9]{0,62}(\.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+\.?
# 内部url，两种
[a-zA-z]+://[^\s]* 
^http://([\w-]+\.)+[\w-]+(/[\w-./?%&=]*)?$
# IPv4地址：
((2(5[0-5]|[0-4]\d))|[0-1]?\d{1,2})(\.((2(5[0-5]|[0-4]\d))|[0-1]?\d{1,2})){3}
```

日期

```sh
# 日期格式：
^\d{4}-\d{1,2}-\d{1,2}
# 一年的12个月(01～09和1～12)：
^(0?[1-9]|1[0-2])$
# 一个月的31天(01～09和1～31)：
^((0?[1-9])|((1|2)[0-9])|30|31)$
```

货币
- 钱的输入格式有四种钱: “10000.00” 和 “10,000.00”, 和没有 “分” 的 “10000” 和 “10,000”：

```sh
^[1-9][0-9]*$
```

正则可视化展示
- 工具地址：[ToolTT在线工具箱](https://tooltt.com/regulex/)
- [wangwl](https://wangwl.net/static/projects/visualRegex#prefix=Z&source=ZidNum)

<iframe frameborder="0" width="846" height="650" src="https://tooltt.com/regulex/r.html#!embed=true&flags=&re=%5E(13%5B0-9%5D%7C14%5B01456879%5D%7C15%5B0-35-9%5D%7C16%5B2567%5D%7C17%5B0-8%5D%7C18%5B0-9%5D%7C19%5B0-35-9%5D)%5Cd%7B8%7D%24"></iframe>

### 地址识别


[addressparser](https://github.com/shibing624/addressparser)
- PyPI version Downloads MIT Python3 Python2.7 GitHub issues Wechat Group
- 中文地址提取工具，支持中国三级区划地址（省、市、区）提取和级联映射，支持地址目的地热力图绘制。
- 适配python2和python3。
- [Demo](https://www.mulanai.com/product/address_extraction/)

```py
#pip install addressparser
location_str = ["徐汇区虹漕路461号58号楼5楼", "泉州市洛江区万安塘西工业区", "朝阳区北苑华贸城"]
import addressparser
df = addressparser.transform(location_str)
#df = addressparser.transform(location_str, pos_sensitive=True) # 带位置索引
#df = addressparser.transform(location_str, cut=True) # 切词模式, jieba
print(df)
# 绘制热力图，pyecharts
from addressparser import drawer
drawer.echarts_draw(processed, "echarts.html")
```

效果

```s
["徐汇区虹漕路461号58号楼5楼", "福建泉州市洛江区万安塘西工业区"]
#        ↓ 转换
|省    |市   |区    |地名                |
|上海市|上海市|徐汇区|虹漕路461号58号楼5楼  |
|福建省|泉州市|洛江区|万安塘西工业区        |
```

数据集：中国行政区划地名

数据源：
- 爬取自[国家统计局](http://www.stats.gov.cn/tjsj/tjbz/tjyqhdmhcxhfdm/)，[中华人民共和国民政局全国行政区划查询平台](http://xzqh.mca.gov.cn/map)
- 数据文件存储在：[addressparser/resources/pca.csv](https://github.com/shibing624/addressparser/blob/master/addressparser/resources/pca.csv)，数据为[2021年统计用区划代码和城乡划分代码](http://www.stats.gov.cn/tjsj/tjbz/tjyqhdmhcxhfdm/2021/index.html)（截止时间：2021-10-31，发布时间：2021-12-30）





# 结束