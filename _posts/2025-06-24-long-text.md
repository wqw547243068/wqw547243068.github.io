---
layout: post
title:  长文本创作专题
date:   2025-06-24 19:10:00
categories: 大模型
tags: 长本文 创作 rag 搜索
excerpt: 大模型辅助长文本创作有哪些问题，怎么解决
mathjax: true
permalink: /long_text
---

* content
{:toc}


# 长文本创作



## 问题

长文本创作的本质是多轮对话

传统AI写作模型，如基于Transformer的GPT系列，虽然在**短文本**生成方面表现出色，但在处理**长文本**时，往往会遇到“**上下文遗忘**”的问题，导致文章前后**逻辑不一致**，**主题偏移**。


## 方案

### LongForm

论文《[The LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction](https://arxiv.org/abs/2304.08460)》介绍了基于 C4 和 Wikipedia 等已有语料库的人工创作文档集合以及这些文档的指令，从而创建了一个适合长文本生成的指令调优数据集。

### Re^3

【2023-2-14】[Generating Longer Stories With Recursive Reprompting and Revision](https://arxiv.org/pdf/2210.06774.pdf), Meta 田渊栋写小说
- We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, **long-range plot coherence and relevance** are more central challenges here. We propose the `Recursive Reprompting and Revision` framework ($Re^3$) to address these challenges


### RecurrentGPT（输出不受限）

【2023-5-30】[ChatGPT能写长篇小说了，ETH提出RecurrentGPT实现交互式超长文本](https://www.toutiao.com/article/7238442944003310084)
- 苏黎世联邦理工和波形智能的团队发布了 RecurrentGPT，一种让大语言模型 (如 ChatGPT 等) 能够模拟 RNN/LSTM，通过 Recurrent Prompting 来实现交互式**超长**文本生成，让利用 ChatGPT 进行长篇小说创作成为了可能。
- [论文地址](https://arxiv.org/abs/2305.13304)
- [项目地址](https://github.com/aiwaves-cn/RecurrentGPT)
- 在线 Demo: [长篇小说写作](https://www.aiwaves.org/recurrentgpt), [交互式小说](https://www.aiwaves.org/interactivefiction)
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/a679b4e41e0d483bae2b1ac35ae2da63~noop.image?_iz=58558&from=article.pc_detail&x-expires=1686034283&x-signature=1GLOG8XAvQwzzXbm0v1ip16bz5Q%3D)

Transformer 大语言模型最明显的限制之一: 输入和输出的**长度限制**。
- 虽然输入端的长度限制可以通过 **VectorDB** 等方式缓解
- 输出内容的长度限制始终是长内容生成的关键障碍。

为解决这一问题，过去很多研究试图使用基于向量化的 State 或 Memory 来让 Transformer 可以进行**循环**计算。这样的方法虽然在长文本建模上展现了一定的优势，但是却要求使用者拥有并可以**修改模型的结构和参数**，这在目前闭源模型遥遥领先的大语言模型时代中是不符合实际的。

RecurrentGPT 则另辟蹊径，利用大语言模型进行**交互式**长文本生成的首个成功实践。它利用 ChatGPT 等大语言模型理解自然语言指令的能力，通过自然语言模拟了循环神经网络（RNNs）的循环计算机制。
- 每一个时间步中，RecurrentGPT 会接收上一个时间步生成的内容、最近生成内容的摘要（短期记忆），历史生成内容中和当前时间步最相关的内容 (长期记忆)，以及一个对下一步生成内容的梗概。RecurrentGPT 根据这些内容生成一段内容，更新其长短时记忆，并最后生成几个对下一个时间步中生成内容的规划，并将当前时间步的输出作为下一个时间步的输入。这样的循环计算机制打破了常规Transformer 模型在生成长篇文本方面的限制，从而实现任意长度文本的生成，而不遗忘过去的信息。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/f1bd9be64d144e18914652db4ce325c8~noop.image?_iz=58558&from=article.pc_detail&x-expires=1686034283&x-signature=4WMRfq0FjPeJxmK0ujy7roS3sbA%3D)



### WriteHERE

WriteHERE 引入**异质递归规划**（Heterogeneous Recursive Planning）技术，有效地解决了这个问题。

异质递归规划核心思想：
- 将写作任务分解为`检索`（Retrieval）、`推理`（Reasoning）和`写作`（Composition）三种异构任务。

每种任务具有不同的信息流模式，例如
- **检索**：从外部获取信息
- **推理**：进行逻辑分析
- **写作**：生成文本

这些任务之间通过`有向无环图`（DAG）建立依赖关系，系统根据任务状态动态调整执行顺序，确保任务按逻辑顺序完成。

这种任务分解和动态调度的方式，使 WriteHERE 能够更好地管理长文本的结构和内容，保持文章的**连贯性**和**一致性**。

同时，WriteHERE 还支持开发者自由调用异构Agent，这意味着用户可以根据自己的需求，定制不同的写作Agent，进一步提升写作效率和质量。

WriteHERE 核心功能：
- 单次生成**超长文本**：WriteHERE 支持生成超过4万字、100页的专业报告，能够满足复杂写作需求。相比于传统的AI写作模型，WriteHERE在长文本生成方面具有显著优势。
- 创意与技术内容生成：WriteHERE 不仅可以生成创意故事、小说等文学作品，还可以生成技术报告、行业分析等专业文档。这使得WriteHERE具有广泛的应用前景。
- **动态信息检索**：在写作过程中，WriteHERE能够实时搜索相关信息，并将这些信息整合到文章中。这大大提高了写作效率和质量。
- **风格一致性**：WriteHERE能够保持一致的写作风格和内容连贯性，避免了传统AI写作模型中常见的风格漂移问题。
- **写作过程可视化**：WriteHERE基于任务依赖图展示写作流程，使得用户可以清晰地了解文章的结构和生成过程。

WriteHERE 技术原理

- **异构任务分解**：WriteHERE将写作过程解构为检索、推理和写作三种异构任务。每种任务具有独特的信息流模式，例如检索任务从外部获取信息，推理任务进行逻辑分析，写作任务生成文本。任务基于递归分解为子任务，直至分解为可直接执行的原子任务。
  - 检索任务：负责从外部知识库或互联网上检索相关信息。检索任务可以使用各种信息检索技术，如关键词搜索、语义搜索等。检索到的信息将作为推理任务和写作任务的输入。
  - 推理任务：负责对检索到的信息进行逻辑分析和推理。推理任务可以使用各种推理技术，如知识图谱推理、逻辑推理等。推理结果将作为写作任务的输入。
  - 写作任务：负责根据推理结果生成文本。写作任务可以使用各种自然语言生成技术，如基于Transformer的语言模型、基于规则的生成方法等。
- 状态化层次调度算法：任务依赖关系用有向无环图（DAG）表示，每个任务具有激活、挂起、静默三种状态。系统根据任务状态动态调整执行顺序，确保任务按逻辑顺序完成，支持实时反馈和调整。
  - 激活状态：表示任务正在执行中。
  - 挂起状态：表示任务已暂停执行，等待其他任务完成后才能继续执行。
  - 静默状态：表示任务已完成或尚未开始执行。
  - 系统会根据任务之间的依赖关系和任务状态，动态调整任务的执行顺序，确保任务按逻辑顺序完成。例如，如果一个写作任务依赖于一个检索任务的结果，那么系统会先执行检索任务，待检索任务完成后，再执行写作任务。
- **数学形式化框架**：将写作系统抽象为五元组，Agent内核、内部记忆、外部数据库、工作空间和输入输出接口。基于数学形式化定义写作规划问题，确保每个任务的可执行性和最终目标的达成。
  - Agent内核：负责执行各种任务，如检索、推理和写作。
  - 内部记忆：用于存储Agent的知识和经验。
  - 外部数据库：用于存储外部信息，如知识库、互联网等。
  - 工作空间：用于存储任务的中间结果。
  - 输入输出接口：用于与外部环境进行交互。
  - 通过数学形式化定义写作规划问题，可以确保每个任务的可执行性和最终目标的达成。例如，可以定义一个目标函数，用于衡量文章的质量和连贯性，然后通过优化算法，找到最优的任务执行顺序和参数配置，使得目标函数达到最大值。

WriteHERE 应用场景：
- 小说创作：生成情节完整、角色丰富的长篇小说，支持创意写作和动态调整情节。例如，可以利用WriteHERE生成一部科幻小说，让AI自动生成故事情节、人物设定和环境描写，从而大大提高创作效率。
- 技术报告：撰写结构化的技术报告，整合数据和逻辑推理。例如，可以利用WriteHERE生成一份关于人工智能技术发展趋势的报告，让AI自动检索相关数据、分析技术发展趋势，并生成结构化的报告。
- 行业分析：生成涵盖行业趋势、市场分析的专业报告。例如，可以利用WriteHERE生成一份关于新能源汽车行业的市场分析报告，让AI自动收集市场数据、分析竞争格局，并生成专业的市场分析报告。
- 学术论文：辅助撰写学术论文，整合文献并生成规范结构。例如，可以利用WriteHERE生成一篇关于深度学习算法的学术论文，让AI自动检索相关文献、分析算法原理，并生成符合学术规范的论文。
- 政策文件：撰写政策文件和白皮书，生成权威性和逻辑性强的文本。例如，可以利用WriteHERE生成一份关于环境保护政策的白皮书，让AI自动收集相关数据、分析政策影响，并生成权威性和逻辑性强的政策文件。

## 案例

### 讲稿撰写


讲稿代写任务要点：
- 1 遵循讲稿主题
- 2 文章逻辑合理
- 3 引用数据正确
- 4 语言表述符合指定风格

分析
- 1 或 4 问题不大
- 2 需要结合主题、外部数据设计文章框架，逻辑严密
- 3 涉及知识库查询、在线搜索，基本信息常规RAG能搞定，但一旦有总结、对比、深度分析，就触发“关联≠因果”的困境，此时，得用更高级的RAG形式，结合agent设计模式

以上这些，无法在一次llm调用中完成

### 写小说






# 结束