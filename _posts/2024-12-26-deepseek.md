---
layout: post
title:  DeepSeek 深度求索
date:   2024-12-26 18:00:00
categories: 大模型
tags: deepseek moe 多模态 
excerpt: 大模型领域扫地僧：DeepSeek（深度求索）介绍，各个版本模型总结
mathjax: true
permalink: /deepseek
---

* content
{:toc}

# DeepSeek

[深度求索（DeepSeek）顶尖人才招聘](https://mp.weixin.qq.com/s/iKVBXclSNporHl-EmaPUkw)

## DeepSeek 介绍

[揭秘DeepSeek:一个更极致的中国技术理想主义故事](https://zhuanlan.zhihu.com/p/720160943)

位于杭州的量化投资基金`幻方`，一家抵达过千亿规模的“顶级基金”
- 2019年，幻方量化成立**AI公司**，自研深度学习训练平台“`萤火一号`”总投资近2亿元，搭载 1100块GPU；
- 2021年，“萤火二号”的投入增加到10亿元，搭载了约1万张英伟达A100显卡。幻方在亚太第一个拿到 A100卡，成为全国少数几家囤有上万张 A100 GPU的机构。
  - 国内拥有超过1万枚GPU的企业不超过**5家**。除几家头部大厂，另一家就是幻方。
  - 1万枚英伟达A100芯片是做自训大模型的算力门槛: 起步就要5000万美金，训练1次需要上千万美金，非百亿美金公司其实很难持续跟进。
  - 成立仅六年的幻方，抵达千亿规模，并被称为“量化四大天王”之一
- 幻方的成长奥秘归结为“选用了一批没有经验但有潜能的人，以及有一个可以让创新发生的组织架构和企业文化”
- 2023年4月11日, 宣布做大模型, DeepSeek （深度求索） 成立
  - 引用了法国新浪潮导演`特吕弗`告诫青年导演的一句话：“务必要疯狂地**怀抱雄心**，且还要疯狂地**真诚**。”
- 2023年5月，才把做大模型的团队独立出来，成立`深度求索`公司，专注于做真正人类级别的人工智能。目标不只是**复刻ChatGPT**，还要去研究和揭秘通用人工智能（AGI）的更多未知之谜。

自从ChatGPT时刻以来，业界弥漫一股“唯GPU论”的情绪，上万张卡加几亿美元，被认为是做大模型的门槛。

`深度求索`创立之初就宣布做AGI，会专注在大模型上，先从**语言大模型**做起，然后再做**视觉**和**多模态**等。
- 从2024年初推出首个大型语言模型 `DeepSeek LLM`，只能对标`GPT-3.5`
- 直到2024年底推出硬碰`GPT-4o`的`DeepSeek V3`，并且进军多模态、推理模型。

中国7家大模型创业公司中，[DeepSeek](https://www.deepseek.com/)（深度求索）最不声不响，但又总能以出其不意的方式被人记住。
- 一年前，这种出其不意源自背后的量化私募巨头`幻方`，大厂外唯一储备**万张**A100芯片的公司
- 一年后，则来自引发中国大模型**价格战**的源头。



### CEO 梁文锋

DeepSeek创始人`梁文锋` 浙江大学电子工程系人工智能方向, 从`幻方时代` 就在幕后**潜心研究技术**的80后创始人
- ![](https://cdn.beekka.com/blogimg/asset/202501/bg2025010222.webp)

在 DeepSeek 时代，依旧延续低调作风，和所有研究员一样，每天 “**看论文，写代码，参与小组讨论**”。

`梁文锋`是当下中国AI界非常罕见
- “兼具强大的**infra工程**能力和**模型研究**能力，又能**调动资源**”
- “既可以从高处做精准判断，又可以在细节上强过一线研究员”的人，他拥有“令人恐怖的学习能力”，同时又“完全不像一个老板，而更像一个极客”。

他是少有把“**是非观**”置于“**利害观**”之前，并提醒看到时代惯性，把“原创式创新”提上日程的人。

[DeepSeek 创始人梁文锋访谈实录](https://zhuanlan.zhihu.com/p/16146997445)
- 【2023-5-24】[疯狂的幻方：一家隐形AI巨头的大模型之路](https://mp.weixin.qq.com/s/Cajwfve7f-z2Blk9lnD0hA)
- 【2024-7-17】[揭秘DeepSeek:一个更极致的中国技术理想主义故事](https://mp.weixin.qq.com/s/r9zZaEgqAa_lml_fOEZmjg)
- 【2024-11-27】英国程序员西蒙·威利森（Simon Willison）专访: [Deepseek: The Quiet Giant Leading China’s AI Race](https://www.chinatalk.media/p/deepseek-ceo-interview-with-chinas)

中国顶级研究者的视野和抱负
- （1）我们做的不是`生成式 AI`，而是通用`人工智能 AGI`。前者只是后者的必经之路，AGI 会在有生之年实现。
- （2）**任何** AI 公司（短期内）都没有碾压对手的技术优势
  - 因为有 OpenAI 指路，又都基于公开论文和代码，大厂和创业公司都会做出自己的大语言模型。
- （3）在颠覆性技术面前，**闭源形成的护城河是短暂的**。即使 OpenAI 闭源，也无法阻止被别人赶超。我们把价值沉淀在团队上，同事在这个过程中得到成长，积累很多know-how，形成可以**创新的组织和文化，才是护城河**。
- （4）我们不会闭源。先有一个强大的技术生态更重要。
- （5）当前是**技术创新**的爆发期，而不是**应用**的爆发期。
  - 大模型应用门槛会越来越低，创业公司在未来20年任何时候下场，也都有机会。
- （6）很多中国公司**习惯别人做技术创新，自己拿过来做应用变现**，等着`摩尔定律`从天而降，躺在家里18个月就会出来更好的硬件和软件。
  - 我们的出发点不是趁机赚一笔，而是走到技术前沿，去推动整个生态发展。中国也要逐步成为贡献者，而不是一直搭便车。
- （7）大部分中国公司习惯 **跟进**，而不是**创新**。中国创新缺的不是**资本**，而是缺乏**信心**，以及不知道怎么组织高密度的人才。
  - 我们没有海外回来的人，都是本土制造。前50名顶尖人才可能不在中国，但也许我们能自己打造这样的人。
- （8）我们每个人对于卡和人的调动不设上限。
  - 如果有想法，每个人随时可以调用训练集群的卡无需审批。
  - 同时因为不存在层级和跨部门，也可以灵活调用所有人，只要对方也有兴趣。
- （9）选人标准一直都是**热爱**和**好奇心**，所以很多人会有一些奇特经历，对做**研究的渴望**远超对**钱的在意**。
- （10）我们在做最难的事。最吸引顶级人才的是去**解决世界上最难的问题**。
  - 其实，顶尖人才在中国被低估。因为整个社会层面的硬核创新太少了，没有机会被识别出来。
- （11）中国产业结构的调整会更依赖**硬核技术创新**。很多人发现过去赚快钱很可能来自时代运气，现在赚不到了，就会更愿意俯身去做真正的创新。
- （12）我是八十年代在广东一个五线城市长大的，父亲是小学老师，九十年代，广东赚钱机会很多，当时有不少家长觉得读书没用。
  - 但现在回去看，观念都变了。因为钱不好赚了，连开出租车的机会可能都没了。
  - 一代人的时间就变了。以后硬核创新会越来越多，因为整个社会群体需要被事实教育。
  - 当这个社会让硬核创新的人功成名就，群体性想法就会改变。
  - 只是还需要一堆**事实**和一个**过程**。


### 团队

OpenAI前政策主管、Anthropic联合创始人`Jack Clark`: 
- DeepSeek “雇佣了一批高深莫测的奇才”，还认为中国制造的大模型，“将和无人机、电动汽车一样，成为不容忽视的力量。”

#### 团队情况

梁文锋：
- 并没有什么高深莫测的奇才，都是一些Top高校的`应届毕业生`、没毕业的博四、博五`实习生`，还有一些毕业才几年的年轻人。都是本土 —— 达摩院背景的`罗福莉`  参考 [罗福莉：天才AI少女“祛魅”记](https://zhuanlan.zhihu.com/p/515631834)
  - 保研北大、在顶会顶刊发文章、拿遍大厂offer、进入阿里达摩院、转行跳槽知名私募公司…
  - 2019年，一位北大硕士，因在NLP国际顶会 ACL 上发表 8 篇论文（其中2篇一作），曾登上知乎热搜
  - 在达摩院，罗福莉主导开发的跨语言预训练模型VECO，成为深度语言模型体系AliceMind八大模型之一，并被顶会ACL2021录用，她也在AliceMind集体开源中挑起大梁。AliceMind 登顶多模态权威榜单VQA Challenge 2021，并在阿里内部数十个核心业务落地，日均调用50亿次，活跃场景超过200个，其中不乏大家熟悉的天猫精灵智能音响等。


#### 招人标准

选人标准: 一直都是**热爱**和**好奇心**，所以很多人会有一些奇特的经历。对做**研究**的渴望远超对**钱**的在意。
- 对顶级人才吸引最大的，肯定是去**解决世界上最难的问题**。其实，顶尖人才在中国是被低估的。因为整个社会层面的**硬核创新太少**了，使得他们没有机会被识别出来。

大模型招人，必卡的条件是什么？

梁文锋：
- 热爱，扎实的基础能力。
- 其他都没那么重要。

投资人说很多适合的人才可能只在OpenAI、FacebookAI Research 等巨头的AI lab里。

你们会去海外挖这类人才吗？

梁文锋：
- 如果追求**短期**目标，找现成**有经验**的人是对的。
- 但如果看**长远**，经验就没那么重要，基础能力、创造性、热爱等更重要。

从这个角度看，国内合适的候选人就不少。

为什么经验没那么重要？

梁文锋：
- 不一定是做过这件事的人才能做这件事。
- 幻方招人有条原则：看**能力**，而不是看**经验**。核心技术岗位，基本以应届和毕业一两年的人为主。

创新业务上，经验是阻碍吗？

梁文锋：
- 做一件事，有经验的人会不假思索告诉你，应该这样做
- 但没有经验的人，会反复摸索、很认真去想应该怎么做，然后找到一个符合当前实际情况的解决办法。
- 核心团队，连我自己，一开始都没有量化经验

什么是好奇心？对 AI 能力边界的好奇。
- 对很多行外人来说，ChatGPT 这波浪潮冲击特别大；
- 但对行内人来说
  - 2012年 AlexNet 带来的冲击已经引领一个新的时代。AlexNet 的错误率远低于当时其他模型，复苏了沉睡几十年的神经网络研究。
  - 虽然具体技术方向一直在变，但模型、数据和算力这三者的组合是不变的
  - 特别是当 2020 年 OpenAI 发布 GPT3 后，方向很清楚，需要大量算力；
  - 但即便 2021 年，我们投入建设萤火二号时，大部分人还是无法理解。

Attention 架构提出多年来，几乎未被成功改过，更遑论大规模验证；对模型结构进行创新，没有路径可依，要经历很多失败，时间、经济成本都耗费巨大。

而 DeepSeek 成功了，它是 7家中国大模型创业公司中，唯一一家放弃“**既要又要**”路线，至今专注研究和技术，未做toC应用的公司，也是唯一一家**未全面考虑商业化**，坚定选择开源路线甚至都没融过资的公司。
- 公司 60 个人, 50 个技术, 10 个工程


## DeepSeek 成就


### 扫地僧

被AI连续轰炸的5月，DeepSeek一跃成名。起因是发布的一款名为`DeepSeek V2`开源模型，提供了一种史无前例的**性价比**：
- 推理成本被降到每百万token仅 1块钱，约等于 Llama3 70B 1/7，GPT-4 Turbo 1/70。

DeepSeek 被迅速冠以“AI界拼多多”之称的同时，字节、腾讯、百度、阿里等大厂也按耐不住，纷纷降价。中国大模型价格战由此一触即发。

成见：
- 美国更擅长从0-1的**技术**创新，而中国更擅长从1-10的**应用**创新。

很多VC对做研究有顾虑，有退出需求，希望尽快做出产品**商业化**，而按照优先做研究的思路，很难从VC那里获得融资。但我们有算力和一个工程师团队，相当于有了一半筹码。
> “一件激动人心的事，或许不能单纯用钱衡量。”

显卡通常会以20%的速度在折损？
- 电费和维护费用其实是很低的，这些支出每年只占硬件造价的1%左右。
- 人工成本不低，但人工成本也是对未来的投资，是公司最大的资产。

事实：
> 与很多大厂烧钱补贴不同，DeepSeek 有利润

DeepSeek 对模型架构进行了**全方位创新**。
- 提出一种崭新的`MLA`（一种**多头潜在注意力机制**）架构，把显存占用降到了过去最常用的`MHA`架构的**5%-13%**
- 独创 `DeepSeekMoESparse` 结构，把计算量降到极致，所有这些最终促成了成本的下降。



### 如何创新

如何让创新真正发生
> “创新往往都是自己产生的，不是刻意安排的，更不是教出来的”

很多家试图模仿你们，却没有成功？

梁文锋：
- 因为仅凭人才这一点，不足以让创新发生。它需要和公司的**文化**和**管理**相匹配。
- 第一年他们什么都做不出来，第二年才开始有点成绩。
- 但我们的考核标准和一般公司不一样: 没有 KPI，也没有所谓的任务。

量化领域，别人一般看重客户**下单量**，我们的销售卖多少和提成不是一开始就算好的，而会更鼓励销售去发展自己的圈子，认识更多人，产生更大影响力。

因为，一个让客户信任的、正直的销售，可能在短时间内做不到让客户来下单，但可以让你觉得他是个靠谱的人。

怎么让新人进入状态?

梁文锋：
- 交给他重要的事，并且不干预他。让他自己想办法，自己发挥。
- 一家公司的基因是很难被模仿的。比如说招没有经验的人，怎么判断他的潜力，招进来之后如何才能让他成长，这些都没法直接模仿。

如何打造一个创新型组织？

梁文锋：
- 创新需要尽可能少的干预和管理，让每个人有自由发挥的空间和试错机会。
- 创新往往都是**自己产生**的，不是**刻意安排**的，更不是教出来的。

如何确保一个人做事是有效率？

梁文锋：
- 招人时确保**价值观一致**，然后通过企业文化来确保步调一致。
- 当然，我们并没有一个成文的企业文化，因为**所有成文东西，又会阻碍创新**。
- 更多时候，是管理者的**以身示范**，遇到一件事，你如何做决策，会成为一种准则。


真正的决定力量往往不是一些现成的**规则**和**条件**，而是一种适应和调整变化的能力。

很多大公司的组织结构已经不能快速响应和快速做事，而且他们很容易让之前的经验和惯性成为束缚，而这波AI新浪潮之下，一定会有一批新公司诞生。

> “创新就是昂贵且低效的，有时候伴随着浪费。”

大模型是一件无休止投入的事，付出的代价顾虑吗？

梁文锋：
- 创新就是昂贵且低效的，有时候伴随着浪费。
- 所以，经济发展到一定程度之后，才能够出现创新。很穷的时候，或者不是创新驱动的行业，成本和效率非常关键。看OpenAI也是烧了很多钱才出来。


## 接入

【2024-12-27】DeepSeek 接入体验方式
- Web形式: [DeepSeek](https://www.deepseek.com/) 免费使用
  - 默认版本: 
  - 联网搜索版本: 
  - 深度思考版本: R1, 对标 OpenAI o1
- API形式: [收费](https://api-docs.deepseek.com/zh-cn/quick_start/pricing)
  - 输入价格: 1 元/百万tokens, 输出价格 2 元/百万tokens
  - 新用户赠送10元

注
- 【2024-12-26】全面升级为 DeepSeek V3

```py
# Please install OpenAI SDK first: `pip3 install openai`

from openai import OpenAI

api_key = 'sk-7284******'

client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

sys_prompt = 'You are a helpful assistant'
sys_prompt = '你是一名数学家'
question = '解此微分方程 xdx+ydy=-xdy+ydx'

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": question},
    ],
    stream=False
)

print(question)
print(response.choices[0].message.content)
```


## 模型


### DeepSeek V2

【2024-5-7】[DeepSeek-V2](https://www.deepseek.com/zh) 全球最强开源通用MoE模型
- DeepSeek-V2 基于 2 千亿 MoE 模型底座，领先性能，超低价格，越级场景体验，已在对话官网和API全面上线
- 技术报告: [浅读 DeepSeek-V2 技术报告](https://zhuanlan.zhihu.com/p/696292840)
- 仓库和技术报告地址：[DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)

DeepSeek-V2 在 DeepSeek 上改进，但并没有沿用主流的“类LLaMA的Dense结构”和“类Mistral的Sparse结构”，而是对Transformer架构中的自注意力机制进行了全方位创新，提出了`MLA`（Multi-head Latent Attention）结构，并使用了MoE技术进一步将计算量降低，大幅提高了推理效率。

特点
- 独创 MLA 结构
- 稀疏结构 DeepSeek-MoE
- 推理成本降低近百倍
- LMSYS榜单中，位列开源模型第一


DeepSeek-V2 包含 236B参数，每个Token激活2.1B参数，支持长达 128K 的上下文长度。
- 与DeepSeek 67B相比，DeepSeek-V2 在性能上取得了显著提升，节省了42.5%的训练成本，减少了93.3%的KV缓存，并将最大生成吞吐量提高到了5.76倍。

深度求索将该 DeepSeek-V2 模型已完全上线至平台服务用户，DeepSeek-V2 API也是物美价廉。并且秉持着最开放的开源精神，深度求索将这次的DeepSeek-V2模型和论文也将完全开源，免费商用。

#### 模型结构

模型结构
- ![](https://pic1.zhimg.com/80/v2-9c998d8bc062c10483e38606f4839814_1440w.webp)



### DeepSeek Coder 


2023年11月，DeepSeek Coder V1发布

2024年6月，DeepSeek Coder V2 全球最强代码开源模型
- 全球首个超越 GPT4-Turbo 的开源代码模型
- BigCodeBench 6月榜单中第二

### DeepSeek VL

自然语言到多模态初探


### DeepSeek R1

【2024-11-20】DeepSeek R1 详见站内专题: [o1](o1)


### DeepSeek V3

DeepSeek V3发布即完全开源，直接用了53页论文把训练细节和盘托出
- 体验地址：[DeepSeek](chat.deepseek.com)
- 技术报告地址：[DeepSeek_V3.pdf](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)
- 抱抱脸开源地址：[DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)
- 参考链接：[公众号文章](https://mp.weixin.qq.com/s/iFZOQsUNkpkXPDvOkE99wQ)
- 【2024-12-26】[国产之光DeepSeek把AI大佬全炸出来了！671B大模型训练只需此前算力1/10，细节全公开](https://mp.weixin.qq.com/s/uho6L_V2IybmUmH8jXmRmw)

DeepSeek V3 是一个参数量为**671B**的MoE模型，激活37B，在14.8T高质量token上进行了预训练。

DeepSeek V3 整个训练过程仅用了不到280万个GPU小时，相比之下，Llama 3 405B的训练时长是3080万GPU小时（p.s. GPU型号也不同）。
- 训练671B的DeepSeek V3的成本是557.6万美元（约合4070万人民币），而只是训练一个7B的Llama 2，就要花费76万美元（约合555万人民币）。
- 官方2048卡集群上，3.7天就能完成这一训练过程

架构方面，DeepSeek V3采用了创新的**负载均衡策略**和**训练目标**。
- DeepSeek-V2架构基础上，提出一种**无辅助损失**的负载均衡策略，能最大限度减少负载均衡而导致的性能下降。
- 该策略为MoE中的每个专家引入了一个偏置项（bias term），并将其添加到相应的亲和度分数中，以确定top-K路由。
- 多Token预测目标（Multi-Token Prediction，MTP）有利于提高模型性能，可以用于推理加速的推测解码。

预训练方面，DeepSeek V3采用FP8训练。
- 设计一个FP8混合精度训练框架，首次验证了FP8训练在极大规模模型上的可行性和有效性。

跨节点MoE训练中的通信瓶颈问题解决
- 设计DualPipe高效流水线并行算法：在单个前向和后向块对内，重叠计算和通信。
- 这种重叠能确保随着模型的进一步扩大，只要保持恒定的计算和通信比率，就仍然可以跨节点使用细粒度专家，实现接近于0的all-to-all通信开销。
- 高效的跨节点all-to-all通信内核等

后训练方面，DeepSeek V3引入了一种创新方法，将推理能力从长思维链模型（DeepSeek R1）中，蒸馏到标准模型上。这在显著提高推理性能的同时，保持了DeepSeek V3的输出风格和长度控制。

DeepSeek V3的MoE由256个**路由专家**和1个**共享专家**组成。在256个路由专家中，每个token会激活8个专家，并确保每个token最多被发送到4个节点。

DeepSeek V3还引入了**冗余专家**（redundant experts）的部署策略，即复制高负载专家并冗余部署。这主要是为了在推理阶段，实现MoE不同专家之间的负载均衡。


在多项测评上，DeepSeek V3达到了开源SOTA，超越Llama 3.1 405B，能和GPT-4o、Claude 3.5 Sonnet等TOP模型正面掰掰手腕

而其价格比 Claude 3.5 Haiku 还便宜，仅为 Claude 3.5 Sonnet的**9%**。

OpenAI创始成员Karpathy对此赞道：
- DeepSeek V3让在有限算力预算上进行模型预训练这件事变得容易。
- DeepSeek V3看起来比Llama 3 405B更强，训练消耗的算力却仅为后者的1/11。

贾扬清
- DeepSeek团队的伟大成就在某种程度上植根于多年的专业知识，这些专业知识部分被许多人忽视了


【2024-12-31】 [2024年，DeepSeek带给硅谷“苦涩的教训”](https://mp.weixin.qq.com/s/74SaMd9urByBhyBiWBFGbg)

`深度求索`共 139 名工程师和研究人员，包括创始人`梁文锋`本人也参与了这个项目。
- 而 OpenAI 有1200名研究人员。Anthropic 有500名研究人员。

独角兽AI公司`scale.ai`创始人 Alex 王 感叹道：
- 中国科技带来的苦涩教训：当美国人休息时，他们在工作，而且以更便宜、更快、更强的产品追上我们。

AI大神`卡帕西`、Meta科学家`田渊栋`、QLora发明人`Tim Dettmers`、OpenAI科学家`Sebastian Raschka`等点赞好评。

除了硅谷在圣诞假期, 被炸出来的大佬们，还有各种评论充斥：
- “这对中国来说，可能比第六代战斗机更具‘斯普特尼克时刻’意义：一款名为 `DeepSeek v3` 的中国AI模型在几乎所有方面都与最新的ChatGPT和Claude模型媲美，甚至常常超越它们，而训练成本却只是极小的一部分（仅550万美元），并且它是开源的（意味着任何人都可以使用、修改和改进它）。”
- “训练成本如此之低尤为重要，因为它彻底改变了谁能参与高级AI开发的**游戏规则**。在此之前，人们普遍认为训练这样的模型需要数亿甚至**数十亿美元**，而DeepSeek仅用**550万美元**就做到了，几乎任何初创公司都能负担得起。意味着DeepSeek刚刚证明了严肃的AI开发并不局限于科技巨头。”

2024年收官之时，这对硅谷是一个强烈的提醒：
> 美国对中国科技封锁，包括最严厉的芯片和AI封锁，结果，资源短缺激发了中国科技企业的创新力。


# 结束