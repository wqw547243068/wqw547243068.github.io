---
layout: post
title:  DeepSeek 深度求索
date:   2024-12-26 18:00:00
categories: 大模型
tags: deepseek moe 多模态 
excerpt: 大模型领域扫地僧：DeepSeek（深度求索）介绍，各个版本模型总结
mathjax: true
permalink: /deepseek
---

* content
{:toc}

# DeepSeek

[深度求索（DeepSeek）顶尖人才招聘](https://mp.weixin.qq.com/s/iKVBXclSNporHl-EmaPUkw)

## DeepSeek 介绍

[揭秘DeepSeek:一个更极致的中国技术理想主义故事](https://zhuanlan.zhihu.com/p/720160943)

位于杭州的量化投资基金`幻方`
- 2021年，幻方在亚太第一个拿到 A100卡，成为全国少数几家囤有上万张 A100 GPU的机构。
- 2023年4月11日, 宣布做大模型
- 2023年5月，才把做大模型的团队独立出来，成立`深度求索`公司。

自从ChatGPT时刻以来，业界弥漫一股“唯GPU论”的情绪，上万张卡加几亿美元，被认为是做大模型的门槛。

`深度求索`创立之初就宣布做AGI，会专注在大模型上，先从**语言大模型**做起，然后再做**视觉**和**多模态**等。
- 从2024年初推出首个大型语言模型 `DeepSeek LLM`，只能对标`GPT-3.5`
- 直到2024年底推出硬碰`GPT-4o`的`DeepSeek V3`，并且进军多模态、推理模型。

中国7家大模型创业公司中，[DeepSeek](https://www.deepseek.com/)（深度求索）最不声不响，但又总能以出其不意的方式被人记住。
- 一年前，这种出其不意源自背后的量化私募巨头`幻方`，大厂外唯一储备**万张**A100芯片的公司
- 一年后，则来自引发中国大模型**价格战**的源头。

2023年5月，DeepSeek （深度求索） 成立

被AI连续轰炸的5月，DeepSeek一跃成名。起因是发布的一款名为`DeepSeek V2`开源模型，提供了一种史无前例的**性价比**：
- 推理成本被降到每百万token仅 1块钱，约等于 Llama3 70B 1/7，GPT-4 Turbo 1/70。

DeepSeek 被迅速冠以“AI界拼多多”之称的同时，字节、腾讯、百度、阿里等大厂也按耐不住，纷纷降价。中国大模型价格战由此一触即发。

成见：
- 美国更擅长从0-1的**技术**创新，而中国更擅长从1-10的**应用**创新。

事实：
> 与很多大厂烧钱补贴不同，DeepSeek 有利润

DeepSeek 对模型架构进行了**全方位创新**。
- 提出一种崭新的`MLA`（一种**多头潜在注意力机制**）架构，把显存占用降到了过去最常用的`MHA`架构的**5%-13%**
- 独创 `DeepSeekMoESparse` 结构，把计算量降到极致，所有这些最终促成了成本的下降。

OpenAI前政策主管、Anthropic联合创始人`Jack Clark`: 
- DeepSeek “雇佣了一批高深莫测的奇才”，还认为中国制造的大模型，“将和无人机、电动汽车一样，成为不容忽视的力量。”

梁文锋：
- 并没有什么高深莫测的奇才，都是一些Top高校的`应届毕业生`、没毕业的博四、博五`实习生`，还有一些毕业才几年的年轻人。都是本土 —— 达摩院背景的`罗福莉`  参考 [罗福莉：天才AI少女“祛魅”记](https://zhuanlan.zhihu.com/p/515631834)
  - 保研北大、在顶会顶刊发文章、拿遍大厂offer、进入阿里达摩院、转行跳槽知名私募公司…
  - 2019年，一位北大硕士，因在NLP国际顶会 ACL 上发表 8 篇论文（其中2篇一作），曾登上知乎热搜
  - 在达摩院，罗福莉主导开发的跨语言预训练模型VECO，成为深度语言模型体系AliceMind八大模型之一，并被顶会ACL2021录用，她也在AliceMind集体开源中挑起大梁。AliceMind 登顶多模态权威榜单VQA Challenge 2021，并在阿里内部数十个核心业务落地，日均调用50亿次，活跃场景超过200个，其中不乏大家熟悉的天猫精灵智能音响等。
- 选人标准: 一直都是**热爱**和**好奇心**，所以很多人会有一些奇特的经历。对做**研究**的渴望远超对**钱**的在意。
- 对顶级人才吸引最大的，肯定是去**解决世界上最难的问题**。其实，顶尖人才在中国是被低估的。因为整个社会层面的**硬核创新太少**了，使得他们没有机会被识别出来。

Attention 架构提出多年来，几乎未被成功改过，更遑论大规模验证；对模型结构进行创新，没有路径可依，要经历很多失败，时间、经济成本都耗费巨大。

而 DeepSeek 成功了，它是 7家中国大模型创业公司中，唯一一家放弃“**既要又要**”路线，至今专注研究和技术，未做toC应用的公司，也是唯一一家**未全面考虑商业化**，坚定选择开源路线甚至都没融过资的公司。
- 公司 60 个人, 50 个技术, 10 个工程

DeepSeek创始人`梁文锋` 浙江大学电子工程系人工智能方向, 从`幻方时代` 就在幕后**潜心研究技术**的80后创始人，在 DeepSeek 时代，依旧延续低调作风，和所有研究员一样，每天 “**看论文，写代码，参与小组讨论**”。

`梁文锋`是当下中国AI界非常罕见
- “兼具强大的**infra工程**能力和**模型研究**能力，又能**调动资源**”
- “既可以从高处做精准判断，又可以在细节上强过一线研究员”的人，他拥有“令人恐怖的学习能力”，同时又“完全不像一个老板，而更像一个极客”。

他是少有把“**是非观**”置于“**利害观**”之前，并提醒看到时代惯性，把“原创式创新”提上日程的人。


## 接入

【2024-12-27】DeepSeek 接入体验方式
- Web形式: [DeepSeek](https://www.deepseek.com/) 免费使用
  - 默认版本: 
  - 联网搜索版本: 
  - 深度思考版本: R1, 对标 OpenAI o1
- API形式: [收费](https://api-docs.deepseek.com/zh-cn/quick_start/pricing)
  - 输入价格: 1 元/百万tokens, 输出价格 2 元/百万tokens
  - 新用户赠送10元

注
- 【2024-12-26】全面升级为 DeepSeek V3

```py
# Please install OpenAI SDK first: `pip3 install openai`

from openai import OpenAI

api_key = 'sk-7284******'

client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

sys_prompt = 'You are a helpful assistant'
sys_prompt = '你是一名数学家'
question = '解此微分方程 xdx+ydy=-xdy+ydx'

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": question},
    ],
    stream=False
)

print(question)
print(response.choices[0].message.content)
```


## 模型


### DeepSeek V2

【2024-5-7】[DeepSeek-V2](https://www.deepseek.com/zh) 全球最强开源通用MoE模型
- DeepSeek-V2 基于 2 千亿 MoE 模型底座，领先性能，超低价格，越级场景体验，已在对话官网和API全面上线
- 技术报告: [浅读 DeepSeek-V2 技术报告](https://zhuanlan.zhihu.com/p/696292840)
- 仓库和技术报告地址：[DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2)

DeepSeek-V2 在 DeepSeek 上改进，但并没有沿用主流的“类LLaMA的Dense结构”和“类Mistral的Sparse结构”，而是对Transformer架构中的自注意力机制进行了全方位创新，提出了`MLA`（Multi-head Latent Attention）结构，并使用了MoE技术进一步将计算量降低，大幅提高了推理效率。

特点
- 独创 MLA 结构
- 稀疏结构 DeepSeek-MoE
- 推理成本降低近百倍
- LMSYS榜单中，位列开源模型第一


DeepSeek-V2 包含 236B参数，每个Token激活2.1B参数，支持长达 128K 的上下文长度。
- 与DeepSeek 67B相比，DeepSeek-V2 在性能上取得了显著提升，节省了42.5%的训练成本，减少了93.3%的KV缓存，并将最大生成吞吐量提高到了5.76倍。

深度求索将该 DeepSeek-V2 模型已完全上线至平台服务用户，DeepSeek-V2 API也是物美价廉。并且秉持着最开放的开源精神，深度求索将这次的DeepSeek-V2模型和论文也将完全开源，免费商用。

#### 模型结构

模型结构
- ![](https://pic1.zhimg.com/80/v2-9c998d8bc062c10483e38606f4839814_1440w.webp)



### DeepSeek Coder 


2023年11月，DeepSeek Coder V1发布

2024年6月，DeepSeek Coder V2 全球最强代码开源模型
- 全球首个超越 GPT4-Turbo 的开源代码模型
- BigCodeBench 6月榜单中第二

### DeepSeek VL

自然语言到多模态初探


### DeepSeek R1

【2024-11-20】DeepSeek R1 详见站内专题: [o1](o1)


### DeepSeek V3

DeepSeek V3发布即完全开源，直接用了53页论文把训练细节和盘托出
- 体验地址：[DeepSeek](chat.deepseek.com)
- 技术报告地址：[DeepSeek_V3.pdf](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf)
- 抱抱脸开源地址：[DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)
- 参考链接：[公众号文章](https://mp.weixin.qq.com/s/iFZOQsUNkpkXPDvOkE99wQ)
- 【2024-12-26】[国产之光DeepSeek把AI大佬全炸出来了！671B大模型训练只需此前算力1/10，细节全公开](https://mp.weixin.qq.com/s/uho6L_V2IybmUmH8jXmRmw)

DeepSeek V3 是一个参数量为**671B**的MoE模型，激活37B，在14.8T高质量token上进行了预训练。

DeepSeek V3 整个训练过程仅用了不到280万个GPU小时，相比之下，Llama 3 405B的训练时长是3080万GPU小时（p.s. GPU型号也不同）。
- 训练671B的DeepSeek V3的成本是557.6万美元（约合4070万人民币），而只是训练一个7B的Llama 2，就要花费76万美元（约合555万人民币）。
- 官方2048卡集群上，3.7天就能完成这一训练过程

架构方面，DeepSeek V3采用了创新的**负载均衡策略**和**训练目标**。
- DeepSeek-V2架构基础上，提出一种**无辅助损失**的负载均衡策略，能最大限度减少负载均衡而导致的性能下降。
- 该策略为MoE中的每个专家引入了一个偏置项（bias term），并将其添加到相应的亲和度分数中，以确定top-K路由。
- 多Token预测目标（Multi-Token Prediction，MTP）有利于提高模型性能，可以用于推理加速的推测解码。

预训练方面，DeepSeek V3采用FP8训练。
- 设计一个FP8混合精度训练框架，首次验证了FP8训练在极大规模模型上的可行性和有效性。

跨节点MoE训练中的通信瓶颈问题解决
- 设计DualPipe高效流水线并行算法：在单个前向和后向块对内，重叠计算和通信。
- 这种重叠能确保随着模型的进一步扩大，只要保持恒定的计算和通信比率，就仍然可以跨节点使用细粒度专家，实现接近于0的all-to-all通信开销。
- 高效的跨节点all-to-all通信内核等

后训练方面，DeepSeek V3引入了一种创新方法，将推理能力从长思维链模型（DeepSeek R1）中，蒸馏到标准模型上。这在显著提高推理性能的同时，保持了DeepSeek V3的输出风格和长度控制。

DeepSeek V3的MoE由256个**路由专家**和1个**共享专家**组成。在256个路由专家中，每个token会激活8个专家，并确保每个token最多被发送到4个节点。

DeepSeek V3还引入了**冗余专家**（redundant experts）的部署策略，即复制高负载专家并冗余部署。这主要是为了在推理阶段，实现MoE不同专家之间的负载均衡。


在多项测评上，DeepSeek V3达到了开源SOTA，超越Llama 3.1 405B，能和GPT-4o、Claude 3.5 Sonnet等TOP模型正面掰掰手腕

而其价格比 Claude 3.5 Haiku 还便宜，仅为 Claude 3.5 Sonnet的**9%**。

OpenAI创始成员Karpathy对此赞道：
- DeepSeek V3让在有限算力预算上进行模型预训练这件事变得容易。
- DeepSeek V3看起来比Llama 3 405B更强，训练消耗的算力却仅为后者的1/11。

贾扬清
- DeepSeek团队的伟大成就在某种程度上植根于多年的专业知识，这些专业知识部分被许多人忽视了


【2024-12-31】 [2024年，DeepSeek带给硅谷“苦涩的教训”](https://mp.weixin.qq.com/s/74SaMd9urByBhyBiWBFGbg)

`深度求索`共 139 名工程师和研究人员，包括创始人`梁文锋`本人也参与了这个项目。
- 而 OpenAI 有1200名研究人员。Anthropic 有500名研究人员。

独角兽AI公司`scale.ai`创始人 Alex 王 感叹道：
- 中国科技带来的苦涩教训：当美国人休息时，他们在工作，而且以更便宜、更快、更强的产品追上我们。

AI大神`卡帕西`、Meta科学家`田渊栋`、QLora发明人`Tim Dettmers`、OpenAI科学家`Sebastian Raschka`等点赞好评。

除了硅谷在圣诞假期, 被炸出来的大佬们，还有各种评论充斥：
- “这对中国来说，可能比第六代战斗机更具‘斯普特尼克时刻’意义：一款名为 `DeepSeek v3` 的中国AI模型在几乎所有方面都与最新的ChatGPT和Claude模型媲美，甚至常常超越它们，而训练成本却只是极小的一部分（仅550万美元），并且它是开源的（意味着任何人都可以使用、修改和改进它）。”
- “训练成本如此之低尤为重要，因为它彻底改变了谁能参与高级AI开发的**游戏规则**。在此之前，人们普遍认为训练这样的模型需要数亿甚至**数十亿美元**，而DeepSeek仅用**550万美元**就做到了，几乎任何初创公司都能负担得起。意味着DeepSeek刚刚证明了严肃的AI开发并不局限于科技巨头。”

2024年收官之时，这对硅谷是一个强烈的提醒：
> 美国对中国科技封锁，包括最严厉的芯片和AI封锁，结果，资源短缺激发了中国科技企业的创新力。


# 结束