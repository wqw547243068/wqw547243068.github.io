---
layout: post
title:  BERTè¯­è¨€æ¨¡å‹
date:   2019-12-16 16:52:00
categories: æ·±åº¦å­¦ä¹  è‡ªç„¶è¯­è¨€å¤„ç†
tags: NLP Transformer BERT Attention ALBERT å¯è§†åŒ– gpu è¿ç§»å­¦ä¹  sentence rasa diet
excerpt: é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åŠBERTçŸ¥è¯†ç‚¹æ±‡æ€»
mathjax: true
permalink: /bert
---

* content
{:toc}


# BERT

- 2018å¹´11æœˆåº•ï¼Œè°·æ­Œå‘å¸ƒäº†åŸºäºåŒå‘ `Transformer` çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ `BERT`ï¼Œè¯¥é¢„è®­ç»ƒæ¨¡å‹èƒ½é«˜æ•ˆæŠ½å–æ–‡æœ¬ä¿¡æ¯å¹¶åº”ç”¨äºå„ç§ NLP ä»»åŠ¡ï¼Œè¯¥ç ”ç©¶å‡­å€Ÿé¢„è®­ç»ƒæ¨¡å‹åˆ·æ–°äº† 11 é¡¹ NLP ä»»åŠ¡çš„å½“å‰æœ€ä¼˜æ€§èƒ½è®°å½•ã€‚
- æŠ€æœ¯åšä¸» Jay Alammar è¿‘æ—¥å‘æ–‡[illustrated-bert](https://jalammar.github.io/illustrated-bert/)ï¼Œé€šè¿‡å›¾è§£æ–¹å¼ç”ŸåŠ¨åœ°è®²è§£äº† BERT çš„æ¶æ„å’Œæ–¹æ³•åŸºç¡€ã€‚
- 2018 å¹´æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹å¤„ç†æ–‡æœ¬ï¼ˆæ›´å‡†ç¡®åœ°è¯´æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œç®€ç§° NLPï¼‰çš„ä¸€ä¸ªè½¬æŠ˜ç‚¹ã€‚
- ![](https://image.jiqizhixin.com/uploads/editor/5442b9f1-17ca-49b3-a259-e5fb52107534/1544732034404.png)
- ![](https://image.jiqizhixin.com/uploads/editor/87b820e3-dc5c-4f9f-97f6-6a01360156b7/1544732034725.png)

ä¸‹æ¸¸ä»»åŠ¡
- ![](https://image.jiqizhixin.com/uploads/editor/41afd366-28b8-4aa1-8464-5f10d253cb48/1544732037865.png)

## BERTä»‹ç»

è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒç”¨åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„ç­–ç•¥ä¸»è¦æœ‰ä¸¤ç§ï¼š
- åŸºäº**ç‰¹å¾**ï¼ˆfeature-baseï¼‰ï¼šä¹Ÿå°±æ˜¯**è¯å‘é‡**ï¼Œé¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒå¥½åè¾“å‡ºçš„è¯å‘é‡ç›´æ¥åº”ç”¨åœ¨ä¸‹æ¸¸æ¨¡å‹ä¸­ã€‚
  - å¦‚ELMoç”¨äº†ä¸€ä¸ª**åŒå‘LSTM**ï¼Œä¸€ä¸ªè´Ÿè´£ç”¨å‰å‡ ä¸ªè¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œå¦ä¸€ä¸ªç›¸åï¼Œç”¨åé¢å‡ ä¸ªè¯æ¥é¢„æµ‹å‰ä¸€ä¸ªè¯ï¼Œä¸€ä¸ªä»å·¦çœ‹åˆ°å³ï¼Œä¸€ä¸ªä»å³çœ‹åˆ°å·¦ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°æ•æ‰åˆ°ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œä¸è¿‡åªèƒ½è¾“å‡ºä¸€ä¸ªè¯å‘é‡ï¼Œéœ€è¦é’ˆå¯¹ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡æ„å»ºæ–°çš„æ¨¡å‹ã€‚
- åŸºäº**å¾®è°ƒ**ï¼ˆfine-tuningï¼‰ï¼šå…ˆä»¥**è‡ªç›‘ç£**çš„å½¢å¼é¢„è®­ç»ƒå¥½ä¸€ä¸ªå¾ˆå¤§çš„æ¨¡å‹ï¼Œç„¶åæ ¹æ®ä¸‹æ¸¸ä»»åŠ¡æ¥ä¸€ä¸ª**è¾“å‡ºå±‚**ï¼Œä¸éœ€è¦å†é‡æ–°å»è®¾è®¡æ¨¡å‹æ¶æ„
  - å¦‚OpenAI-GPTï¼Œä½†æ˜¯GPTç”¨çš„æ˜¯ä¸€ä¸ª**å•å‘transformer**ï¼Œè®­ç»ƒæ—¶ç”¨å‰é¢å‡ ä¸ªè¯æ¥é¢„æµ‹åé¢ä¸€ä¸ªè¯ï¼Œåªèƒ½ä»å·¦å¾€å³çœ‹ï¼Œä¸èƒ½å¤Ÿå¾ˆå¥½çš„æ•æ‰åˆ°ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ã€‚

ELMoè™½ç„¶ç”¨äº†ä¸¤ä¸ªå•å‘çš„LSTMæ¥æ„æˆä¸€ä¸ªåŒå‘çš„æ¶æ„ï¼Œèƒ½å¤Ÿæ•æ‰åˆ°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä½†æ˜¯åªèƒ½è¾“å‡ºè¯å‘é‡ï¼Œä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹è¿˜æ˜¯è¦è‡ªå·±é‡æ–°æ„å»ºï¼Œè€ŒGPTè™½ç„¶æ˜¯åŸºäºå¾®è°ƒï¼Œç›´æ¥æ¥ä¸ªè¾“å‡ºå±‚å°±èƒ½ç”¨äº†ï¼Œä½†æ˜¯æ˜¯å•å‘çš„æ¨¡å‹ï¼Œåªèƒ½åŸºäºä¸Šæ–‡é¢„æµ‹ä¸‹æ–‡ï¼Œæ²¡æœ‰åŠæ³•å¾ˆå¥½çš„æ•æ‰åˆ°æ•´ä¸ªå¥å­çš„ä¿¡æ¯ã€‚

**BERT**ï¼ˆBidirectional Encoder Representations from Transformersï¼‰æŠŠè¿™ä¸¤ä¸ªæ¨¡å‹çš„æ€æƒ³èåˆäº†èµ·æ¥
- é¦–å…ˆï¼Œç”¨åŸºäº**å¾®è°ƒ**çš„ç­–ç•¥ï¼Œåœ¨ä¸‹æ¸¸æœ‰ç›‘ç£ä»»åŠ¡é‡Œé¢åªéœ€è¦æ¢ä¸ªè¾“å‡ºå±‚å°±è¡Œ
- å…¶æ¬¡ï¼Œè®­ç»ƒæ—¶ç”¨äº†ä¸€ä¸ªtransformerçš„encoderæ¥åŸºäº**åŒå‘**çš„ä¸Šä¸‹æ–‡æ¥è¡¨ç¤º**è¯å…ƒ**

ELMoã€GPTå’ŒBERTçš„åŒºåˆ«
- ![](https://pic2.zhimg.com/80/v2-59c61eca79848c14285c302564cfe3d9_720w.jpg)

BERTå¾ˆå¥½çš„èåˆäº†ELMoå’ŒGPTçš„ä¼˜ç‚¹ï¼Œè®ºæ–‡ä¸­æåˆ°åœ¨11ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼ˆæ–‡æœ¬åˆ†ç±»ã€è‡ªç„¶è¯­è¨€æ¨æ–­ã€é—®ç­”ã€æ–‡æœ¬æ ‡è®°ï¼‰éƒ½å–å¾—äº†SOTAçš„æˆç»©ã€‚

## BERTç»“æ„

BERTç»“æ„
- ![](https://pic1.zhimg.com/80/v2-b80e8506c78fbf64fe80b8203abff7e8_1440w.jpg)

### è¾“å…¥æ ¼å¼

Bert çš„è¾“å…¥æ ¼å¼å¦‚ä¸‹ï¼š

```
tokens     : [CLS] Sentence 1 [SEP] Sentence 2 [SEP]
input_ids  :  101     3322     102     9987     102
segment_ids:   0        0       0        1       1
```

æ–‡æœ¬ç‰¹å¾
- **å•å¥**æˆ–è€…**å¥å­å¯¹**(æˆ–å«ä¸Šä¸‹å¥)ç»è¿‡ Word Piece Tokenize åå˜æˆ tokensï¼Œç„¶åé€šè¿‡ä¸€ä¸ª**è¯è¡¨**(vocab)è¢«æ˜ å°„æˆ input_idsã€‚
- ä¸Šä¸‹å¥ä¿¡æ¯ä¼šè¢«ç¼–ç æˆ segment_idsï¼Œè¿åŒ input_ids ä¸€èµ·è¿›å…¥ Bert
- ç»è¿‡å¤šå±‚ transformer è¢«è½¬æ¢æˆ Vector Representationï¼Œæ–‡æœ¬ä¿¡æ¯ä¹Ÿå°±è¢«ç¼–ç åˆ°å‘é‡ä¸­äº†ã€‚

#### tokens é•¿åº¦é™åˆ¶

æ³¨æ„
- tokens æ€»é•¿åº¦çš„**é™åˆ¶**(Seq Length)ï¼š
  - **ç¡¬æ€§é™åˆ¶**: ä¸èƒ½è¶…è¿‡ <span style='color:red'>512</span>ï¼›
  - **èµ„æºé™åˆ¶**: **GPUæ˜¾å­˜**æ¶ˆè€—ä¼šéšç€ Seq Length çš„å¢åŠ è€Œæ˜¾è‘—å¢åŠ ã€‚
- æ‰€ä»¥ï¼Œtokensæ€»é•¿åº¦ä¸ max batch sizeæˆ**åæ¯”ä¾‹**å…³ç³»

Google ç»™å‡ºçš„åœ¨å•ä¸ª Titan X GPU(12G RAM) ä¸Š Bert çš„èµ„æºæ¶ˆè€—æƒ…å†µã€‚
- ![](https://pic2.zhimg.com/80/v2-ebd0eb2aab87ca8357953bdce3f608f5_1440w.jpg)

#### å¦‚ä½•å¤„ç†æˆªæ–­

çœŸå®æ•°æ®é›†ä¸­ï¼Œæ¯æ¡æ•°æ®æ–‡æœ¬é•¿åº¦ä¸åŒï¼Œä¸€èˆ¬å‘ˆç°**é•¿å°¾åˆ†å¸ƒ**ï¼Œéœ€è¦ç®€å•åˆ†æåï¼Œç¡®å®šä¸€ä¸ª Seq Lengthã€‚

ä»¥ Google QUEST Q&A Labeling æ¯”èµ›ä¸ºä¾‹ï¼Œæ–‡æœ¬ç‰¹å¾åŒ…æ‹¬ question_titleã€question_body å’Œ answerï¼Œtokenize ååˆ†å¸ƒ
- ![](https://pic3.zhimg.com/80/v2-7f5802f65d1e427daebdbc61d84b75c2_1440w.jpg)
- ![](https://pic1.zhimg.com/80/v2-983e93c94c883f2d0f6770b60e60e300_1440w.jpg)
- ä½¿ç”¨ question_title + question_body + answer çš„æ–¹å¼ï¼Œåˆ™æœ‰75%å·¦å³çš„æ ·æœ¬æ»¡è¶³512çš„é•¿åº¦é™åˆ¶ï¼Œå¦å¤–**25%**éœ€è¦åštruncateæˆªæ–­å¤„ç†ï¼›
- ä½¿ç”¨ question_title + question_body å’Œ answer **åˆ†å¼€ç¼–ç **çš„æ–¹å¼ï¼Œåˆ™æœ‰92%å·¦å³çš„æ ·æœ¬æ»¡è¶³512çš„é•¿åº¦é™åˆ¶ï¼Œä¸è¿‡è¿™æ—¶å€™éœ€è¦ä½¿ç”¨ä¸¤ä¸ª Bertï¼›

å¸¸ç”¨æˆªæ–­ç­–ç•¥æœ‰ä¸‰ç§ï¼š
- pre-truncate å¤´éƒ¨é˜¶æ®µ
- post-truncate å°¾éƒ¨é˜¶æ®µ
- middle-truncate (head + tail)ï¼šæœ€ä¼˜

ä¸‰ç§ç­–ç•¥çš„æ•ˆæœï¼Œhead + tail æœ€ä¼˜ï¼Œç¬¦åˆç›´è§‰ï¼Œä¸€èˆ¬æ–‡ç« æˆ–æ®µè½çš„**å¼€å¤´ç»“å°¾**å¾€å¾€ä¼šåŒ…å«é‡è¦ä¿¡æ¯ï¼Œæˆªæ‰ä¸­é—´éƒ¨åˆ†ä¿¡æ¯ä¸¢å¤±æœ€å°‘ã€‚
- å½“ç„¶ï¼Œè¿™å¹¶éç»å¯¹ï¼Œå»ºè®®ä¸‰ç§æ–¹æ³•éƒ½å°è¯•ï¼Œæ¯”è¾ƒåæŒ‘é€‰ï¼Œæˆ–è€…éƒ½ä¿ç•™ï¼ŒåæœŸåšèåˆã€‚
- å¦å¤–ï¼Œæ ¹æ®å…·ä½“çš„åœºæ™¯**è‡ªå®šä¹‰**æˆªæ–­ç­–ç•¥ï¼Œæ¯”å¦‚åœ¨ä¸Šé¢ä¾‹å­ä¸­ï¼Œå¯ä»¥é€šè¿‡è®¾å®š question_title ä¸è¶…è¿‡ 50ï¼Œquestion_body ä¸è¶…è¿‡ 200ï¼Œanswer ä¸è¶…è¿‡ 200 ç­‰ã€‚

### Encoder

[BERT](https://zh-v2.d2l.ai/chapter_natural-language-processing-pretraining/bert.html#subsec-bert-input-rep)

ä¸ TransformerEncoder ä¸åŒï¼Œ BERT Encoder ä½¿ç”¨**ç‰‡æ®µåµŒå…¥**å’Œå¯å­¦ä¹ çš„**ä½ç½®åµŒå…¥**ã€‚
- nn.Parameter ä¼ å…¥çš„æ˜¯ä¸€ä¸ª**éšæœºæ•°**

```py
#@save
class BERTEncoder(nn.Module):
    """BERTç¼–ç å™¨"""
    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,
                 ffn_num_hiddens, num_heads, num_layers, dropout,
                 max_len=1000, key_size=768, query_size=768, value_size=768,
                 **kwargs):
        super(BERTEncoder, self).__init__(**kwargs)
        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)
        self.segment_embedding = nn.Embedding(2, num_hiddens)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module(f"{i}", d2l.EncoderBlock(
                key_size, query_size, value_size, num_hiddens, norm_shape,
                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))
        # åœ¨BERTä¸­ï¼Œä½ç½®åµŒå…¥æ˜¯å¯å­¦ä¹ çš„ï¼Œå› æ­¤æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿé•¿çš„ä½ç½®åµŒå…¥å‚æ•°
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))

    def forward(self, tokens, segments, valid_lens):
        # åœ¨ä»¥ä¸‹ä»£ç æ®µä¸­ï¼ŒXçš„å½¢çŠ¶ä¿æŒä¸å˜ï¼šï¼ˆæ‰¹é‡å¤§å°ï¼Œæœ€å¤§åºåˆ—é•¿åº¦ï¼Œnum_hiddensï¼‰
        X = self.token_embedding(tokens) + self.segment_embedding(segments)
        X = X + self.pos_embedding.data[:, :X.shape[1], :]
        for blk in self.blks:
            X = blk(X, valid_lens)
        return X
```

### ç‰¹æ®Šç¬¦å·

Bert è¾“å…¥ç«¯ï¼Œé€šè¿‡æ·»åŠ  special tokens

Google QUEST Q&A Labeling æ¯”èµ›ä¸¾ä¾‹ï¼Œæ¯”å¦‚å¯¹äºæ¯æ¡é—®ç­”æ•°æ®ï¼Œæœ‰ä¸€ä¸ªç±»åˆ«ä¿¡æ¯ï¼Œå¯ä»¥æ˜¯ cultureã€science ç­‰

è®¾è®¡ä¸¤ä¸ª special tokens `[CAT=CULTURE]` å’Œ `[CAT=SCIENCE]` åŠ å…¥åˆ°æ–‡æœ¬ä¸­ï¼š

```js
tokens     : [CLS] [CAT=CULTURE] question [SEP] answer [SEP]
input_ids  :  101       1          3322    102   9987   102
segment_ids:   0        0           0       0      1     1
```

æ–°å¢çš„ special tokens éœ€è¦ç›¸åº”åœ°åœ¨è¯è¡¨ä¸­æ·»åŠ ï¼Œå®ç°æ–¹æ³•ä¹Ÿå¾ˆç®€å•ï¼Œå…·ä½“å¯ä»¥æŸ¥çœ‹ç›¸å…³æ¡†æ¶çš„æ–‡æ¡£ã€‚

### è¾“å‡º

Bert è¾“å‡ºç«¯
- ç›´æ¥åš embedding
- å¦ä¸€ç§æ˜¯ç›´æ¥åœ¨ Bert è¾“å‡ºç«¯å¯¹ meta ç‰¹å¾è¿›è¡Œ embeddingï¼Œç„¶åä¸æ–‡æœ¬ç‰¹å¾çš„ Vector Representation è¿›è¡Œèåˆã€‚

ä¸€ä¸ªç®€å•çš„ä»£ç å®ç°ç¤ºæ„ï¼š

```py
emb = nn.Embedding(10, 32)    # åˆå§‹åŒ–ä¸€ä¸ª Embedding å±‚
meta_vector = emb(cat)    # å°†ç±»åˆ«ç¼–ç æˆ vector
logits = torch.cat([txt_vector, meta_vector], dim=-1)    # æ–‡æœ¬å‘é‡å’Œç±»åˆ«å‘é‡èåˆ
```

ä¸¤ç§æ–¹å¼éƒ½å¯ä»¥å°è¯•

### å‘é‡è¡¨ç¤º Vector Representation

æ–‡æœ¬ç‰¹å¾ç»è¿‡ Encoder ç¼–ç åå˜æˆäº†å•¥ï¼Ÿ
- ä»¥12å±‚çš„ bert-base ä¸ºä¾‹ï¼Œå¾—åˆ°äº† transformer 12å±‚çš„ hidden statesï¼Œæ¯ä¸€å±‚çš„ç»´åº¦ä¸º `B x L x H `
  - `B` è¡¨ç¤º batch sizeï¼Œ`L` è¡¨ç¤º Seq Lengthï¼Œ`H` è¡¨ç¤º Hidden dim

ä¸€èˆ¬æœ€åä¸€å±‚ç¬¬ä¸€ä¸ª token (ä¹Ÿå³ `[CLS]`) å¯¹åº”çš„å‘é‡å¯ä»¥ä½œä¸ºæ•´ä¸ªå¥å­(æˆ–å¥å­å¯¹)çš„å‘é‡è¡¨ç¤ºï¼ŒåŒ…å«äº†ä»æ–‡æœ¬ä¸­æå–çš„æœ‰æ•ˆä¿¡æ¯ã€‚ä½†åœ¨æ¯”èµ›ä¸­å¯ä»¥çœ‹åˆ°å„ç§èŠ±å¼æ“ä½œï¼Œå¹¶ä¸”éƒ½å¾—åˆ°äº†æ˜æ˜¾çš„æ•ˆæœæå‡ï¼Œæ¯”å¦‚ï¼š
- å–æœ€åä¸€å±‚æ‰€æœ‰ token å¯¹åº”çš„å‘é‡åšèåˆï¼›
- å–æ‰€æœ‰å±‚çš„ç¬¬ä¸€ä¸ª token å¯¹åº”çš„å‘é‡åšèåˆï¼›
- å–æœ€åå››å±‚çš„æ‰€æœ‰ token å¯¹åº”çš„å‘é‡ï¼ŒåŠ æƒé‡(å¯å­¦ä¹ )èåˆï¼›
- ...

![](https://pic3.zhimg.com/80/v2-0a675ad3cd5c24a6f3338b01f2554bc2_1440w.jpg)

### åˆ†ç±»å™¨ Classifier

åˆ†ç±»å™¨ä¸€èˆ¬æ˜¯ä¸€æˆ–å¤šå±‚çš„**å…¨è¿æ¥ç½‘ç»œ**ï¼Œæœ€ç»ˆæŠŠç‰¹å¾çš„å‘é‡è¡¨ç¤ºæ˜ å°„åˆ° label (target) ç»´åº¦ä¸Šã€‚è¿™éƒ¨åˆ†æ¯”è¾ƒçµæ´»ï¼Œæ¯”å¦‚ä½¿ç”¨ä¸åŒçš„æ¿€æ´»å‡½æ•°ã€Batch Normalizationã€dropoutã€ä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°ç”šè‡³æ ¹æ®æ¯”èµ› metric è‡ªå®šä¹‰æŸå¤±å‡½æ•°ç­‰ã€‚

å°æŠ€å·§ï¼š

ï¼ˆ1ï¼‰**Multi-Sample Dropout**

ä½¿ç”¨è¿ç»­çš„ dropoutï¼ŒåŠ å¿«æ¨¡å‹æ”¶æ•›ï¼Œå¢åŠ æ³›åŒ–èƒ½åŠ›ï¼Œè¯¦ç»†è§è®ºæ–‡ï¼Œä»£ç å®ç°å¦‚ä¸‹ï¼š

```py
dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])

for j, dropout in enumerate(dropouts):
    if j == 0:
        logit = self.fc(dropout(h))
    else:
        logit += self.fc(dropout(h))
```

ï¼ˆ2ï¼‰**è¾…åŠ©ä»»åŠ¡**

å¤šä»»åŠ¡å­¦ä¹ åœ¨ NLP é¢†åŸŸå¾ˆå¸¸è§ï¼Œé€šè¿‡è®¾è®¡ä¸€äº›ä¸ç›®æ ‡ä»»åŠ¡ç›¸å…³çš„è¾…åŠ©ä»»åŠ¡ï¼Œæœ‰æ—¶ä¹Ÿå¯ä»¥è·å¾—ä¸é”™çš„æå‡ã€‚åœ¨ Jigsaw Unintended Bias in Toxicity Classification æ¯”èµ›ä¸­ï¼Œ1st place solution å°±é‡‡ç”¨äº†è¾…åŠ©ä»»åŠ¡çš„æ–¹æ³•ã€‚
- ![](https://pic4.zhimg.com/v2-47e66fc3817e1d2ceb5dfed653b3f323_r.jpg)

å›¾ä¸­ Target æ˜¯æ¯”èµ›è¦é¢„æµ‹çš„ç›®æ ‡ï¼Œis male mentionedã€is christian mentioned ç­‰æ˜¯ä¸€äº›**è¾…åŠ©ç›®æ ‡**ï¼Œé€šè¿‡è¿™äº›è¾…åŠ©ç›®æ ‡æä¾›çš„ç›‘ç£ä¿¡å·ï¼Œä¹Ÿå¯ä»¥å¯¹æ¨¡å‹çš„è®­ç»ƒå’Œæœ€ç»ˆæ•ˆæœæä¾›å¸®åŠ©ã€‚

### é‡‡æ ·

å½“ä¸€ç§ç¥ç»ç½‘ç»œæ¡†æ¶å¾ˆå¼ºå¤§æ—¶(æ¯”å¦‚ transformer ç»“æ„)ï¼Œå¯¹ç½‘ç»œç»“æ„çš„å°çš„æ”¹è¿›(æ¯”å¦‚åœ¨ classifier ä¸­å¢åŠ å‡ å±‚å…¨è¿æ¥å±‚ã€æ”¹å˜ä¸€ä¸‹æ¿€æ´»å‡½æ•°ã€ä½¿ç”¨dropoutç­‰)æ”¶ç›Šéƒ½æ˜¯éå¸¸å°çš„ï¼Œè¿™æ—¶å€™å¤§çš„æå‡ç‚¹å¾€å¾€æ˜¯åœ¨æ•°æ®å±‚é¢ã€‚

### æ–‡æœ¬æ‰©å¢

æ¯”è¾ƒå¸¸ç”¨ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ˜¯åŸºäºç¿»è¯‘çš„ï¼Œå«åš back translation ï¼ˆ**å›è¯‘æ³•**ï¼‰ã€‚åŸç†ç®€å•ï¼Œæ˜¯å°†æ–‡æœ¬ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ï¼Œç„¶åå†ç¿»è¯‘å›æ¥ï¼Œæ¯”å¦‚ï¼š

```shell
# åŸå§‹æ–‡æœ¬
Take care not to believe your own bullshit, see On Bullshit.
# EN -> DE -> EN
Be careful not to trust your own nonsense, see On Bullshit.
# EN -> FR -> EN
Be careful not to believe your own bullshit, see On Bullshit.
# EN -> ES -> EN
Be careful not to believe in your own shit, see Bullshit.
```

å› ä¸º Google ç¿»è¯‘å·²ç»åšåˆ°äº†å¾ˆé«˜çš„å‡†ç¡®ç‡ï¼Œæ‰€ä»¥ back translation åŸºæœ¬ä¸ä¼šå‡ºç°å¤ªå¤§çš„è¯­ä¹‰æ”¹å˜ï¼Œä¸è¿‡éœ€è¦æœ‰åŠæ³•æåˆ° Google ç¿»è¯‘çš„ apiï¼Œç›®å‰ä¼¼ä¹æ²¡æœ‰å…è´¹å¯ç”¨çš„äº†ã€‚

## BERTè¦ç‚¹

Bertç»†èŠ‚ï¼š
- åœ¨è¾“å…¥ä¸Šï¼ŒBertçš„è¾“å…¥æ˜¯ä¸¤ä¸ªsegmentï¼Œå…¶ä¸­æ¯ä¸ªsegmentå¯ä»¥åŒ…å«å¤šä¸ªå¥å­ï¼Œä¸¤ä¸ªsegmentç”¨\[SEP]æ‹¼æ¥èµ·æ¥ã€‚
- æ¨¡å‹ç»“æ„ä¸Šï¼Œä½¿ç”¨ Transformerï¼Œè¿™ç‚¹è·ŸRobertaæ˜¯ä¸€è‡´çš„ã€‚
- å­¦ä¹ ç›®æ ‡ä¸Šï¼Œä½¿ç”¨ä¸¤ä¸ªç›®æ ‡ï¼š
  1. Masked Language Model(`MLM`) **æ©ç è¯­è¨€æ¨¡å‹**: å…¶ä¸­**15%**çš„tokenè¦è¢«Maskï¼Œåœ¨è¿™15%é‡Œï¼Œæœ‰80%è¢«æ›¿æ¢æˆ\[Mask]æ ‡è®°ï¼Œæœ‰10%è¢«éšæœºæ›¿æ¢æˆå…¶ä»–tokenï¼Œæœ‰10%ä¿æŒä¸å˜ã€‚
  1. Next Sentence Prediction(`NSP`) **ä¸‹ä¸€å¥é¢„æµ‹**: åˆ¤æ–­segmentå¯¹ä¸­ç¬¬äºŒä¸ªæ˜¯ä¸æ˜¯ç¬¬ä¸€ä¸ªçš„åç»­ã€‚éšæœºé‡‡æ ·å‡º**50%**æ˜¯å’Œ50%ä¸æ˜¯ã€‚
- Optimizations ç®—æ³•ä¼˜åŒ–:
  - Adam, beta1=0.9, beta2=0.999, epsilon=1e-6, L2 weight decay=0.01
  - learning rate, å‰10000æ­¥ä¼šå¢é•¿åˆ°1e-4, ä¹‹åå†**çº¿æ€§ä¸‹é™**ã€‚
  - dropout=0.1
  - GELUæ¿€æ´»å‡½æ•°
  - è®­ç»ƒæ­¥æ•°ï¼š1M
  - mini-batch: 256
  - è¾“å…¥é•¿åº¦: 512
- Data
  - BookCorpus + English Wiki = 16GB


## BERTè¿›é˜¶æ€è€ƒ

### BERTå­¦åˆ°äº†ä»€ä¹ˆ

ACL 2019 æœ€æ–°æ”¶å½•çš„è®ºæ–‡ï¼š[What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document) [ç†è§£BERTæ¯ä¸€å±‚éƒ½å­¦åˆ°äº†ä»€ä¹ˆ](https://zhuanlan.zhihu.com/p/74515580)ï¼Œ [ä»£ç ](https://github.com/ganeshjawahar/interpret_bert)
- Fregeæ—©åœ¨1965å¹´çš„ç»„åˆåŸåˆ™é‡Œè°ˆåˆ°ï¼Œå¤æ‚è¡¨è¾¾å¼çš„æ„ä¹‰ç”±å…¶å­è¡¨è¾¾å¼çš„æ„ä¹‰ä»¥åŠæ„ä¹‰å¦‚ä½•ç»„åˆçš„è§„åˆ™å…±åŒå†³å®šã€‚
- æœ¬æ–‡æ€è·¯ä¸åˆ†æå·ç§¯ç¥ç»ç½‘ç»œæ¯å±‚å­¦ä¹ åˆ°çš„è¡¨å¾ç±»ä¼¼ï¼Œä¸»è¦æ˜¯æ¢ç´¢äº†BERTçš„æ¯ä¸€å±‚åˆ°åº•æ•æ‰åˆ°äº†ä»€ä¹ˆæ ·çš„ä¿¡æ¯è¡¨å¾ã€‚ä½œè€…é€šè¿‡ä¸€ç³»åˆ—çš„å®éªŒè¯æ˜BERTå­¦ä¹ åˆ°äº†ä¸€äº›**ç»“æ„åŒ–**çš„è¯­è¨€ä¿¡æ¯ï¼Œæ¯”å¦‚
  - BERTçš„**ä½å±‚**ç½‘ç»œå°±å­¦ä¹ åˆ°äº†**çŸ­è¯­**çº§åˆ«çš„**ä¿¡æ¯è¡¨å¾**
  - BERTçš„**ä¸­å±‚**ç½‘ç»œå°±å­¦ä¹ åˆ°äº†ä¸°å¯Œçš„**è¯­è¨€å­¦**ç‰¹å¾
  - BERTçš„**é«˜å±‚**ç½‘ç»œåˆ™å­¦ä¹ åˆ°äº†ä¸°å¯Œçš„**è¯­ä¹‰**ä¿¡æ¯ç‰¹å¾ã€‚
- [å›¾ç¤º](https://pic2.zhimg.com/80/v2-602f7d353a057e56327a631e396934b1_720w.jpg)
  - ![](https://pic2.zhimg.com/80/v2-602f7d353a057e56327a631e396934b1_720w.jpg)

### BERTå¯è§†åŒ–

ã€2022-7-22ã€‘[æœ€å…¨æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å·¥å…·](https://mp.weixin.qq.com/s/QQgKfYi-m9psUOJUZUxZ0Q)
- æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸€ç›´å¤„äºé»‘åŒ£å­çŠ¶æ€ï¼Œæœ‰å¾ˆå¤šåŒå­¦é—®æˆ‘å…·ä½“æ€ä¹ˆè§£é‡Šï¼Ÿå…¶å®å¾ˆå¤šè¿˜æ˜¯æ— æ³•å¯è§£é‡Šï¼Œä½†æ˜¯é€šè¿‡å¯è§†åŒ–ï¼Œå…·ä½“å¯ä»¥çŸ¥é“æ·±åº¦å­¦ä¹ åœ¨è®­ç»ƒè¿‡ç¨‹åˆ°åº•å­¦ä¹ äº†å“ªäº›ç‰¹å¾ï¼Ÿåˆ°åº•å¯¹è¯¥ç›®æ ‡çš„å“ªäº›ç‰¹å¾æ„Ÿå…´è¶£ï¼Ÿ
  - 1.æ·±åº¦å­¦ä¹ ç½‘ç»œç»“æ„ç”»å›¾å·¥å…·ï¼Œ[åœ°å€](https://cbovar.github.io/ConvNetDraw/)
  - 2.caffeå¯è§†åŒ–å·¥å…·,è¾“å…¥ï¼šcaffeé…ç½®æ–‡ä»¶ è¾“å‡ºï¼šç½‘ç»œç»“æ„; [åœ°å€](http://ethereon.github.io/netscope/#/editor)
  - 3.æ·±åº¦å­¦ä¹ å¯è§†åŒ–å·¥å…·Visual DL; Visual DLæ˜¯ç™¾åº¦å¼€å‘çš„ï¼ŒåŸºäºecharå’ŒPaddlePaddleï¼Œæ”¯æŒPaddlePaddleï¼ŒPyTorchå’ŒMXNetç­‰ä¸»æµæ¡†æ¶ã€‚psï¼šè¿™ä¸ªæ˜¯æˆ‘æœ€å–œæ¬¢çš„ï¼Œæ¯•ç«Ÿecharçš„æ¸²æŸ“èƒ½åŠ›ä¸é”™å“ˆå“ˆå“ˆï¼Œå¯æƒœä¸æ”¯æŒcaffeå’Œtensorflowã€‚[åœ°å€](https://github.com/PaddlePaddle/VisualDL)
  - 4.ç»“æ„å¯è§†åŒ–å·¥å…· PlotNeuralNet, è¨å°”å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸“ä¸šçš„ä¸€ä¸ªå­¦ç”Ÿå¼€å‘ã€‚[åœ°å€](https://github.com/HarisIqbal88/PlotNeuralNet)
  - [CNN Explainer](https://poloclub.github.io/cnn-explainer/) [github](https://github.com/poloclub/cnn-explainer), äº¤äº’å¯è§†åŒ–CNNç±»ç¥ç»ç½‘ç»œï¼Œä½¿ç”¨ TensorFlow.js åŠ è½½é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¯è§†åŒ–æ•ˆæœï¼Œäº¤äº’æ–¹é¢åˆ™ä½¿ç”¨ Svelte ä½œä¸ºæ¡†æ¶å¹¶ä½¿ç”¨ D3.js è¿›è¡Œå¯è§†åŒ–ã€‚æœ€ç»ˆçš„æˆå“å³ä½¿å¯¹äºå®Œå…¨ä¸æ‡‚çš„æ–°æ‰‹æ¥è¯´ï¼Œä¹Ÿæ²¡æœ‰ä½¿ç”¨é—¨æ§›ã€‚
  - [VisualDL](https://github.com/PaddlePaddle/VisualDL), a visualization analysis tool of PaddlePaddleï¼›ç±»ä¼¼tensorboard
    - ![](https://user-images.githubusercontent.com/48054808/103188111-1b32ac00-4902-11eb-914e-c2368bdb8373.gif)
- NLPå¯è§†åŒ–
  - [NLPReViz](https://nlpreviz.github.io/): An Interactive Tool for Natural Language Processing on Clinical Text
  - [NLPVis](https://github.com/shusenl/nlpvis) webç³»ç»Ÿï¼Œvisualize the attention of neural network based natural language models.
    - ![](https://github.com/shusenl/nlpvis/raw/master/teaser.png?raw=true)
  - [Visualizing BERT](https://home.ttic.edu/~kgimpel/viz-bert/viz-bert.html)
  - kaggleä¸Šå¤–å›½äººåˆ†äº«çš„[Visualizing BERT embeddings with t-SNE](https://www.kaggle.com/wqw547243068/visualizing-bert-embeddings-with-t-sne/edit)
  - SHAP
    - ![](https://www.nowhere.co.jp/blog/wp-content/uploads/2022/07/SHAP-for-translation.png)
  - [BertViz](https://github1s.com/jessevig/bertviz)æ˜¯BERTå¯è§†åŒ–å·¥å…·åŒ…ï¼Œæ”¯æŒ[transformers](https://github.com/huggingface/transformers) åº“çš„å¤§éƒ¨åˆ†æ¨¡å‹ (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, BART, etc.)ï¼Œç»§æ‰¿äº[Tensor2Tensor visualization tool](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization)

#### BERTViz

```shell
# pipå®‰è£…
pip install bertviz
# æºç å®‰è£…
git clone https://github.com/jessevig/bertviz.git
cd bertviz
python setup.py develop
```

åŠŸèƒ½ï¼š
- headviewï¼šæŒ‡å®šå±‚çš„æ³¨æ„åŠ›å±‚å¯è§†åŒ–
  - [interactive Colab Notebook](https://colab.research.google.com/drive/1PEHWRHrvxQvYr9NFRC-E_fr3xDq1htCj)
  - ![](https://upload-images.jianshu.io/upload_images/22279029-4a9d09dd11e565a3.png)
- modelviewï¼šæ•´ä¸ªæ¨¡å‹çš„æ³¨æ„åŠ›å¯è§†åŒ–
  - [interactive Colab Notebook](https://colab.research.google.com/drive/1c73DtKNdl66B0_HF7QXuPenraDp0jHRS)
  - ![](https://upload-images.jianshu.io/upload_images/22279029-59e2434e45d87166.png)
- neuralviewï¼šQKVç¥ç»å…ƒå¯è§†åŒ–, åŠå¦‚ä½•è®¡ç®—æ³¨æ„åŠ›
  - [interactive Colab Notebook](https://colab.research.google.com/drive/1m37iotFeubMrp9qIf9yscXEL1zhxTN2b)
  - ![](https://upload-images.jianshu.io/upload_images/22279029-7088fd1e5fef41e5.png)


```python
from transformers import AutoTokenizer, AutoModel
from bertviz import model_view

model_version = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_version)
model = AutoModel.from_pretrained(model_version, output_attentions=True)

inputs = tokenizer.encode("The cat sat on the mat", return_tensors='pt')
outputs = model(inputs)
attention = outputs[-1]  # Output includes attention weights when output_attentions=True
tokens = tokenizer.convert_ids_to_tokens(inputs[0]) 

# headå¯è§†åŒ–
from bertviz import head_view
head_view(attention, tokens)
head_view(attention, tokens, layer=2, heads=[3,5])

# modelå¯è§†åŒ–
model_view(attention, tokens)
model_view(attention, tokens, display_mode="light") # èƒŒæ™¯è®¾ç½®
model_view(attention, tokens, include_layers=[5, 6]) # åªæ˜¾ç¤º5-6å±‚

# neuralå¯è§†åŒ–
# Import specialized versions of models (that return query/key vectors)
from bertviz.transformers_neuron_view import BertModel, BertTokenizer

from bertviz.neuron_view import show

model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
model_type = 'bert'
show(model, model_type, tokenizer, sentence_a, sentence_b, layer=2, head=0)

```

jupyter notebookç‰ˆæœ¬ï¼š

```python
# bertå¯è§†åŒ–  pip install bertviz
from transformers import BertTokenizer, BertModel
#from bertv_master.bertviz import head_view
from bertviz import head_view


# åœ¨ jupyter notebook æ˜¾ç¤ºvisualzation 
def call_html():
  import IPython
  display(IPython.core.display.HTML('''<script src="/static/components/requirejs/require.js"></script><script>// <![CDATA[
requirejs.config({ paths: { base: '/static/base', "d3": "https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min", jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min', }, });
// ]]></script>'''))

# è¨˜å¾—æˆ‘ä»¬æ˜¯ä½¿ç”¨ä¸­æ–‡ BERT
model_version = 'bert-base-chinese'
model = BertModel.from_pretrained(model_version, cache_dir="./transformers/", output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version)
 
# æƒ…å¢ƒ 1 çš„å¥å­
sentence_a = "è€çˆ¸å«å°å®å»ä¹°é…±æ²¹ï¼Œ"
sentence_b = "å›æ¥æ…¢äº†å°±éª‚ä»–ã€‚"
 
# å¾—åˆ°tokensåè¾“å…¥BERTæ¨¡å‹è·å–æ³¨æ„åŠ›æƒé‡(attention)
inputs = tokenizer.encode_plus(sentence_a,sentence_b,return_tensors='pt', add_special_tokens=True)
token_type_ids = inputs['token_type_ids']
input_ids = inputs['input_ids']
attention = model(input_ids, token_type_ids=token_type_ids)[-1]
input_id_list = input_ids[0].tolist() # Batch index 0
tokens = tokenizer.convert_ids_to_tokens(input_id_list)
call_html()

# ç”¨BertVizå¯è§†åŒ–
head_view(attention, tokens)
```


#### SHAP

SHapley Additive exPlanations 
- ![](https://www.nowhere.co.jp/blog/wp-content/uploads/2022/07/SHAP-for-translation.png)

All the code and outputs are provided at the end of this blood. Please refer to it while reading and feel free to run it in [Google Colab notebook](https://colab.research.google.com/gist/Cheto01/1285f09b81d043856b6ba2b13f2ec942/shap-example.ipynb)

ï¼ˆ1ï¼‰Explaining a transformers-based model â€”â€” åˆ†ç±»æ¨¡å‹

SHAP or ECCO have the potential to provide substantial syntactic information captured from an NLP modelâ€™s attention.

SHAP can be used to visualize and interpret what a complex transformers-based model sees when applied to a classification task.

```python
#import the necessary libraries
import pandas as pd
import shap
import sklearn

import transformers
import datasets
import torch
import numpy as np
import scipy as sp

# load a BERT sentiment analysis model
tokenizer = transformers.DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
model = transformers.DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english"
).cuda()

# define a prediction function
def f(x):
    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=500, truncation=True) for v in x]).cuda()
    outputs = model(tv)[0].detach().cpu().numpy()
    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T
    val = sp.special.logit(scores[:,1]) # use one vs rest logit units
    return val

# build an explainer using a token masker
explainer = shap.Explainer(f, tokenizer)

# explain the model's predictions on IMDB reviews
imdb_train = datasets.load_dataset("imdb")["train"]
shap_values = explainer(imdb_train[:30], fixed_context=1, batch_size=2)

# plot a sentence's explanation
shap.plots.text(shap_values[27])
```

This visualization uses the probability scores of the modelâ€™s prediction. By taking into account the underlying patterns inside the layers, it uses the neuron activations values, more precisely the non-negative matrix factorization.


ï¼ˆ2ï¼‰NLP Model for Translation

Imagining what happens inside a sequence-to-sequence model could be described easier than a classifier. Our intuition for a translation could even show us more precisely which token or words have been mistranslated.

By hovering over the translated text, you will notice that those neuron activations carry embeddings scores. The non-latent embedding scores can be translated into embedding space where word similarity can easily be visualized. For example, in the embedding space, for â€œfatherâ€, â€œbrotherâ€ will be what â€œsisterâ€ is for â€œmotherâ€. This can be visualized even through a sentiment classification where each word is represented by its embedding score.  

```python
import numpy as np
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import shap
import torch

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-fr")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-fr").cuda()

s=["In my family, we are six: my father, my mother, my elder sister, my younger brother and sister"]

explainer = shap.Explainer(model,tokenizer)

shap_values = explainer(s)
```


### è¯­ä¹‰åŒ¹é…

#### Sentence BERT

ã€2020-3-25ã€‘[Sentence-Bertè®ºæ–‡ç¬”è®°](https://zhuanlan.zhihu.com/p/113133510)
- è®ºæ–‡: [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)ï¼Œä»£ç  [sentence-transformers](https://github.com/UKPLab/sentence-transformers)

Bertæ¨¡å‹å·²ç»åœ¨NLPå„å¤§ä»»åŠ¡ä¸­éƒ½å±•ç°å‡ºäº†å¼ºè€…çš„å§¿æ€ã€‚åœ¨**è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—**ï¼ˆsemantic textual similarityï¼‰ä»»åŠ¡ä¸Šä¹Ÿä¸ä¾‹å¤–ï¼Œä½†ç”±äºbertæ¨¡å‹è§„å®šï¼Œåœ¨è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦æ—¶ï¼Œéœ€è¦å°†ä¸¤ä¸ªå¥å­åŒæ—¶è¿›å…¥æ¨¡å‹ï¼Œè¿›è¡Œä¿¡æ¯äº¤äº’ï¼Œè¿™é€ æˆå¤§é‡çš„è®¡ç®—å¼€é”€ã€‚
- ä¾‹å¦‚ï¼Œæœ‰1wä¸ªå¥å­ï¼Œæˆ‘ä»¬æƒ³è¦æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å¥å­å¯¹ï¼Œéœ€è¦è®¡ç®—ï¼ˆ10000*9999/2ï¼‰æ¬¡ï¼Œéœ€è¦å¤§çº¦**65å°æ—¶**ã€‚
Bertæ¨¡å‹çš„æ„é€ ä½¿å¾—å®ƒæ—¢ä¸é€‚åˆ**è¯­ä¹‰ç›¸ä¼¼åº¦**æœç´¢ï¼Œä¹Ÿä¸é€‚åˆ**éç›‘ç£**ä»»åŠ¡ï¼Œæ¯”å¦‚èšç±»ã€‚
- é—®ç­”ç³»ç»Ÿä»»åŠ¡ä¸­ï¼Œå¾€å¾€ä¼šäººä¸ºé…ç½®ä¸€äº›å¸¸ç”¨å¹¶ä¸”æè¿°æ¸…æ™°çš„é—®é¢˜åŠå…¶å¯¹åº”çš„å›ç­”ï¼Œå³â€œ**æ ‡å‡†é—®**â€ã€‚å½“ç”¨æˆ·è¿›è¡Œæé—®æ—¶ï¼Œå¸¸å¸¸å°†ç”¨æˆ·çš„é—®é¢˜ä¸æ‰€æœ‰é…ç½®å¥½çš„æ ‡å‡†é—®è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œæ‰¾å‡ºä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸ä¼¼çš„æ ‡å‡†é—®ï¼Œå¹¶è¿”å›å…¶ç­”æ¡ˆç»™ç”¨æˆ·ï¼Œè¿™æ ·å°±å®Œæˆäº†ä¸€æ¬¡é—®ç­”æ“ä½œã€‚
- å¦‚æœä½¿ç”¨bertæ¨¡å‹ï¼Œé‚£ä¹ˆæ¯æ¬¡ç”¨æˆ·é—®é¢˜éƒ½éœ€è¦ä¸æ ‡å‡†é—®åº“è®¡ç®—ä¸€éã€‚åœ¨å®æ—¶äº¤äº’çš„ç³»ç»Ÿä¸­ï¼Œæ˜¯ä¸å¯èƒ½ä¸Šçº¿çš„ã€‚
Sentence-BERTç½‘ç»œç»“æ„å¯ä»¥è§£å†³bertæ¨¡å‹çš„ä¸è¶³ã€‚ç®€å•é€šä¿—åœ°è®²ï¼Œå€Ÿé‰´**å­ªç”Ÿç½‘ç»œ**æ¨¡å‹çš„æ¡†æ¶ï¼Œå°†ä¸åŒçš„å¥å­è¾“å…¥åˆ°ä¸¤ä¸ªbertæ¨¡å‹ä¸­ï¼ˆä½†è¿™ä¸¤ä¸ªbertæ¨¡å‹æ˜¯å‚æ•°å…±äº«çš„ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºæ˜¯åŒä¸€ä¸ªbertæ¨¡å‹ï¼‰ï¼Œè·å–åˆ°æ¯ä¸ªå¥å­çš„å¥å­è¡¨å¾å‘é‡ï¼›è€Œæœ€ç»ˆè·å¾—çš„**å¥å­è¡¨å¾å‘é‡**ï¼Œå¯ä»¥ç”¨äº**è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—**ï¼Œä¹Ÿå¯ä»¥ç”¨äºæ— ç›‘ç£**èšç±»**ä»»åŠ¡ã€‚
- å¯¹äºåŒæ ·çš„10000ä¸ªå¥å­ï¼Œæˆ‘ä»¬æƒ³è¦æ‰¾å‡ºæœ€ç›¸ä¼¼çš„å¥å­å¯¹ï¼Œåªéœ€è¦è®¡ç®—10000æ¬¡ï¼Œéœ€è¦å¤§çº¦5ç§’å°±å¯è®¡ç®—å®Œå…¨ã€‚ä»65å°æ—¶åˆ°5ç§’é’Ÿï¼Œè¿™çœŸæ˜¯ææ€–çš„å·®è·ã€‚

Sentence-BERT ç½‘ç»œç»“æ„
- æ–‡ä¸­å®šä¹‰äº†ä¸‰ç§é€šè¿‡bertæ¨¡å‹æ±‚å¥å­å‘é‡çš„ç­–ç•¥ï¼Œåˆ†åˆ«æ˜¯**CLSå‘é‡**ï¼Œ**å¹³å‡**æ± åŒ–å’Œ**æœ€å¤§å€¼**æ± åŒ–ã€‚
- ï¼ˆ1ï¼‰**CLSå‘é‡**ç­–ç•¥ï¼Œå°±æ˜¯å°†bertæ¨¡å‹ä¸­ï¼Œå¼€å§‹æ ‡è®°ã€clsã€‘å‘é‡ï¼Œä½œä¸ºæ•´å¥è¯çš„å¥å‘é‡ã€‚
- ï¼ˆ2ï¼‰**å¹³å‡æ± åŒ–**ç­–ç•¥ï¼Œå°±æ˜¯å°†å¥å­é€šè¿‡bertæ¨¡å‹å¾—åˆ°çš„å¥å­ä¸­æ‰€æœ‰çš„å­—å‘é‡è¿›è¡Œæ±‚å‡å€¼æ“ä½œï¼Œæœ€ç»ˆå°†å‡å€¼å‘é‡ä½œä¸ºæ•´å¥è¯çš„å¥å‘é‡ã€‚
- ï¼ˆ3ï¼‰**æœ€å¤§å€¼æ± åŒ–**ç­–ç•¥ï¼Œå°±æ˜¯å°†å¥å­é€šè¿‡bertæ¨¡å‹å¾—åˆ°çš„å¥å­ä¸­æ‰€æœ‰çš„å­—å‘é‡è¿›è¡Œæ±‚æœ€å¤§å€¼æ“ä½œï¼Œæœ€ç»ˆå°†æœ€å¤§å€¼å‘é‡ä½œä¸ºæ•´å¥è¯çš„å¥å‘é‡ã€‚

bertæ¨¡å‹å¾®è°ƒæ—¶ï¼Œè®¾ç½®äº†ä¸‰ä¸ªç›®æ ‡å‡½æ•°ï¼Œç”¨äºä¸åŒä»»åŠ¡çš„è®­ç»ƒä¼˜åŒ–ï¼Œå‡è®¾ä¸¤ä¸ªå¾…æ¯”è¾ƒçš„å‘é‡åˆ†åˆ«ä¸º u å’Œ v 
- ï¼ˆ1ï¼‰Classification Objective Functionï¼šuã€vä»¥åŠ`|u-v|`æ‹¼æ¥èµ·æ¥ï¼ŒåŠ softmax
  - ![](https://www.zhihu.com/equation?tex=o+%3D+softmax%28W_%7Bt%7D%28u%2Cv%2C%7Cu-v%7C%29%29)
  - ![](https://pic1.zhimg.com/80/v2-75a858c4c766a4a3158fa3970badc628_720w.jpg)
- ï¼ˆ2ï¼‰Regression Objective Functionï¼šç›®æ ‡å‡½æ•°æ˜¯ç›´æ¥å¯¹ä¸¤å¥è¯çš„å¥å­å‘é‡ u å’Œ v è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
  - ![](https://pic3.zhimg.com/80/v2-1aeacdd9a1d3ee03655f08304d00f726_720w.jpg)
- ï¼ˆ3ï¼‰Triplet Objective Function: å°†åŸæ¥çš„ä¸¤ä¸ªè¾“å…¥ï¼Œå˜æˆä¸‰ä¸ªå¥å­è¾“å…¥ã€‚
  - ç»™å®šä¸€ä¸ªé”šå®šå¥ a ï¼Œä¸€ä¸ªè‚¯å®šå¥ p å’Œä¸€ä¸ªå¦å®šå¥ n ï¼Œæ¨¡å‹é€šè¿‡ä½¿ a-p çš„è·ç¦»å°äº a-n çš„è·ç¦»ï¼Œæ¥ä¼˜åŒ–æ¨¡å‹ã€‚ä½¿å…¶ç›®æ ‡å‡½æ•°oæœ€å°
  - ![](https://www.zhihu.com/equation?tex=max%28%7C%7Cs_%7Ba%7D-s_%7Bp%7D%7C%7C-%7C%7Cs_%7Ba%7D-s_%7Bn%7D%7C%7C%2B%5Cepsilon%2C+0%29)

å¤§é‡å®éªŒæ¯”è¾ƒä¸‰ç§æ±‚å¥å­å‘é‡ç­–ç•¥çš„å¥½åï¼Œè®¤ä¸º**å¹³å‡æ± åŒ–**ç­–ç•¥æœ€ä¼˜ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ•ˆæœéªŒè¯ã€‚è™½ç„¶æ•ˆæœæ²¡æœ‰bertè¾“å…¥ä¸¤å¥è¯çš„æ•ˆæœå¥½ï¼Œä½†æ˜¯æ¯”å…¶ä»–æ–¹æ³•è¿˜æ˜¯è¦å¥½çš„ï¼Œå¹¶ä¸”é€Ÿåº¦å¾ˆå¿«ã€‚

### BERTé™ç»´

#### BERT-flow

BERT-flow æ¥è‡ªè®ºæ–‡ã€Š[On the Sentence Embeddings from Pre-trained Language Models](https://arxiv.org/abs/2011.05864)ã€‹ï¼ŒEMNLP 2020ï¼Œä¸»è¦æ˜¯ç”¨flowæ¨¡å‹æ ¡æ­£äº†BERTå‡ºæ¥çš„å¥å‘é‡çš„åˆ†å¸ƒï¼Œä»è€Œä½¿å¾—è®¡ç®—å‡ºæ¥çš„cosç›¸ä¼¼åº¦æ›´ä¸ºåˆç†ä¸€äº›ã€‚

ç”¨å¥å‘é‡åšç›¸ä¼¼åº¦è®¡ç®—ã€ç´¢å¼•æ—¶ï¼Œå¸¸å¸¸ç”¨åˆ°ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ ¹æ®æ•°å€¼æ’åºï¼Œç„¶è€Œï¼Œä¸æ˜¯æ‰€æœ‰å‘é‡éƒ½é€‚åˆç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
- ä½™å¼¦ç›¸ä¼¼åº¦ï¼šä¸¤ä¸ªå‘é‡x,yçš„å†…ç§¯çš„å‡ ä½•æ„ä¹‰å°±æ˜¯â€œå„è‡ªçš„æ¨¡é•¿ä¹˜ä»¥å®ƒä»¬çš„å¤¹è§’ä½™å¼¦â€ï¼Œå³ä¸¤ä¸ªå‘é‡çš„å†…ç§¯å¹¶é™¤ä»¥å„è‡ªçš„æ¨¡é•¿
- å‡è®¾ï¼šå‘é‡çš„â€œå¤¹è§’ä½™å¼¦â€æœ¬èº«æ˜¯å…·æœ‰é²œæ˜çš„å‡ ä½•æ„ä¹‰çš„ï¼Œä½†ä¸Šå¼å³ç«¯åªæ˜¯åæ ‡çš„è¿ç®—ï¼Œåæ ‡ä¾èµ–äºæ‰€é€‰å–çš„åæ ‡åŸºï¼ŒåŸºåº•ä¸åŒï¼Œå†…ç§¯å¯¹åº”çš„åæ ‡å…¬å¼å°±ä¸ä¸€æ ·ï¼Œä»è€Œä½™å¼¦å€¼çš„åæ ‡å…¬å¼ä¹Ÿä¸ä¸€æ ·ã€‚å³ä½™å¼¦å…¬å¼åªåœ¨â€œ**æ ‡å‡†æ­£äº¤åŸº**â€ä¸‹æˆç«‹ï¼Œå¦‚æœè¿™ç»„åŸºæ˜¯æ ‡å‡†æ­£äº¤åŸºï¼Œæ¯ä¸ªåˆ†é‡éƒ½æ˜¯ç‹¬ç«‹çš„ã€å‡åŒ€çš„ï¼ŒåŸºå‘é‡é›†è¡¨ç°å‡ºâ€œ**å„é¡¹åŒæ€§**â€ã€‚

BERTå‘é‡ç”¨ä½™å¼¦å€¼æ¥æ¯”è¾ƒç›¸ä¼¼åº¦ï¼Œè¡¨ç°ä¸å¥½ï¼ŒåŸå› å¯èƒ½å°±æ˜¯å¥å‘é‡æ‰€å±çš„åæ ‡ç³»å¹¶é**æ ‡å‡†æ­£äº¤åŸº**ã€‚ä¸æ»¡è¶³å„é¡¹åŒæ€§æ—¶ï¼Œéœ€è¦é€šè¿‡ä¸€äº›æ–¹æ³•è½¬æ¢ï¼Œæ¯”å¦‚ï¼šBERT-flowç”¨äº†flowæ¨¡å‹ã€‚

flowæ¨¡å‹æ˜¯ä¸€ä¸ªå‘é‡å˜æ¢æ¨¡å‹ï¼Œå®ƒå¯ä»¥å°†è¾“å…¥æ•°æ®çš„åˆ†å¸ƒè½¬åŒ–ä¸º**æ ‡å‡†æ­£æ€åˆ†å¸ƒ**ï¼Œè€Œæ˜¾ç„¶æ ‡å‡†æ­£æ€åˆ†å¸ƒæ˜¯å„å‘åŒæ€§çš„

flowæ¨¡å‹æœ¬èº«å¾ˆå¼±ï¼ŒBERT-flowé‡Œè¾¹ä½¿ç”¨çš„flowæ¨¡å‹æ›´å¼±ï¼Œæ‰€ä»¥flowæ¨¡å‹ä¸å¤§å¯èƒ½åœ¨BERT-flowä¸­å‘æŒ¥è‡³å…³é‡è¦çš„ä½œç”¨ã€‚åè¿‡æ¥æƒ³ï¼Œé‚£å°±æ˜¯ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°æ›´ç®€å•ç›´æ¥çš„æ–¹æ³•è¾¾åˆ°BERT-flowçš„æ•ˆæœã€‚


#### BERT-whitening

[ä½ å¯èƒ½ä¸éœ€è¦BERT-flowï¼šä¸€ä¸ªçº¿æ€§å˜æ¢åª²ç¾BERT-flow](https://kexue.fm/archives/8069)
- [BERT-whitening](https://github.com/bojone/BERT-whitening)ï¼šé€šè¿‡ç®€å•çš„å‘é‡**ç™½åŒ–**æ¥æ”¹å–„å¥å‘é‡è´¨é‡ï¼Œå¯ä»¥åª²ç¾ç”šè‡³è¶…è¿‡BERT-flowçš„æ•ˆæœ, ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œå¯ä»¥è½»æ¾å¥—åˆ°ä»»æ„çš„å¥å‘é‡æ¨¡å‹ä¸­
  - ã€Š[Whitening Sentence Representations for Better Semantics and Faster Retrieval](https://arxiv.org/abs/2103.15316)ã€‹

å°†å¥å‘é‡çš„**å‡å€¼**å˜æ¢ä¸º0ã€**åæ–¹å·®çŸ©é˜µ**å˜æ¢ä¸ºå•ä½é˜µ, ç›¸å½“äºä¼ ç»Ÿæ•°æ®æŒ–æ˜ä¸­çš„**ç™½åŒ–**æ“ä½œï¼ˆWhiteningï¼‰ï¼Œæ‰€ä»¥è¯¥æ–¹æ³•ç¬”è€…ç§°ä¹‹ä¸º**BERT-whitening**

åæ–¹å·®çŸ©é˜µ Î£ æ˜¯ä¸€ä¸ª**åŠæ­£å®š**å¯¹ç§°çŸ©é˜µï¼ŒåŠæ­£å®šå¯¹ç§°çŸ©é˜µéƒ½å…·æœ‰å¦‚ä¸‹å½¢å¼çš„SVDåˆ†è§£: Î£=UÎ›UâŠ¤ï¼Œå…¶ä¸­Uæ˜¯ä¸€ä¸ªæ­£äº¤çŸ©é˜µï¼Œè€ŒÎ›æ˜¯ä¸€ä¸ªå¯¹è§’é˜µï¼Œå¹¶ä¸”å¯¹è§’çº¿å…ƒç´ éƒ½æ˜¯æ­£çš„ï¼Œå› æ­¤ç›´æ¥è®© $\boldsymbol{W}^{-1}=\sqrt{\boldsymbol{\Lambda}} \boldsymbol{U}^{\top}$ å°±å¯ä»¥å®Œæˆæ±‚è§£ï¼š $\boldsymbol{W}=\boldsymbol{U} \sqrt{\mathbf{\Lambda}^{-1}}$

```python
def compute_kernel_bias(vecs):
    """è®¡ç®—kernelå’Œbias
    vecs.shape = [num_samples, embedding_size]ï¼Œ
    æœ€åçš„å˜æ¢ï¼šy = (x + bias).dot(kernel)
    """
    mu = vecs.mean(axis=0, keepdims=True)
    cov = np.cov(vecs.T)
    u, s, vh = np.linalg.svd(cov)
    W = np.dot(u, np.diag(1 / np.sqrt(s)))
    return W, -mu
```

[Githubé“¾æ¥](https://github.com/bojone/BERT-whitening)

ç”¨ä¸€ä¸ªç®€å•çº¿æ€§å˜æ¢çš„ BERT-whitening å–å¾—äº†è·ŸBERT-flowåª²ç¾çš„ç»“æœã€‚é™¤äº†STS-Bä¹‹å¤–ï¼Œä¸­æ–‡ä¸šåŠ¡æ•°æ®å†…åšäº†ç±»ä¼¼çš„æ¯”è¾ƒï¼Œç»“æœéƒ½è¡¨æ˜BERT-flowå¸¦æ¥çš„æå‡è·ŸBERT-whiteningæ˜¯ç›¸è¿‘çš„

ä»¿ç…§PCAé™ç»´ï¼Œæ•ˆæœæ›´å¥½

```python
def compute_kernel_bias(vecs, n_components=256):
    """è®¡ç®—kernelå’Œbias
    vecs.shape = [num_samples, embedding_size]ï¼Œ
    æœ€åçš„å˜æ¢ï¼šy = (x + bias).dot(kernel)
    """
    mu = vecs.mean(axis=0, keepdims=True)
    cov = np.cov(vecs.T)
    u, s, vh = np.linalg.svd(cov)
    W = np.dot(u, np.diag(1 / np.sqrt(s)))
    return W[:, :n_components], -mu
```

å°†baseç‰ˆæœ¬çš„**768ç»´**åªä¿ç•™å‰**256ç»´**ï¼Œæ•ˆæœæœ‰æ‰€æå‡ï¼Œå¹¶ä¸”ç”±äºé™ç»´ï¼Œå‘é‡æ£€ç´¢é€Ÿåº¦è‚¯å®šä¹Ÿèƒ½å¤§å¤§åŠ å¿«ï¼›ç±»ä¼¼åœ°ï¼Œå°†largeç‰ˆçš„**1024ç»´**åªä¿ç•™å‰**384ç»´**ï¼Œé‚£ä¹ˆé™ç»´çš„åŒæ—¶ä¹Ÿæå‡äº†æ•ˆæœã€‚

è¿™ä¸ªç»“æœè¡¨æ˜ï¼š
- æ— ç›‘ç£è®­ç»ƒå‡ºæ¥çš„å¥å‘é‡å…¶å®æ˜¯â€œ**é€šç”¨å‹**â€çš„ï¼Œå¯¹äºç‰¹å®šé¢†åŸŸå†…çš„åº”ç”¨ï¼Œé‡Œè¾¹æœ‰å¾ˆå¤šç‰¹å¾æ˜¯å†—ä½™çš„ï¼Œå‰”é™¤è¿™äº›å†—ä½™ç‰¹å¾ï¼Œå¾€å¾€èƒ½è¾¾åˆ°æé€Ÿåˆææ•ˆçš„æ•ˆæœã€‚

è€Œflowæ¨¡å‹æ˜¯**å¯é€†çš„ã€ä¸é™ç»´**çš„ï¼Œè¿™åœ¨æŸäº›åœºæ™¯ä¸‹æ˜¯å¥½å¤„ï¼Œä½†åœ¨ä¸å°‘åœºæ™¯ä¸‹ä¹Ÿæ˜¯ç¼ºç‚¹ï¼Œå› ä¸ºå®ƒæ— æ³•å‰”é™¤å†—ä½™ç»´åº¦ï¼Œé™åˆ¶äº†æ€§èƒ½ï¼Œæ¯”å¦‚GANçš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä¸€ä¸ª256ç»´çš„é«˜æ–¯å‘é‡å°±å¯ä»¥éšæœºç”Ÿæˆ1024Ã—1024çš„äººè„¸å›¾ï¼Œè¿™è¡¨æ˜è¿™äº›äººè„¸å›¾å…¶å®åªæ˜¯æ„æˆäº†ä¸€ä¸ªç›¸å½“ä½ç»´çš„æµå½¢ï¼Œä½†æ˜¯å¦‚æœç”¨flowæ¨¡å‹æ¥åšï¼Œå› ä¸ºè¦ä¿è¯å¯é€†æ€§ï¼Œå°±å¾—å¼ºè¡Œç”¨1024Ã—1024Ã—3é‚£ä¹ˆå¤šç»´çš„é«˜æ–¯å‘é‡æ¥éšæœºç”Ÿæˆï¼Œè®¡ç®—æˆæœ¬å¤§å¤§å¢åŠ ï¼Œè€Œä¸”æ•ˆæœè¿˜ä¸Šä¸å»ã€‚


#### SimBERT è¿½ä¸€ç§‘æŠ€

[é±¼ä¸ç†ŠæŒå…¼å¾—ï¼šèåˆæ£€ç´¢å’Œç”Ÿæˆçš„SimBERTæ¨¡å‹](https://kexue.fm/archives/7427)

`è¿½ä¸€ç§‘æŠ€`å¼€æ”¾äº†ä¸€ä¸ªåä¸º[SimBERT](https://github.com/ZhuiyiTechnology/pretrained-models#simbert-base)çš„æ¨¡å‹æƒé‡ï¼Œä»¥Googleå¼€æºçš„BERTæ¨¡å‹ä¸ºåŸºç¡€ï¼ŒåŸºäºå¾®è½¯çš„[UniLM](https://arxiv.org/abs/1905.03197)æ€æƒ³è®¾è®¡äº†èæ£€ç´¢ä¸ç”Ÿæˆäºä¸€ä½“çš„ä»»åŠ¡ï¼Œæ¥è¿›ä¸€æ­¥å¾®è°ƒåå¾—åˆ°çš„æ¨¡å‹ï¼Œæ‰€ä»¥å®ƒåŒæ—¶å…·å¤‡ç›¸ä¼¼é—®**ç”Ÿæˆ**å’Œç›¸ä¼¼å¥**æ£€ç´¢**èƒ½åŠ›ã€‚ä¸è¿‡å½“æ—¶é™¤äº†æ”¾å‡ºä¸€ä¸ªæƒé‡æ–‡ä»¶å’Œç¤ºä¾‹è„šæœ¬ä¹‹å¤–ï¼Œæœªå¯¹æ¨¡å‹åŸç†å’Œè®­ç»ƒè¿‡ç¨‹åšè¿›ä¸€æ­¥è¯´æ˜

`UniLM`æ˜¯ä¸€ä¸ªèåˆNLUå’ŒNLGèƒ½åŠ›çš„Transformeræ¨¡å‹ï¼Œç”±å¾®è½¯åœ¨å»å¹´5æœˆä»½æå‡ºæ¥çš„ï¼Œä»Šå¹´2æœˆä»½åˆ™å‡çº§åˆ°äº†v2ç‰ˆæœ¬ã€‚ä¹‹å‰çš„æ–‡ç« ã€Šä»è¯­è¨€æ¨¡å‹åˆ°Seq2Seqï¼šTransformerå¦‚æˆï¼Œå…¨é Maskã€‹å°±ç®€å•ä»‹ç»è¿‡UniLMï¼Œå¹¶ä¸”å·²ç»é›†æˆåˆ°äº†bert4kerasä¸­ã€‚

UniLM æ ¸å¿ƒæ˜¯é€šè¿‡ç‰¹æ®Šçš„Attention Maskæ¥èµ‹äºˆæ¨¡å‹å…·æœ‰Seq2Seqçš„èƒ½åŠ›ã€‚

å‡å¦‚è¾“å…¥æ˜¯â€œ`ä½ æƒ³åƒå•¥`â€ï¼Œç›®æ ‡å¥å­æ˜¯â€œ`ç™½åˆ‡é¸¡`â€
- é‚£ UNILM å°†è¿™ä¸¤ä¸ªå¥å­æ‹¼æˆä¸€ä¸ªï¼š`[ CLS] ä½  æƒ³ åƒ å•¥ [ SEP] ç™½ åˆ‡ é¸¡ [ SEP]`
- ç„¶å, æ¥å¦‚å›¾çš„Attention Mask
  - ![](https://kexue.fm/usr/uploads/2019/09/1625339461.png)
- `[ CLS] ä½  æƒ³ åƒ å•¥ [ SEP]`è¿™å‡ ä¸ªtokenä¹‹é—´æ˜¯åŒå‘Attentionï¼Œè€Œ`ç™½ åˆ‡ é¸¡ [ SEP]`åˆ™æ˜¯å•å‘Attentionï¼Œä»è€Œå…è®¸é€’å½’åœ°é¢„æµ‹`ç™½ åˆ‡ é¸¡ [ SEP]`è¿™å‡ ä¸ªtokenï¼Œæ‰€ä»¥å®ƒå…·å¤‡æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚
- ![](https://kexue.fm/usr/uploads/2019/09/1879768703.png)

[SimBERT](https://github.com/ZhuiyiTechnology/simbert)å±äºæœ‰ç›‘ç£è®­ç»ƒï¼Œè®­ç»ƒè¯­æ–™æ˜¯è‡ªè¡Œæ”¶é›†åˆ°çš„ç›¸ä¼¼å¥å¯¹ï¼Œé€šè¿‡ä¸€å¥æ¥é¢„æµ‹å¦ä¸€å¥çš„ç›¸ä¼¼å¥ç”Ÿæˆä»»åŠ¡æ¥æ„å»ºSeq2Seqéƒ¨åˆ†ï¼Œç„¶åå‰é¢ä¹Ÿæåˆ°è¿‡[ CLS]çš„å‘é‡äº‹å®ä¸Šå°±ä»£è¡¨ç€è¾“å…¥çš„å¥å‘é‡ï¼Œæ‰€ä»¥å¯ä»¥åŒæ—¶ç”¨å®ƒæ¥è®­ç»ƒä¸€ä¸ªæ£€ç´¢ä»»åŠ¡
- ![](https://kexue.fm/usr/uploads/2020/05/2840550561.png)
- å‡è®¾`SENT_a`å’Œ`SENT_b`æ˜¯ä¸€ç»„ç›¸ä¼¼å¥ï¼Œé‚£ä¹ˆåœ¨åŒä¸€ä¸ªbatchä¸­ï¼ŒæŠŠ`[ CLS] SENT_a [ SEP] SENT_b [ SEP]` å’Œ `[ CLS] SENT_b [ SEP] SENT_a [ SEP]`éƒ½åŠ å…¥è®­ç»ƒï¼Œåšä¸€ä¸ªç›¸ä¼¼å¥çš„ç”Ÿæˆä»»åŠ¡ï¼Œè¿™æ˜¯Seq2Seqéƒ¨åˆ†ã€‚

å…³é”®å°±æ˜¯â€œ`[ CLS]`å‘é‡äº‹å®ä¸Šå°±ä»£è¡¨ç€è¾“å…¥çš„å¥å‘é‡â€ï¼Œæ‰€ä»¥å¯ä»¥åšä¸€äº›NLUç›¸å…³çš„äº‹æƒ…ã€‚æœ€åçš„lossæ˜¯Seq2Seqå’Œç›¸ä¼¼å¥åˆ†ç±»ä¸¤éƒ¨åˆ†lossä¹‹å’Œã€‚

å®æ–½ï¼š
- æ•°æ®æ¥æºæ˜¯çˆ¬å–ç™¾åº¦çŸ¥é“æ¨èçš„ç›¸ä¼¼é—®ï¼Œç„¶åç»è¿‡ç®€å•ç®—æ³•è¿‡æ»¤ã€‚å¦‚æœæœ¬èº«æœ‰å¾ˆå¤šé—®å¥ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å¸¸è§çš„æ£€ç´¢ç®—æ³•æ£€ç´¢å‡ºä¸€äº›ç›¸ä¼¼å¥ï¼Œä½œä¸ºè®­ç»ƒæ•°æ®ç”¨ã€‚æ€»è€Œè¨€ä¹‹ï¼Œè®­ç»ƒæ•°æ®æ²¡æœ‰ç‰¹åˆ«ä¸¥æ ¼è¦æ±‚ï¼Œç†è®ºä¸Šæœ‰ä¸€å®šçš„ç›¸ä¼¼æ€§éƒ½å¯ä»¥ã€‚
- è‡³äºè®­ç»ƒç¡¬ä»¶ï¼Œå¼€æºçš„æ¨¡å‹æ˜¯åœ¨ä¸€å¼ TITAN RTXï¼ˆ22Gæ˜¾å­˜ï¼Œbatch_size=128ï¼‰ä¸Šè®­ç»ƒäº†4å¤©å·¦å³ï¼Œæ˜¾å­˜å’Œæ—¶é—´å…¶å®ä¹Ÿæ²¡æœ‰ç¡¬æ€§è¦æ±‚ï¼Œè§†å®é™…æƒ…å†µè€Œå®šï¼Œå¦‚æœæ˜¾å­˜æ²¡é‚£ä¹ˆå¤§ï¼Œé‚£ä¹ˆé€‚å½“é™ä½batch_sizeå³å¯ï¼Œå¦‚æœè¯­æ–™æœ¬èº«ä¸æ˜¯å¾ˆå¤šï¼Œé‚£ä¹ˆè®­ç»ƒæ—¶é—´ä¹Ÿä¸ç”¨é‚£ä¹ˆé•¿ï¼ˆå¤§æ¦‚æ˜¯èƒ½å®Œæ•´éå†å‡ éæ•°æ®é›†å³å¯ï¼‰ã€‚

#### SimCSE å¯¹æ¯”å­¦ä¹  -- simbert ç®€åŒ–

ä¸­æ–‡ä»»åŠ¡è¿˜æ˜¯SOTAå—ï¼Ÿç»™SimCSEè¡¥å……äº†ä¸€äº›å®éªŒ

`è‹å‰‘æ—`æ„æ€çš„â€œBERT-whiteningâ€çš„æ–¹æ³•ä¸€åº¦æˆä¸ºäº†è¯­ä¹‰ç›¸ä¼¼åº¦çš„æ–°SOTAï¼Œç„¶è€Œä¸ä¹…ä¹‹åï¼ŒArxivä¸Šå‡ºç°äº†è‡³å°‘æœ‰ä¸¤ç¯‡ç»“æœæ˜æ˜¾ä¼˜äºBERT-whiteningçš„æ–°è®ºæ–‡ã€‚
- ç¬¬ä¸€ç¯‡æ˜¯ã€Š[Generating Datasets with Pretrained Language Models](https://arxiv.org/pdf/2104.07540.pdf)ã€‹ï¼Œè¿™ç¯‡å€ŸåŠ©æ¨¡æ¿ä»GPT2_XLä¸­æ— ç›‘ç£åœ°æ„é€ äº†æ•°æ®å¯¹æ¥è®­ç»ƒ**ç›¸ä¼¼åº¦**æ¨¡å‹ï¼Œè™½ç„¶æœ‰ä¸€å®šå¯å‘è€Œä¸”æ•ˆæœè¿˜å¯ä»¥ï¼Œä½†æ˜¯å¤ç°çš„æˆæœ¬å’Œå˜æ•°éƒ½å¤ªå¤§ã€‚
- å¦ä¸€ç¯‡åˆ™æ˜¯ã€Š[SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821)ã€‹ï¼Œå®ƒæå‡ºçš„`SimCSE`åœ¨è‹±æ–‡æ•°æ®ä¸Šæ˜¾è‘—è¶…è¿‡äº†BERT-flowå’ŒBERT-whiteningï¼Œå¹¶ä¸”æ–¹æ³•ç‰¹åˆ«ç®€å•ï½

[SimCSE](https://github.com/bojone/SimCSE)å¯ä»¥çœ‹æˆæ˜¯`SimBERT`çš„**ç®€åŒ–ç‰ˆ**ï¼š
- 1ã€[SimCSE](https://github.com/bojone/SimCSE)å»æ‰äº†`SimBERT`çš„ç”Ÿæˆéƒ¨åˆ†ï¼Œä»…ä¿ç•™æ£€ç´¢æ¨¡å‹ï¼›
- 2ã€ç”±äºSimCSEæ²¡æœ‰æ ‡ç­¾æ•°æ®ï¼Œæ‰€ä»¥æŠŠæ¯ä¸ªå¥å­è‡ªèº«è§†ä¸ºç›¸ä¼¼å¥ä¼ å…¥ã€‚
å³ï¼š(è‡ªå·±,è‡ªå·±) ä½œä¸º**æ­£ä¾‹**ã€(è‡ªå·±,åˆ«äºº) ä½œä¸º**è´Ÿä¾‹**æ¥è®­ç»ƒå¯¹æ¯”å­¦ä¹ æ¨¡å‹ï¼Œè¿™é‡Œçš„â€œè‡ªå·±â€å¹¶éå®Œå…¨ä¸€æ ·ï¼Œè€Œæ˜¯é‡‡ç”¨ä¸€äº›æ•°æ®æ‰©å¢æ‰‹æ®µï¼Œè®©æ­£ä¾‹çš„ä¸¤ä¸ªæ ·æœ¬æœ‰æ‰€å·®å¼‚ï¼Œä½†æ˜¯åœ¨NLPä¸­å¦‚ä½•åšæ•°æ®æ‰©å¢æœ¬èº«åˆæ˜¯ä¸€ä¸ªéš¾æçš„é—®é¢˜ï¼ŒSimCSEåˆ™æå‡ºäº†ä¸€ä¸ªæä¸ºç®€å•çš„æ–¹æ¡ˆï¼š**ç›´æ¥æŠŠDropoutå½“ä½œæ•°æ®æ‰©å¢**ï¼

å®éªŒç»“æœ
- è‹±æ–‡è¯­æ–™ï¼šSimCSE**æ˜æ˜¾ä¼˜äº**BERT-flowå’ŒBERT-whitening
- ä¸­æ–‡è¯­æ–™ï¼šé™¤äº†PAWSXè¿™ä¸ªâ€œå¼‚ç±»â€å¤–ï¼ŒSimCSEç›¸æ¯”BERT-whiteningç¡®å®æœ‰å‹å€’æ€§ä¼˜åŠ¿ï¼Œæœ‰äº›ä»»åŠ¡è¿˜èƒ½å¥½10ä¸ªç‚¹ä»¥ä¸Šï¼Œåœ¨BQä¸ŠSimCSEè¿˜æ¯”æœ‰ç›‘ç£è®­ç»ƒè¿‡çš„SimBERTè¦å¥½ï¼Œè€Œä¸”åƒSimBERTè¿™ç§å·²ç»ç»è¿‡ç›‘ç£è®­ç»ƒçš„æ¨¡å‹è¿˜èƒ½è·å¾—è¿›ä¸€æ­¥çš„æå‡ï¼Œè¿™äº›éƒ½è¯´æ˜ç¡®å®å¼ºå¤§

### BERTçœŸçš„æœ‰ç†è§£èƒ½åŠ›å—

ã€2019-07-12ã€‘
- ACL 2019ï¼Œ[è®ºæ–‡1ï¼šProbing Neural Network Comprehension of Natural Language Arguments](https://arxiv.org/abs/1907.07355), å°æ¹¾æˆåŠŸå¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸ä¿¡æ¯å·¥ç¨‹ç³»æ™ºèƒ½çŸ¥è¯†ç®¡ç†å®éªŒå®¤ï¼Œä¸€ä½œ Timothy Nivenï¼ŒARCTä¸€ä¸ªæ•°æ®é›†
  - [å°æ¹¾å°å“¥ä¸€ç¯‡è®ºæ–‡æŠŠ BERT æ‹‰ä¸‹ç¥å›](https://zhuanlan.zhihu.com/p/74652696)
  - [æŠŠBERTæ‹‰ä¸‹ç¥å›ï¼ACLè®ºæ–‡åªé ä¸€ä¸ªâ€œNotâ€ï¼Œå°±æŠŠAIé˜…è¯»ç†è§£éª¤é™åˆ°ç›²çŒœæ°´å¹³](https://bbs.huaweicloud.com/blogs/107657)
  - We are surprised to find that BERT's peak performance of 77% on the **Argument Reasoning Comprehension Task** reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an **adversarial dataset** on which <font color='red'>all models achieve random accuracy</font>. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.
- ACL 2019, [è®ºæ–‡2ï¼šRight for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference](https://arxiv.org/abs/1902.01007)ï¼Œåªä¸è¿‡ï¼Œæ•°æ®é›†ä¸ä¸€æ ·äº†ã€‚è¿™ç¯‡è®ºæ–‡é‡ŒBERTæ˜¯åœ¨**å¤šç±»å‹è¯­è¨€æ¨ç†æ•°æ®é›†** (MNLI) ä¸Šè®­ç»ƒçš„ï¼Œè€Œæµ‹è¯•é›†åˆ™æ˜¯ç ”ç©¶å›¢é˜Ÿè‡ªåˆ¶çš„HANSæ•°æ®é›†ï¼Œä¸€äº›ç®€å•çš„å¥å­å˜æ¢ï¼Œå°±èƒ½è®©AIåšå‡ºé”™è¯¯çš„åˆ¤æ–­ã€‚
  - A machine learning system can score well on a given test set by relying on heuristics that are effective for **frequent example types** but break down in more challenging cases. We study this issue within **natural language inference** (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a **controlled evaluation set** called `HANS` (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area
- è®ºæ–‡3ï¼šWhat does BERT Learn from Multiple-Choice Reading Comprehension Datasets? åœ¨æ•°æ®é›†ä¸­æ·»åŠ å¹²æ‰°æ–‡æœ¬ï¼Œç»“æœæ˜¾ç¤ºBERTçš„è¡¨ç°éå¸¸å·®
- ç ”ç©¶è®¤ä¸ºï¼ŒåŒ…æ‹¬BERTåœ¨å†…ï¼Œè®¸å¤šæ¨¡å‹çš„æˆåŠŸéƒ½æ˜¯å»ºç«‹åœ¨**è™šå‡çš„çº¿ç´¢**ä¸Šã€‚å›¢é˜Ÿç”¨äº†å»å¹´è¯ç”Ÿçš„`è§‚ç‚¹æ¨ç†ç†è§£ä»»åŠ¡` (ARCT) è€ƒéªŒäº†BERTã€‚ç»“æœå‘ç°ï¼Œåªè¦åšä¸ª**å¯¹æŠ—æ•°æ®é›†**ï¼Œå‡†ç¡®ç‡å°±ä»77%é™åˆ°53%ï¼Œå‡ ä¹ç­‰åŒäºéšæœºçŒœ
  - å¯¹æŠ—å¹¶ä¸æ˜¯æŠŠoå˜æˆ0ã€æŠŠIå˜æˆ1çš„å°ä¼ä¿©
  - BERTæ˜¯ä¾é æ•°æ®é›†é‡Œ â€œ**è™šå‡çš„ç»Ÿè®¡å­¦çº¿ç´¢** (Spurious Statistical Cues) â€æ¥æ¨ç†çš„ï¼ŒBERTæ˜¯åˆ©ç”¨äº†ä¸€äº›çº¿ç´¢è¯æ¥åˆ¤æ–­ï¼Œç‰¹åˆ«æ˜¯â€œNotâ€è¿™ä¸ªè¯, å®ƒå¹¶ä¸èƒ½çœŸæ­£åˆ†æå‡ºå¥å­ä¹‹é—´çš„é€»è¾‘å…³ç³»ã€‚
  - `è§‚ç‚¹æ¨ç†ç†è§£ä»»åŠ¡` (ARCT) ï¼Œæ˜¯Habernalå’Œå°ä¼™ä¼´ä»¬æå‡ºçš„é˜…è¯»ç†è§£ä»»åŠ¡ï¼Œè€ƒå¯Ÿçš„æ˜¯è¯­è¨€æ¨¡å‹çš„**æ¨ç†èƒ½åŠ›**ï¼Œä¸­é€‰äº†NAACL 2018ã€‚ä¸€ä¸ª`è§‚ç‚¹`ï¼ŒåŒ…å«`å‰æ` (Premise) ï¼Œå’Œ`ä¸»å¼ ` (Claim) ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè§‚ç‚¹åˆæœ‰å®ƒçš„`åŸå› ` (Reasoning) ï¼Œä»¥åŠå®ƒçš„`ä½è¯` (Warrant) ã€‚
- è¿™ç¯‡ACLè®ºæ–‡æ‰“å‡»äº†ä»¥BERTä¸ºé¦–çš„ä¼—å¤šé˜…è¯»ç†è§£æ¨¡å‹

Redditè¯„è®ºåŒºè¡¥å……è¯´ï¼š
> - æ¯éš”å‡ ä¸ªæœˆå°±ä¼šå¬åˆ°æœ‰å…³NLPçš„æ–°è¿›å±•ï¼Œæ›´æ–°ã€æ›´å¥½çš„æ¨¡å‹å±‚å‡ºä¸ç©·ã€‚ä½†å½“æœ‰äººå®é™…ç”¨æ•°æ®é›†æµ‹è¯•æ—¶ï¼Œä¼šå‘ç°è¿™äº›æ¨¡å‹å¹¶æ²¡æœ‰çœŸæ­£å­¦ä¹ åˆ°ä»€ä¹ˆã€‚ä¼˜åŒ–æ¨¡å‹çš„ç«èµ›è¯¥æ”¾ç¼“è„šæ­¥äº†ï¼Œæˆ‘ä»¬æ›´åº”è¯¥ä»”ç»†ç ”ç©¶ç ”ç©¶æ•°æ®é›†ï¼Œçœ‹çœ‹å®ƒä»¬æ˜¯å¦çœŸçš„æœ‰æ„ä¹‰ã€‚
> - å¹¶ä¸å¦è®¤BERTå’Œå…¶ä»–æ–°æ¨¡å‹çš„ä»·å€¼ï¼Œä½†æ˜¯å¹¶ä¸ç›¸ä¿¡ä¸€äº›Benchmarkã€‚


# BERTå˜ç§

## æ”¹è¿›æ€»ç»“

- ã€2022-5-26ã€‘[BERTæ¨¡å‹çš„ä¼˜åŒ–æ”¹è¿›æ–¹æ³•](https://zhuanlan.zhihu.com/p/514849987), è®ºæ–‡ã€ŠBERTæ¨¡å‹çš„ä¸»è¦ä¼˜åŒ–æ”¹è¿›æ–¹æ³•ç ”ç©¶ç»¼è¿°ã€‹çš„é˜…è¯»ç¬”è®°ï¼Œå¯¹ BERTä¸»è¦ä¼˜åŒ–æ”¹è¿›æ–¹æ³•è¿›è¡Œäº†ç ”ç©¶æ¢³ç†ã€‚

åŸºç¡€å›é¡¾
- BERTæ˜¯ç”±Google AIäº2018å¹´10æœˆæå‡ºçš„ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­è¨€è¡¨ç¤ºæ¨¡å‹ã€‚BERT å‘å¸ƒæ—¶ï¼Œåœ¨11ç§ä¸åŒçš„NLPæµ‹è¯•ä»»åŠ¡ä¸­å–å¾—æœ€ä½³æ•ˆæœï¼ŒNLPé¢†åŸŸè¿‘æœŸé‡è¦çš„ç ”ç©¶æˆæœã€‚
  - ![](https://pic4.zhimg.com/80/v2-c1d76b60d55e808946c73c54556753b3_1440w.jpg)
- BERTæ¨¡å‹ç»“æ„æ˜¯**Transformerç¼–ç å™¨**ã€‚Transformeræ˜¯ç”± Ashish ç­‰äº2017å¹´æå‡ºçš„ï¼Œç”¨äºGoogleæœºå™¨ç¿»è¯‘ï¼ŒåŒ…å«ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ä¸¤éƒ¨åˆ†ã€‚
BERT æ¨¡å‹ä½¿ç”¨ä¸¤ä¸ªé¢„è®­ç»ƒç›®æ ‡æ¥å®Œæˆæ–‡æœ¬å†…å®¹ç‰¹å¾çš„å­¦ä¹ ã€‚
- **æ©è—è¯­è¨€æ¨¡å‹**ï¼ˆMasked Language Modelï¼ŒMLMï¼‰é€šè¿‡å°†å•è¯æ©ç›–ï¼Œä»è€Œå­¦ä¹ å…¶ä¸Šä¸‹æ–‡å†…å®¹ç‰¹å¾æ¥é¢„æµ‹è¢«æ©ç›–çš„å•è¯
- **ç›¸é‚»å¥é¢„æµ‹**ï¼ˆNext Sentence Predicationï¼ŒNSPï¼‰é€šè¿‡å­¦ä¹ å¥å­é—´å…³ç³»ç‰¹å¾ï¼Œé¢„æµ‹ä¸¤ä¸ªå¥å­çš„ä½ç½®æ˜¯å¦ç›¸é‚»

### BERT è¿›åŒ–å›¾

[Natural Language Processing](https://github.com/javiabellan/nlp/tree/master#readme)

| Model | Creator | Date | Breif description | ğŸ¤— |
| :-: | :-: | :-: | --- | :-: |
| [**1st Transfor.**](https://arxiv.org/abs/1706.03762) | Google | Jun. 2017 | Transforer encoder & decoder |  |
| [**ULMFiT**](https://arxiv.org/abs/1801.06146) | Fast.ai | Jan. 2018 | Regular LSTM |  |
| [**ELMo**](https://arxiv.org/abs/1802.05365) | AllenNLP | Feb. 2018 | Bidirectional LSTM |  |
| **GPT** | OpenAI | Jun. 2018 | Transformer decoder on LM | âœ” |
| [**BERT**](https://arxiv.org/abs/1810.04805) | Google | Oct. 2018 | Transformer encoder on MLM (& NSP) | âœ” |
| [**TransformerXL**](https://arxiv.org/abs/1901.02860) | Google | Jan. 2019 | Recurrent transformer decoder | âœ” |
| [**XLM/mBERT**](https://arxiv.org/abs/1901.07291) | Facebook | Jan. 2019 | Multilingual LM | âœ” |
| **Transf. ELMo** | AllenNLP | Jan. 2019 |  |  |
| **GPT-2** | OpenAI | Feb. 2019 | Good text generation | âœ” |
| [**ERNIE**](https://arxiv.org/abs/1904.09223) | Baidu | Apr. 2019 |  |  |
| [**ERNIE**](https://arxiv.org/abs/1905.07129) | Tsinghua | May. 2019 | Transformer with Knowledge Graph |  |
| [**XLNet**](https://arxiv.org/abs/1906.08237) | Google | Jun. 2019 | BERT + Transformer-XL | âœ” |
| [**RoBERTa**](https://arxiv.org/abs/1907.11692) | Facebook | Jul. 2019 | BERT without NSP | âœ” |
| **DistilBERT** | Hug. Face | Aug. 2019 | Compressed BERT | âœ” |
| [**MiniBERT**](https://arxiv.org/abs/1909.00100) | Google | Aug. 2019 | Compressed BERT |  |
| [**MultiFiT**](https://arxiv.org/abs/1909.04761) | Fast.ai | Sep. 2019 | Multi-lingual ULMFiT (QRNN) [post](http://nlp.fast.ai/classification/2019/09/10/multifit.html) |  |
| [**CTRL**](https://arxiv.org/abs/1909.05858) | Salesforce | Sep. 2019 | Controllable text generation | âœ” |
| [**MegatronLM**](https://arxiv.org/abs/1909.08053) | Nvidia | Sep. 2019 | Big models with parallel training |  |
| [**ALBERT**](https://arxiv.org/abs/1909.11942) | Google | Sep. 2019 | Reduce BERT params (param sharing) | âœ” |
| **DistilGPT-2** | Hug. Face | Oct. 2019 | Compressed GPT-2 | âœ” |
| [**T5**](https://arxiv.org/abs/1910.10683) | Google | Oct. 2019 | Text-to-Text Transfer Transformer | âœ” |
| [**ELECTRA**](https://openreview.net/pdf?id=r1xMH1BtvB) | ? | Dec. 2019 | An efficient LM pretraining |  |
| [**Reformer**](https://arxiv.org/abs/2001.04451) | Google | Jan. 2020 | The Efficient Transformer |  |
| [**Meena**](https://arxiv.org/abs/2001.09977) | Google | Jan. 2020 | A Human-like Open-Domain Chatbot |  |

| Model | 2L | 3L | 6L | 12L | 18L | 24L | 36L | 48L | 54L | 72L |
| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |
| **1st Transformer** |  |  |  | yes |  |  |  |  |  |  |
| **ULMFiT** |  | yes |  |  |  |  |  |  |  |  |
| **ELMo** | yes |  |  |  |  |  |  |  |  |  |
| **GPT** |  |  |  | 110M |  |  |  |  |  |  |
| **BERT** |  |  |  | 110M |  | 340M |  |  |  |  |
| **Transformer-XL** |  |  |  |  | 257M |  |  |  |  |  |
| **XLM/mBERT** |  |  | Yes | Yes |  |  |  |  |  |  |
| **Transf. ELMo** |  |  |  |  |  |  |  |  |  |  |
| **GPT-2** |  |  |  | 117M |  | 345M | 762M | 1542M |  |  |
| **ERNIE** |  |  | Yes |  |  |  |  |  |  |  |
| **XLNet**: |  |  |  | 110M |  | 340M |  |  |  |  |
| **RoBERTa** |  |  |  | 125M |  | 355M |  |  |  |  |
| **MegatronLM** |  |  |  |  |  | 355M |  |  | 2500M | 8300M |
| **DistilBERT** |  |  | 66M |  |  |  |  |  |  |  |
| **MiniBERT** |  | Yes |  |  |  |  |  |  |  |  |
| **ALBERT** |  |  |  |  |  |  |  |  |  |  |
| **CTRL** |  |  |  |  |  |  |  | 1630M |  |  |
| **DistilGPT-2** |  |  | 82M |  |  |  |  |  |  |  |


### BERTå®¶æ—

- `BERT`ï¼šMLM å’Œ NSPä»»åŠ¡
  - åŸºäº Transformer Encoder æ„å»ºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡ Masked Lanauge Model(`MLM`) å’Œ Next Sentence Prediction(`NSP`) ä¸¤ä¸ªä»»åŠ¡åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šè®­ç»ƒå¾—åˆ°çš„
  - å¼€æºçš„ Bert æ¨¡å‹åˆ†ä¸º base å’Œ largeï¼Œå·®å¼‚åœ¨æ¨¡å‹å¤§å°ä¸Šã€‚å¤§æ¨¡å‹æœ‰æ›´å¤§çš„å‚æ•°é‡ï¼Œæ€§èƒ½ä¹Ÿæœ‰ä¼šå‡ ä¸ªç™¾åˆ†ç‚¹çš„æå‡ï¼Œå½“ç„¶éœ€è¦æ¶ˆè€—æ›´å¤šçš„ç®—åŠ›
- `BERT-WWM`ï¼šmaskç­–ç•¥ç”±token çº§åˆ«å‡çº§ä¸º**è¯çº§åˆ«**
- `Roberta`ï¼šBERT**ä¼˜åŒ–**ç‰ˆï¼Œæ›´å¤šæ•°æ®+è¿­ä»£æ­¥æ•°+å»é™¤NSP+åŠ¨æ€mask
- `XLNet`ï¼šæ¨¡å‹ç»“æ„å’Œè®­ç»ƒæ–¹å¼ä¸Šä¸BERTå·®åˆ«è¾ƒå¤§
  - Bert çš„ MLM åœ¨é¢„è®­ç»ƒæ—¶æœ‰ MASK æ ‡ç­¾ï¼Œä½†åœ¨æ¨ç†æ—¶å´æ²¡æœ‰ï¼Œå¯¼è‡´è®­ç»ƒå’Œæ¨ç†å‡ºç°ä¸ä¸€è‡´ï¼›
  - å¹¶ä¸” MLM ä¸å±äº Autoregressive LMï¼Œä¸èƒ½åš**ç”Ÿæˆç±»**ä»»åŠ¡ã€‚
  - XLNet é‡‡ç”¨ PLM(Permutation Language Model) é¿å…äº† MASK æ ‡ç­¾çš„ä½¿ç”¨ï¼Œä¸”å±äº Autoregressive LMï¼Œå¯ä»¥åšç”Ÿæˆä»»åŠ¡ã€‚
  - Bert ä½¿ç”¨ Transformer ç»“æ„å¯¹æ–‡æœ¬çš„é•¿åº¦**æœ‰é™åˆ¶**ï¼Œä¸ºæ›´å¥½åœ°å¤„ç†é•¿æ–‡æœ¬ï¼ŒXLNet é‡‡ç”¨å‡çº§ç‰ˆçš„ `Transformer-XL`ã€‚
- `Albert`ï¼šBERT**ç®€åŒ–**ç‰ˆï¼Œæ›´å°‘çš„æ•°æ®ï¼Œå¾—åˆ°æ›´å¥½çš„ç»“æœï¼ˆ70% å‚æ•°é‡çš„å‰Šå‡ï¼Œæ¨¡å‹æ€§èƒ½æŸå¤±<3% ï¼‰ï¼›ä¸¤ä¸ªæ–¹é¢å‡å°‘æ¨¡å‹çš„å‚æ•°é‡ï¼š
  - å¯¹ Vocabulary Embedding è¿›è¡ŒçŸ©é˜µåˆ†è§£ï¼Œå°†åŸæ¥çŸ©é˜µ`V x E`åˆ†è§£æˆä¸¤ä¸ªçŸ©é˜µ`V x H`å’Œ`H x E`ï¼ˆH << Eï¼‰ã€‚
  - è·¨å±‚å‚æ•°å…±äº«ï¼Œæ¯å±‚çš„ attention map æœ‰ç›¸ä¼¼çš„patternï¼Œå¯ä»¥è€ƒè™‘å…±äº«ã€‚

è¿™äº›æ¨¡å‹çš„æ€§èƒ½åœ¨ä¸åŒæ•°æ®é›†ä¸Šæœ‰å·®å¼‚ï¼Œä½†æ€»ä½“è€Œè¨€ XLNet å’Œ Roberta ä¼šæ¯” Bert æ•ˆæœç•¥å¥½ï¼Œlarge ä¼šæ¯” base ç•¥å¥½ï¼Œæ›´å¤šæƒ…å†µä¸‹ï¼Œå®ƒä»¬ä¼šè¢«ä¸€èµ·ä½¿ç”¨ï¼Œæœ€ååš ensembleã€‚

### åˆ†æ”¯1ï¼šæ”¹è¿›é¢„è®­ç»ƒ

è‡ªç„¶è¯­è¨€ä¸°å¯Œå¤šå˜ï¼Œç ”ç©¶è€…é’ˆå¯¹æ›´ä¸°å¯Œå¤šå˜çš„æ–‡æœ¬è¡¨è¾¾å½¢å¼ï¼Œåœ¨è¿™ä¸¤ä¸ªè®­ç»ƒç›®æ ‡çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥å®Œå–„å’Œæ”¹è¿›ï¼Œæå‡äº†æ¨¡å‹çš„æ–‡æœ¬ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚

#### 1.1 æ”¹è¿›æ©è—è¯­è¨€æ¨¡å‹ â€”â€” å…¨è¯è¦†ç›–ï¼ˆwwm/ERNIE/SpanBERTï¼‰

BERTæ¨¡å‹ä¸­ï¼Œå¯¹æ–‡æœ¬çš„é¢„å¤„ç†éƒ½æŒ‰ç…§**æœ€å°å•ä½**è¿›è¡Œäº†åˆ‡åˆ†ã€‚ä¾‹å¦‚å¯¹äºè‹±æ–‡æ–‡æœ¬çš„é¢„å¤„ç†é‡‡ç”¨äº†Googleçš„ wordpiece æ–¹æ³•ä»¥è§£å†³å…¶æœªç™»å½•è¯çš„é—®é¢˜ã€‚
- MLMä¸­æ©ç›–çš„å¯¹è±¡å¤šæ•°æƒ…å†µä¸‹ä¸º**è¯æ ¹**ï¼ˆsubwordï¼‰ï¼Œå¹¶ä¸æ˜¯å®Œæ•´çš„è¯ï¼›
- å¯¹äºä¸­æ–‡åˆ™ç›´æ¥æŒ‰**å­—**åˆ‡åˆ†ï¼Œç›´æ¥å¯¹å•ä¸ªå­—è¿›è¡Œæ©ç›–ã€‚è¿™ç§æ©ç›–ç­–ç•¥å¯¼è‡´äº†æ¨¡å‹å¯¹äºè¯è¯­ä¿¡æ¯å­¦ä¹ çš„ä¸å®Œæ•´ã€‚
  - æ±‰è¯­çš„è¯è¯­å‡ ä¹å¯ä»¥ç”¨æ— ç©·æ— å°½æ¥å½¢å®¹ã€‚ä½†ä¸­æ–‡å¸¸ç”¨çš„æ±‰å­—ä¹Ÿå°±**4000**å¤šä¸ªï¼ŒBERTè®­ç»ƒå¾ˆå®¹æ˜“å®ç°ï¼Œæ‰€ä»¥æœ€åˆBERTå¯¹ä¸­æ–‡ä»»åŠ¡æ˜¯ä»¥**æ±‰å­—**ä¸ºå•ä½å®ç°è®­ç»ƒçš„ã€‚
  - é—®é¢˜ï¼šæ—¢ç„¶æ˜¯ä»¥æ±‰å­—ä¸ºå•ä½è®­ç»ƒçš„ï¼Œå…¶è®­ç»ƒå‡ºçš„å°±æ˜¯å­¤é›¶é›¶çš„æ±‰å­—å‘é‡ï¼Œè€Œåœ¨ç°ä»£æ±‰è¯­ä¸­ï¼Œå•ä¸ªæ±‰å­—æ˜¯è¿œä¸è¶³ä»¥è¡¨è¾¾åƒè¯è¯­æˆ–è€…çŸ­è¯­é‚£æ ·ä¸°å¯Œçš„è¯­ä¹‰çš„ï¼Œè¿™å°±é€ æˆäº†BERTåœ¨å¾ˆå¤šä¸­æ–‡ä»»åŠ¡ä¸Šè¡¨ç°å¹¶ä¸ç†æƒ³çš„æƒ…å†µã€‚
é’ˆå¯¹è¿™ä¸€ä¸è¶³ï¼Œå¤§éƒ¨åˆ†ç ”ç©¶è€…æ”¹è¿›äº†MLMçš„æ©ç›–ç­–ç•¥ã€‚

`BERT-WWM` æ¨¡å‹ä¸­ï¼Œæå‡ºäº†**å…¨è¯è¦†ç›–**çš„æ–¹å¼ã€‚
- 2019å¹´7æœˆï¼Œå“ˆå·¥å¤§å‘å¸ƒçš„ `BERT-Chinese-wwm` åˆ©ç”¨ä¸­æ–‡åˆ†è¯ï¼Œå°†ç»„æˆä¸€ä¸ªå®Œæ•´è¯è¯­çš„æ‰€æœ‰å•å­—åŒæ—¶æ©ç›–ã€‚
- 2019å¹´4æœˆï¼Œ`ERNIE` æ‰©å±•äº†ä¸­æ–‡å…¨è¯æ©ç›–ç­–ç•¥ï¼Œæ‰©å±•åˆ°å¯¹äºä¸­æ–‡åˆ†è¯ã€çŸ­è¯­åŠå‘½åå®ä½“çš„å…¨è¯æ©ç›–ã€‚
  - ç™¾åº¦ç‰ˆï¼šå…¨è¯maskï¼Œ**éšå¼**è¾“å…¥çŸ¥è¯†ä¿¡æ¯ï¼Œmaskè®­ç»ƒé˜¶æ®µå°†çŸ¥è¯†å®ä½“å…¨éƒ¨mask
  - æ¸…åç‰ˆï¼šçŸ¥è¯†å›¾è°±ï¼Œ**æ˜¾å¼**åŠ å…¥çŸ¥è¯†ä¿¡æ¯ï¼Œåœ¨è¾“å…¥é˜¶æ®µ
- `SpanBERT` é‡‡ç”¨äº†å‡ ä½•åˆ†å¸ƒæ¥éšæœºé‡‡æ ·è¢«æ©ç›–çš„çŸ­è¯­ç‰‡æ®µï¼Œé€šè¿‡Spanè¾¹ç•Œè¯å‘é‡æ¥é¢„æµ‹æ©ç›–è¯

##### ï¼ˆ1ï¼‰BERT-WWM

BERT-WWMæ˜¯ 2019å¹´7æœˆ æ¨å‡ºçš„ï¼Œå®ƒæ˜¯ç”±å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤é’ˆå¯¹ä¸­æ–‡åšå‡ºçš„æ”¹è¿›ï¼ŒWWM çš„æ„æ€æ˜¯Whole Word Maskingï¼Œå…¶å®å°±æ˜¯ERNIEæ¨¡å‹ä¸­çš„**çŸ­è¯­çº§åˆ«**é®ç›–ï¼ˆphrase-level maskingï¼‰è®­ç»ƒã€‚

BERT-WWMå±äºBERT-baseæ¨¡å‹ç»“æ„ï¼Œç”±12å±‚Transformersæ„æˆã€‚
- è®­ç»ƒç¬¬ä¸€é˜¶æ®µï¼ˆæœ€å¤§é•¿åº¦ä¸º128ï¼‰é‡‡ç”¨çš„ batch sizeä¸º2560ï¼Œè®­ç»ƒäº†100Kæ­¥ã€‚
- è®­ç»ƒç¬¬äºŒé˜¶æ®µï¼ˆæœ€å¤§é•¿åº¦ä¸º512ï¼‰é‡‡ç”¨çš„ batch sizeä¸º384ï¼Œè®­ç»ƒäº†100Kæ­¥ã€‚

åç»­åˆå‡ºäº†ä¸€ä¸ª BERT-WWM-ext ï¼Œå®ƒæ˜¯BERT-WWMçš„ä¸€ä¸ª**å‡çº§ç‰ˆ**ï¼Œç›¸æ¯”äºBERT-WWMçš„æ”¹è¿›æ˜¯å¢åŠ äº†è®­ç»ƒæ•°æ®é›†åŒæ—¶ä¹Ÿå¢åŠ äº†è®­ç»ƒæ­¥æ•°ï¼Œæ€§èƒ½ä¹Ÿå¾—åˆ°äº†ä¸€å®šç¨‹åº¦çš„æå‡ã€‚å®ƒä¹Ÿå±äºBERT-baseæ¨¡å‹ç»“æ„ï¼Œç”±12å±‚Transformersæ„æˆã€‚
- è®­ç»ƒç¬¬ä¸€é˜¶æ®µï¼ˆæœ€å¤§é•¿åº¦ä¸º128ï¼‰é‡‡ç”¨çš„batch sizeä¸º2560ï¼Œè®­ç»ƒäº†1000Kæ­¥ã€‚
- è®­ç»ƒç¬¬äºŒé˜¶æ®µï¼ˆæœ€å¤§é•¿åº¦ä¸º512ï¼‰é‡‡ç”¨çš„batch sizeä¸º384ï¼Œè®­ç»ƒäº†400Kæ­¥ã€‚

ä¸ºä»€ä¹ˆè¦å•ç‹¬è¯´è¿™ä¸ªå‘¢ï¼Ÿå› ä¸ºæå‡ºBERT-WWMçš„è®ºæ–‡ä¸­ï¼Œä½œè€…å›¢é˜Ÿè¿˜åŒæ—¶è®­ç»ƒå‡ºäº†ï¼šBERT-WWMã€BERT-WWM-extã€RoBERTa-WWM-extå’ŒRoBERTa-WWM-ext-largeæ¨¡å‹ï¼Œå¹¶ä¸”å¯¹æ¯”äº†è¿™äº›æ¨¡å‹çš„æ€§èƒ½ï¼Œå…¶ä¸­RoBERTa-WWM-ext-largeçš„å¼ºæ‚æ€§èƒ½ä¹Ÿè®©å¾ˆå¤šäººå¯¹å®ƒé’çæœ‰åŠ ï¼Œå³ä¾¿æ˜¯æ¨¡å‹å‘å¸ƒä¸€å¹´ä¹‹ä¹…çš„ä»Šå¤©å®ƒä¹Ÿéš¾é€¢æ•Œæ‰‹ã€‚è¿™äº›æ¨¡å‹çš„å‘å¸ƒä¸ºNLPä»ä¸šè€…ã€å­¦ä¹ è€…å’Œçˆ±å¥½è€…æä¾›äº†æå¤§çš„æ–¹ä¾¿ï¼Œä½¿ç”¨èµ·æ¥ä¹Ÿå¾ˆç®€å•ï¼Œåœ¨æ­¤æ„Ÿè°¢ä½œè€…ä»¬ã€‚

##### ï¼ˆ2ï¼‰ERNIE

ERNIEçš„æ”¹è¿›æ€è·¯ï¼šç»™æ¨¡å‹åŠ ä¸Š**è¯è¯­**ä¿¡æ¯ï¼Œå³ç™¾åº¦å®£ç§°çš„è®©æ¨¡å‹å­¦åˆ°â€œçŸ¥è¯†â€

é‚£è¦æ€æ ·åŠ å…¥çŸ¥è¯†ä¿¡æ¯å‘¢ï¼Ÿæœ‰ä¸¤ç§æ–¹æ³•ï¼š
- ç¬¬ä¸€ç§æ˜¯**æ˜¾å¼**çš„æ–¹æ³•ï¼Œå³é€šè¿‡æŸç§æ‰‹æ®µï¼Œåœ¨**è¾“å…¥**é˜¶æ®µï¼Œå°±ç»™æ¨¡å‹è¾“å…¥çŸ¥è¯†å®ä½“ã€‚ï¼ˆæ¸…åç‰ˆï¼‰
- ç¬¬äºŒç§æ˜¯**éšå¼**çš„æ–¹æ³•ï¼Œå³åœ¨masked**è®­ç»ƒ**é˜¶æ®µï¼Œå°†åŒ…å«æœ‰æŸä¸€ä¸ªçŸ¥è¯†å®ä½“çš„æ±‰å­—å…¨éƒ½Maskæ‰ï¼Œè®©æ¨¡å‹æ¥çŒœæµ‹è¿™ä¸ªè¯è¯­ï¼Œä»è€Œå­¦ä¹ åˆ°çŸ¥è¯†å®ä½“ã€‚ï¼ˆç™¾åº¦ç‰ˆï¼‰
ç™¾åº¦çš„ERNIEé€‰å–çš„æ˜¯ç¬¬äºŒç§æ–¹æ¡ˆï¼Œå…¶å®æ¸…åå¤§å­¦ä¹Ÿæ¨å‡ºäº†ä¸€ä¸ªåŒåçš„ERNIEæ¨¡å‹ï¼Œé‡‡å–çš„å°±æ˜¯ç¬¬ä¸€ç§æ–¹æ³•ï¼Œä½†è²Œä¼¼è¡¨ç°æ•ˆæœå’Œåæ°”éƒ½ä¸å¦‚ç™¾åº¦çš„ERNIEï¼Œæ•…åœ¨æ­¤ä¸å¤šè°ˆã€‚ç™¾åº¦æ˜¯å›½å†…æœç´¢å¼•æ“å¸¸å¹´çš„éœ¸ä¸»ï¼Œå…¶å¯¹è¯­ä¹‰ç†è§£å’ŒçŸ¥è¯†å›¾è°±çš„ç ”ç©¶å¯è°“æ˜¯ç‚‰ç«çº¯é’ï¼Œæ‰€ä»¥ç™¾åº¦å°†çŸ¥è¯†å›¾è°±å¼•å…¥äº†BERTä¸­ï¼Œå½¢æˆäº†ERNIEã€‚

å®é™…ä¸Šï¼Œè‹±æ–‡ä»»åŠ¡ä¹Ÿå¯ä»¥é€šè¿‡ERNIEçš„æ–¹å¼æ¥æ”¹è¿›ï¼Œæ©ç›–æ•´ä¸ªè‹±æ–‡çŸ­è¯­æˆ–è€…å®ä½“ï¼Œè®©BERTå­¦ä¹ åˆ°çŸ¥è¯†
- ![](https://pic4.zhimg.com/80/v2-3477503af72ce5ad3dd9c3b233447837_1440w.jpg)

ERNIRçš„maskæ˜¯ç”±ä¸‰ä¸ªé˜¶æ®µå­¦ä¹ ç»„æˆ
- ç¬¬ä¸€ä¸ªé˜¶æ®µï¼Œé‡‡ç”¨çš„æ˜¯ BERTæ¨¡å¼çš„ word pieceï¼ˆè¯æ ¹ï¼‰ çº§åˆ«çš„ mask (basic-level masking)
- å…¶æ¬¡ï¼ŒåŠ å…¥**çŸ­è¯­**çº§åˆ«çš„mask(phrase-level masking)
- æœ€åï¼Œå†åŠ å…¥**å®ä½“**çº§åˆ«çš„mask(entity-level masking)ã€‚

![](https://pic1.zhimg.com/80/v2-39571e3fc839399476c5fb3995b32944_1440w.png)

åœ¨è¿™ä¸ªåŸºç¡€ä¸Šï¼Œå€ŸåŠ©ç™¾åº¦åœ¨ä¸­æ–‡çš„ç¤¾åŒºçš„å¼ºå¤§èƒ½åŠ›ï¼Œä¸­æ–‡çš„ERNIRè¿˜ç”¨äº†å„ç§æ··æ‚æ•°æ®é›†(Heterogeneous Data)ã€‚

ERNIE 2.0
- ç™¾åº¦åœ¨ä¹‹å‰çš„æ¨¡å‹ä¸Šåšäº†æ–°çš„æ”¹è¿›ï¼Œè¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯èµ°**å¤šä»»åŠ¡**çš„æ€æƒ³ï¼Œå¼•å…¥äº†å¤šå¤§7ä¸ªä»»åŠ¡æ¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”é‡‡ç”¨çš„æ˜¯é€æ¬¡å¢åŠ ä»»åŠ¡çš„æ–¹å¼æ¥é¢„è®­ç»ƒï¼Œå…·ä½“çš„ä»»åŠ¡å¦‚ä¸‹é¢å›¾ä¸­æ‰€ç¤ºï¼Œå›¾ä¸­çº¢æ¡†ã€è“æ¡†ã€ç»¿æ¡†é‡Œé¢çš„å°±æ˜¯ä¸ƒç§ä»»åŠ¡çš„åç§°ï¼š
- ![](https://pic3.zhimg.com/80/v2-63bc4e5877bd7b1944483286aa883522_1440w.jpg)
- å› ä¸ºä¸åŒçš„ä»»åŠ¡è¾“å…¥ä¸åŒï¼Œå› æ­¤ä½œè€…è¿˜å¼•å…¥äº†Task Embeddingï¼Œæ¥åŒºåˆ†ä¸åŒçš„ä»»åŠ¡ã€‚çœŸå°±ä¸‡ç‰©çš†å¯Embeddingå‘—ã€‚
- è®­ç»ƒçš„æ–¹æ³•æ˜¯å…ˆè®­ç»ƒä»»åŠ¡1ï¼Œä¿å­˜æ¨¡å‹ï¼Œç„¶ååŠ è½½åˆšä¿å­˜çš„æ¨¡å‹ï¼Œå†åŒæ—¶è®­ç»ƒä»»åŠ¡1å’Œä»»åŠ¡2ï¼Œä¾æ¬¡ç±»æ¨ï¼Œåˆ°æœ€ååŒæ—¶è®­ç»ƒ7ä¸ªä»»åŠ¡ã€‚

æ•ˆæœï¼š
- 2.0åœ¨æ•ˆæœä¸Šæ¯”1.0ç‰ˆæœ¬å…¨é¢æå‡ï¼Œå…¶ä¸­ï¼Œåœ¨é˜…è¯»ç†è§£çš„ä»»åŠ¡ä¸Šæå‡éå¸¸å¤§ã€‚

#### 1.2 å¼•å…¥é™å™ªè‡ªç¼–ç å™¨ â€”â€” BART

MLM å°†åŸæ–‡ä¸­çš„è¯ç”¨`[MASK]`æ ‡è®°**éšæœº**æ›¿æ¢ï¼Œè¿™æœ¬èº«æ˜¯å¯¹æ–‡æœ¬è¿›è¡Œäº†ç ´åï¼Œç›¸å½“äºåœ¨æ–‡æœ¬ä¸­æ·»åŠ äº†å™ªå£°ï¼Œç„¶åé€šè¿‡è®­ç»ƒè¯­è¨€æ¨¡å‹æ¥è¿˜åŸæ–‡æœ¬ï¼Œæ¶ˆé™¤å™ªå£°ã€‚

`DAE` æ˜¯ä¸€ç§å…·æœ‰é™å™ªåŠŸèƒ½çš„**è‡ªç¼–ç å™¨**ï¼Œæ—¨åœ¨å°†å«æœ‰å™ªå£°çš„è¾“å…¥æ•°æ®è¿˜åŸä¸ºå¹²å‡€çš„åŸå§‹æ•°æ®ã€‚å¯¹äºè¯­è¨€æ¨¡å‹æ¥è¯´ï¼Œå°±æ˜¯åœ¨åŸå§‹è¯­è¨€ä¸­åŠ å…¥å™ªå£°æ•°æ®ï¼Œå†é€šè¿‡æ¨¡å‹å­¦ä¹ è¿›è¡Œå™ªå£°çš„å»é™¤ä»¥æ¢å¤åŸå§‹æ–‡æœ¬ã€‚

`BART`å¼•å…¥äº†**é™å™ªè‡ªç¼–ç å™¨**ï¼Œä¸°å¯Œäº†æ–‡æœ¬çš„ç ´åæ–¹å¼ã€‚ä¾‹å¦‚éšæœºæ©ç›–ï¼ˆåŒ MLM ä¸€è‡´ï¼‰æŸäº›è¯ã€éšæœºåˆ æ‰æŸäº›è¯æˆ–ç‰‡æ®µã€æ‰“ä¹±æ–‡æ¡£é¡ºåºï¼ˆå«æ—‹è½¬ï¼‰ç­‰ï¼Œå°†æ–‡æœ¬è¾“å…¥åˆ°ç¼–ç å™¨ä¸­åï¼Œåˆ©ç”¨ä¸€ä¸ªè§£ç å™¨ç”Ÿæˆç ´åä¹‹å‰çš„åŸå§‹æ–‡æ¡£ã€‚
- ![](https://pic3.zhimg.com/80/v2-27d3a08c1e6cff3d55f201ac7adc97ee_1440w.jpg)


#### 1.3 å¼•å…¥æ›¿ä»£è¯æ£€æµ‹ â€”â€” ELECTRA

MLM å¯¹æ–‡æœ¬ä¸­çš„`[MASK]`æ ‡è®°çš„è¯è¿›è¡Œé¢„æµ‹ï¼Œä»¥è¯•å›¾æ¢å¤åŸå§‹æ–‡æœ¬ã€‚å…¶é¢„æµ‹ç»“æœå¯èƒ½å®Œå…¨æ­£ç¡®ï¼Œä¹Ÿå¯èƒ½é¢„æµ‹å‡ºä¸€ä¸ªä¸å±äºåŸæ–‡æœ¬ä¸­çš„è¯ã€‚

`ELECTRA`å¼•å…¥äº†**æ›¿ä»£è¯**æ£€æµ‹ï¼Œæ¥é¢„æµ‹ä¸€ä¸ªç”±è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„å¥å­ä¸­å“ªäº›è¯æ˜¯åŸæœ¬å¥å­ä¸­çš„è¯ï¼Œå“ªäº›è¯æ˜¯è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¸”ä¸å±äºåŸå¥å­ä¸­çš„è¯ã€‚
- ELECTRA ä½¿ç”¨ä¸€ä¸ªå°å‹çš„ MLM æ¨¡å‹ä½œä¸º**ç”Ÿæˆå™¨**ï¼ˆGeneratorï¼‰ï¼Œæ¥å¯¹åŒ…å«\[MASK]çš„å¥å­è¿›è¡Œé¢„æµ‹ã€‚
- å¦å¤–è®­ç»ƒä¸€ä¸ªåŸºäºäºŒåˆ†ç±»çš„åˆ¤åˆ«å™¨ï¼ˆDiscriminatorï¼‰æ¥å¯¹ç”Ÿæˆå™¨ç”Ÿæˆçš„å¥å­è¿›è¡Œåˆ¤æ–­ã€‚

![](https://pic3.zhimg.com/80/v2-91eed3257d2330631a8506ca5b1d17be_1440w.jpg)

#### 1.4 æ”¹è¿›ç›¸é‚»å¥é¢„æµ‹

åœ¨å¤§å¤šæ•°åº”ç”¨åœºæ™¯ä¸‹ï¼Œæ¨¡å‹ä»…éœ€è¦é’ˆå¯¹å•ä¸ªå¥å­å®Œæˆå»ºæ¨¡ï¼Œèˆå¼ƒNSPè®­ç»ƒç›®æ ‡æ¥ä¼˜åŒ–æ¨¡å‹å¯¹äºå•ä¸ªå¥å­çš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›ã€‚
- åˆ é™¤NSPï¼šNSPä»…ä»…è€ƒè™‘äº†ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸é‚»ï¼Œè€Œæ²¡æœ‰å…¼é¡¾åˆ°å¥å­åœ¨æ•´ä¸ªæ®µè½ã€ç¯‡ç« ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚
- æ”¹è¿›NSPï¼šé€šè¿‡é¢„æµ‹å¥å­ä¹‹é—´çš„é¡ºåºå…³ç³»ï¼Œä»è€Œå­¦ä¹ å…¶ä½ç½®ä¿¡æ¯ã€‚

### åˆ†æ”¯2ï¼šèåˆèåˆå¤–éƒ¨çŸ¥è¯†

å½“ä¸‹çŸ¥è¯†å›¾è°±çš„ç›¸å…³ç ”ç©¶å·²ç»å–å¾—äº†æå¤§çš„è¿›å±•ï¼Œå¤§é‡çš„å¤–éƒ¨çŸ¥è¯†åº“éƒ½å¯ä»¥åº”ç”¨åˆ° NLP çš„ç›¸å…³ç ”ç©¶ä¸­ã€‚
 
#### 2.1 åµŒå…¥å®ä½“å…³ç³»çŸ¥è¯†
 
å®ä½“å…³ç³»ä¸‰å…ƒç»„æ˜¯çŸ¥è¯†å›¾è°±çš„æœ€åŸºæœ¬çš„ç»“æ„ï¼Œä¹Ÿæ˜¯å¤–éƒ¨çŸ¥è¯†æœ€ç›´æ¥å’Œç»“æ„åŒ–çš„è¡¨è¾¾ã€‚
- `K-BERT`ä»BERTæ¨¡å‹è¾“å…¥å±‚å…¥æ‰‹ï¼Œå°†å®ä½“å…³ç³»çš„ä¸‰å…ƒç»„æ˜¾å¼åœ°åµŒå…¥åˆ°è¾“å…¥å±‚ä¸­ã€‚
- ![](https://pic1.zhimg.com/80/v2-4f699efff5260493029a417af59c51ac_1440w.jpg)
 
#### 2.2 ç‰¹å¾å‘é‡æ‹¼æ¥çŸ¥è¯†

BERTå¯ä»¥å°†ä»»æ„æ–‡æœ¬è¡¨ç¤ºä¸ºç‰¹å¾å‘é‡çš„å½¢å¼ï¼Œå› æ­¤å¯ä»¥è€ƒè™‘é‡‡ç”¨**å‘é‡æ‹¼æ¥**çš„æ–¹å¼åœ¨ BERT æ¨¡å‹ä¸­èåˆå¤–éƒ¨çŸ¥è¯†ã€‚
- `SemBERT`åˆ©ç”¨è¯­ä¹‰è§’è‰²æ ‡æ³¨å·¥å…·ï¼Œè·å–æ–‡æœ¬ä¸­çš„è¯­ä¹‰è§’è‰²å‘é‡è¡¨ç¤ºï¼Œä¸åŸå§‹BERTæ–‡æœ¬è¡¨ç¤ºèåˆã€‚
 
#### 2.3 è®­ç»ƒç›®æ ‡èåˆçŸ¥è¯†
 
åœ¨çŸ¥è¯†å›¾è°±æŠ€æœ¯ä¸­ï¼Œå¤§é‡ä¸°å¯Œçš„å¤–éƒ¨çŸ¥è¯†è¢«ç”¨æ¥ç›´æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå½¢æˆäº†å¤šç§è®­ç»ƒä»»åŠ¡ã€‚
- `ERNIE`ä»¥DAEçš„æ–¹å¼åœ¨BERTä¸­å¼•å…¥äº†**å®ä½“å¯¹é½**è®­ç»ƒç›®æ ‡ï¼ŒWKLMé€šè¿‡éšæœºæ›¿æ¢ç»´åŸºç™¾ç§‘æ–‡æœ¬ä¸­çš„å®ä½“ï¼Œè®©æ¨¡å‹é¢„æµ‹æ­£è¯¯ï¼Œä»è€Œåœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­åµŒå…¥çŸ¥è¯†ã€‚

### åˆ†æ”¯3ï¼šæ”¹è¿›Transformer
 
ç”±äºTransformerç»“æ„è‡ªèº«çš„é™åˆ¶ï¼ŒBERTç­‰ä¸€ç³»åˆ—é‡‡ç”¨ Transformer çš„æ¨¡å‹æ‰€èƒ½å¤„ç†çš„æœ€å¤§æ–‡æœ¬é•¿åº¦ä¸º 512ä¸ªtokenã€‚
 
#### 3.1 æ”¹è¿› Encoder MASKçŸ©é˜µ
 
BERT ä½œä¸ºä¸€ç§åŒå‘ç¼–ç çš„è¯­è¨€æ¨¡å‹ï¼Œå…¶â€œåŒå‘â€ä¸»è¦ä½“ç°åœ¨ Transformerç»“æ„çš„ MASK çŸ©é˜µä¸­ã€‚Transformer åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰ï¼Œåˆ©ç”¨MASK çŸ©é˜µæä¾›ä¸€ç§â€œæ³¨æ„â€æœºåˆ¶ï¼Œå³ MASK çŸ©é˜µå†³å®šäº†æ–‡æœ¬ä¸­å“ªäº›è¯å¯ä»¥äº’ç›¸â€œçœ‹è§â€ã€‚
 
`UniLM`é€šè¿‡å¯¹è¾“å…¥æ•°æ®ä¸­çš„ä¸¤ä¸ªå¥å­è®¾è®¡ä¸åŒçš„ MASK çŸ©é˜µæ¥å®Œæˆç”Ÿæˆæ¨¡å‹çš„å­¦ä¹ ã€‚å¯¹äºç¬¬ä¸€ä¸ªå¥å­ï¼Œé‡‡ç”¨è·Ÿ BERT ä¸­çš„ Transformer-Encoder ä¸€è‡´çš„ç»“æ„ï¼Œæ¯ä¸ªè¯éƒ½èƒ½å¤Ÿâ€œæ³¨æ„â€åˆ°å…¶â€œä¸Šæ–‡â€å’Œâ€œä¸‹æ–‡â€ä¿¡æ¯ã€‚
 
å¯¹äºç¬¬äºŒä¸ªå¥å­ï¼Œå…¶ä¸­çš„æ¯ä¸ªè¯åªèƒ½â€œæ³¨æ„â€åˆ°ç¬¬ä¸€å¥è¯ä¸­çš„æ‰€æœ‰è¯å’Œå½“å‰å¥å­çš„â€œä¸Šæ–‡â€ä¿¡æ¯ã€‚åˆ©ç”¨è¿™ç§å·§å¦™çš„è®¾è®¡ï¼Œæ¨¡å‹è¾“å…¥çš„ç¬¬ä¸€å¥è¯å’Œç¬¬äºŒå¥è¯å½¢æˆäº†ç»å…¸çš„â€œSeq2Seqâ€çš„æ¨¡å¼ï¼Œä»è€Œå°† BERT æˆåŠŸç”¨äºè¯­è¨€ç”Ÿæˆä»»åŠ¡ã€‚
 
#### 3.2 Encoder + Decoderè¯­è¨€ç”Ÿæˆ
 
`BART`æ¨¡å‹åŒæ ·é‡‡ç”¨Encoder+Decoderç»“æ„ï¼Œå€ŸåŠ©DAEè¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ–¹å¼ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°é¢„æµ‹å’Œç”Ÿæˆè¢«â€œå™ªå£°â€ç ´åçš„æ–‡æœ¬ï¼Œä»è€Œä¹Ÿå¾—åˆ°å…·æœ‰æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚
 
### åˆ†æ”¯4ï¼šé‡åŒ–ä¸å‹ç¼©

#### 4.1 æ¨¡å‹è’¸é¦
 
å¯¹ BERT è’¸é¦çš„ç ”ç©¶ä¸»è¦å­˜åœ¨äºä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
*   åœ¨é¢„è®­ç»ƒé˜¶æ®µè¿˜æ˜¯å¾®è°ƒé˜¶æ®µä½¿ç”¨è’¸é¦
*   å­¦ç”Ÿæ¨¡å‹çš„é€‰æ‹©
*   è’¸é¦çš„ä½ç½®
 
- `DistilBERT`åœ¨é¢„è®­ç»ƒé˜¶æ®µè’¸é¦ï¼Œå…¶å­¦ç”Ÿæ¨¡å‹å…·æœ‰ä¸BERTç»“æ„ï¼Œä½†å±‚æ•°å‡åŠã€‚
- `TinyBERT`ä¸ºBERTçš„åµŒå…¥å±‚ã€è¾“å‡ºå±‚ã€Transformerä¸­çš„éšè—å±‚ã€æ³¨æ„åŠ›çŸ©é˜µéƒ½è®¾è®¡äº†æŸå¤±å‡½æ•°ï¼Œæ¥å­¦ä¹  BERT ä¸­å¤§é‡çš„è¯­è¨€çŸ¥è¯†ã€‚
 
![](https://pic3.zhimg.com/80/v2-50248129fc9e5102fb355d979742b65a_1440w.jpg)
 
#### 4.2 æ¨¡å‹å‰ªæ
 
å‰ªæï¼ˆPruningï¼‰æ˜¯æŒ‡å»æ‰æ¨¡å‹ä¸­ä¸å¤ªé‡è¦çš„æƒé‡æˆ–ç»„ä»¶ï¼Œä»¥æå‡æ¨ç†é€Ÿåº¦ã€‚ç”¨äº BERT çš„å‰ªææ–¹æ³•ä¸»è¦æœ‰æƒé‡ä¿®å‰ªå’Œç»“æ„ä¿®å‰ªã€‚

## BERT fine-tune

bert çš„ finetune ä¸»è¦å­˜åœ¨ä¸¤ç±»åº”ç”¨åœºæ™¯ï¼š**åˆ†ç±»**å’Œ**é˜…è¯»ç†è§£**ã€‚

- ã€2021-8-25ã€‘[ä¸­æ–‡è¯­æ–™çš„ Bert å¾®è°ƒ Bert Chinese Finetune](https://kuhungio.me/2019/bert-chinese-finetune/)
- ã€2023-11-15ã€‘[BERTå¾®è°ƒä¸å®æ—¶é¢„æµ‹](https://zhuanlan.zhihu.com/p/335305116)

[Bert](https://github.com/google-research/bert) æ–‡æ¡£æœ¬èº«å¯¹ finetune è¿›è¡Œäº†è¾ƒä¸ºè¯¦ç»†çš„æè¿°ï¼Œä½†å¯¹äºä¸ç†Ÿæ‚‰å®˜æ–¹æ ‡å‡†æ•°æ®é›†çš„å·¥ç¨‹å¸ˆæ¥è¯´ï¼Œæœ‰ä¸€å®šçš„ä¸Šæ‰‹éš¾åº¦ã€‚

éšç€ [Bert as service](https://github.com/hanxiao/bert-as-service) ä»£ç çš„å¼€æºï¼Œä½¿ç”¨ Bert åˆ†ç±»æˆ–é˜…è¯»ç†è§£çš„å‰¯äº§ç‰©â€“è¯ç©ºé—´ï¼Œæˆä¸ºä¸€ä¸ªæ›´å…·å®ç”¨ä»·å€¼çš„æ–¹å‘ã€‚

### é¢„è®­ç»ƒæ¨¡å‹

- ä¸‹è½½ [BERT-Base, Chinese](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip): Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters

### æ•°æ®å¤„ç†è¿‡ç¨‹

BERTæ•°æ®å¤„ç†è¿‡ç¨‹ï¼š<font color='blue'>åŸå§‹æ–‡æœ¬ â†’ åˆ†è¯ â†’ åŠ ç‰¹æ®Šæ ‡è®° â†’ æ˜ å°„ä¸ºid â†’ åˆå¹¶</font>
- åŸå§‹æ–‡æœ¬
- **åˆ†è¯**ï¼šä¸­è‹±æ–‡ä¸åŒ
  - è‹±æ–‡ï¼šè¯æ€§/è¯å¹²è¿˜åŸ
  - ä¸­æ–‡ï¼šåˆ†è¯ï¼Œberté»˜è®¤ä»¥å•å­—åˆ‡å‰²
- åŠ **ç‰¹æ®Šæ ‡è®°**
  - æ ¹æ®ä»»åŠ¡ä¸åŒï¼ŒåŠ `[CLS]`,`[SEP]`,`[PAD]`ç­‰
- **æ˜ å°„ä¸ºid**
  - å°†ä»¥ä¸Šæ‰€æœ‰å­—ç¬¦æ˜ å°„ä¸ºidç¼–å·åºåˆ—
  - æ³¨æ„ï¼šencoderå’Œdecoderå¯¹åº”ä¸åŒçš„å­—å…¸
- å¥å­**å‘é‡åŒ–**
  - æ ¹æ®è¯­æ–™é•¿åº¦ï¼Œè®¾ç½®é«˜äºåˆ†è¯åæœ€å¤§é•¿åº¦çš„é˜ˆå€¼max_lenï¼Œä½œä¸ºå¥å­é•¿åº¦ç»´åº¦

æ ¼å¼ï¼š
- (a) For sequence pairs: **å¥å­å¯¹**
  - sentence: `is this jackson ville ? || no it is not .`
  - tokens:   `[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]`
  - type_ids: `0     0  0    0    0   0  0 0     1  1  1  1   1 1`
-  (b) For single sequences: **å•å¥å½¢å¼**
  - sentence: the dog is hairy .
  - tokens:   `[CLS] the dog is hairy . [SEP]`
  - type_ids:   `0     0   0   0  0     0  0`
- è¾“å‡ºæ ¼å¼ï¼š`[guid, text_a, text_b, label]`
  - åä¸¤ä¸ªå­—æ®µå¯é€‰
- è¾“å…¥æ¨¡å‹
  - input_idsï¼šå¥å­idå‘é‡ï¼Œmax_lenç»´
  - input_maskï¼šå¥å­æ©ç æ¨¡æ¿ï¼Œ0ï¼Œ1æ ‡è®°ï¼Œ0è¡¨ç¤ºç©ºç™½å¡«å……
  - segment_idsï¼šä¸¤ä¸ªå¥å­åˆ†éš”ä½ç½®
  - label_idï¼šåˆ†ç±»ç›®æ ‡å¯¹åº”çš„id

### æ•°æ®å‡†å¤‡

å°†è¯­æ–™åˆ‡åˆ†ä¸º train.tsv , test.tsvä»¥åŠ dev.tsvä¸‰ä¸ªæ–‡ä»¶ã€‚
- train.tsv è®­ç»ƒé›†
- dev.tsv éªŒè¯é›†

æ•°æ®æ ¼å¼
- ç¬¬ä¸€åˆ—ä¸º labelï¼Œç¬¬äºŒåˆ—ä¸ºå…·ä½“å†…å®¹ï¼Œtab åˆ†éš”ã€‚
- å› æ¨¡å‹æœ¬èº«åœ¨å­—ç¬¦çº§åˆ«åšå¤„ç†ï¼Œå› è€Œæ— éœ€åˆ†è¯ã€‚

```
fashion	è¡¬è¡«å’Œå®ƒä¸€èµ·ç©¿,è®©ä½ å‡é¾„åå²!è¶Šæ´»è¶Šå¹´è½»!å¤ªç¾äº†!...
houseliving	95ã¡ç®€çº¦ç¾å¼å°ä¸‰å±…,è¿‡ç²¾ç¾åˆ«è‡´ã€æ‚ ç„¶è‡ªå¾—çš„å°æ—¥å­! å±‹ä¸»çš„å®¢...
game	èµ›å­£æœ«ç”¨ä»–ä»¬ä¸¤å¤©ä¸Šä¸€æ®µï¼Œ7.20æœ€å¼ºLOLä¸Šåˆ†è‹±é›„æ¨èï¼ å„ä½å°ä¼™...
```


### åˆ†ç±»ä»»åŠ¡

åˆ†ç±»å®¹æ˜“è·å¾—æ ·æœ¬ï¼Œä»¥åˆ†ç±»ä¸ºä¾‹åšæ¨¡å‹å¾®è°ƒ
- å®Œæ•´ä»£ç  [TextClassifier_Transformer](https://github.com/Vincent131499/TextClassifier_Transformer)

#### æ•´ä½“æµç¨‹

ä¿®æ”¹ run_classifier.py
- **é›†æˆæŠ½è±¡ç±»** DataProcessorï¼Œå®ç°é‡Œé¢çš„å‡ ç§æ–¹æ³•ï¼š
  - get_train_examplesï¼šè·å–è®­ç»ƒé›†æ•°æ®
  - get_dev_examplesï¼šè·å–éªŒè¯é›†æ•°æ®
  - get_test_examplesï¼šè·å–æµ‹è¯•é›†æ•°æ®
  - get_labelsï¼šè·å–åˆ†ç±»æ ‡ç­¾é›†åˆ

#### DataProcessor

æ–°å¢ä»»åŠ¡å®šä¹‰ `DemoProcessor`
- get_labels å®šä¹‰label

```python
class DemoProcessor(DataProcessor):
    """ä»»åŠ¡ç›¸å…³çš„æ•°æ®é›†ï¼Œå¤„ç†ç±»"""
    def __init__(self):
        self.labels = set() # labelé›†åˆ
    
    def get_train_examples(self, data_dir):
        """è¯»å–è®­ç»ƒé›†"""
        # _read_csvåªæ˜¯ç®€å•æŒ‰ç…§tabåˆ†éš”æˆlist
        # _create_exampleså°†ä»¥ä¸Šlistè½¬æˆæ ‡å‡†æ ·æœ¬æ ¼å¼
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")
    def get_dev_examples(self, data_dir):
        """è¯»å–éªŒè¯é›†"""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")
    def get_test_examples(self, data_dir):
        """è¯»å–æµ‹è¯•é›†"""
        return self._create_examples(
          self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")
    # æ–°å¢çš„ä»»åŠ¡ 
    def get_labels(self):
        """è·å–ç›®æ ‡å€¼é›†åˆ"""
        #return list(self.labels) # å·æ‡’æ–¹å¼
        #return ['-1', '0' , '1']
        return ["fashion", "houseliving","game"] # æ ¹æ® label è‡ªå®šä¹‰
    
    def _create_examples(self, lines, set_type):
        """ä»è®­ç»ƒé›†/éªŒè¯é›†ä¸­è¯»å–æ ·æœ¬"""
        examples = []
        for (i, line) in enumerate(lines):
            # æ ¼å¼ï¼š[label text]
            # Only the test set has a header æµ‹è¯•é›†æœ‰headerè¡¨å¤´ä¿¡æ¯
            if set_type == "test" and i == 0:
                continue
            guid = "%s-%s" % (set_type, i) # æ ·æœ¬å”¯ä¸€id
            if set_type == "test":
                text_a = tokenization.convert_to_unicode(line[0])
                label = "0" # æµ‹è¯•é›†ç»™é»˜è®¤label
            else: # å°†æ‰€æœ‰å­—ç¬¦è½¬æˆunicode
                text_a = tokenization.convert_to_unicode(line[1])
                label = tokenization.convert_to_unicode(line[0])
                examples.append(
                  InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
        # è¾“å‡ºæ ¼å¼ï¼š[guid, text_a, text_b, label]
        return examples
```

#### processors

æ·»åŠ  DemoProcessor

```python
  processors = {
      "cola": ColaProcessor,
      "mnli": MnliProcessor,
      "mrpc": MrpcProcessor,
      "xnli": XnliProcessor,
      "demo": DemoProcessor, # æ–°å¢ä»»åŠ¡
  }
```

#### è®­ç»ƒ

è®­ç»ƒè„šæœ¬ train.sh

```py
BERT_DIR=`pwd`
# æ¨¡å‹ç›®å½•
export BERT_BASE_DIR=${BERT_DIR}/chinese_L-12_H-768_A-12
# æ•°æ®ç›®å½•
export DATA_DIR=${BERT_DIR}/data 

python run_classifier.py \
  --task_name="demo" \
  --do_train=true \
  --do_eval=true \
  --data_dir=$DATA_DIR \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/output/
```

#### è¾“å‡º

output ç›®å½•ä¼šæœ‰ä»¥ä¸‹è¾“å‡º:

```shell
***** Eval results *****
  eval_accuracy = xx
  eval_loss = xx
  global_step = xx
  loss = xx
```

æœ€ç»ˆï¼Œå¾®è°ƒåçš„æ¨¡å‹ä¿å­˜åœ¨output_diræŒ‡å‘çš„æ–‡ä»¶å¤¹ä¸­ã€‚


#### æ¨¡å‹å¯¼å‡º

run_classifier.py æ˜¯å•æ¬¡è¿è¡Œï¼Œå¦‚æœæŠŠ do_predict å‚æ•°è®¾ç½®æˆ Trueï¼Œä¹Ÿå¯ä»¥é¢„æµ‹ï¼Œä½†è¾“å…¥æ ·æœ¬æ˜¯åŸºäºæ–‡ä»¶ï¼Œä¸æ”¯æŒå°†æ¨¡å‹æŒä¹…åŒ–åœ¨å†…å­˜é‡Œè¿›è¡Œservingï¼Œå› æ­¤éœ€è¦æ”¹ä¸€äº›ä»£ç ï¼Œè¾¾åˆ°ï¼š
- å°†æ¨¡å‹åŠ è½½åˆ°å†…å­˜ï¼Œå³ï¼šå…è®¸ä¸€æ¬¡åŠ è½½ï¼Œå¤šæ¬¡è°ƒç”¨ã€‚
- è¯»å–éæ–‡ä»¶ä¸­çš„æ ·æœ¬è¿›è¡Œé¢„æµ‹ã€‚ä»æ ‡å‡†è¾“å…¥æµè¯»å–æ ·æœ¬è¾“å…¥ã€‚

åœ¨run_classifier.pyæ·»åŠ ä¸‹é¢çš„ä»£ç 

```py
flags.DEFINE_string(
    "export_dir", None,
    "The dir where the exported model will be written.")
flags.DEFINE_bool(
    "do_export", False,
    "Whether to export the model.")
```

ç¼–å†™å¯¼å‡ºè„šæœ¬ explot.sh

```sh
BERT_DIR=`pwd`
export BERT_BASE_DIR=${BERT_DIR}/chinese_L-12_H-768_A-12
export DATA_DIR=${BERT_DIR}/data

python run_classifier.py \
  --task_name=sentiment \
  --do_train=false \
  --do_eval=false \
  --do_predict=true \
  --data_dir=$DATA_DIR \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=output \
  --do_export=true \
  --export_dir=exported
```


æ·»åŠ å‡½æ•°

```py
def serving_input_fn():
    label_ids = tf.placeholder(tf.int32, [None], name='label_ids')
    input_ids = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name='input_ids')
    input_mask = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name='input_mask')
    segment_ids = tf.placeholder(tf.int32, [None, FLAGS.max_seq_length], name='segment_ids')
    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({
        'label_ids': label_ids,
        'input_ids': input_ids,
        'input_mask': input_mask,
        'segment_ids': segment_ids,
    })()
    return input_fn
```

main()å‡½æ•°æ·»åŠ  do_export é€‰é¡¹

```py
if FLAGS.do_export:
      estimator._export_to_tpu = False
      estimator.export_savedmodel(FLAGS.export_dir, serving_input_fn)
```

æ‰§è¡Œexport.shè„šæœ¬ï¼Œæ‰§è¡Œå®Œæˆå¯ä»¥åœ¨exportç›®å½•ä¸‹çœ‹åˆ°ä¸€ä¸ªæ–°çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¡¨ç¤ºå¯¼å‡ºæˆåŠŸ

#### å®æ—¶é¢„æµ‹

`python test_serving.py` ä¹‹åå°±å¯ä»¥æ„‰å¿«çš„å¿«é€Ÿç¦»çº¿é¢„æµ‹

```py
import tensorflow as tf
import tokenization

class InputExample(object):
  """A single training/test example for simple sequence classification."""

  def __init__(self, guid, text_a, text_b=None, label=None):
    """Constructs a InputExample.
    Args:
      guid: Unique id for the example.
      text_a: string. The untokenized text of the first sequence. For single
        sequence tasks, only this sequence must be specified.
      text_b: (Optional) string. The untokenized text of the second sequence.
        Only must be specified for sequence pair tasks.
      label: (Optional) string. The label of the example. This should be
        specified for train and dev examples, but not for test examples.
    """
    self.guid = guid
    self.text_a = text_a
    self.text_b = text_b
    self.label = label

class PaddingInputExample(object):
  """Fake example so the num input examples is a multiple of the batch size.
  When running eval/predict on the TPU, we need to pad the number of examples
  to be a multiple of the batch size, because the TPU requires a fixed batch
  size. The alternative is to drop the last batch, which is bad because it means
  the entire output data won't be generated.
  We use this class instead of `None` because treating `None` as padding
  battches could cause silent errors.
  """


class InputFeatures(object):
  """A single set of features of data."""

  def __init__(self,
               input_ids,
               input_mask,
               segment_ids,
               label_id,
               is_real_example=True):
    self.input_ids = input_ids
    self.input_mask = input_mask
    self.segment_ids = segment_ids
    self.label_id = label_id
    self.is_real_example = is_real_example

def _truncate_seq_pair(tokens_a, tokens_b, max_length):
  """Truncates a sequence pair in place to the maximum length."""

  # This is a simple heuristic which will always truncate the longer sequence
  # one token at a time. This makes more sense than truncating an equal percent
  # of tokens from each, since if one sequence is very short then each token
  # that's truncated likely contains more information than a longer sequence.
  while True:
    total_length = len(tokens_a) + len(tokens_b)
    if total_length <= max_length:
      break
    if len(tokens_a) > len(tokens_b):
      tokens_a.pop()
    else:
      tokens_b.pop()


def convert_single_example(ex_index, example, label_list, max_seq_length,
                           tokenizer):
    """Converts a single `InputExample` into a single `InputFeatures`."""

    if isinstance(example, PaddingInputExample):
        return InputFeatures(
            input_ids=[0] * max_seq_length,
            input_mask=[0] * max_seq_length,
            segment_ids=[0] * max_seq_length,
            label_id=0,
            is_real_example=False)

    label_map = {}
    for (i, label) in enumerate(label_list):
        label_map[label] = i

    tokens_a = tokenizer.tokenize(example.text_a)
    tokens_b = None
    if example.text_b:
        tokens_b = tokenizer.tokenize(example.text_b)

    if tokens_b:
        # Modifies `tokens_a` and `tokens_b` in place so that the total
        # length is less than the specified length.
        # Account for [CLS], [SEP], [SEP] with "- 3"
        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
    else:
        # Account for [CLS] and [SEP] with "- 2"
        if len(tokens_a) > max_seq_length - 2:
            tokens_a = tokens_a[0:(max_seq_length - 2)]

    # The convention in BERT is:
    # (a) For sequence pairs:
    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1
    # (b) For single sequences:
    #  tokens:   [CLS] the dog is hairy . [SEP]
    #  type_ids: 0     0   0   0  0     0 0
    #
    # Where "type_ids" are used to indicate whether this is the first
    # sequence or the second sequence. The embedding vectors for `type=0` and
    # `type=1` were learned during pre-training and are added to the wordpiece
    # embedding vector (and position vector). This is not *strictly* necessary
    # since the [SEP] token unambiguously separates the sequences, but it makes
    # it easier for the model to learn the concept of sequences.
    #
    # For classification tasks, the first vector (corresponding to [CLS]) is
    # used as the "sentence vector". Note that this only makes sense because
    # the entire model is fine-tuned.
    tokens = []
    segment_ids = []
    tokens.append("[CLS]")
    segment_ids.append(0)
    for token in tokens_a:
        tokens.append(token)
        segment_ids.append(0)
    tokens.append("[SEP]")
    segment_ids.append(0)

    if tokens_b:
        for token in tokens_b:
            tokens.append(token)
            segment_ids.append(1)
        tokens.append("[SEP]")
        segment_ids.append(1)

    input_ids = tokenizer.convert_tokens_to_ids(tokens)

    # The mask has 1 for real tokens and 0 for padding tokens. Only real
    # tokens are attended to.
    input_mask = [1] * len(input_ids)

    # Zero-pad up to the sequence length.
    while len(input_ids) < max_seq_length:
        input_ids.append(0)
        input_mask.append(0)
        segment_ids.append(0)

    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length

    # debug xmxoxo 2019/3/13
    # print(ex_index,example.text_a)

    label_id = label_map[example.label]
    if ex_index < 5:
        tf.logging.info("*** Example ***")
        tf.logging.info("guid: %s" % (example.guid))
        tf.logging.info("tokens: %s" % " ".join(
            [tokenization.printable_text(x) for x in tokens]))
        tf.logging.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
        tf.logging.info("input_mask: %s" % " ".join([str(x) for x in input_mask]))
        tf.logging.info("segment_ids: %s" % " ".join([str(x) for x in segment_ids]))
        tf.logging.info("label: %s (id = %d)" % (example.label, label_id))

    feature = InputFeatures(
        input_ids=input_ids,
        input_mask=input_mask,
        segment_ids=segment_ids,
        label_id=label_id,
        is_real_example=True)
    return feature

if __name__ == '__main__':
    predict_fn = tf.contrib.predictor.from_saved_model('exported/1607510113')
    label_list = ["-1", "0", "1"]
    max_seq_length = 128
    tokenizer = tokenization.FullTokenizer(vocab_file='./chinese_L-12_H-768_A-12/vocab.txt', do_lower_case=True)
    print('æ¨¡å‹åŠ è½½å®Œæ¯•ï¼æ­£åœ¨ç›‘å¬ã€‹ã€‹ã€‹')
    while True:
        question = input("> ")
        predict_example = InputExample("id", question, None, '0')
        feature = convert_single_example(100, predict_example, label_list,
                                         max_seq_length, tokenizer)

        prediction = predict_fn({
            "input_ids": [feature.input_ids],
            "input_mask": [feature.input_mask],
            "segment_ids": [feature.segment_ids],
            "label_ids": [feature.label_id],
        })
        probabilities = prediction["probabilities"]
        label = label_list[probabilities.argmax()]
        print(label)
```


### æ€»ç»“

Bert é¢„è®­ç»ƒåçš„ finetuneï¼Œæ˜¯ä¸€ç§å¾ˆé«˜æ•ˆçš„æ–¹å¼ï¼ŒèŠ‚çœæ—¶é—´ï¼ŒåŒæ—¶æé«˜æ¨¡å‹åœ¨å‚ç›´è¯­æ–™çš„è¡¨ç°ã€‚finetune è¿‡ç¨‹ï¼Œå®é™…ä¸Šä¸éš¾ã€‚è¾ƒå¤§çš„éš¾ç‚¹åœ¨äºæ•°æ®å‡†å¤‡å’Œ pipeline çš„è®¾è®¡ã€‚ä»å•†ä¸šè§’åº¦è®²ï¼Œåº”ç€é‡è€ƒè™‘ finetune ä¹‹åï¼Œæ¨¡å‹æœ‰æ•ˆæ€§çš„è¯æ˜ï¼Œä»¥åŠåœ¨ä¸šåŠ¡åœºæ™¯ä¸­çš„åº”ç”¨ã€‚å¦‚æœè¯„ä¼°æŒ‡æ ‡å’Œä¸šåŠ¡åœºæ™¯éƒ½å·²ç¼•æ¸…ï¼Œé‚£ä¹ˆä¸å¦¨ä¸€è¯•ã€‚

[Github åœ°å€](https://github.com/kuhung/bert_finetune)

## MLMæ”¹è¿›

- åŸºäºMLMï¼Œåšå„ç§æ”¹è¿›å°è¯•

MLMï¼Œå…¨ç§°â€œMasked Language Modelâ€ï¼Œå¯ä»¥ç¿»è¯‘ä¸ºâ€œæ©ç è¯­è¨€æ¨¡å‹â€ï¼Œå®é™…ä¸Šå°±æ˜¯ä¸€ä¸ªå®Œå½¢å¡«ç©ºä»»åŠ¡ï¼Œéšæœº Mask æ‰æ–‡æœ¬ä¸­çš„æŸäº›å­—è¯ï¼Œç„¶åè¦æ¨¡å‹å»é¢„æµ‹è¢« Mask çš„å­—è¯ã€‚å…¶ä¸­è¢« Mask æ‰çš„éƒ¨åˆ†ï¼Œå¯ä»¥æ˜¯ç›´æ¥éšæœºé€‰æ‹©çš„ Tokenï¼Œä¹Ÿå¯ä»¥æ˜¯éšæœºé€‰æ‹©è¿ç»­çš„èƒ½ç»„æˆä¸€æ•´ä¸ªè¯çš„ Tokenï¼Œåè€…ç§°ä¸º `WWM`ï¼ˆWhole Word Maskingï¼‰ã€‚

è®ºæ–‡ BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language ModelæŒ‡å‡º MLM å¯ä»¥ä½œä¸ºä¸€èˆ¬çš„ç”Ÿæˆæ¨¡å‹ç”¨ï¼Œè®ºæ–‡ Spelling Error Correction with Soft-Masked BERT åˆ™å°† MLM ç”¨äºæ–‡æœ¬çº é”™ã€‚

### ç»“åˆäººå·¥æ¨¡æ¿

GPT-3 çš„è®ºæ–‡å«åš Language Models are Few-Shot Learners [1]ï¼Œæ ‡é¢˜é‡Œè¾¹å·²ç»æ²¡æœ‰ Gã€Pã€T å‡ ä¸ªå•è¯äº†ï¼Œåªä¸è¿‡å®ƒè·Ÿå¼€å§‹çš„ GPT æ˜¯ä¸€è„‰ç›¸æ‰¿çš„ï¼Œå› æ­¤è¿˜æ˜¯ä»¥ GPT ç§°å‘¼å®ƒã€‚é¡¾åæ€ä¹‰ï¼ŒGPT-3 ä¸»æ‰“çš„æ˜¯ **Few-Shot Learning**ï¼Œä¹Ÿå°±æ˜¯**å°æ ·æœ¬å­¦ä¹ **ã€‚æ­¤å¤–ï¼ŒGPT-3 çš„å¦ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¤§ï¼Œæœ€å¤§çš„ç‰ˆæœ¬å¤šè¾¾ 1750 äº¿å‚æ•°ï¼Œæ˜¯ BERT Base çš„ä¸€åƒå¤šå€ã€‚

æ­£å› å¦‚æ­¤ï¼Œå‰äº›å¤© Arxiv ä¸Šçš„ä¸€ç¯‡è®ºæ–‡ It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners [2] ä¾¿å¼•èµ·äº†ç¬”è€…çš„æ³¨æ„ï¼Œæ„è¯‘è¿‡æ¥å°±æ˜¯â€œè°è¯´ä¸€å®šè¦å¤§çš„ï¼Ÿå°æ¨¡å‹ä¹Ÿå¯ä»¥åšå°æ ·æœ¬å­¦ä¹ â€ã€‚

[å¿…é¡»è¦GPT-3å—ï¼Ÿä¸ï¼ŒBERTçš„MLMæ¨¡å‹ä¹Ÿèƒ½å°æ ·æœ¬å­¦ä¹ ](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247512167&idx=1&sn=cc7695d92362e3b18a6e8969fb14dc27&chksm=96ea6fe7a19de6f1be86b965e268df1b9c6320810cf32b6d64ddd3d238bf9088be41fb36adfe#rd)ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åä¸º**Pattern-Exploiting Training**ï¼ˆ`PET`ï¼‰ çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡äººå·¥æ„å»ºçš„æ¨¡ç‰ˆä¸BERTçš„MLMæ¨¡å‹ç»“åˆï¼Œèƒ½å¤Ÿèµ·åˆ°éå¸¸å¥½çš„é›¶æ ·æœ¬ã€å°æ ·æœ¬ä¹ƒè‡³åŠç›‘ç£å­¦ä¹ æ•ˆæœï¼Œè€Œä¸”è¯¥æ€è·¯æ¯”è¾ƒä¼˜é›…æ¼‚äº®ï¼Œå› ä¸ºå®ƒå°†é¢„è®­ç»ƒä»»åŠ¡å’Œä¸‹æ¸¸ä»»åŠ¡ç»Ÿä¸€èµ·æ¥äº†ã€‚

å°†ä»»åŠ¡è½¬æˆå®Œå½¢å¡«ç©º
- MLM çš„ä¸€ä¸ªç²¾å½©åº”ç”¨ï¼šç”¨äºå°æ ·æœ¬å­¦ä¹ æˆ–åŠç›‘ç£å­¦ä¹ ï¼ŒæŸäº›åœºæ™¯ä¸‹ç”šè‡³èƒ½åšåˆ°é›¶æ ·æœ¬å­¦ä¹ ã€‚

![](https://pic1.zhimg.com/80/v2-886f03f3e90c8e65f98329f4375b1408_720w.jpg)

ä¸€äº›ç®€å•çš„æ¨ç†ä»»åŠ¡ä¹Ÿå¯ä»¥åšè¿™æ ·çš„è½¬æ¢ï¼Œå¸¸è§çš„æ˜¯ç»™å®šä¸¤ä¸ªå¥å­ï¼Œåˆ¤æ–­è¿™ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸å®¹ï¼Œæ¯”å¦‚â€œæˆ‘å»äº†åŒ—äº¬â€è·Ÿâ€œæˆ‘å»äº†ä¸Šæµ·â€å°±æ˜¯çŸ›ç›¾çš„ï¼Œâ€œæˆ‘å»äº†åŒ—äº¬â€è·Ÿâ€œæˆ‘åœ¨å¤©å®‰é—¨å¹¿åœºâ€æ˜¯ç›¸å®¹çš„ï¼Œå¸¸è§çš„åšæ³•å°±æ˜¯å°†ä¸¤ä¸ªå¥å­æ‹¼æ¥èµ·æ¥è¾“å…¥åˆ°æ¨¡å‹åšï¼Œä½œä¸ºä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ã€‚å¦‚æœè¦è½¬æ¢ä¸ºå®Œå½¢å¡«ç©ºï¼Œé‚£è¯¥æ€ä¹ˆæ„é€ å‘¢ï¼Ÿä¸€ç§æ¯”è¾ƒè‡ªç„¶çš„æ„å»ºæ–¹å¼æ˜¯ï¼š

>- æˆ‘å»äº†åŒ—äº¬ï¼Ÿ______ï¼Œæˆ‘å»äº†ä¸Šæµ·ã€‚
>- æˆ‘å»äº†åŒ—äº¬ï¼Ÿ______ï¼Œæˆ‘åœ¨å¤©å®‰é—¨å¹¿åœºã€‚

å…¶ä¸­ç©ºä½ä¹‹å¤„çš„å€™é€‰è¯ä¸º { æ˜¯çš„ï¼Œä¸æ˜¯ }

ç»™è¾“å…¥çš„æ–‡æœ¬å¢åŠ ä¸€ä¸ªå‰ç¼€æˆ–è€…åç¼€æè¿°ï¼Œå¹¶ä¸” Mask æ‰æŸäº› Tokenï¼Œè½¬æ¢ä¸ºå®Œå½¢å¡«ç©ºé—®é¢˜ï¼Œè¿™æ ·çš„è½¬æ¢åœ¨åŸè®ºæ–‡ä¸­ç§°ä¸º Patternï¼Œè¿™ä¸ªè½¬æ¢è¦å°½å¯èƒ½ä¸åŸæ¥çš„å¥å­ç»„æˆä¸€å¥è‡ªç„¶çš„è¯ï¼Œä¸èƒ½è¿‡äºç”Ÿç¡¬ï¼Œå› ä¸ºé¢„è®­ç»ƒçš„ MLM æ¨¡å‹å°±æ˜¯åœ¨è‡ªç„¶è¯­è¨€ä¸Šè¿›è¡Œçš„ã€‚


### æ¨¡æ¿è‡ªåŠ¨ç”Ÿæˆï¼ˆPETï¼‰

ã€2021-6-17ã€‘[P-tuningï¼šè‡ªåŠ¨æ„å»ºæ¨¡ç‰ˆï¼Œé‡Šæ”¾è¯­è¨€æ¨¡å‹æ½œèƒ½](https://zhuanlan.zhihu.com/p/364141928)

ç„¶è€Œï¼Œäººå·¥æ„å»ºè¿™æ ·çš„æ¨¡ç‰ˆæœ‰æ—¶å€™ä¹Ÿæ˜¯æ¯”è¾ƒå›°éš¾çš„ï¼Œè€Œä¸”ä¸åŒçš„æ¨¡ç‰ˆæ•ˆæœå·®åˆ«ä¹Ÿå¾ˆå¤§ï¼Œå¦‚æœèƒ½å¤Ÿé€šè¿‡å°‘é‡æ ·æœ¬æ¥è‡ªåŠ¨æ„å»ºæ¨¡ç‰ˆï¼Œä¹Ÿæ˜¯éå¸¸æœ‰ä»·å€¼çš„ã€‚

æœ€è¿‘Arxivä¸Šçš„è®ºæ–‡ã€Š[GPT Understands, Too](https://arxiv.org/abs/2103.10385)ã€‹æå‡ºäº†åä¸º**P-tuning**çš„æ–¹æ³•ï¼ŒæˆåŠŸåœ°å®ç°äº†æ¨¡ç‰ˆçš„**è‡ªåŠ¨æ„å»º**ã€‚ä¸ä»…å¦‚æ­¤ï¼Œå€ŸåŠ©P-tuningï¼ŒGPTåœ¨SuperGLUEä¸Šçš„æˆç»©é¦–æ¬¡è¶…è¿‡äº†åŒç­‰çº§åˆ«çš„BERTæ¨¡å‹ï¼Œè¿™é¢ è¦†äº†ä¸€ç›´ä»¥æ¥â€œGPTä¸æ“…é•¿NLUâ€çš„ç»“è®ºï¼Œä¹Ÿæ˜¯è¯¥è®ºæ–‡å‘½åçš„ç¼˜ç”±ã€‚


P-tuningé‡æ–°å®¡è§†äº†å…³äºæ¨¡ç‰ˆçš„å®šä¹‰ï¼Œæ”¾å¼ƒäº†â€œæ¨¡ç‰ˆç”±è‡ªç„¶è¯­è¨€æ„æˆâ€è¿™ä¸€å¸¸è§„è¦æ±‚ï¼Œä»è€Œå°†æ¨¡ç‰ˆçš„æ„å»ºè½¬åŒ–ä¸ºè¿ç»­å‚æ•°ä¼˜åŒ–é—®é¢˜ï¼Œè™½ç„¶ç®€å•ï¼Œä½†å´æœ‰æ•ˆ

æ¨¡ç‰ˆçš„åæ€
- é¦–å…ˆï¼Œæˆ‘ä»¬æ¥æƒ³ä¸€ä¸‹â€œä»€ä¹ˆæ˜¯æ¨¡ç‰ˆâ€ã€‚ç›´è§‚æ¥çœ‹ï¼Œæ¨¡ç‰ˆå°±æ˜¯ç”±è‡ªç„¶è¯­è¨€æ„æˆçš„å‰ç¼€/åç¼€ï¼Œé€šè¿‡è¿™äº›æ¨¡ç‰ˆæˆ‘ä»¬ä½¿å¾—ä¸‹æ¸¸ä»»åŠ¡è·Ÿé¢„è®­ç»ƒä»»åŠ¡ä¸€è‡´ï¼Œè¿™æ ·æ‰èƒ½æ›´åŠ å……åˆ†åœ°åˆ©ç”¨åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ï¼Œèµ·åˆ°æ›´å¥½çš„é›¶æ ·æœ¬ã€å°æ ·æœ¬å­¦ä¹ æ•ˆæœã€‚
- ç­‰ç­‰ï¼Œæˆ‘ä»¬çœŸçš„åœ¨ä¹æ¨¡ç‰ˆæ˜¯ä¸æ˜¯â€œè‡ªç„¶è¯­è¨€â€æ„æˆçš„å—ï¼Ÿ
- å¹¶ä¸æ˜¯ã€‚æœ¬è´¨ä¸Šæ¥è¯´ï¼Œæˆ‘ä»¬å¹¶ä¸å…³å¿ƒæ¨¡ç‰ˆé•¿ä»€ä¹ˆæ ·ï¼Œæˆ‘ä»¬åªéœ€è¦çŸ¥é“æ¨¡ç‰ˆç”±å“ªäº›tokenç»„æˆï¼Œè¯¥æ’å…¥åˆ°å“ªé‡Œï¼Œæ’å…¥åèƒ½ä¸èƒ½å®Œæˆæˆ‘ä»¬çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œè¾“å‡ºçš„å€™é€‰ç©ºé—´æ˜¯ä»€ä¹ˆã€‚æ¨¡ç‰ˆæ˜¯ä¸æ˜¯è‡ªç„¶è¯­è¨€ç»„æˆçš„ï¼Œå¯¹æˆ‘ä»¬æ ¹æœ¬æ²¡å½±å“ï¼Œâ€œè‡ªç„¶è¯­è¨€â€çš„è¦æ±‚ï¼Œåªæ˜¯ä¸ºäº†æ›´å¥½åœ°å®ç°â€œä¸€è‡´æ€§â€ï¼Œä½†ä¸æ˜¯å¿…é¡»çš„ã€‚äºæ˜¯ï¼ŒP-tuningè€ƒè™‘äº†å¦‚ä¸‹å½¢å¼çš„æ¨¡ç‰ˆï¼š

![](https://pic1.zhimg.com/80/v2-a8313077087b511186ccb280a2e08a20_720w.jpg)

â–² P-tuningç›´æ¥ä½¿ç”¨[unused*]çš„tokenæ¥æ„å»ºæ¨¡ç‰ˆï¼Œä¸å…³å¿ƒæ¨¡ç‰ˆçš„è‡ªç„¶è¯­è¨€æ€§

è¿™é‡Œçš„[u1]ï½[u6]ï¼Œä»£è¡¨BERTè¯è¡¨é‡Œè¾¹çš„ [unused1]ï½[unused6]ï¼Œä¹Ÿå°±æ˜¯ç”¨å‡ ä¸ªä»æœªè§è¿‡çš„tokenæ¥æ„æˆæ¨¡æ¿ï¼Œè¿™é‡Œçš„tokenæ•°ç›®æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œæ”¾åœ¨å‰é¢è¿˜æ˜¯åé¢ä¹Ÿå¯ä»¥è°ƒæ•´ã€‚æ¥ç€ï¼Œä¸ºäº†è®©â€œæ¨¡ç‰ˆâ€å‘æŒ¥ä½œç”¨ï¼Œæˆ‘ä»¬ç”¨æ ‡æ³¨æ•°æ®æ¥æ±‚å‡ºè¿™ä¸ªæ¨¡æ¿ã€‚

æ ¹æ®æ ‡æ³¨æ•°æ®é‡çš„å¤šå°‘ï¼Œä¼˜åŒ–æ€è·¯åˆåˆ†ä¸¤ç§æƒ…å†µè®¨è®ºã€‚
- ç¬¬ä¸€ç§ï¼Œæ ‡æ³¨æ•°æ®**æ¯”è¾ƒå°‘**ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å›ºå®šæ•´ä¸ªæ¨¡å‹çš„æƒé‡ï¼Œåªä¼˜åŒ–[unused1]ï½[unused6]è¿™å‡ ä¸ªtokençš„Embeddingï¼Œæ¢å¥è¯è¯´ï¼Œå…¶å®æˆ‘ä»¬å°±æ˜¯è¦å­¦6ä¸ªæ–°çš„Embeddingï¼Œä½¿å¾—å®ƒèµ·åˆ°äº†æ¨¡ç‰ˆçš„ä½œç”¨ã€‚è¿™æ ·ä¸€æ¥ï¼Œå› ä¸ºæ¨¡å‹æƒé‡å‡ ä¹éƒ½è¢«å›ºå®šä½äº†ï¼Œè®­ç»ƒèµ·æ¥å¾ˆå¿«ï¼Œè€Œä¸”å› ä¸ºè¦å­¦ä¹ çš„å‚æ•°å¾ˆå°‘ï¼Œå› æ­¤å“ªæ€•æ ‡æ³¨æ ·æœ¬å¾ˆå°‘ï¼Œä¹Ÿèƒ½æŠŠæ¨¡ç‰ˆå­¦å‡ºæ¥ï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚
- ç¬¬äºŒç§ï¼Œæ ‡æ³¨æ•°æ®**å¾ˆå……è¶³**ã€‚è¿™æ—¶å€™å¦‚æœè¿˜æŒ‰ç…§ç¬¬ä¸€ç§çš„æ–¹æ¡ˆæ¥ï¼Œå°±ä¼šå‡ºç°æ¬ æ‹Ÿåˆçš„æƒ…å†µï¼Œå› ä¸ºåªæœ‰6ä¸ªtokençš„å¯ä¼˜åŒ–å‚æ•°å®åœ¨æ˜¯å¤ªå°‘äº†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æ”¾å¼€æ‰€æœ‰æƒé‡å¾®è°ƒï¼ŒåŸè®ºæ–‡åœ¨SuperGLUEä¸Šçš„å®éªŒå°±æ˜¯è¿™æ ·åšçš„ã€‚è¯»è€…å¯èƒ½ä¼šæƒ³ï¼šè¿™æ ·è·Ÿç›´æ¥åŠ ä¸ªå…¨è¿æ¥å¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼ŸåŸè®ºæ–‡çš„ç»“æœæ˜¯è¿™æ ·åšæ•ˆæœæ›´å¥½ï¼Œå¯èƒ½è¿˜æ˜¯å› ä¸ºè·Ÿé¢„è®­ç»ƒä»»åŠ¡æ›´ä¸€è‡´äº†å§ã€‚

åŸä½œè€…åœ¨SuperGLUEä¸Šçš„å®éªŒç»“æœï¼Œæ˜¾ç¤ºå‡ºå¦‚æœé…åˆP-tuningï¼Œé‚£ä¹ˆï¼š
- 1ï¼‰GPTã€BERTçš„æ•ˆæœç›¸æ¯”ç›´æ¥finetuneéƒ½æœ‰æ‰€æå‡ï¼›
- 2ï¼‰GPTçš„æ•ˆæœè¿˜èƒ½è¶…è¿‡äº†BERTã€‚

![](https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bc%7Ccc%7D++%5Chline++%26+%5Ctext%7B%E9%AA%8C%E8%AF%81%E9%9B%86%7D+%26+%5Ctext%7B%E6%B5%8B%E8%AF%95%E9%9B%86%7D+%5C%5C++%5Chline++%5Ctext%7B%E5%B0%8F%E6%A0%B7%E6%9C%AC%E7%9B%B4%E6%8E%A5%E5%BE%AE%E8%B0%83%7D+%26+88.93%5C%25+%26+89.34%5C%25+%5C%5C++%5Ctext%7BVAT%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%7D+%26+89.83%5C%25+%26+90.37%5C%25+%5C%5C++%5Chline++%5Ctext%7BPET%E9%9B%B6%E6%A0%B7%E6%9C%AC%7D+%26+85.17%5C%25+%26+84.27%5C%25+%5C%5C++%5Ctext%7BPET%E6%97%A0%E7%9B%91%E7%9D%A3%7D+%26+88.05%5C%25+%26+87.53%5C%25+%5C%5C++%5Ctext%7BPET%E5%B0%8F%E6%A0%B7%E6%9C%AC%7D+%26+89.29%5C%25+%26+89.18%5C%25+%5C%5C++%5Ctext%7BPET%E5%8D%8A%E7%9B%91%E7%9D%A3%7D+%26+90.09%5C%25+%26+89.76%5C%25+%5C%5C++%5Chline++%5Ctext%7BBERT+%2B+P-tuning%7D+%26+89.81%5C%25+%26+89.75%5C%25+%5C%5C++%5Ctext%7BGPT+%2B+P-tuning%7D+%26+89.30%5C%25+%26+88.51%5C%25+%5C%5C++%5Chline++%5Cend%7Barray%7D%5C%5C)

å…¶ä¸­â€œå°æ ·æœ¬â€åªç”¨åˆ°äº†â€œå°‘é‡æ ‡æ³¨æ ·æœ¬â€ï¼Œâ€œæ— ç›‘ç£â€åˆ™ç”¨åˆ°äº†â€œå¤§é‡æ— æ ‡æ³¨æ ·æœ¬â€ï¼Œâ€œåŠç›‘ç£â€åˆ™ç”¨åˆ°äº†â€œå°‘é‡æ ‡æ³¨æ ·æœ¬+å¤§é‡æ— æ ‡æ³¨æ ·æœ¬â€ï¼Œâ€œP-tuningâ€éƒ½æ˜¯å°æ ·æœ¬ï¼ŒPETçš„å‡ ä¸ªä»»åŠ¡æŠ¥å‘Šçš„æ˜¯æœ€ä¼˜çš„äººå·¥æ¨¡ç‰ˆçš„ç»“æœï¼Œå…¶å®è¿˜æœ‰æ›´å·®çš„äººå·¥æ¨¡ç‰ˆã€‚ä»å°æ ·æœ¬è§’åº¦æ¥çœ‹ï¼ŒP-tuningç¡®å®å–å¾—äº†æœ€ä¼˜çš„å°æ ·æœ¬å­¦ä¹ æ•ˆæœï¼›ä»æ¨¡ç‰ˆæ„å»ºçš„è§’åº¦æ¥çœ‹ï¼ŒP-tuningç¡®å®ä¹Ÿæ¯”äººå·¥æ„å»ºçš„æ¨¡ç‰ˆè¦å¥½å¾—å¤šï¼›ä»æ¨¡å‹è§’åº¦çœ‹ï¼ŒP-tuningç¡®å®å¯ä»¥å°†GPTçš„åˆ†ç±»æ€§èƒ½å‘æŒ¥åˆ°è·ŸBERTç›¸è¿‘ï¼Œä»è€Œæ­ç¤ºäº†GPTä¹Ÿæœ‰å¾ˆå¼ºçš„NLUèƒ½åŠ›çš„äº‹å®ã€‚

å®Œæ•´[ä»£ç ](https://github.com/bojone/P-tuning), åŸè®ºæ–‡ä¹Ÿå¼€æºäº†[ä»£ç ](https://github.com/THUDM/P-tuning)

## ã€2019-7-26ã€‘RoBERTa â€”â€” æ•°æ®é‡+è®­ç»ƒæ–¹å¼


### RoBERTa ä»‹ç»

ã€2020-5-9ã€‘[Roberta: Bertè°ƒä¼˜](https://zhuanlan.zhihu.com/p/260693956)

Robertaï¼Œæ˜¯Robustly Optimized BERT Approachçš„ç®€ç§°ã€‚
- Robustlyç”¨è¯å¾ˆèµï¼Œæ—¢æœ‰â€œé²æ£’çš„â€ï¼Œåˆæœ‰â€ä½“åŠ›çš„â€ã€‚Robertaæ˜¯å®éªŒä¸ºåŸºç¡€çš„è®ºæ–‡ï¼Œæœ‰ç‚¹ä½“åŠ›æ´»çš„æ„æ€ï¼Œä½†æ˜¯ç»“æœåˆéå¸¸çš„é²æ£’å¯ä¿¡èµ–ã€‚
- ã€2019-7-26ã€‘åç››é¡¿å¤§å­¦ [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)
- ä»£ç åœ¨å·¥å…·åŒ… [bert4keras](https://github.com/bojone/bert4keras) é‡Œ

### RoBERTa æ”¹è¿›ç‚¹

BERT æ¨¡å‹çš„æ”¹è¿›ç‰ˆ: RoBERTa,GLUEï¼ŒSQuADï¼ŒRACE ä¸‰ä¸ªæ¦œä¸Šå–å¾— SOTA

RoBERTa æ˜¯BERTçš„æˆåŠŸå˜ç§ä¹‹ä¸€ï¼Œä¸»è¦æœ‰4ä¸ªç®€å•æœ‰æ•ˆçš„å˜åŒ–ï¼š
- 1ï¼‰**å»é™¤NSP**ä»»åŠ¡ï¼›
- 2ï¼‰å¤§è¯­æ–™ä¸æ›´é•¿çš„è®­ç»ƒæ­¥æ•°ï¼šbatch sizeæ›´å¤§ï¼Œæ•°æ®æ›´å¤šï¼›
- 3ï¼‰æ›´é•¿çš„è®­ç»ƒå¥å­ï¼›
- 4ï¼‰Maskingç­–ç•¥â€”â€”é™æ€ä¸åŠ¨æ€ï¼š**åŠ¨æ€**æ”¹å˜ `[MASK]` æ¨¡å¼ã€‚å¤åˆ¶å¤šä»½æ•°æ®

Bert çš„ä¼˜åŒ–ç‰ˆï¼Œæ¨¡å‹ç»“æ„ä¸ Bert å®Œå…¨ä¸€æ ·ï¼Œåªæ˜¯åœ¨æ•°æ®é‡å’Œè®­ç»ƒæ–¹æ³•ä¸Šåšäº†æ”¹è¿›ã€‚ç®€å•è¯´å°±æ˜¯æ›´å¤§çš„æ•°æ®é‡ï¼Œæ›´å¥½çš„è®­ç»ƒæ–¹å¼ï¼Œè®­ç»ƒå¾—æ›´ä¹…ä¸€äº›ã€‚
- ç›¸æ¯”åŸç”Ÿ Bert çš„16Gè®­ç»ƒæ•°æ®ï¼ŒRoBerta è®­ç»ƒæ•°æ®é‡è¾¾åˆ°äº†161Gï¼›
- å»é™¤äº† NSP ä»»åŠ¡ï¼Œç ”ç©¶è¡¨æ˜ NSP ä»»åŠ¡å¤ªè¿‡ç®€å•ï¼Œä¸ä»…ä¸èƒ½æå‡åå€’æœ‰æŸæ¨¡å‹æ€§èƒ½ï¼›
- MLM æ¢æˆ Dynamic Masking LMï¼›
- æ›´å¤§çš„ Batch size ä»¥åŠå…¶ä»–è¶…å‚æ•°çš„è°ƒä¼˜ï¼›

RoBERTa åœ¨ BERT çš„åŸºç¡€ä¸Šå–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„ç»“æœã€‚è€Œä¸”ï¼ŒRoBERTa å·²ç»æŒ‡å‡ºï¼Œ**NSP ä»»åŠ¡å¯¹äº BERT çš„è®­ç»ƒæ¥è¯´ç›¸å¯¹æ²¡ç”¨**ã€‚

ç»“è®ºï¼š
- NSP ä¸æ˜¯å¿…é¡» loss
- Maskæ–¹å¼è™½ä¸æ˜¯æœ€ä¼˜ä½†æ˜¯å·²æ¥è¿‘ã€‚
- å¢å¤§batch sizeå’Œå¢å¤§è®­ç»ƒæ•°æ®èƒ½å¸¦æ¥è¾ƒå¤§çš„æå‡ã€‚

ç”±äºRobertaå‡ºè‰²çš„æ€§èƒ½ï¼Œç°åœ¨å¾ˆå¤šåº”ç”¨éƒ½æ˜¯åŸºäºRobertaè€Œä¸æ˜¯åŸå§‹çš„Bertå»å¾®è°ƒäº†ã€‚

ï¼ˆ1ï¼‰åŠ¨æ€ mask
- Bertä¸­æ˜¯åœ¨è®­ç»ƒæ•°æ®ä¸­é™æ€çš„æ ‡ä¸ŠMaskæ ‡è®°ï¼Œç„¶ååœ¨è®­ç»ƒä¸­æ˜¯ä¸å˜çš„ï¼Œè¿™ç§æ–¹å¼å°±æ˜¯**é™æ€**çš„ã€‚
- Robertaå°è¯•äº†ä¸€ç§åŠ¨æ€çš„æ–¹å¼ï¼Œè¯´æ˜¯åŠ¨æ€ï¼Œå…¶å®ä¹Ÿæ˜¯ç”¨é™æ€çš„æ–¹å¼å®ç°çš„ï¼ŒæŠŠæ•°æ®å¤åˆ¶10ä»½ï¼Œæ¯ä¸€ä»½ä¸­é‡‡ç”¨ä¸åŒçš„Maskã€‚è¿™æ ·å°±æœ‰äº†10ç§ä¸åŒçš„Maskæ•°æ®ã€‚
- ä»ç»“æœä¸­ï¼Œå¯ä»¥çœ‹åˆ°åŠ¨æ€maskèƒ½å¸¦æ¥å¾®å°çš„æå‡ã€‚

ï¼ˆ2ï¼‰NSPä»»åŠ¡

Bertçš„æ¨¡å‹è¾“å…¥ä¸­æ˜¯ç”±ä¸¤ä¸ªsegmentç»„æˆçš„ï¼Œå› è€Œå°±æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š
- ä¸¤ä¸ªsegmentæ˜¯ä¸æ˜¯å¿…è¦ï¼Ÿ
- ä¸ºä»€ä¹ˆæ˜¯segmentè€Œä¸æ˜¯å•ä¸ªçš„å¥å­ï¼Ÿ

å› æ­¤è®¾ç½®äº†å››ä¸ªå®éªŒï¼š
- Segment-Pair + NSP
- Sentence-Pair + NSP: åªç”¨äº†sentenceä»¥åï¼Œè¾“å…¥çš„é•¿åº¦ä¼šå˜å°‘ï¼Œä¸ºäº†ä½¿å¾—æ¯ä¸€æ­¥è®­ç»ƒè§åˆ°çš„tokenæ•°ç±»ä¼¼ï¼Œåœ¨è¿™é‡Œä¼šå¢å¤§batch size
- Full-Sentence: æ¯ä¸€ä¸ªæ ·æœ¬éƒ½æ˜¯ä»ä¸€ä¸ªæ–‡æ¡£ä¸­è¿ç»­sampleå‡ºæ¥çš„ï¼Œå¦‚æœè·¨è¿‡æ–‡æ¡£è¾¹ç•Œï¼Œå°±æ·»åŠ ä¸€ä¸ª[SEP]çš„æ ‡è®°ï¼Œæ²¡æœ‰NSPæŸå¤±ã€‚
- Doc-Sentence: ç±»ä¼¼äºFull-Sentenceï¼Œä½†æ˜¯ä¸ä¼šè·¨è¿‡æ–‡æ¡£è¾¹ç•Œã€‚
ä»å®éªŒç»“æœä¸­å¯ä»¥çœ‹åˆ°ï¼Œæ”¹ç”¨Sentence-Pairä¼šå¸¦æ¥ä¸€ä¸ªè¾ƒå¤§çš„æŸå¤±ã€‚çŒœæµ‹æ˜¯å› ä¸ºè¿™æ ·æ— æ³•æ•æ‰long-termçš„ä¾èµ–ã€‚

å¦å¤–ï¼ŒFull-Sentenceå’ŒDoc-Sentenceèƒ½å¤Ÿå¸¦æ¥å¾®å°çš„æå‡ï¼Œè¯´æ˜NSPä¸æ˜¯å¿…é¡»çš„ã€‚
- è¿™ç‚¹è·ŸBertä¸­çš„æ¶ˆèå®éªŒç»“è®ºç›¸åï¼Œä½†æ˜¯è¯·æ³¨æ„å®ƒä»¬çš„è¾“å…¥è¿˜æ˜¯ä¸åŒçš„ï¼ŒåŸå§‹Bertä¸­çš„è¾“å…¥æ˜¯Segment-Pairï¼Œæœ‰50%/50%çš„é‡‡æ ·ï¼Œè€ŒFull/Doc-Sentenceä¸­åˆ™æ˜¯ä»æ–‡ç« ä¸­è¿ç»­sampleæ¥çš„å¥å­ã€‚

å› ä¸ºDoc-Sentenceä¼šå¯¼è‡´ä¸åŒçš„batch_sizeï¼ˆå› ä¸ºè¦ä¿è¯æ¯ä¸ªbatchè§åˆ°çš„tokenæ•°ç±»ä¼¼ï¼‰ï¼Œæ‰€ä»¥åœ¨Robertaä¸­ï¼Œä½¿ç”¨Full-Sentenceæ¨¡å¼ã€‚

ï¼ˆ3ï¼‰Large-Batch

ç°åœ¨è¶Šæ¥è¶Šå¤šçš„å®éªŒè¡¨æ˜å¢å¤§batch_sizeä¼šä½¿å¾—æ”¶æ•›æ›´å¿«ï¼Œæœ€åçš„æ•ˆæœæ›´å¥½ã€‚åŸå§‹çš„Bertä¸­ï¼Œbatch_size=256ï¼ŒåŒæ—¶è®­ç»ƒ1M stepsã€‚

åœ¨Robertaä¸­ï¼Œå®éªŒäº†ä¸¤ä¸ªè®¾ç½®ï¼š
- batch_size=2k, è®­ç»ƒ125k stepsã€‚
- batch_size=8k, è®­ç»ƒ31k stepsã€‚
ä»ç»“æœä¸­çœ‹ï¼Œbatch_size=2kæ—¶ç»“æœæœ€å¥½ã€‚

### RoBERTa éƒ¨ç½²

```sh
pip install git+https://www.github.com/bojone/bert4keras.git
```


## BERT å‹ç¼©è’¸é¦


### ã€2019-9-23ã€‘TinyBERT


BERT å¼ºå¤§æ¯«æ— ç–‘é—®ï¼Œä½†ç”±äºæ¨¡å‹è¿‡äºåºå¤§ï¼Œå•ä¸ªæ ·æœ¬è®¡ç®—ä¸€æ¬¡çš„å¼€é”€åŠ¨è¾„ä¸Šç™¾æ¯«ç§’ï¼Œå¾ˆéš¾åº”ç”¨åˆ°å®é™…ç”Ÿäº§ä¸­ã€‚

TinyBERT æ˜¯åä¸ºã€åç§‘è”åˆæå‡ºçš„ä¸€ç§ä¸ºåŸºäº transformer çš„æ¨¡å‹ä¸“é—¨è®¾è®¡çš„**çŸ¥è¯†è’¸é¦**æ–¹æ³•ï¼Œæ¨¡å‹å¤§å°ä¸åˆ° BERT çš„ **1/7**ï¼Œä½†é€Ÿåº¦æé«˜äº† **9 å€**ï¼Œè€Œä¸”æ€§èƒ½æ²¡æœ‰å‡ºç°æ˜æ˜¾ä¸‹é™ã€‚
- ICLR 2020 [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/pdf/1909.10351.pdf)
- æºç  [TinyBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT)

æœ¬æ–‡å¤ç°äº† TinyBERT çš„ç»“æœï¼Œè¯æ˜äº† Tiny BERT åœ¨é€Ÿåº¦æé«˜çš„åŒæ—¶ï¼Œå¯¹å¤æ‚çš„è¯­ä¹‰åŒ¹é…ä»»åŠ¡ï¼Œæ€§èƒ½æ²¡æœ‰æ˜¾è‘—ä¸‹é™ã€‚

TinyBERT æ¨¡å‹ç»“æ„
- ![](https://pic4.zhimg.com/80/v2-4c4c0b519fb52f2c17e12711ee4ebf1f_1440w.webp)


### ã€2019-8-31ã€‘MiniBERT

Googleæ¨å‡ºçš„ä¸€ç§è½»é‡çº§ç‰ˆæœ¬çš„BERTæ¨¡å‹ï¼ˆå³MiniBERTï¼‰ï¼Œè¯¥æ¨¡å‹åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¹…å‡å°‘äº†å‚æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼Œä»è€Œæ˜¾è‘—ç¼©çŸ­äº†é¢„è®­ç»ƒæ—¶é—´
- [Small and Practical BERT Models for Sequence Labeling](https://arxiv.org/pdf/1909.00100.pdf)

ä»£ç 
- [Mini-BERT for Text Classification](https://github.com/nunonmg/minibert-text-classification)


### ã€2021ã€‘ AutoTinyBERT

ä¸€æ¬¡æ€§ç¥ç»æ¶æ„æœç´¢ï¼ˆNASï¼‰æ¥è‡ªåŠ¨æœç´¢æ¶æ„è¶…å‚æ•°ã€‚
- è®ºæ–‡ï¼š[AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models](http://arxiv.org/abs/2107.13686)
- ä»£ç ï¼š[AutoTinyBERT](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/AutoTinyBERT)

åœ¨ GLUE å’Œ SQuAD åŸºå‡†æµ‹è¯•ä¸­çš„æœ‰æ•ˆæ€§ã€‚
- AutoTinyBERT ä¼˜äºåŸºäº **SOTA æœç´¢**çš„åŸºçº¿ï¼ˆNAS-BERTï¼‰å’ŒåŸºäº **SOTA è’¸é¦**çš„æ–¹æ³•ï¼ˆä¾‹å¦‚ DistilBERTã€TinyBERTã€MiniLM å’Œ MobileBERTï¼‰ã€‚

æ­¤å¤–ï¼ŒåŸºäºè·å¾—çš„æ¶æ„ï¼Œæå‡ºäº†ä¸€ç§æ›´é«˜æ•ˆçš„å¼€å‘æ–¹æ³•ï¼Œç”šè‡³æ¯”å•ä¸ª PLM çš„å¼€å‘é€Ÿåº¦è¿˜è¦å¿«ã€‚


## ã€2019-9-28ã€‘ALBERT è½»é‡çº§bert 30%å‚æ•°

ã€2019-9-28ã€‘è°·æ­ŒLabå‘å¸ƒæ–°çš„é¢„è®­ç»ƒæ¨¡å‹"ALBERT"å…¨é¢åœ¨SQuAD 2.0ã€GLUEã€RACEç­‰ä»»åŠ¡ä¸Šè¶…è¶Šäº†BERTã€XLNetã€RoBERTaï¼Œå†æ¬¡åˆ·æ–°äº†æ’è¡Œæ¦œ
- [ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://arxiv.org/pdf/1909.11942.pdf)
- [ALBERT](https://github.com/google-research/ALBERT) code
- ![](https://pic4.zhimg.com/80/v2-237353aaeb0f6a2a8fe24bcd585cc65b_720w.jpg)

Albert çš„ç›®çš„æ˜¯æƒ³å¯¹ Bert ç˜¦èº«ï¼Œå¸Œæœ›ç”¨æ›´ç®€å•çš„æ¨¡å‹ï¼Œæ›´å°‘çš„æ•°æ®ï¼Œå¾—åˆ°æ›´å¥½çš„ç»“æœã€‚å®ƒä¸»è¦ä»ä¸¤ä¸ªæ–¹é¢å‡å°‘æ¨¡å‹å‚æ•°é‡ï¼š
- å¯¹ Vocabulary Embedding è¿›è¡Œ**çŸ©é˜µåˆ†è§£**ï¼Œå°†åŸæ¥çš„çŸ©é˜µV x Eåˆ†è§£æˆä¸¤ä¸ªçŸ©é˜µ V x H å’Œ H x Eï¼ˆH << Eï¼‰ã€‚
- **è·¨å±‚å‚æ•°å…±äº«**ï¼Œæ¯å±‚çš„ attention map æœ‰ç›¸ä¼¼çš„patternï¼Œå¯ä»¥è€ƒè™‘å…±äº«ã€‚

ALBERTæœ€å°çš„å‚æ•°åªæœ‰åå‡ M, æ•ˆæœè¦æ¯”BERTä½1-2ä¸ªç‚¹ï¼Œæœ€å¤§çš„xxlargeä¹Ÿå°±200å¤šMã€‚

æ•ˆæœæ˜¯ **70%**å‚æ•°é‡çš„å‰Šå‡ï¼Œæ¨¡å‹æ€§èƒ½æŸå¤± < **3%**ï¼Œä½†æœ‰å¾ˆå¤šæ–‡ç« æŒ‡å‡º Albert è®¡ç®—é‡å¹¶æ²¡æœ‰å‡å°‘å¤ªå¤šï¼Œå¹¶ä¸”å¦‚æœè¾¾åˆ°è¶…è¿‡ Bert æ€§èƒ½ï¼Œéœ€è¦ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ä»¥åŠç›¸åº”çš„ç®—åŠ›ã€‚

ã€2020-5-20ã€‘[BERTçš„youxiuå˜ä½“ï¼šALBERTè®ºæ–‡å›¾è§£ä»‹ç»](https://zhuanlan.zhihu.com/p/142416395)

ALBERT(ä¸€ä¸ªLite BERT)(Lanç­‰äººï¼Œ2019)ä¸»è¦è§£å†³äº†æ›´é«˜çš„**å†…å­˜æ¶ˆè€—**å’ŒBERT**è®­ç»ƒé€Ÿåº¦æ…¢**çš„é—®é¢˜ã€‚ 

ALBERTå¼•å…¥äº†ä¸¤ç§å‚æ•°å‡å°‘æŠ€æœ¯ã€‚ 
- é¦–å…ˆæ˜¯åµŒå…¥åˆ†è§£ï¼Œå®ƒå°†åµŒå…¥çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªå°çš„çŸ©é˜µã€‚ 
- å…¶æ¬¡æ˜¯è·¨å±‚å‚æ•°å…±äº«ï¼Œè·¨ALBERTçš„æ¯ä¸€å±‚å…±äº«transformeræƒé‡ï¼Œè¿™å°†æ˜¾ç€å‡å°‘å‚æ•°ã€‚ 
æ­¤å¤–ï¼Œä»–ä»¬è¿˜æå‡ºäº†**å¥åºé¢„æµ‹**(SOP)ä»»åŠ¡æ¥ä»£æ›¿ä¼ ç»Ÿçš„**NSP**é¢„è®­ç»ƒä»»åŠ¡ã€‚

ALBERTä½œä¸ºBERTçš„ä¸€ä¸ªå˜ä½“ï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŸºç¡€ä¸Šï¼Œå¤§å¤§å‡å°‘äº†æ¨¡å‹çš„å‚æ•°ï¼Œä½¿å¾—å®ç”¨å˜å¾—æ›´åŠ æ–¹ä¾¿ï¼Œæ˜¯ç»å…¸çš„BERTå˜ä½“ä¹‹ä¸€ã€‚
 
è€ƒè™‘ä¸‹é¢ç»™å‡ºçš„å¥å­ã€‚ä½œä¸ºäººç±»ï¼Œå½“æˆ‘ä»¬é‡åˆ°â€œ**apple**â€è¿™ä¸ªè¯æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
*   æŠŠâ€œappleâ€è¿™ä¸ªè¯å’Œæˆ‘ä»¬å¯¹â€œappleâ€è¿™ä¸ªæ°´æœçš„è¡¨å¾è”ç³»èµ·æ¥
*   æ ¹æ®ä¸Šä¸‹æ–‡å°†â€œappleâ€ä¸æ°´æœè”ç³»åœ¨ä¸€èµ·ï¼Œè€Œä¸æ˜¯ä¸å…¬å¸è”ç³»åœ¨ä¸€èµ·
*   ç†è§£â€œ_he ate an apple_â€
*   åœ¨å­—ç¬¦çº§ï¼Œå•è¯çº§å’Œå¥å­çº§ç†è§£å®ƒ
 
![](https://pic2.zhimg.com/v2-e7dda49cb43c1d2f5e4bbe757b0a5a19_b.jpg)
 
NLPæœ€æ–°å‘å±•çš„åŸºæœ¬å‰ææ˜¯èµ‹äºˆæœºå™¨å­¦ä¹ è¿™äº›è¡¨ç¤ºçš„èƒ½åŠ›ã€‚  
2018å¹´ï¼Œè°·æ­Œå‘å¸ƒäº†BERTï¼Œè¯•å›¾åŸºäºä¸€äº›æ–°çš„æƒ³æ³•æ¥å­¦ä¹ è¿™ä¸ªè¡¨ç¤ºï¼š
 
### å›é¡¾ BERT

**1. æ©ç è¯­è¨€å»ºæ¨¡**
 
è¯­è¨€å»ºæ¨¡æ–¹æ³•åŒ…æ‹¬ï¼šé¢„æµ‹å•è¯çš„ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬å½“å‰å•è¯æ—¶é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚
- ![](https://pic2.zhimg.com/v2-e394582d3b192c7b9e6ce6a23a5ea67d_b.jpg)
 
BERTä½¿ç”¨**æ©ç è¯­è¨€æ¨¡å‹**ï¼Œæ–‡æ¡£ä¸­éšæœºåœ°å¯¹å•è¯è¿›è¡Œæ©ç ï¼Œå¹¶è¯•å›¾æ ¹æ®å‘¨å›´ä¸Šä¸‹æ–‡æ¥é¢„æµ‹å®ƒä»¬ã€‚
- ![](https://pic3.zhimg.com/v2-cceaeae52d02d52a4a2a27dac0662d9a_b.jpg)
 
 **2. ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹**  
 
â€œä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹â€ç›®çš„æ˜¯æ£€æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦è¿è´¯ã€‚
- ![](https://pic2.zhimg.com/v2-a66f48660c3d0d00c8c06011f0897fc1_b.jpg)
 
è®­ç»ƒæ•°æ®ä¸­çš„è¿ç»­å¥è¢«ç”¨ä½œä¸€ä¸ªæ­£æ ·æœ¬ã€‚å¯¹äºè´Ÿæ ·æœ¬ï¼Œå–ä¸€ä¸ªå¥å­ï¼Œç„¶ååœ¨å¦ä¸€ä¸ªæ–‡æ¡£ä¸­éšæœºæŠ½å–ä¸€ä¸ªå¥å­æ”¾åœ¨å®ƒçš„æ—è¾¹ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼ŒBERTæ¨¡å‹è¢«è®­ç»ƒæ¥è¯†åˆ«ä¸¤ä¸ªå¥å­æ˜¯å¦å¯ä»¥åŒæ—¶å‡ºç°ã€‚
 
**3. Transformerç»“æ„**
 
ä¸ºäº†è§£å†³ä¸Šè¿°ä¸¤é¡¹ä»»åŠ¡ï¼ŒBERTä½¿ç”¨äº†å¤šå±‚Transformeræ¨¡å—ä½œä¸ºç¼–ç å™¨ã€‚å•è¯å‘é‡è¢«ä¼ é€’åˆ°å„ä¸ªå±‚ï¼Œä»¥æ•è·å…¶å«ä¹‰ï¼Œå¹¶ä¸ºåŸºæœ¬æ¨¡å‹ç”Ÿæˆå¤§å°ä¸º768çš„å‘é‡ã€‚
- ![](https://pic2.zhimg.com/v2-2797503b2f0e9275bd2d0466e47e66a5_b.jpg)
 
Jay Alammaræœ‰ä¸€ç¯‡éå¸¸å¥½çš„æ–‡ç« ï¼š[http://jalammar.github.io/bert/](https://link.zhihu.com/?target=http%3A//jalammar.github.io/bert/)ï¼Œæ›´æ·±å…¥åœ°é˜è¿°äº†Transformerçš„å†…éƒ¨æœºåˆ¶ã€‚
 
### BERT é—®é¢˜
 
BERTå‘å¸ƒåï¼Œåœ¨æ’è¡Œæ¦œä¸Šäº§ç”Ÿäº†è®¸å¤šNLPä»»åŠ¡çš„æœ€æ–°æˆæœã€‚ä½†æ˜¯ï¼Œæ¨¡å‹éå¸¸å¤§ï¼Œå¯¼è‡´äº†ä¸€äº›é—®é¢˜ã€‚â€œALBERTâ€è®ºæ–‡å°†è¿™äº›é—®é¢˜åˆ†ä¸ºä¸¤ç±»ï¼š
 
1ã€**å†…å­˜é™åˆ¶å’Œé€šä¿¡å¼€é”€**ï¼š
 
è€ƒè™‘ä¸€ä¸ªåŒ…å«ä¸€ä¸ªè¾“å…¥èŠ‚ç‚¹ã€ä¸¤ä¸ªéšè—èŠ‚ç‚¹å’Œä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹çš„ç®€å•ç¥ç»ç½‘ç»œã€‚å³ä½¿æ˜¯è¿™æ ·ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œï¼Œç”±äºæ¯ä¸ªèŠ‚ç‚¹çš„æƒé‡å’Œåå·®ï¼Œä¹Ÿä¼šæœ‰7ä¸ªå‚æ•°éœ€è¦å­¦ä¹ ã€‚
 
![](https://pic1.zhimg.com/v2-955ed3bb42debfc88c5d35ae147d4b70_b.jpg)

BERT-largeæ¨¡å‹æ˜¯ä¸€ä¸ªå¤æ‚çš„æ¨¡å‹ï¼Œå®ƒæœ‰24ä¸ªéšå«å±‚ï¼Œåœ¨å‰é¦ˆç½‘ç»œå’Œæ³¨æ„å¤´ä¸­æœ‰å¾ˆå¤šèŠ‚ç‚¹ï¼Œæ‰€ä»¥æœ‰3.4äº¿ä¸ªå‚æ•°ã€‚å¦‚æœä½ æƒ³åœ¨BERTçš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œä½ éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºçš„éœ€æ±‚æ¥ä»é›¶å¼€å§‹è¿›è¡Œè®­ç»ƒå¹¶åœ¨å…¶ä¸Šè¿›è¡Œè¿­ä»£
 
![](https://pic3.zhimg.com/v2-c30e166f19a018432a2354ac329b2eaa_b.jpg)
 
è¿™äº›è®¡ç®—éœ€æ±‚ä¸»è¦æ¶‰åŠgpuå’ŒTPUsï¼Œä½†æ˜¯è¿™äº›è®¾å¤‡æœ‰å†…å­˜é™åˆ¶ã€‚æ‰€ä»¥ï¼Œæ¨¡å‹çš„å¤§å°æ˜¯æœ‰é™åˆ¶çš„ã€‚  
 
åˆ†å¸ƒå¼è®­ç»ƒæ˜¯è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æµè¡Œæ–¹æ³•ã€‚æˆ‘ä»¬ä»¥BERT-largeä¸Šçš„æ•°æ®å¹¶è¡Œæ€§ä¸ºä¾‹ï¼Œå…¶ä¸­è®­ç»ƒæ•°æ®è¢«åˆ†åˆ°ä¸¤å°æœºå™¨ä¸Šã€‚æ¨¡å‹åœ¨ä¸¤å°æœºå™¨ä¸Šå¯¹æ•°æ®å—è¿›è¡Œè®­ç»ƒã€‚å¦‚å›¾æ‰€ç¤ºï¼Œä½ å¯ä»¥æ³¨æ„åˆ°åœ¨æ¢¯åº¦åŒæ­¥è¿‡ç¨‹ä¸­è¦ä¼ è¾“çš„å¤§é‡å‚æ•°ï¼Œè¿™ä¼šå‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚åŒæ ·çš„ç“¶é¢ˆä¹Ÿé€‚ç”¨äºæ¨¡å‹çš„å¹¶è¡Œæ€§ï¼Œå³æˆ‘ä»¬åœ¨ä¸åŒçš„æœºå™¨ä¸Šå­˜å‚¨æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†ã€‚
 
![](https://pic2.zhimg.com/v2-2ecac204965c9acbdaec48a3c4f413e9_b.jpg)
 
2ã€**æ¨¡å‹é€€åŒ–**  
 
æœ€è¿‘åœ¨NLPç ”ç©¶ç¤¾åŒºçš„è¶‹åŠ¿æ˜¯ä½¿ç”¨è¶Šæ¥è¶Šå¤§çš„æ¨¡å‹ï¼Œä»¥è·å¾—åœ¨æ’è¡Œæ¦œä¸Šçš„æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ALBERT çš„ç ”ç©¶è¡¨æ˜ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ”¶ç›Šé€€åŒ–ã€‚
 
åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…åšäº†ä¸€ä¸ªæœ‰è¶£çš„å®éªŒã€‚
 
> å¦‚æœæ›´å¤§çš„æ¨¡å‹å¯ä»¥å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ï¼Œä¸ºä»€ä¹ˆä¸å°†æœ€å¤§çš„BERTæ¨¡å‹(BERT-large)çš„éšå«å±‚å•å…ƒå¢åŠ ä¸€å€ï¼Œä»1024ä¸ªå•å…ƒå¢åŠ åˆ°2048ä¸ªå•å…ƒå‘¢?
 
ä»–ä»¬ç§°ä¹‹ä¸ºâ€œBERT-xlargeâ€ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæ— è®ºæ˜¯åœ¨è¯­è¨€å»ºæ¨¡ä»»åŠ¡è¿˜æ˜¯åœ¨é˜…è¯»ç†è§£æµ‹è¯•(RACE)ä¸­ï¼Œè¿™ä¸ªæ›´å¤§çš„æ¨¡å‹çš„è¡¨ç°éƒ½ä¸å¦‚BERT-largeæ¨¡å‹ã€‚
 
![](https://pic1.zhimg.com/v2-32e5c234b4128584c752d11cb3751a48_b.jpg)
 
ä»åŸæ–‡ç»™å‡ºçš„å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ€§èƒ½æ˜¯å¦‚ä½•ä¸‹é™çš„ã€‚BERT-xlargeçš„æ€§èƒ½æ¯”BERT-largeå·®ï¼Œå°½ç®¡å®ƒæ›´å¤§å¹¶ä¸”æœ‰æ›´å¤šçš„å‚æ•°ã€‚  
 
![](https://pic3.zhimg.com/v2-1a9c618bd33e5fd93ce6d98ef1f50f66_b.jpg)
 
### ALBERT åŸç†

ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼š
- å½“æˆ‘ä»¬è®©ä¸€ä¸ªæ¨¡å‹çš„å‚æ•°å˜å¤šçš„æ—¶å€™ï¼Œä¸€å¼€å§‹æ¨¡å‹æ•ˆæœæ˜¯æé«˜çš„è¶‹åŠ¿ï¼Œä½†ä¸€æ—¦å¤æ‚åˆ°äº†ä¸€å®šçš„ç¨‹åº¦ï¼Œæ¥ç€å†å»å¢åŠ å‚æ•°åè€Œä¼šè®©æ•ˆæœé™ä½ï¼Œè¿™ä¸ªç°è±¡å«ä½œâ€œmodel degratation"ã€‚

albertè¦è§£å†³çš„é—®é¢˜ï¼š
1. è®©æ¨¡å‹çš„å‚æ•°æ›´å°‘ 
2. ä½¿ç”¨æ›´å°‘çš„å†…å­˜ 
3. æå‡æ¨¡å‹çš„æ•ˆæœ

ALBERTæå‡ºäº†ä¸‰ç§ä¼˜åŒ–ç­–ç•¥ï¼Œåšåˆ°äº†æ¯”BERTæ¨¡å‹å°å¾ˆå¤šçš„æ¨¡å‹ï¼Œä½†æ•ˆæœåè€Œè¶…è¶Šäº†BERTï¼Œ XLNetã€‚
- **ä½ç§©åˆ†è§£** `Factorized Embedding Parameterization`. é’ˆå¯¹äºVocabulary Embeddingã€‚åœ¨BERTã€XLNetä¸­ï¼Œè¯è¡¨çš„embedding size(E)å’Œtransformerå±‚çš„hidden size(H)æ˜¯ç­‰åŒçš„ï¼Œæ‰€ä»¥E=Hã€‚ä½†å®é™…ä¸Šè¯åº“çš„å¤§å°ä¸€èˆ¬éƒ½å¾ˆå¤§ï¼Œè¿™å°±å¯¼è‡´æ¨¡å‹å‚æ•°ä¸ªæ•°å°±ä¼šå˜å¾—å¾ˆå¤§ã€‚é€šè¿‡å¯¹Embedding éƒ¨åˆ†é™ç»´æ¥è¾¾åˆ°é™ä½å‚æ•°çš„ä½œç”¨ã€‚åœ¨æœ€åˆçš„BERTä¸­ï¼Œä»¥Baseä¸ºä¾‹ï¼ŒEmbeddingå±‚çš„ç»´åº¦ä¸éšå±‚çš„ç»´åº¦ä¸€æ ·éƒ½æ˜¯768ï¼Œè€Œè¯çš„åˆ†å¸ƒå¼è¡¨ç¤ºï¼Œå¾€å¾€å¹¶ä¸éœ€è¦è¿™ä¹ˆé«˜çš„ç»´åº¦ï¼Œå¦‚Word2Vecæ—¶ä»£å°±å¤šé‡‡ç”¨50æˆ–300è¿™æ ·çš„ç»´åº¦ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ä»–ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºfactorizationçš„æ–¹æ³•ã€‚ä»–ä»¬æ²¡æœ‰ç›´æ¥æŠŠone-hotæ˜ å°„åˆ°hidden layer, è€Œæ˜¯å…ˆæŠŠone-hotæ˜ å°„åˆ°ä½ç»´ç©ºé—´ä¹‹åï¼Œå†æ˜ å°„åˆ°hidden layerã€‚è¿™å…¶å®ç±»ä¼¼äºåšäº†çŸ©é˜µçš„åˆ†è§£ã€‚
  - ![](https://www.zhihu.com/equation?tex=+O%28V+%5Ctimes+H%29+%5Cto+O%28V+%5Ctimes+E+%2B+E+%5Ctimes+H%29+)
  - Vï¼šè¯è¡¨å¤§å°ï¼›Hï¼šéšå±‚ç»´åº¦ï¼›Eï¼šè¯å‘é‡ç»´åº¦
  - ä»¥ BERT-Base ä¸ºä¾‹ï¼ŒBaseä¸­çš„Hidden size ä¸º768ï¼Œ è¯è¡¨å¤§å°ä¸º3wï¼Œæ­¤æ—¶çš„å‚æ•°é‡ä¸ºï¼š768 * 3w = 23040000ã€‚ å¦‚æœå°† Embedding çš„ç»´åº¦æ”¹ä¸º 128ï¼Œé‚£ä¹ˆæ­¤æ—¶Embeddingå±‚çš„å‚æ•°é‡ä¸ºï¼š 128 * 3w + 128 * 768 = 3938304ã€‚äºŒè€…çš„å·®ä¸º19101696ï¼Œå¤§çº¦ä¸º19Mã€‚æˆ‘ä»¬çœ‹åˆ°ï¼Œå…¶å®Embeddingå‚æ•°é‡ä»åŸæ¥çš„23Må˜ä¸ºäº†ç°åœ¨çš„4Mï¼Œä¼¼ä¹å˜åŒ–ç‰¹åˆ«å¤§ï¼Œç„¶è€Œå½“æˆ‘ä»¬æ”¾åˆ°å…¨å±€æ¥çœ‹çš„è¯ï¼ŒBERT-Baseçš„å‚æ•°é‡åœ¨110Mï¼Œé™ä½19Mä¹Ÿä¸èƒ½äº§ç”Ÿä»€ä¹ˆé©å‘½æ€§çš„å˜åŒ–ã€‚å› æ­¤ï¼Œå¯ä»¥è¯´Embeddingå±‚çš„å› å¼åˆ†è§£å…¶å®å¹¶ä¸æ˜¯é™ä½å‚æ•°é‡çš„ä¸»è¦æ‰‹æ®µã€‚
- **å±‚é—´å‚æ•°å…±äº«** `Cross-layer parameter sharing`. Zhenzhongåšå£«æå‡ºæ¯ä¸€å±‚çš„layerå¯ä»¥å…±äº«å‚æ•°ï¼Œè¿™æ ·ä¸€æ¥å‚æ•°çš„ä¸ªæ•°ä¸ä¼šä»¥å±‚æ•°çš„å¢åŠ è€Œå¢åŠ ã€‚æ‰€ä»¥æœ€åå¾—å‡ºæ¥çš„æ¨¡å‹ç›¸æ¯”BERT-largeå°18å€ä»¥ä¸Šã€‚
  - æœ¬è´¨ä¸Šå°±æ˜¯å¯¹å‚æ•°å…±äº«æœºåˆ¶åœ¨Transformerå†…çš„æ¢è®¨ã€‚Transformerä¸¤å¤§ä¸»è¦çš„ç»„ä»¶ï¼šFFNä¸å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚ä¼ ç»Ÿ Transformer çš„æ¯ä¸€å±‚å‚æ•°éƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒåŒ…æ‹¬å„å±‚çš„ self-attentionã€å…¨è¿æ¥
  - ALBERT å°†æ‰€æœ‰å±‚çš„å‚æ•°è¿›è¡Œå…±äº«ï¼Œç›¸å½“äºåªå­¦ä¹ ç¬¬ä¸€å±‚çš„å‚æ•°ï¼Œå¹¶åœ¨å‰©ä¸‹çš„æ‰€æœ‰å±‚ä¸­é‡ç”¨è¯¥å±‚çš„å‚æ•°ï¼Œè€Œä¸æ˜¯æ¯ä¸ªå±‚éƒ½å­¦ä¹ ä¸åŒçš„å‚æ•°
- **SOPæ›¿ä»£NSP** `Inter-sentence coherence loss`. 
  - BERT è®­ç»ƒä¸­æå‡ºäº†next sentence prediction loss, ç»™å®šä¸¤ä¸ªsentence segments, ç„¶åè®©BERTå»é¢„æµ‹å®ƒä¿©ä¹‹é—´çš„å…ˆåé¡ºåº
  - ä½†ALBERTæå‡ºè¿™ç§æœ‰é—®é¢˜ï¼Œè¿™ç§è®­ç»ƒæ–¹å¼ç”¨å¤„ä¸æ˜¯å¾ˆå¤§ã€‚ æ‰€ä»¥ä½¿ç”¨ setence-order prediction loss (SOP)ï¼Œå…¶å®æ˜¯åŸºäºä¸»é¢˜çš„å…³è”å»é¢„æµ‹æ˜¯å¦ä¸¤ä¸ªå¥å­è°ƒæ¢äº†é¡ºåºã€‚

æ¨¡å‹å‹ç¼©æœ‰å¾ˆå¤šæ‰‹æ®µï¼ŒåŒ…æ‹¬å‰ªæï¼Œå‚æ•°å…±äº«ï¼Œä½ç§©åˆ†è§£ï¼Œç½‘ç»œç»“æ„è®¾è®¡ï¼ŒçŸ¥è¯†è’¸é¦ç­‰ã€‚ALBERT ä¹Ÿæ²¡èƒ½é€ƒå‡ºè¿™ä¸€æ¡†æ¶ï¼Œå®ƒå…¶å®æ˜¯ä¸€ä¸ªç›¸å½“å·¥ç¨‹åŒ–çš„æ€æƒ³
 
Alberç›¸å¯¹äºåŸå§‹BERTæ¨¡å‹ä¸»è¦æœ‰ä¸‰ç‚¹æ”¹è¿›ï¼š
- embedding å±‚å‚æ•°å› å¼åˆ†è§£
- è·¨å±‚å‚æ•°å…±äº«
- å°† NSP ä»»åŠ¡æ”¹ä¸º SOP ä»»åŠ¡

ALBERTåœ¨BERT çš„åŸºç¡€ä¸Šæå‡ºäº†ä¸€äº›æ–°é¢–çš„æƒ³æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼š
 
1ã€**è·¨å±‚å‚æ•°å…±äº«**
 
BERT-largeæ¨¡å‹æœ‰24å±‚ï¼Œè€Œå®ƒçš„åŸºç¡€ç‰ˆæœ¬æœ‰12å±‚ã€‚éšç€å±‚æ•°çš„å¢åŠ ï¼Œå‚æ•°çš„æ•°é‡å‘ˆæŒ‡æ•°å¢é•¿ã€‚
 
![](https://pic4.zhimg.com/v2-da47ac3ebd17d165d957b4959294118f_b.jpg)
 
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒALBERTä½¿ç”¨äº†è·¨å±‚å‚æ•°å…±äº«çš„æ¦‚å¿µã€‚ä¸ºäº†è¯´æ˜è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹12å±‚çš„BERT-baseæ¨¡å‹çš„ä¾‹å­ã€‚æˆ‘ä»¬åªå­¦ä¹ ç¬¬ä¸€ä¸ªå—çš„å‚æ•°ï¼Œå¹¶åœ¨å‰©ä¸‹çš„11ä¸ªå±‚ä¸­é‡ç”¨è¯¥å—ï¼Œè€Œä¸æ˜¯ä¸º12ä¸ªå±‚ä¸­æ¯ä¸ªå±‚éƒ½å­¦ä¹ ä¸åŒçš„å‚æ•°ã€‚  
 
![](https://pic1.zhimg.com/v2-e5457046cbaeb14334cc8d9de72f128c_b.jpg)
 
æˆ‘ä»¬å¯ä»¥åªå…±äº«feed-forwardå±‚çš„å‚æ•°ï¼Œåªå…±äº«æ³¨æ„åŠ›å‚æ•°ï¼Œä¹Ÿå¯ä»¥å…±äº«æ•´ä¸ªå—çš„å‚æ•°ã€‚è®ºæ–‡å¯¹æ•´ä¸ªå—çš„å‚æ•°è¿›è¡Œäº†å…±äº«ã€‚  
 
ä¸BERT-baseçš„1.1äº¿ä¸ªå‚æ•°ç›¸æ¯”ï¼ŒALBERTæ¨¡å‹åªæœ‰3100ä¸‡ä¸ªå‚æ•°ï¼Œè€Œä½¿ç”¨ç›¸åŒçš„å±‚æ•°å’Œ768ä¸ªéšè—å•å…ƒã€‚å½“åµŒå…¥å°ºå¯¸ä¸º128æ—¶ï¼Œå¯¹ç²¾åº¦çš„å½±å“å¾ˆå°ã€‚ç²¾åº¦çš„ä¸»è¦ä¸‹é™æ˜¯ç”±äºfeed-forwardå±‚çš„å‚æ•°å…±äº«ã€‚å…±äº«æ³¨æ„åŠ›å‚æ•°çš„å½±å“æ˜¯æœ€å°çš„ã€‚
 
![](https://pic3.zhimg.com/v2-5e3a3fe7c409ca694d28ab4f68e16356_b.jpg)
 
è·¨å±‚å‚æ•°ç­–ç•¥å¯¹æ€§èƒ½çš„å½±å“
 
2ã€**å¥å­é¡ºåºé¢„æµ‹ (SOP)**
 
BERTå¼•å…¥äº†ä¸€ä¸ªå«åšâ€œ**ä¸‹ä¸€ä¸ªå¥å­é¢„æµ‹**â€çš„äºŒåˆ†ç±»æŸå¤±ã€‚è¿™æ˜¯ä¸“é—¨ä¸ºæé«˜ä½¿ç”¨å¥å­å¯¹ï¼Œå¦‚â€œè‡ªç„¶è¯­è¨€æ¨æ–­â€çš„ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½è€Œåˆ›å»ºçš„ã€‚åŸºæœ¬æµç¨‹ä¸ºï¼š
*   ä»è®­ç»ƒè¯­æ–™åº“ä¸­å–å‡ºä¸¤ä¸ªè¿ç»­çš„æ®µè½ä½œä¸ºæ­£æ ·æœ¬
*   ä»ä¸åŒçš„æ–‡æ¡£ä¸­éšæœºåˆ›å»ºä¸€å¯¹æ®µè½ä½œä¸ºè´Ÿæ ·æœ¬

![](https://pic2.zhimg.com/v2-90f2e0b33156326e46387959ce388c29_b.jpg)
 
åƒROBERTAå’ŒXLNETè¿™æ ·çš„è®ºæ–‡å·²ç»é˜æ˜äº†NSPçš„æ— æ•ˆæ€§ï¼Œå¹¶ä¸”å‘ç°å®ƒå¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“æ˜¯ä¸å¯é çš„ã€‚åœ¨å–æ¶ˆNSPä»»åŠ¡ä¹‹åï¼Œå¤šä¸ªä»»åŠ¡çš„æ€§èƒ½éƒ½å¾—åˆ°äº†æé«˜ã€‚  
 
å› æ­¤ï¼ŒALBERTæå‡ºäº†å¦ä¸€ä¸ªä»»åŠ¡**â€œå¥å­é¡ºåºé¢„æµ‹â€**ã€‚å…³é”®æ€æƒ³æ˜¯:
*   ä»åŒä¸€ä¸ªæ–‡æ¡£ä¸­å–ä¸¤ä¸ªè¿ç»­çš„æ®µè½ä½œä¸ºä¸€ä¸ªæ­£æ ·æœ¬
*   äº¤æ¢è¿™ä¸¤ä¸ªæ®µè½çš„é¡ºåºï¼Œå¹¶ä½¿ç”¨å®ƒä½œä¸ºä¸€ä¸ªè´Ÿæ ·æœ¬
 
![](https://pic3.zhimg.com/v2-c7b91c17a1b42283817b17a2d566a6d6_b.jpg)
 
è¿™ä½¿å¾—æ¨¡å‹èƒ½å­¦ä¹ åˆ°æ›´ç»†ç²’åº¦çš„å…³äºæ®µè½çº§çš„ä¸€è‡´æ€§çš„åŒºåˆ«ã€‚
 
ALBERTæ¨æµ‹NSPæ˜¯æ— æ•ˆçš„ï¼Œå› ä¸ºä¸æ©ç è¯­è¨€å»ºæ¨¡ç›¸æ¯”ï¼Œå®ƒå¹¶ä¸æ˜¯ä¸€é¡¹å›°éš¾çš„ä»»åŠ¡ã€‚åœ¨å•ä¸ªä»»åŠ¡ä¸­ï¼Œå®ƒæ··åˆäº†ä¸»é¢˜é¢„æµ‹å’Œè¿è´¯æ€§é¢„æµ‹ã€‚ä¸»é¢˜é¢„æµ‹éƒ¨åˆ†å¾ˆå®¹æ˜“å­¦ä¹ ï¼Œå› ä¸ºå®ƒä¸æ©ç è¯­è¨€å»ºæ¨¡çš„æŸå¤±æœ‰é‡å ã€‚å› æ­¤ï¼Œå³ä½¿NSPæ²¡æœ‰å­¦ä¹ è¿è´¯æ€§é¢„æµ‹ï¼Œå®ƒä¹Ÿä¼šç»™å‡ºæ›´é«˜çš„åˆ†æ•°ã€‚
 
SOPæé«˜äº†ä¸‹æ¸¸å¤šå¥ç¼–ç ä»»åŠ¡(SQUAD 1.1, 2.0, MNLI, SST-2, RACE)çš„æ€§èƒ½ã€‚
 
![](https://pic3.zhimg.com/v2-c9ce732b00678780cb2ae1fc9e219612_b.jpg)

åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨SOPä»»åŠ¡ä¸Šï¼Œä¸€ä¸ªç»è¿‡NSPè®­ç»ƒçš„æ¨¡å‹ç»™å‡ºçš„åˆ†æ•°åªæ¯”éšæœºåŸºçº¿ç•¥å¥½ä¸€ç‚¹ï¼Œä½†æ˜¯ç»è¿‡SOPè®­ç»ƒçš„æ¨¡å‹å¯ä»¥éå¸¸æœ‰æ•ˆåœ°è§£å†³NSPä»»åŠ¡ã€‚è¿™å°±è¯æ˜SOPèƒ½å¸¦æ¥æ›´å¥½çš„å­¦ä¹ è¡¨ç°ã€‚
 
3ã€**åµŒå…¥å‚æ•°åˆ†è§£**
 
åœ¨BERTä¸­ï¼Œä½¿ç”¨çš„embeddings(word piece embeddings)å¤§å°è¢«é“¾æ¥åˆ°transformerå—çš„éšè—å±‚å¤§å°ã€‚Word piece embeddingsä½¿ç”¨äº†å¤§å°ä¸º30,000çš„è¯æ±‡è¡¨çš„ç‹¬çƒ­ç¼–ç è¡¨ç¤ºã€‚è¿™äº›è¢«ç›´æ¥æŠ•å°„åˆ°éšè—å±‚çš„éšè—ç©ºé—´ã€‚
 
å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤§å°ä¸º30Kçš„è¯æ±‡è¡¨ï¼Œå¤§å°ä¸ºE=768çš„word-piece embeddingå’Œå¤§å°ä¸ºH=768çš„éšå«å±‚ã€‚å¦‚æœæˆ‘ä»¬å¢åŠ äº†å—ä¸­çš„éšè—å•å…ƒå°ºå¯¸ï¼Œé‚£ä¹ˆæˆ‘ä»¬è¿˜éœ€è¦ä¸ºæ¯ä¸ªåµŒå…¥æ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ã€‚è¿™ä¸ªé—®é¢˜åœ¨XLNETå’ŒROBERTAä¸­ä¹Ÿå¾ˆæ™®éã€‚

![](https://pic4.zhimg.com/v2-262f2f051d69dda14c1e437524f84e43_b.jpg)
 
ALBERTé€šè¿‡å°†å¤§çš„è¯æ±‡è¡¨åµŒå…¥çŸ©é˜µåˆ†è§£æˆä¸¤ä¸ªå°çš„çŸ©é˜µæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™å°†éšè—å±‚çš„å¤§å°ä¸è¯æ±‡è¡¨åµŒå…¥çš„å¤§å°åˆ†å¼€ã€‚è¿™å…è®¸æˆ‘ä»¬åœ¨ä¸æ˜¾è‘—å¢åŠ è¯æ±‡è¡¨åµŒå…¥çš„å‚æ•°å¤§å°çš„æƒ…å†µä¸‹å¢åŠ éšè—çš„å¤§å°ã€‚
 
![](https://pic1.zhimg.com/v2-e9fb494e0067ae3770d09c7299e6aa94_b.jpg)
 
æˆ‘ä»¬å°†ç‹¬çƒ­ç¼–ç å‘é‡æŠ•å½±åˆ°E=100çš„ä½ç»´åµŒå…¥ç©ºé—´ï¼Œç„¶åå°†è¿™ä¸ªåµŒå…¥ç©ºé—´æŠ•å½±åˆ°éšå«å±‚ç©ºé—´H=768ã€‚
 
### ç»“æœ
 
*   æ¯”BERT-largeæ¨¡å‹ç¼©å°äº†18xçš„å‚æ•°
*   è®­ç»ƒåŠ é€Ÿ1.7x
*   åœ¨GLUE, RACEå’ŒSQUADå¾—åˆ°SOTAç»“æœï¼š
  *   RACEï¼š89.4%\[æå‡45.3%\]
  *   GLUE Benchmarkï¼š89.4
  *   SQUAD2.0 f1 scoreï¼š92.2

ALBERTä¸BERTæ¨¡å‹ä¹‹é—´å‚æ•°æƒ…å†µ

![](https://pic3.zhimg.com/80/v2-7f6261989fc1b9b8d2ce05d4249911ce_720w.jpg)

åœ¨benchmarkä¸Šçš„æ•ˆæœ
![](https://pic1.zhimg.com/80/v2-5e320cd88fbc16d4038eebfaf586a9f4_720w.jpg)
![](https://pic4.zhimg.com/80/v2-969d2eefa07339b637d6333817d13a0f_720w.jpg)


**æ€»ç»“**
 
ALBERTæ ‡å¿—ç€æ„å»ºè¯­è¨€æ¨¡å‹çš„é‡è¦ä¸€æ­¥ï¼Œè¯¥æ¨¡å‹ä¸ä»…è¾¾åˆ°äº†SOTAï¼Œè€Œä¸”å¯¹ç°å®ä¸–ç•Œçš„åº”ç”¨ä¹Ÿæ˜¯å¯è¡Œçš„ã€‚
 
- è‹±æ–‡åŸæ–‡ï¼š[https://amitness.com/2020/02/al](https://amitness.com/2020/02/albert-visual-summary/)
- albertçš„[ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹](https://github.com/brightmart/albert_zh)


### ALBERT å¤šæ ‡ç­¾å®è·µ

ã€2023-11-15ã€‘[å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±» ALBERT](https://zhuanlan.zhihu.com/p/164873441)

æ ‡ç­¾ä¸¤ä¸¤ä¹‹é—´çš„å…³ç³»ï¼šæœ‰çš„ independentï¼Œæœ‰çš„ non independentã€‚

å‡è®¾ä¸ªäººçˆ±å¥½çš„é›†åˆä¸€å…±æœ‰6ä¸ªå…ƒç´ ï¼šè¿åŠ¨ã€æ—…æ¸¸ã€è¯»ä¹¦ã€å·¥ä½œã€ç¡è§‰ã€ç¾é£Ÿã€‚
- ä¸€ä¸ªäººçš„çˆ±å¥½æœ‰è¿™å…¶ä¸­çš„ä¸€ä¸ªæˆ–è€…å¤šä¸ª â€”â€” å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡
- ![](https://pic3.zhimg.com/80/v2-d0e019b7596cf3c9dc237861c2c3f5ae_1440w.webp)

å¤‡æ³¨
- å¸¸è§„æ–‡æœ¬åˆ†ç±»ä¸­çš„äº¤å‰ç†µä¸º `tf.nn.softmax_cross_entropy_with_logits`ï¼›
- å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œäº¤å‰ç†µåˆ™ä¸º `tf.nn.sigmoid_cross_entropy_with_logits` 

åŸå› ï¼š
- `tf.nn.sigmoid_cross_entropy_with_logits`æµ‹é‡ç¦»æ•£åˆ†ç±»ä»»åŠ¡ä¸­çš„æ¦‚ç‡è¯¯å·®ï¼Œå…¶ä¸­æ¯ä¸ªç±»æ˜¯**ç‹¬ç«‹è€Œä¸äº’æ–¥**ã€‚è¿™é€‚ç”¨äº**å¤šæ ‡ç­¾**åˆ†ç±»é—®é¢˜ã€‚
- `tf.nn.softmax_cross_entropy_with_logits`æµ‹é‡ç¦»æ•£åˆ†ç±»ä»»åŠ¡ä¸­çš„æ¦‚ç‡è¯¯å·®ï¼Œå…¶ä¸­ç±»ä¹‹é—´æ˜¯**ç‹¬ç«‹ä¸”äº’æ–¥**ï¼ˆæ¯ä¸ªæ¡ç›®æ°å¥½åœ¨ä¸€ä¸ªç±»ä¸­ï¼‰ã€‚è¿™é€‚ç”¨å¤šåˆ†ç±»é—®é¢˜ã€‚

ä»£ç 
- [classifier_multi_label](https://github.com/hellonlp/classifier-multi-label/tree/master/classifier_multi_label)



### ALBERT å¾®è°ƒ


#### huggingface finetune

åŸºäºhuggingfaceçš„finetune
- [huggingface transformers-ä½¿ç”¨Albertè¿›è¡Œä¸­æ–‡æ–‡æœ¬åˆ†ç±»](https://blog.csdn.net/u013230189/article/details/108836511)
- [nlp08_huggingface_transformers_albert.ipynb](https://github.com/chongzicbo/nlp-ml-dl-notes/blob/master/code/textclassification/nlp08_huggingface_transformers_albert.ipynb)

```py
import torch
from transformers import BertTokenizer,BertModel,BertConfig
import numpy as np
from torch.utils import data
from sklearn.model_selection import train_test_split
import pandas as pd
# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
pretrained = 'voidful/albert_chinese_small' #ä½¿ç”¨smallç‰ˆæœ¬Albert
tokenizer = BertTokenizer.from_pretrained(pretrained)
model = BertModel.from_pretrained(pretrained)
config = BertConfig.from_pretrained(pretrained)

inputtext = "ä»Šå¤©å¿ƒæƒ…æƒ…å¾ˆå¥½å•Šï¼Œä¹°äº†å¾ˆå¤šä¸œè¥¿ï¼Œæˆ‘ç‰¹åˆ«å–œæ¬¢ï¼Œç»ˆäºæœ‰äº†è‡ªå·±å–œæ¬¢çš„ç”µå­äº§å“ï¼Œè¿™æ¬¡æ€»ç®—å¯ä»¥å¥½å¥½å­¦ä¹ äº†"
tokenized_text = tokenizer.encode(inputtext)
input_ids = torch.tensor(tokenized_text).view(-1,len(tokenized_text))
outputs = model(input_ids)

# è¾“å‡ºå­—å‘é‡è¡¨ç¤ºå’Œå¥å‘é‡
print(outputs[0].shape,outputs[1].shape)
```

#### Pytorchç‰ˆæœ¬

Pytorchç‰ˆæœ¬å®ç° [albert_pytorch](https://github.com/lonePatient/albert_pytorch/blob/master/README_zh.md)

```sh
pip install boto3
```

ç›®å½•ç»“æ„

```sh
callback
convert_albert_tf_checkpoint_to_pytorch.py # æ¨¡å‹æ ¼å¼è½¬æ¢ tensorflow -> pytorch
dataset # æ•°æ®é›†ç›®å½•
__init__.py
LICENSE
metrics # è¯„æµ‹å·¥å…·
model # æ¨¡å‹æºç 
outputs # æ¨¡å‹ä¿å­˜
prepare_lm_data_mask.py # æ•°æ®å¤„ç†ï¼šMASK
prepare_lm_data_ngram.py # æ•°æ®å¤„ç†ï¼šn-gram
prev_trained_model # é¢„è®­ç»ƒæ¨¡å‹ç›®å½•
processors # å¤„ç† glue.py GLUEæ•°æ®é›†é€‚é…å·¥å…·
README.md
README_zh.md
run_classifier.py # ä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡
run_pretraining.py # é¢„è®­ç»ƒä»»åŠ¡
scripts # å¾®è°ƒè„šæœ¬
tools
```

##### é¢„è®­ç»ƒ

æ•°æ®å‡†å¤‡: n-gram
- å°†æ–‡æœ¬æ•°æ®è½¬åŒ–ä¸ºä¸€è¡Œä¸€å¥æ ¼å¼ï¼Œå¹¶ä¸”ä¸åŒdocumentä¹‹é—´ä½¿ç”¨`\n`åˆ†å‰²
- äº§ç”Ÿn-gram maskingæ•°æ®é›†

```sh
python prepare_lm_data_ngram.py \
    --data_dir=dataset/ \
    --vocab_path=vocab.txt \
    --data_name=albert \
    --max_ngram=3 \
    --do_data
```

å¯åŠ¨é¢„è®­ç»ƒ

```sh
python run_pretraining.py \
    --data_dir=dataset/ \
    --vocab_path=configs/vocab.txt \
    --data_name=albert \
    --config_path=configs/albert_config_base.json \
    --output_dir=outputs/ \
    --data_name=albert \
    --share_type=all
```

##### åŒ¹é…

æµ‹è¯•ç»“æœ

é—®é¢˜åŒ¹é…è¯­ä»»åŠ¡ï¼šLCQMC(Sentence Pair Matching)

| æ¨¡å‹ | å¼€å‘é›†(Dev) | æµ‹è¯•é›†(Test) |
| --- | --- | --- |
| albert_base(tf) | 86.4 | 86.3 | 
| albert_base(pytorch) | 87.4 | 86.4 |
| albert_tiny | 85.1| 85.3 |


##### åˆ†ç±»

æ•°æ®å‡†å¤‡
- ä¸‹è½½ sts-2 æ•°æ®é›†ï¼Œåˆ°ç›®å½• dataset/sts-2

```sh
# question \t sentiment
has all the depth of a wading pool .    0
a movie with a real anarchic flair .    1
```

å¯åŠ¨è®­ç»ƒ
- `sh scripts/run_classifier_sst2.sh`

è®­ç»ƒå®Œæ¯•
- V100 32G GPUä¸Šï¼Œçº¦20minå®Œæˆè®­ç»ƒ
- æ¨¡å‹ä¿å­˜ç›®å½• `outputs/sst-2_output/albert`

#### tf finetune

ã€2021-2-17ã€‘[Albertå¾®è°ƒå®æˆ˜](https://zhuanlan.zhihu.com/p/351105268)

[TextClassifier_Transformer](https://github.com/Vincent131499/TextClassifier_Transformer/tree/master/dat)
- åŸºäºè°·æ­Œå¼€æºçš„BERTç¼–å†™çš„æ–‡æœ¬åˆ†ç±»å™¨ï¼ˆåŸºäºå¾®è°ƒæ–¹å¼ï¼‰ï¼Œå¯è‡ªç”±åŠ è½½NLPé¢†åŸŸçŸ¥åçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹`BERT`ã€ `Roberta`ã€`ALBert`åŠå…¶wwmç‰ˆæœ¬ï¼ŒåŒæ—¶é€‚é…ERNIE1.0.

è¯¥é¡¹ç›®æ”¯æŒä¸¤ç§é¢„æµ‹æ–¹å¼ï¼š
- ï¼ˆ1ï¼‰çº¿ä¸‹å®æ—¶é¢„æµ‹
- ï¼ˆ2ï¼‰æœåŠ¡ç«¯å®æ—¶é¢„æµ‹

##### è¯­æ–™å‡†å¤‡

å®Œæ•´ç‰ˆ [TextClassifier_Transformer](https://github.com/Vincent131499/TextClassifier_Transformer/tree/master/dat)
- train.tsv
- test.tsv
- dev.tsv


æ•°æ®æ ·ä¾‹

```
label txt
1 æ‰‹æœºå¾ˆå¥½ï¼Œæ¼‚äº®æ—¶å°š
1 åšå·¥ä¸€èˆ¬
```


##### æ¨¡å‹

```sh
# ä¸‹è½½æºç 
git clone https://github.com/google-research/ALBERT
# ä¸‹è½½æ¨¡å‹ base
wget https://storage.googleapis.com/albert_models/albert_base_zh.tar.gz
# è§£å‹æ¨¡å‹
tar xvf albert_base_zh.tar.gz
```

##### ä»£ç ä¿®æ”¹

æ‰“å¼€`classifier_utils.py`æ–‡ä»¶ï¼Œæ·»åŠ ä»£ç 

```py
class SentimentProcessor(DataProcessor):
  """Processor for the sentiment data set (GLUE version)."""

  def get_train_examples(self, data_dir):
    """See base class."""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

  def get_dev_examples(self, data_dir):
    """See base class."""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")

  def get_test_examples(self, data_dir):
    """See base class."""
    return self._create_examples(
        self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

  def get_labels(self):
    """See base class."""
    return ["-1", "0", "1"]

  def _create_examples(self, lines, set_type):
    """Creates examples for the training and dev sets."""
    examples = []
    for (i, line) in enumerate(lines):
      # Only the test set has a header
      if i == 0:
          continue
      guid = "%s-%s" % (set_type, i)
      text_a = tokenization.convert_to_unicode(line[1])
      if set_type == "test":
        label = "0"
      else:
        label = tokenization.convert_to_unicode(line[0])
      examples.append(
          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
    return examples
```

ç„¶åæ‰“å¼€`run_classifier.py`æ–‡ä»¶ï¼Œæ·»åŠ ä¸€ä¸ªprocessors

```py
processors = {
      "cola": classifier_utils.ColaProcessor,
      "mnli": classifier_utils.MnliProcessor,
      "mismnli": classifier_utils.MisMnliProcessor,
      "mrpc": classifier_utils.MrpcProcessor,
      "rte": classifier_utils.RteProcessor,
      "sst-2": classifier_utils.Sst2Processor,
      "sts-b": classifier_utils.StsbProcessor,
      "qqp": classifier_utils.QqpProcessor,
      "qnli": classifier_utils.QnliProcessor,
      "wnli": classifier_utils.WnliProcessor,
      "entiment": classifier_utils.SentimentProcessor,
  }
```

Albert ä»£ç ä¸Bertç»“æ„å¤§éƒ¨åˆ†æ˜¯ç›¸ä¼¼çš„ï¼Œä½†æ˜¯ç›®å½•ç»“æ„ä¸ä¸€æ ·ï¼ŒæŠŠè¿è¡Œè„šæœ¬å†™åœ¨å·¥ç¨‹çš„å¤–é¢

```sh
BERT_DIR=`pwd`
export BERT_BASE_DIR=${BERT_DIR}/albert/albert_base
export DATA_DIR=${BERT_DIR}/albert/data

python albert/run_classifier.py \
  --task_name=sentiment \
  --do_train=true \
  --do_eval=true \
  --spm_model_file="" \
  --do_predict=false \
  --data_dir=$DATA_DIR \
  --vocab_file=$BERT_BASE_DIR/vocab_chinese.txt \
  --albert_config_file=$BERT_BASE_DIR/albert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/model.ckpt-best \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=albert/output
```

ã€2023-11-14ã€‘å®æµ‹
- run_classifier.py ç›¸å…³çš„å‡ ä¸ªæ–‡ä»¶é‡Œimportéœ€è¦ä¿®æ”¹

```py
from albert import modeling.py
# æ”¹ä¸º
import modeling.py
```

Mac ä¸‹æ‰§è¡Œ[æŠ¥é”™](https://github.com/conda/conda/issues/9678)

```
Illegal instruction: 4  python albert/run_classifier.py
```

åŸå› 
>numpy 1.18 was compiled with flags that enable certain CPU features (such as AVX) that are not present on older CPUs.

Linux tensorflow 2 ä¸‹ï¼Œæ‰§è¡Œä»ç„¶æŠ¥é”™
- ï¼ˆ1ï¼‰
  - contrib åœ¨TensorFlow 2ä¸­ä¸å†æ”¯æŒ
- ï¼ˆ2ï¼‰tokenization.py 27è¡Œ,tensorflow_hubä¸å†æ”¯æŒ
    

modeling.py æ–‡ä»¶

```py
# from tensorflow.contrib import layers as contrib_layers # æ³¨é‡Šæ‰
# 429 è¡Œä¿®æ”¹
def layer_norm(input_tensor, name=None):
  """Run layer normalization on the last dimension of the tensor."""
  return tf.keras.layers.layer_norm(
      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)
  #return contrib_layers.layer_norm(
  #    inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)

```

```sh
import tensorflow_hub as hub
ModuleNotFoundError: No module named 'tensorflow_hub'
```


è®­ç»ƒç»“æœä¿å­˜åœ¨outputæ–‡ä»¶å¤¹ä¸­ï¼ŒæŸ¥çœ‹è®­ç»ƒç»“æœå¦‚ä¸‹

```sh
eval_accuracy = 0.8332304
eval_loss = 0.4143196
global_step = 1000
loss = 0.41322827
best = 0.609635591506958
```

## XLNet

XLNet å¯¹ Bert åšäº†è¾ƒå¤§çš„æ”¹åŠ¨ï¼ŒäºŒè€…åœ¨**æ¨¡å‹ç»“æ„**å’Œ**è®­ç»ƒæ–¹å¼**ä¸Šéƒ½æœ‰ä¸å°çš„å·®å¼‚ã€‚
- Bert çš„ MLM åœ¨é¢„è®­ç»ƒæ—¶æœ‰ MASK æ ‡ç­¾ï¼Œä½†åœ¨ä½¿ç”¨æ—¶å´æ²¡æœ‰ï¼Œå¯¼è‡´è®­ç»ƒå’Œä½¿ç”¨æ—¶å‡ºç°ä¸ä¸€è‡´ï¼›å¹¶ä¸” MLM ä¸å±äº Autoregressive LMï¼Œä¸èƒ½åšç”Ÿæˆç±»ä»»åŠ¡ã€‚
- XLNet é‡‡ç”¨ PML(Permutation Language Model) é¿å…äº† MASK æ ‡ç­¾çš„ä½¿ç”¨ï¼Œä¸”å±äº Autoregressive LMï¼Œå¯ä»¥åšç”Ÿæˆä»»åŠ¡ã€‚

Bert ä½¿ç”¨çš„ Transformer ç»“æ„å¯¹æ–‡æœ¬çš„**é•¿åº¦æœ‰é™åˆ¶**ï¼Œä¸ºæ›´å¥½åœ°å¤„ç†é•¿æ–‡æœ¬ï¼ŒXLNet é‡‡ç”¨å‡çº§ç‰ˆçš„ Transformer-XLã€‚

è¦ç‚¹
- å…¨æ’åˆ—è¯­è¨€æ¨¡å‹
- transformer-XL
- è·Ÿå¤šçš„æ•°æ®

Yangç­‰äºº(2019å¹´)è®¤ä¸ºï¼Œç°æœ‰çš„åŸºäºè‡ªç¼–ç çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(ä¾‹å¦‚BERT)ä¼šé­å—**é¢„è®­ç»ƒ**å’Œ**å¾®è°ƒ**é˜¶æ®µçš„å·®å¼‚ï¼Œå› ä¸ºmaskingç¬¦å·\[MASK]æ°¸è¿œä¸ä¼šå‡ºç°åœ¨å¾®è°ƒé˜¶æ®µã€‚ 

ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œä»–ä»¬æå‡ºäº†XLNetï¼Œå®ƒåŸºäºTransformer-XL(Daiç­‰äººï¼Œ2019)ã€‚ XLNetä¸»è¦é€šè¿‡ä¸¤ç§æ–¹å¼è¿›è¡Œä¿®æ”¹ã€‚ 
- é¦–å…ˆæ˜¯æ‰€æœ‰æ’åˆ—ä¸Šçš„è¾“å…¥**å› å¼åˆ†è§£**çš„æœ€å¤§åŒ–æœŸæœ›ä¼¼ç„¶ï¼Œåœ¨æ­¤å°†å®ƒä»¬ç§°ä¸º**æ’åˆ—è¯­è¨€æ¨¡å‹**(PLM)ã€‚ 
- å…¶æ¬¡æ˜¯å°†**è‡ªç¼–ç **è¯­è¨€æ¨¡å‹æ›´æ”¹ä¸º**è‡ªå›å½’**æ¨¡å‹ï¼Œè¿™ä¸ä¼ ç»Ÿçš„ç»Ÿè®¡è¯­è¨€æ¨¡å‹ç›¸ä¼¼

## ERNIEï¼ˆèåˆçŸ¥è¯†ï¼‰

æ€è·¯ï¼š
- ç™¾åº¦ç‰ˆï¼š**å…¨è¯**mask
- æ¸…åç‰ˆï¼š**çŸ¥è¯†å›¾è°±**èå…¥

ERNIE(é€šè¿‡çŸ¥è¯†æ•´åˆå¢å¼ºè¡¨ç¤º)(Sunç­‰äººï¼Œ2019a)æ—¨åœ¨ä¼˜åŒ–BERTçš„maskingè¿‡ç¨‹ï¼Œå…¶ä¸­åŒ…æ‹¬**å®ä½“**çº§maskingå’Œ**çŸ­è¯­**çº§maskingã€‚ ä¸åœ¨è¾“å…¥ä¸­é€‰æ‹©éšæœºå•è¯ä¸åŒï¼Œå®ä½“çº§maskå°†maské€šå¸¸ç”±å¤šä¸ªå•è¯ç»„æˆçš„å‘½åå®ä½“ã€‚çŸ­è¯­çº§maskæ˜¯maskè¿ç»­çš„å•è¯ï¼Œç±»ä¼¼äºN-gram maskç­–ç•¥

## ã€2019-12-5ã€‘NeZha æŒªå’ï¼ˆåä¸ºï¼‰

[åä¸ºå¼€æºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€Œå“ªå’ã€ï¼šç¼–ç ã€æ©ç å‡çº§ï¼Œæå‡å¤šé¡¹ä¸­æ–‡ NLP ä»»åŠ¡æ€§èƒ½](https://www.leiphone.com/category/yanxishe/YmSMHZUOCekn9Cyr.html)

ã€2019-12-5ã€‘åä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤è¯­éŸ³è¯­ä¹‰å›¢é˜Ÿä¸æµ·æ€ã€äº‘BUç­‰å›¢é˜Ÿåˆä½œï¼Œå…±åŒç ”ç©¶å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„è®­ç»ƒæŠ€æœ¯ï¼Œå‘å¸ƒäº†è‡ªå·±çš„ä¸­æ–‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹NEZHA(NEural ContextualiZed Representation for CHinese LAnguage Understandingï¼Œä¸­æ–‡ï¼šå“ªå’)ã€‚

- NEZHA [è®ºæ–‡åœ°å€](https://arxiv.org/pdf/1909.00204.pdf)
- å…³äºçŸ¥è¯†è’¸é¦æ¨¡å‹ TinyBERT è¯¦ç»†è§£è¯»ï¼Œå¯å‚è€ƒ[å¾€æœŸå†…å®¹](https://mp.weixin.qq.com/s/f2vxlhaGW1wnu8UYrvh-tA)
- Github [å¼€æºåœ°å€](https://github.com/huawei-noah/Pretrained-Language-Model)ï¼ˆåŒ…å« NEZHA ä¸ TinyBERT )  

NEZHAæ˜¯åŸºäºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹BERTçš„æ”¹è¿›æ¨¡å‹ï¼ŒBERTé€šè¿‡ä½¿ç”¨å¤§é‡æ— ç›‘ç£æ–‡æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼Œå…¶åŒ…å«ä¸¤ä¸ªé¢„è®­ç»ƒä»»åŠ¡ï¼šMasked Language Modelingï¼ˆMLMï¼‰å’Œ Next Sentence Prediction ï¼ˆNSPï¼‰ï¼Œåˆ†åˆ«é¢„æµ‹å¥å­é‡Œè¢«Maskçš„å­—ï¼ˆåœ¨æ„é€ è®­ç»ƒæ•°æ®æ—¶ï¼Œå¥å­é‡Œçš„éƒ¨åˆ†å­—è¢«Maskï¼‰å’Œåˆ¤æ–­è®­ç»ƒå¥å¯¹é‡Œé¢æ˜¯ä¸æ˜¯çœŸå®çš„ä¸Šä¸‹å¥ã€‚

ä¸‰å¤´å…­è‡‚ NEZHAï¼ˆå“ªå’ï¼‰
- **å‡½æ•°å¼**ç›¸å¯¹ä½ç½®ç¼–ç 
- å…¨è¯è¦†ç›–çš„å®ç°

### å‡½æ•°å¼ç›¸å¯¹ä½ç½®ç¼–ç 

ä½ç½®ç¼–ç æœ‰**å‡½æ•°å¼**å’Œ**å‚æ•°å¼**ä¸¤ç§
- å‡½æ•°å¼é€šè¿‡å®šä¹‰å‡½æ•°ç›´æ¥è®¡ç®—å°±å¯ä»¥äº†ã€‚
- å‚æ•°å¼ä¸­ä½ç½®ç¼–ç æ¶‰åŠä¸¤ä¸ªæ¦‚å¿µï¼Œä¸€ä¸ªæ˜¯è·ç¦»ï¼›äºŒæ˜¯ç»´åº¦ã€‚å…¶ä¸­ï¼ŒWord Embedding ä¸€èˆ¬æœ‰å‡ ç™¾ç»´ï¼Œæ¯ä¸€ç»´å„æœ‰ä¸€ä¸ªå€¼ï¼Œä¸€ä¸ªä½ç½®ç¼–ç çš„å€¼æ­£æ˜¯é€šè¿‡ä½ç½®å’Œç»´åº¦ä¸¤ä¸ªå‚æ•°æ¥ç¡®å®šã€‚

NEZHA é¢„è®­ç»ƒæ¨¡å‹åˆ™é‡‡ç”¨äº†**å‡½æ•°å¼**ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œå…¶è¾“å‡ºä¸æ³¨æ„åŠ›å¾—åˆ†çš„è®¡ç®—æ¶‰åŠåˆ°ä»–ä»¬ç›¸å¯¹ä½ç½®çš„æ­£å¼¦å‡½æ•°ï¼Œè¿™ä¸€çµæ„Ÿæ­£æ˜¯æ¥æºäº Transformer çš„ç»å¯¹ä½ç½®ç¼–ç ï¼Œè€Œç›¸å¯¹ä½ç½®ç¼–ç åˆ™è§£å†³äº†åœ¨ Transformer ä¸­ï¼Œæ¯ä¸ªè¯ä¹‹é—´å› ä¸ºäº’ä¸çŸ¥é“ç›¸éš”çš„è·ç¦»å¼•å‘çš„ä¸€ç³»åˆ—èµ„æºå ç”¨é—®é¢˜ã€‚

Transformer æœ€æ—©åªè€ƒè™‘äº†**ç»å¯¹ä½ç½®ç¼–ç **ï¼Œè€Œä¸”æ˜¯å‡½æ•°å¼çš„ï¼›åæ¥ BERT çš„æå‡ºå°±ä½¿ç”¨äº†å‚æ•°å¼ï¼Œè€Œå‚æ•°å¼è®­ç»ƒåˆ™ä¼šå—æ”¶åˆ°å¥å­é•¿åº¦çš„å½±å“ï¼ŒBERT èµ·åˆè®­ç»ƒçš„å¥å­æœ€é•¿ä¸º 512ï¼Œå¦‚æœåªè®­ç»ƒåˆ° 128 é•¿åº¦çš„å¥å­ï¼Œåœ¨ 128~520 ä¹‹é—´çš„ä½ç½®å‚æ•°å°±æ— æ³•è·å¾—ï¼Œæ‰€ä»¥å¿…é¡»è¦è®­ç»ƒæ›´é•¿çš„è¯­æ–™æ¥ç¡®å®šè¿™ä¸€éƒ¨åˆ†çš„å‚æ•°ã€‚

è€Œåœ¨ NEZHA æ¨¡å‹ä¸­ï¼Œè·ç¦»å’Œç»´åº¦éƒ½æ˜¯ç”±æ­£å¼¦å‡½æ•°å¯¼å‡ºçš„ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´æ˜¯å›ºå®šçš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä½ç½®ç¼–ç çš„æ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªæ­£å¼¦ï¼Œä¸åŒç»´åº¦çš„æ­£å¼¦å‡½æ•°å…·æœ‰ä¸åŒçš„æ³¢é•¿ï¼Œè€Œé€‰æ‹©å›ºå®šæ­£å¼¦å‡½æ•°ï¼Œåˆ™å¯ä»¥ä½¿è¯¥æ¨¡å‹å…·æœ‰æ›´å¼ºçš„æ‰©å±•æ€§ï¼›å³å½“å®ƒé‡åˆ°æ¯”è®­ç»ƒä¸­åºåˆ—é•¿åº¦æ›´é•¿çš„åºåˆ—æ—¶ï¼Œä¾ç„¶å¯ä»¥å‘æŒ¥ä½œç”¨ã€‚

### å…¨è¯è¦†ç›–

ç°åœ¨çš„ç¥ç»ç½‘ç»œæ¨¡å‹æ— è®ºæ˜¯åœ¨è¯­è¨€æ¨¡å‹è¿˜æ˜¯æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œéƒ½ä¼šç”¨åˆ°ä¸€ä¸ªè¯è¡¨ï¼›è€Œåœ¨ Softmax æ—¶ï¼Œæ¯ä¸ªè¯éƒ½è¦å°è¯•æ¯”è¾ƒä¸€ä¸‹ã€‚æ¯æ¬¡è¿ç®—æ—¶ï¼Œæ‰€æœ‰è¯è¦éƒ½åœ¨è¯è¡¨ä¸­å¯¹æ¯”ä¸€éï¼Œå¾€å¾€ä¸€ä¸ªè¯è¡¨ä¼šåŒ…å«å‡ ä¸‡ä¸ªè¯ï¼Œè€Œæœºå™¨ç¿»è¯‘åˆ™ç»å¸¸è¾¾åˆ°å…­ä¸ƒä¸‡ä¸ªè¯ï¼Œå› æ­¤ï¼Œè¯è¡¨æ˜¯è¯­è¨€æ¨¡å‹è¿ç®—ä¸­è¾ƒå¤§çš„ç“¶é¢ˆã€‚

è€Œ NEZHA é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™é‡‡ç”¨äº†å…¨è¯è¦†ç›–ï¼ˆWWMï¼‰ç­–ç•¥ï¼Œå½“ä¸€ä¸ªæ±‰å­—è¢«è¦†ç›–æ—¶ï¼Œå±äºåŒä¸€ä¸ªæ±‰å­—çš„å…¶ä»–æ±‰å­—éƒ½è¢«ä¸€èµ·è¦†ç›–ã€‚è¯¥ç­–ç•¥è¢«è¯æ˜æ¯” BERT ä¸­çš„éšæœºè¦†ç›–è®­ç»ƒï¼ˆå³æ¯ä¸ªç¬¦å·æˆ–æ±‰å­—éƒ½è¢«éšæœºå±è”½ï¼‰æ›´æœ‰æ•ˆã€‚
- ![](https://static.leiphone.com/uploads/new/images/20191205/5de8d5448fe36.jpg?imageView2/2/w/740)

### æ··åˆç²¾åº¦è®­ç»ƒåŠ LAMB ä¼˜åŒ–å™¨

åœ¨ NEZHA æ¨¡å‹çš„é¢„è®­ç»ƒä¸­ï¼Œç ”ç©¶è€…é‡‡ç”¨äº†æ··åˆç²¾åº¦è®­ç»ƒæŠ€æœ¯ã€‚è¯¥æŠ€æœ¯å¯ä»¥ä½¿è®­ç»ƒé€Ÿåº¦æé«˜ 2-3 å€ï¼ŒåŒæ—¶ä¹Ÿå‡å°‘äº†æ¨¡å‹çš„ç©ºé—´æ¶ˆè€—ï¼Œä»è€Œå¯ä»¥åˆ©ç”¨è¾ƒå¤§çš„æ‰¹é‡ã€‚

ä¼ ç»Ÿçš„æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒä½¿ç”¨ FP32ï¼ˆå³å•ç²¾åº¦æµ®ç‚¹æ ¼å¼ï¼‰æ¥è¡¨ç¤ºè®­ç»ƒä¸­æ¶‰åŠçš„æ‰€æœ‰å˜é‡ï¼ˆåŒ…æ‹¬æ¨¡å‹å‚æ•°å’Œæ¢¯åº¦ï¼‰ï¼›è€Œæ··åˆç²¾åº¦è®­ç»ƒåœ¨è®­ç»ƒä¸­é‡‡ç”¨äº†å¤šç²¾åº¦ã€‚å…·ä½“è€Œè¨€ï¼Œå®ƒé‡ç‚¹ä¿è¯æ¨¡å‹ä¸­æƒé‡çš„å•ç²¾åº¦å‰¯æœ¬ï¼ˆç§°ä¸ºä¸»æƒé‡ï¼‰ï¼Œå³åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œå°†ä¸»æƒå€¼èˆå…¥ FP16ï¼ˆå³åŠç²¾åº¦æµ®ç‚¹æ ¼å¼ï¼‰ï¼Œå¹¶ä½¿ç”¨ FP16 æ ¼å¼å­˜å‚¨çš„æƒå€¼ã€æ¿€æ´»å’Œæ¢¯åº¦æ‰§è¡Œå‘å‰å’Œå‘åä¼ é€’ï¼›æœ€åå°†æ¢¯åº¦è½¬æ¢ä¸º FP32 æ ¼å¼ï¼Œå¹¶ä½¿ç”¨ FP32 æ¢¯åº¦æ›´æ–°ä¸»æƒé‡ã€‚



## ã€2020-9-15ã€‘MacBERT â€”â€” RoBERTaæ”¹è¿›


### MacBERT ä»‹ç»

MacBERT å…¨ç§°ï¼šMLM as correction BERTï¼Œåœ¨å¤šä¸ªæ–¹é¢å¯¹RoBERTaè¿›è¡Œäº†æ”¹è¿›ï¼Œå°¤å…¶æ˜¯é‡‡ç”¨ MLM ä½œä¸ºæ ¡æ­£(Mac)çš„maskedç­–ç•¥ã€‚ç”¨**ç›¸ä¼¼å•è¯**maskï¼Œå‡è½»äº†é¢„è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µä¸¤è€…ä¹‹é—´çš„å·®è·ï¼Œè¿™å·²è¢«è¯æ˜å¯¹ä¸‹æ¸¸ä»»åŠ¡æ˜¯æœ‰æ•ˆçš„
- NSPä»»åŠ¡åŒ`ALBERT`çš„sentence-order predicionï¼ˆ`SOP`ï¼‰ä»»åŠ¡ï¼Œé¢„æµ‹è¿™ä¸¤ä¸ªå¥å­å¯¹æ˜¯æ­£åºè¿˜æ˜¯é€†åºã€‚
- MacBERTä¸»è¦æ˜¯ä¿®æ”¹BERT `MLM`ä»»åŠ¡
  - å¼•å…¥**çº é”™å‹**æ©ç è¯­è¨€æ¨¡å‹ï¼ˆMacï¼‰é¢„è®­ç»ƒä»»åŠ¡ï¼Œç¼“è§£â€œé¢„è®­ç»ƒ-ä¸‹æ¸¸ä»»åŠ¡â€ä¸ä¸€è‡´é—®é¢˜ã€‚
  - å¦‚ä½•é€‰æ‹©ç›¸ä¼¼è¯ï¼ŸSynonyms å·¥å…·åŸºäºword2vecè®¡ç®—
- MacBERTåœ¨å¤šç§NLPä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æ€§èƒ½æå‡ã€‚
- EMNLPï¼Œå“ˆå·¥å¤§ï¼Œ[Revisiting Pre-trained Models for Chinese Natural Language Processing](https://www.aclweb.org/anthology/2020.findings-emnlp.58)ï¼Œè¾¾åˆ° sota
- [MacBERT](https://github.com/ymcui/MacBERT)
- [MacBERT: ä¸­æ–‡è‡ªç„¶è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹](https://zhuanlan.zhihu.com/p/333202482)

å‘å±•
- 2020/9/15 è®ºæ–‡"Revisiting Pre-Trained Models for Chinese Natural Language Processing" è¢«Findings of EMNLP å½•ç”¨ä¸ºé•¿æ–‡ã€‚
- 2020/11/3 é¢„è®­ç»ƒå¥½çš„ä¸­æ–‡MacBERTå·²å‘å¸ƒï¼Œä½¿ç”¨æ–¹æ³•ä¸BERTä¸€è‡´

### MacBERT æ¨¡å‹

MacBERTä¸BERTå…±äº«ç›¸åŒçš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œä½†æœ‰ä¸€äº›ä¿®æ”¹ã€‚ å¯¹äºMLMä»»åŠ¡ä¿®æ”¹ã€‚
- ç”¨**å…¨è¯**maskedä»¥åŠ **Ngram** maskedç­–ç•¥æ¥é€‰æ‹©å€™é€‰tokenæ¥maskedï¼Œå•è¯çº§åˆ«çš„unigramåˆ°4-gramçš„æ¯”ä¾‹ä¸º40ï¼…ï¼Œ30ï¼…ï¼Œ20ï¼…ï¼Œ10ï¼…ã€‚
- ä¸ç”¨\[MASK] tokenè¿›è¡Œmaskï¼Œå› ä¸ºåœ¨token å¾®è°ƒé˜¶æ®µ**ä»æœªå‡ºç°è¿‡**\[MASK]ï¼Œæè®®ç”¨**ç±»ä¼¼å•è¯**è¿›è¡Œmaskingã€‚ é€šè¿‡ä½¿ç”¨åŸºäºword2vec(Mikolov et alã€‚ï¼Œ2013)ç›¸ä¼¼åº¦è®¡ç®—çš„åŒä¹‰è¯å·¥å…·åŒ…(Wang and Huï¼Œ2017)è·å¾—ç›¸ä¼¼çš„å•è¯ã€‚ å¦‚æœé€‰æ‹©ä¸€ä¸ªN-gramè¿›è¡Œmaskedï¼Œåˆ†åˆ«æ‰¾åˆ°ç›¸ä¼¼çš„å•è¯ã€‚ åœ¨æå°‘æ•°æƒ…å†µä¸‹ï¼Œå½“æ²¡æœ‰ç›¸ä¼¼çš„å•è¯æ—¶ï¼Œä¼šé™çº§ä»¥ä½¿ç”¨**éšæœºå•è¯**æ›¿æ¢ã€‚
- å¯¹15ï¼…æ¯”ä¾‹çš„è¾“å…¥å•è¯è¿›è¡Œmaskingï¼Œå…¶ä¸­80ï¼…æ›¿æ¢ä¸ºç›¸ä¼¼çš„å•è¯ï¼Œ10ï¼…å°†æ›¿æ¢ä¸ºéšæœºå•è¯ï¼Œå…¶ä½™10ï¼…åˆ™ä¿ç•™åŸå§‹å•è¯ã€‚
å¯¹äºç±»ä¼¼NSPçš„ä»»åŠ¡ï¼Œæ‰§è¡ŒALBERT (Lanç­‰äººï¼Œ2019)å¼•å…¥çš„å¥å­é¡ºåºé¢„æµ‹(SOP)ä»»åŠ¡ï¼Œå…¶ä¸­é€šè¿‡åˆ‡æ¢ä¸¤ä¸ªè¿ç»­å¥å­çš„åŸå§‹é¡ºåºæ¥åˆ›å»ºè´Ÿæ ·æœ¬ã€‚

MacBERTä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒï¼Œç”¨è°·æ­Œå®˜æ–¹çš„Chinese BERT-baseè¿›è¡Œå‚æ•°åˆå§‹åŒ–
- ä½†æ˜¯å¯¹äºlargeç‰ˆæœ¬æ˜¯é‡æ–°è®­ç»ƒçš„ï¼Œå› ä¸ºè°·æ­Œå®˜æ–¹æ²¡æœ‰å‘å¸ƒChinese BERTçš„largeç‰ˆæœ¬ã€‚

è®ºæ–‡æ¨¡å‹(Hugging Face): Â 
- [hfl/chinese-macbert-base](https://huggingface.co/hfl/chinese-macbert-base) ; Â 
- [hfl/chinese-macbert-large](https://huggingface.co/hfl/chinese-macbert-large)

### MacBERT æ•ˆæœ

åœ¨å“ˆå·¥å¤§è®¯é£è”åˆå®éªŒå®¤å‘å¸ƒçš„ä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£æ•°æ®ï¼ˆCMRC 2018æ•°æ®é›†ï¼‰ç­‰å¤šä¸ªæ•°æ®é›†ä¸Šå–å¾—æœ€ä½³ï¼Œè¶…è¿‡ RoBERTaï¼ŒELECTRA



### MacBERT å®è·µ


```py
model_name = 'hfl/chinese-macbert-base'
model_name = 'hfl/chinese-macbert-large'

tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)
```

## DIET(RASA)

RASAæ¨å‡º DIET æ¨¡å‹ â€”â€” æ„å›¾è¯†åˆ«+å®ä½“æŠ½å– sota

ã€2020-5-10ã€‘[DIETæ¨¡å‹ rasa èŠå¤©æœºå™¨äººæ ¸å¿ƒæ¨¡å‹è®ºæ–‡](https://zhuanlan.zhihu.com/p/162995854), RASA 1.8 ä¸­å¼•å…¥
- [DIET: Lightweight Language Understanding for Dialogue Systems](https://arxiv.org/pdf/2004.09936.pdf)
- ã€2023-11-15ã€‘æºç :[diet_classifier](https://github.com/RasaHQ/rasa/blob/main/rasa/nlu/classifiers/diet_classifier.py)
- æ•°æ®é›†ï¼š[dataset](https://github.com/RasaHQ/DIET-paper/tree/master/data)
  - [ATIS](https://github.com/RasaHQ/DIET-paper/tree/master/data/ATIS)
  - [NLU-Evaluation-Data](https://github.com/RasaHQ/DIET-paper/tree/master/data)
  - [SNIPS](https://github.com/RasaHQ/DIET-paper/tree/master/data/SNIPS)
- [Introducing DIET: state-of-the-art architecture that outperforms fine-tuning BERT and is 6X faster to train](https://rasa.com/blog/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/)

DIET æ¨¡å‹æ˜¯ Dual Intent and Entity Transformer ç®€ç§°, è§£å†³äº†å¯¹è¯ç†è§£é—®é¢˜ä¸­çš„2ä¸ªé—®é¢˜ï¼Œ**æ„å›¾åˆ†ç±»**å’Œ**å®ä½“è¯†åˆ«**ã€‚
- DIETä½¿ç”¨**çº¯ç›‘ç£**æ–¹å¼ï¼Œæ²¡æœ‰ä»»ä½•é¢„è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œ**æ— é¡»**å¤§è§„æ¨¡é¢„è®­ç»ƒæ˜¯å…³é”®ï¼Œæ€§èƒ½å¥½äº fine-tuning Bert, ä½†æ˜¯è®­ç»ƒé€Ÿåº¦æ˜¯bertçš„**6å€**ã€‚

å¯¹è¯ç³»ç»Ÿä¸­NLUçš„2ä¸ªä»»åŠ¡ï¼š**æ„å›¾åˆ†ç±»**å’Œ**å®ä½“è¯†åˆ«**ï¼Œ Goo et al è®¤ä¸ºå•ç‹¬è®­ç»ƒè¿™2ä¸ªtaskä¼šå¯¼è‡´**é”™è¯¯ä¼ æ’­**ï¼Œå…¶æ•ˆæœä¸å¦‚2ä¸ªä»»åŠ¡åŒæ—¶ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹ï¼Œ2ä¸ªä»»åŠ¡çš„æ•ˆæœä¼šç›¸äº’åŠ å¼ºã€‚
- å¤§å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä¸Šæ€§èƒ½å¾ˆå¥½ï¼Œä½†æ˜¯åœ¨è®­ç»ƒå’Œå¾®è°ƒé˜¶æ®µéƒ½éœ€è¦**å¤§é‡**è®¡ç®—æ€§èƒ½ã€‚

DIETæ¨¡å‹å…³é”®åŠŸèƒ½æ˜¯å°†é¢„è®­ç»ƒæ¨¡å‹çš„å¾—åˆ°çš„**è¯å‘é‡**ï¼Œå’Œå¯è‡ªç”±ç»„åˆçš„ç¨€ç–å•è¯ç‰¹å¾å’Œn-gramç‰¹å¾ç»“åˆèµ·æ¥
- DIETä»£ç ä¸­ï¼Œè¿™2ä¸ªæ˜¯dense-feature å’Œsparse-featureç‰¹å¾ã€‚

ä¸€èˆ¬ç”¨ ELMOï¼ŒBERTï¼ŒGPT ç­‰æ¨¡å‹ä½œä¸ºè¿ç§»å­¦ä¹ æ¨¡å‹ï¼Œç„¶åç”¨æˆ–ä¸ç”¨fine-tuningè·å–å‘é‡ï¼Œç”¨äºä¸‹ä¸€ä¸ªæ¨¡å‹
- ä½†æ˜¯è¿™äº›æ¨¡å‹**é€Ÿåº¦æ…¢ï¼Œè®­ç»ƒæˆæœ¬é«˜**ï¼Œä¸å¤ªé€‚åˆç°å®çš„AIå¯¹è¯ç³»ç»Ÿã€‚

Hen-derson et al. (2019b) æå‡ºäº†ä¸€ä¸ªæ›´ç®€æ´çš„æ¨¡å‹ï¼Œä½¿ç”¨å•è¯å’Œå¥å­çº§åˆ«çš„encodingè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¯”BERTå’ŒELMOæ•ˆæœè¦å¥½ï¼ŒDIETæ¨¡å‹æ˜¯å¯¹æ­¤è¿›ä¸€æ­¥ç ”ç©¶

è”åˆçš„æ„å›¾åˆ†ç±»å’Œå‘½åå®ä½“è¯†åˆ«
- Zhang and Wang (2016) æå‡ºäº†ä¸€ç§ç”±**åŒå‘é—¨æ§é€’å½’å•å…ƒ**ï¼ˆBiGRUï¼‰ç»„æˆçš„è”åˆæ¶æ„ã€‚æ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ç”¨äºå®ä½“æ ‡è®°ï¼Œæœ€åæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ç”¨äºæ„å›¾åˆ†ç±»ã€‚
- Liuand Lane (2016); Varghese et al. (2020) and Gooet al. (2018) ï¼‰æå‡ºäº†ä¸€ç§åŸºäºæ³¨æ„åŠ›çš„**åŒå‘é•¿æœŸçŸ­æœŸè®°å¿†**ï¼ˆBiLSTMï¼‰ï¼Œç”¨äºè”åˆæ„å›¾åˆ†ç±»å’ŒNERã€‚ 
- Haihong et al.(2019)å¼•å…¥äº†ä¸€ä¸ª**å…±åŒå…³æ³¨ç½‘ç»œ**ï¼Œç”¨äºæ¯ä¸ªä»»åŠ¡ä¹‹é—´å…±äº«ä¿¡æ¯ã€‚ 
- Chenet al. (2019)æå‡ºäº†**è”åˆBERT**ï¼Œè¯¥BERTå»ºç«‹åœ¨BERTä¹‹ä¸Šï¼Œå¹¶ä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è¿›è¡Œè®­ç»ƒã€‚ä»–ä»¬ä½¿ç”¨ç¬¬ä¸€ä¸ªï¼ˆCLSï¼‰çš„éšè—çŠ¶æ€è¿›è¡Œæ„å›¾åˆ†ç±»ã€‚ä½¿ç”¨å…¶ä»–tokençš„æœ€ç»ˆéšè—çŠ¶æ€æ¥é¢„æµ‹å®ä½“æ ‡ç­¾ã€‚
- Vanzoï¼ˆ2019ï¼‰æå‡ºäº†ä¸€ç§ç”±BiLSTMå•å…ƒç»„æˆçš„åˆ†å±‚è‡ªä¸‹è€Œä¸Šçš„**ä½“ç³»ç»“æ„**ï¼Œä»¥æ•è·è¯­ä¹‰æ¡†æ¶çš„è¡¨ç¤ºå½¢å¼ã€‚ä»–ä»¬ä»ä»¥è‡ªä¸‹è€Œä¸Šçš„æ–¹å¼å †å çš„å„ä¸ªå±‚å­¦ä¹ çš„è¡¨ç¤ºå½¢å¼ï¼Œé¢„æµ‹å¯¹è¯è¡Œä¸ºï¼Œæ„å›¾å’Œå®ä½“æ ‡ç­¾ã€‚

DIETé‡‡ç”¨transformer-based æ¡†æ¶ï¼Œä½¿ç”¨çš„å¤šä»»åŠ¡åƒå½¢å¼ï¼Œæ­¤å¤–è¿˜è¿›è¡Œäº†æ¶ˆèæµ‹è¯•ã€‚

DIET æ¶æ„å›¾
- ![](https://pic2.zhimg.com/80/v2-f678ceccf7625d6d364431ea10552575_1440w.webp)
- æ„å›¾æ˜¯play_game, å®ä½“æ˜¯ping pongï¼ŒFFWæ˜¯å…±äº«æƒé‡çš„

è¾“å…¥ç‰¹å¾åˆ†ä¸º2éƒ¨åˆ†ï¼Œç¨ å¯†ç‰¹å¾ dense featureså’Œç¨€ç–ç‰¹å¾sparse featuresã€‚

ç¨€ç–ç‰¹å¾æ˜¯n-grams(n<=5)çš„one-hot æˆ–è€…multi-hotç¼–ç ï¼Œä½†æ˜¯ç¨€ç–ç‰¹å¾åŒ…å«å¾ˆå¤šå†—ä½™ç‰¹å¾ï¼Œä¸ºäº†é¿å…è¿‡æ‹Ÿåˆï¼Œå¯¹æ­¤åŠ å…¥äº†dropoutã€‚ ç¨ å¯†ç‰¹å¾æ¥è‡ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œ ä¾‹å¦‚Bertæˆ–GloVeã€‚

ä½¿ç”¨ConveRTä½œä¸ºå¥å­çš„ç¼–ç å™¨ï¼ŒConveRTçš„_CLSå‘é‡ä½œä¸ºDIETçš„åˆå§‹è¾“å…¥ã€‚è¿™å°±æ˜¯ä½œä¸ºå•è¯ä¿¡æ¯ä¹‹å¤–çš„å¥å­ä¿¡æ¯ç‰¹å¾äº†ã€‚å¦‚æœä½¿ç”¨BERTï¼Œæˆ‘ä»¬ä½¿ç”¨ BERT `[CLS]` tokenï¼Œ å¯¹äºGloVeï¼Œä½¿ç”¨å¥å­æ‰€æœ‰tokençš„å‡å€¼ä½œä¸º_CLS_ï¼Œ

ç¨€ç–ç‰¹å¾é€šè¿‡ä¸€ä¸ªå…¨è¿æ¥è®¡ç®—ï¼Œå…¶ä¸­å…¨è¿æ¥çš„æƒé‡æ˜¯å’Œå…¶å®ƒåºåˆ—çš„æ—¶é—´æ­¥å…¨è¿æ¥å…±äº«çš„ï¼Œç›®çš„æ˜¯ä¸ºäº†è®©ç¨€ç–ç‰¹å¾çš„ç»´åº¦å’Œç¨ å¯†ç‰¹å¾çš„ç»´åº¦ä¸€è‡´ï¼Œ ç„¶åå°†ç¨€ç–ç‰¹å¾çš„FFWçš„è¾“å‡ºå’Œç¨ å¯†ç‰¹å¾çš„å‘é‡è¿›è¡Œconcatæ‹¼æ¥ï¼Œå› ä¸ºtransformerè¦æ±‚è¾“å…¥çš„å‘é‡ç»´åº¦éœ€è¦ä¸€è‡´ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ‹¼æ¥åé¢å†æ¥ä¸€ä¸ªFFWï¼ŒFFWçš„æƒé‡ä¹Ÿæ˜¯å…±äº«çš„ã€‚åœ¨å®éªŒä¸­ï¼Œè¿™ä¸€å±‚çš„ç»´åº¦æ˜¯256.

transformeræ¨¡å—ï¼Œä½¿ç”¨2å±‚layerï¼Œä½¿ç”¨tokençš„ç›¸å¯¹ä½ç½®ç¼–ç ã€‚

DIET: ä¸€ç§ç”¨äºæ„å›¾å’Œå®ä½“å»ºæ¨¡çš„çµæ´»ä½“ç³»ç»“æ„ã€‚
- DIETåœ¨NLU-Benchmarkæ•°æ®é›†ä¸Šè¡¨ç°çš„state of the artã€‚ 
- æ­¤å¤–ï¼Œä½¿ç”¨å„ç§é¢„è®­ç»ƒæ–¹æ³•çš„åµŒå…¥çš„æœ‰æ•ˆæ€§ã€‚å‘ç°æ²¡æœ‰å•ä¸€çš„embddingæ¨¡å‹æ€»æ˜¯åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šè¡¨ç°å‡ºæœ€å¥½çš„æ•ˆæœï¼Œè¿™å‡¸æ˜¾äº†æ¨¡å—åŒ–ä½“ç³»ç»“æ„çš„é‡è¦æ€§
- æ­¤å¤–ï¼ŒåƒGloVeè¿™æ ·çš„åˆ†å¸ƒå¼æ¨¡å‹ä¸å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹BERTä»ç„¶å…·æœ‰ä¸åŒçš„ä¼˜åŠ¿ï¼Œåœ¨ä¸ä½¿ç”¨ä»»ä½•é¢„è®­ç»ƒåµŒå…¥çš„æƒ…å†µä¸‹ï¼ŒDIETä»è¡¨ç°å‡ºstate of the art
- æœ€åï¼Œåœ¨NLU-Benchmarkæ•°æ®é›†ä¸Šï¼Œç”¨äºDIETçš„é¢„è®­ç»ƒåµŒå…¥é›†ä¼˜äºå¯¹DIETå†…çš„BERTè¿›è¡Œfine-tuneï¼Œå¹¶ä¸”è®­ç»ƒé€Ÿåº¦å¿«äº†å…­å€ã€‚


## å…¶å®ƒ

UniLMã€MASS ã€SpanBERT å’Œ ELECTRA

## èµ„æ–™

- [Bertæ—¶ä»£çš„åˆ›æ–°ï¼ˆåº”ç”¨ç¯‡ï¼‰ï¼šBertåœ¨NLPå„é¢†åŸŸçš„åº”ç”¨è¿›å±•](https://zhuanlan.zhihu.com/p/68446772)


# ç»“æŸ