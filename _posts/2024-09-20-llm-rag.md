---
layout: post
title:   大模型落地技术：检索增强生成 LLM RAG
date:   2024-09-20 16:52:00
categories: 大模型
tags: llm RAG
excerpt: 大模型工业落地的技术经验：RAG 检索增强生成及其改进版本
mathjax: true
permalink: /rag
---

* content
{:toc}


# RAG 检索增强生成

更多LLM技术落地方案见站内专题：[大模型应用技术方案](llm_solution)

## LLM 问题

LLM 通过大量数据训练，回答任何问题或完成任务，利用其参数化记忆。这些模型有一个知识**截止日期**，取决于上次训练的时间。
- 被问及超出其知识范围或知识截止日期后发生的事件时，模型会产生**幻觉**。

Meta 研究人员发现，通过提供与手头任务相关的信息，模型在完成任务时表现**显著改善**。

例如，询问模型关于截止日期之后发生的事件，则提供该事件作为背景信息并随后提问将帮助模型正确回答问题。

由于LLM具有有限的上下文窗口长度，在处理当前任务时**只能传递最相关的知识**。添加到上下文中数据质量影响着模型生成响应结果的质量。

机器学习从业者在RAG流程不同阶段使用多种技术来改善LLM性能。


## RAG 解决什么问题

RAG 解决什么问题
- **知识时效性**：大型语言模型通常是基于一定时间范围内的数据进行训练的，因此它们难以获取最新的信息。外挂知识库可以包含最新的数据，使得模型能够回答关于最新事件的问题。
- **知识覆盖范围**：即使是非常大的语言模型，也无法覆盖所有领域的知识。通过外挂知识库，可以针对特定领域或主题提供详细和专业的信息。
- 提高准确性和可靠性：在没有额外知识的情况下，模型可能会基于其训练数据生成不准确或误导性的回答。外挂知识库可以提供准确的信息源，从而提高回答的可靠性。
- **减少幻觉**（Hallucination）：幻觉是指模型生成看似合理但实际上不准确的答案。通过检索相关的真实信息，外挂知识库可以帮助减少这种幻觉现象。
- **跨领域**知识融合：在处理需要跨领域知识的问题时，外挂知识库可以提供不同领域的知识，帮助模型生成综合性的回答。
- 数据**长度**问题，最初大模型一次性处理的token数量有限，通过RAG技术能是的大模型一次性处理更长的文本
- 数据**安全**问题

## RAG 介绍

科里·祖
> “检索增强生成是用您（系统）从其他地方检索到的附加信息来补充用户输入到 ChatGPT 等大型语言模型 (LLM) 的过程。然后，法学硕士可以使用该信息来增强其生成的响应。” 


检索增强生成（简称 `RAG`）是 Meta 于 2020 年推广的一种架构，通过将**相关信息**与**问题/任务细节**一起传递给模型来提高 LLM 的性能。
- 【2020-9-28】[Retrieval Augmented Generation: Streamlining the creation of intelligent natural language processing models](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)

RAG模型直接把Retrieve augmentation加到名字里。

解决的痛点：
- 之前的研究（如REALM）基于MLM模型，只做**提取**任务，其他任务受限，用generator可以赋予其更大的能力。

两个模型变种：`RAG-Sequence`和`RAG-Token`
- `RAG-Sequence`，先找到k个最详尽的例子（k个z），然后根据每个z作为条件去预测完整个句子，然后概率求和。在每一次去预测的时候都是只看一个doc的。先乘后加。对应的decode需要额外处理
  - ![](https://pic4.zhimg.com/80/v2-68b1299d6a4b92e5cab3e0fcd4561af7_1440w.webp)
- `RAG-Token`，先找到k个最详尽的例子（k个z），但是每预测一个token的时候都要看k个doc然后求和。先加后乘，对应的decode需要额外处理
  - ![](https://pic3.zhimg.com/v2-70ae3fec7b473b30b4a5692a45982a86_b.jpg)

retriever 用 DPR，generator 用 BART，联合训练。预测时候有一些小小操作详情见原[论文](https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/)

[REALM](https://huggingface.co/docs/transformers/model_doc/realm) 和 [RAG](https://huggingface.co/docs/transformers/model_doc/rag) 在huggingface上都已经被实现了，直接使用。

`FiD`和`FiD-KD`，简单高效的Generator端的改进
- `FiD` 全称是 Fusion-in-Decoder，作者的一篇短文被提出（小彩蛋：通讯作者是当时cached LM的一作）
  - 主要改进 Generator端，方法一张图：
  - ![](https://pic1.zhimg.com/80/v2-7adbf779c5e97a23bb387a09a6122f48_1440w.webp)
  - 基于seq2seq的模型，拆开两半用，第一步用encoder将检索出来的N个段落分别与Question进行拼接的N个拼接编码，之后就和seq2seq剩余的部分一样了，cross attention，causual mask什么的使用decoder把他们拼起来的结果解码即可。结果还是很不错的，简单有效刷SOTA。
- `FiD-KD`: 发完FiD不久，统一团队对FiD迅速补充提出了KD的FiD。
  - 解决的痛点: 很难去监督retrieve出来的结果，因为几乎没有对到底检索出来哪些文档有有监督的标注，之前的REALM和RAG也对这个根本没有监督。FiD-KD对此进行改进。
  - FiD中将所有的encoding拼接之后接下来继续完成seq2seq，会进行一下cross attention，（如果对cross attention不熟悉那么就可以去看看huggingface transformers的BART源码），cross attention的attention值的大小是一个很好的监督信号。cross attention的值这个可以对检索出来的文章进行一个打分，进而作为信号被我们用来拉近retriever的结果和retriever打分的分布。具体的attention怎么处理的，怎样拉近分布比较好详情见论文。

UniK-QA，扩展Open-domain QA的知识来源
- 之前的KBQA（涉及结构化知识）和开放域问答（涉及非结构化知识）是分别分开研究的，使用的技术也不一样，同时也限制了知识的来源。作者希望他们之间能通过统一使得QA的性能得以提高。所以作者提出了一个pipeline：首先将结构化文本进行flatten表示为非结构化文本“一段话”（也就是传统的开放域问答所使用的知识形式）

TLM，使用检索来进行专用化的高效预训练
- 2021年的11月，杨植麟团队提出了一种新的预训练范式，Task-driven Language Modeling (TLM)，根据下游需要完成的任务来有针对性的进行上游预训练数据的筛选。这种针对性的预训练可以大幅度加速下游的微调时间，从而取得更好的效果。

CASPER，检索examples作为辅助

RETRO，retrieve-based LM领域的大厂强心剂
- jalammar写的很简洁，直接看就可以很快地了解RETRO在干什么，几个takeaway是：
  - 1）不训练检索器就很好使，RETRO直接用冻结的BERT作为检索器，效果也很好。
  - 2）cross attention固然好用，像RETRO这样设计新的深度融合可能会更好，这也是可以做的一个蓝海方向。

更多：[检索、提示：检索增强的（Retrieval Augmented）自然语言处理](https://zhuanlan.zhihu.com/p/470784563)

【2024-3-7】[大模型RAG行业适配过程中的一些思考](https://zhuanlan.zhihu.com/p/682625461?utm_psn=1748870868057579520)

对齐方式上来看:人向大模型对齐和大模型向人对齐。
- **人->大模型**对齐：prompt工程，通过提示词工程提升，进行问题优化，或引入BPO机制，进行提示词预训练。
- **大模型->人**对齐：RLHF、SFT、RAG 等方式，通过对于人类习惯、领域知识的增强与补充，使模型的输出更加符合人类的预期和需求。

实际工程中也会通过以上融合的方式，使大模型能够更加符合人类对于问答的预期

RAG 生成准确且符合上下文的答案，同时减少模型幻觉。
- RAG的基本原理: 首先对现有文档进行检索，然后基于检索到的信息对生成的答案进行修正。
- RAG主要作用: 提高LLM的处理效率，并有效控制tokens的长度
- 主流框架: LlamaIndex和langchain等，通过RAG对于领域知识的引入，可以减少大模幻觉问题。并且，由于无需打开模型调参的工作机制，与SFT等对齐方式相比，具有更加快速、便捷的优点。

准确率计算：
- `RAG准确率` = `LLM准确率` * `语义搜索准确率` * `RAG信息保存率`

RAG的准确性影响因素
- 上下文脱节问题：如果文本有多层上下文信息，chunk之间会使得内容上上下文脱节
- 位置因素失效问题：一般文字位置代表一定的重要性，但是在向量库存储的模式下，位置关键因素会失效
- 连续信息不完整问题：由于连接不再完整，语序容易被打乱
- 描述信息丢失问题。


## RAG 方案


### RAG 进化路线


<div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers tags lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; modified=\&quot;2023-12-09T12:31:12.223Z\&quot; agent=\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\&quot; etag=\&quot;eeN8-CU8CjzQvVej0-xF\&quot; version=\&quot;22.1.5\&quot;&gt;\n  &lt;diagram name=\&quot;第 1 页\&quot; id=\&quot;YUrH7kkdw6S7EPocWAtV\&quot;&gt;\n    &lt;mxGraphModel dx=\&quot;1238\&quot; dy=\&quot;789\&quot; grid=\&quot;1\&quot; gridSize=\&quot;10\&quot; guides=\&quot;1\&quot; tooltips=\&quot;1\&quot; connect=\&quot;1\&quot; arrows=\&quot;1\&quot; fold=\&quot;1\&quot; page=\&quot;1\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;827\&quot; pageHeight=\&quot;1169\&quot; math=\&quot;0\&quot; shadow=\&quot;0\&quot;&gt;\n      &lt;root&gt;\n        &lt;mxCell id=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;CRyWcW9bKPYmjVe2kgWn-2\&quot; value=\&quot;RAG优化路线图\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontStyle=0;fontSize=22;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;294.5\&quot; y=\&quot;20\&quot; width=\&quot;180\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;EXEZnx_Qc3e6ulj9dSyI-57\&quot; value=\&quot;相关信息&amp;lt;font color=&amp;quot;#333333&amp;quot;&amp;gt;+&amp;lt;/font&amp;gt;问题/任务细节&amp;lt;font color=&amp;quot;#666666&amp;quot;&amp;gt;一起传给LLM&amp;lt;/font&amp;gt;\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#6666FF;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;124.5\&quot; y=\&quot;120\&quot; width=\&quot;230\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;O1_MGZhKnQJ_RoEkMdeZ-1\&quot; value=\&quot;RAG&amp;lt;br&amp;gt;检索增强生产\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;151\&quot; y=\&quot;150\&quot; width=\&quot;115.5\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;O1_MGZhKnQJ_RoEkMdeZ-2\&quot; value=\&quot;2020.9.26，Meta推广\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;146.5\&quot; y=\&quot;180\&quot; width=\&quot;140\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;O1_MGZhKnQJ_RoEkMdeZ-4\&quot; value=\&quot;PromptsRoyale\&quot; style=\&quot;rounded=1;whiteSpace=wrap;fillColor=#fff2cc;strokeColor=#d6b656;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;151\&quot; y=\&quot;250\&quot; width=\&quot;115.5\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;O1_MGZhKnQJ_RoEkMdeZ-5\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;spacing=3;strokeWidth=2;strokeColor=#999999;exitX=0.5;exitY=1;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;O1_MGZhKnQJ_RoEkMdeZ-1\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;272\&quot; y=\&quot;449\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;209\&quot; y=\&quot;250\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;O1_MGZhKnQJ_RoEkMdeZ-6\&quot; value=\&quot;Web&amp;amp;nbsp;\&quot; style=\&quot;edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];labelBackgroundColor=none;\&quot; vertex=\&quot;1\&quot; connectable=\&quot;0\&quot; parent=\&quot;O1_MGZhKnQJ_RoEkMdeZ-5\&quot;&gt;\n          &lt;mxGeometry x=\&quot;-0.0968\&quot; y=\&quot;5\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;18\&quot; y=\&quot;8\&quot; as=\&quot;offset\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n      &lt;/root&gt;\n    &lt;/mxGraphModel&gt;\n  &lt;/diagram&gt;\n&lt;/mxfile&gt;\n&quot;}"></div>
<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>

模型：
> `Knn-LM` -> `REALM` -> `DPR` -> `RAG` -> `FID` -> `COG` -> `GenRead` -> `REPLUG` -> `Adaptive retrieval`

详情见[文章](https://blog.csdn.net/qq_52852138/article/details/133019348)

【2023-10-30】[ACL2023](https://acl2023-retrieval-lm.github.io/)陈丹琦等《基于检索的大语言模型及其应用》
- 【2023-7-10】[陈丹琦ACL学术报告来了！详解大模型「外挂」数据库7大方向3大挑战，3小时干货满满](https://www.qbitai.com/2023/07/67259.html)
- ![](https://pic1.zhimg.com/80/v2-8fda0e178a75c4c4f7e832e78ffae8c4_1440w.webp)

GPT 模型三个问题：
- 1、参数量过大，如果基于新数据重训练，计算成本过高；
- 2、记忆力不行（面对长文本，记了下文忘了上文），时间一长会产生幻觉，且容易泄露数据；
- 3、目前的参数量，不可能记住所有知识。

这种情况下，**外部检索语料库**被提出，即给大语言模型“外挂”一个数据库，让它随时能通过查找资料来回答问题，而且由于这种数据库随时能更新，也不用担心重训的成本问题。
- ![](https://www.qbitai.com/wp-content/uploads/replace/09f5a03d57c982249ed0e440e53fec9d.png)

训练方式上，则着重介绍
- **独立训练**（independent training，语言模型和检索模型分开训练）、**连续学习**（sequential training）、**多任务学习**（joint training）等方法

应用方面，这类模型涉及的也就比较多了
- 不仅可以用在代码生成、分类、知识密集型NLP等任务上
- 而且通过微调、强化学习、基于检索的提示词等方法就能使用

RAG要解决的几大难题。
- 其一，**小语言模型**+（不断扩张的）**大数据库**，本质上是否意味着语言模型的参数量依旧很大？如何解决这一问题？
  - 模型参数量只有70亿参数量，但外挂的数据库却能达到2T…
  - ![](https://www.qbitai.com/wp-content/uploads/replace/79f264e4ffb852cb45f75b3e5a2fc434.png)
- 其二，相似性搜索的效率。如何设计算法使得搜索效率最大化，是目前非常活跃的一个研究方向。
  - ![](https://www.qbitai.com/wp-content/uploads/replace/18596970d8efb294c3e3283072902ef4.png)
- 其三，完成复杂语言任务。包括开放式文本生成任务，以及复杂的文本推理任务在内，如何用基于检索的语言模型完成这一任务，也是需要持续探索的方向。
  - ![](https://www.qbitai.com/wp-content/uploads/replace/88ca24b2488cf19b327633bad16ca4ea.png)


### RAG 流程

将原始文件拆解后, 每个部分都会生成相应embedding 并且 存放到vector store 中. 当查询发送给 vector store 时, 查询也会转换为 embedding , 然后 vector store 返回与查询最相似 的 embeddings
- ![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jq9bEbitg1Pv4oASwEQwJg.png)

RAG 包含三个阶段：数据准备、检索和生成。
- **数据准备**阶段：确定数据源、从数据源中提取数据、清理数据并将其存储到数据库中。
  - 识别数据来源、从来源中提取数据、清洗数据并将其存储在数据库中
  - 向量存储器：存储文本、图像、音频等非结构化数据，并基于语义相似性搜索该类别下的内容。
- **检索**阶段：根据手任务从数据库中检索相关数据。
  - 关键词搜索：简单的检索数方法，数据根据关键词进行索引，并且搜索引擎返回包含这些关键的文档。
  - 关键词搜索适用于存储**结构化数据**（如表格、文档等）并使用关键词对数据进行搜索。
  - 图数据库以节点和边的形式存储数据。适用于存储结构化数据（如表格、文档等），并通过数据之间的关系进行搜索
  - 搜索引擎：从公共搜索引擎（如Google、Bing等）或内部擎（如Elasticsearch、Solr等）中检索RAG管道中的数据；搜索引擎适用于从网络上检索数据并使用关键字对其进行搜索。
  - 可将来自**搜索引擎**的数据与**其他数据库**（如向量存储、图数据库等）中获取到的数据相结合，以提高输出质量。推荐结合多种策略（如语义搜索 + 关键字匹配）的混合方法
  - 矢量数据库中对嵌入式数据进行相似性搜索
- **生成**阶段：利用检索到的数据和任务生成输出结果。
  - 检索到相关数据，就会连同用户的查询或任务一起传递给生成器（LLM）。LLM 使用检索到的数据和用户的查询或任务生成输出
  - 输出质量取决于数据的质量和检索策略。

### LLM+RAG

【2023-9-26】[一文纵览LLM+RAG 的方法实现](https://mp.weixin.qq.com/s/ifp2i71Psn86ZCEzffsF0Q)

LLM+RAG 方法实现主要有以下几种方法：
1. **作为Prompt一部分**：比较简单，外接检索器，将检索器召回的内容直接放置到预先配置的Prompt模板中，当成背景知识让LLM来直接输出。这种方法实现最简单，也是当前比较常用的做法。
2. **KNN+LLM**：该方法最大的一个特点就是推理时采用 模型**分布输出** + 检索**Top tokens**，作为两个next-token 进行**融合解码**。通过 Embdding 召回外部知识库和Query token 相似的token 。
3. **自回归检索+解码**：先让模型解码出tokens，然后检索该tokens相似的doc，拼接在Prompt中，进行next-token 预测，完成自回归的解码。

第一种办法中涉及私域知识库问答，关于建索引部分可参考文章：分享闭域知识问答下检索相关的方法和实践。
- 如果不是私域问答，最简单的方案是直接用query 调用外部搜索引擎，取到Top 的结果内容拼接到Prompt 中拿到模型的答案。

路线图见原文: [一文纵览LLM+RAG 的方法实现](https://mp.weixin.qq.com/s/ifp2i71Psn86ZCEzffsF0Q)

## 评估指标

一般有检索的**上下文相关性**、答案的**忠实度**（faithfulness）和答案**相关性**（answer_relevancy）

两大类：[OpenAI：最大化LLM性能的技术综述](https://zhuanlan.zhihu.com/p/667439436?utm_psn=1709306396712189953)
- 一类是衡量LLM如何回答问题的
  - 第一个度量是**忠实度**（Faithfulness），检查生成答案的**事实**准确性，将答案分解为事实，并将这些事实与检索的内容相对应。如果无法对应，则认为是模型的“幻觉”。这个度量提供了一个数值，如果数值低于某个阈值，表明可能出现了幻觉。
- 另一类则是衡量检索内容与问题的相关性。
  - **答案相关性**（Answer Relevancy）度量模型生成的答案是否真正针对用户提出的问题。有时候模型可能会生成一个很好地利用了检索内容的答案，但实际上与用户的原始问题毫不相关。这个度量就是为了衡量这一点。如果发现答案在事实上是准确的，但相关性却很低，这意味着模型可能过于关注内容本身，而忽视了问题的实际要求。这就表明可能需要进行提示工程（Prompt Engineering），或者采取其他措施来促使模型更多关注问题本身，并决定是否应该使用相关内容。
  - **内容相关性**方面，需要关注上下文精确度（Context Precision），这是衡量检索内容信噪比的指标。它评估每一块内容与答案的对应关系，以确定内容的使用是否真正对答案有帮助。此外，上下文召回（Context Recall）度量模型是否检索到了回答问题所需的所有相关信息。这是反映搜索优化程度的一个指标，如果召回率很低，表明我们可能需要优化搜索功能，引入重排序，或者对嵌入进行精调（Fine-tuning），以确保能够检索到更相关的内容。


【2023-11-1】[评估检索增强生成（RAG）：TRULENS + MILVUS](https://mp.weixin.qq.com/s/xAn827Q31fdQ8t7jkLIoUQ)
-  Evaluations for Retrieval Augmented Generation: TruLens + Milvus

构建 RAG 的各种配置和参数，包括：索引类型、嵌入模型、top k 和 chunk 大小参数
- **前 k 个**：经常讨论的一个参数，控制检索到的上下文分块数量。更高的前 k 个提供更高机会检索到所需信息，也增加语言模型融入不相关信息到其回答中的可能性。对简单问题而言，较低的前 k 个通常性能最佳。
- **分块大小**：控制每个检索上下文的大小。对更复杂问题而言，较大分块大小可能更有帮助，而简单问题只需要很小一部分信息即可回答，较小分块就足够了。

参数选择没有无一刀切的解决方案。
- 性能可能因数据规模和类型、使用的语言模型、应用等而大相径庭。

### TruLens

需要评估工具来评估质量。 [TruLens](https://www.trulens.org) 的用武之地。

[TruLens](https://www.trulens.org) 是一个开源库，用于评估和跟踪语言模型应用(如RAG)的性能。通过TruLens，还可以利用语言模型本身来评估输出、检索质量等。

构建语言模型应用时，多数人最关心的问题是**幻想**。RAG 在很大程度上通过为语言模型提供检索上下文来确保准确信息，但无法百分百保证。因此评估对验证应用中不存在幻想至关重要。

TruLens 提供了三项测试：**上下文相关度**、**准确性**和**答案相关度**。
- （1）**上下文相关度**
  - 任何 RAG 应用第一步是检索；为验证检索质量，要确保每个上下文块与输入查询相关。这非常关键，因为语言模型将使用该上下文生成答案，所以上下文中的任何不相关信息都可能被编织成幻想。
- （2）**准确性**
  - 检索上下文后，它被语言模型形成答案。语言模型往往偏离提供的事实，对正确的答案进行夸张或扩展。为验证应用的准确性，应将回复分为独立语句，并在检索上下文中独立查证每个语句的证据支持。
- （3）**答案相关度**
  - 最后，回复仍须有助于回答原始问题。通过评估最终回复与用户输入的相关度来验证这一点。

无幻想 RAG
- 通过对上述三项达到满意的评估，可以对应用的**正确性**做出细微陈述；它在知识库限度内经验证无幻想。换言之，如果向量数据库仅包含准确信息，则 RAG 提供的答案也准确。

代码见[原文](https://mp.weixin.qq.com/s/xAn827Q31fdQ8t7jkLIoUQ)

[TruLens](https://github.com/truera/trulens) provides a set of tools for developing and monitoring neural nets, including large language models. This includes both tools for evaluation of LLMs and LLM-based applications with `TruLens-Eval` and deep learning explainability with `TruLens-Explain`. 

How it works
- ![](https://www.trulens.org/img/trulens-diagram.svg)

```sh
pip install trulens-eval
pip install trulens
```

使用方法介绍  官方文档[langchain_quickstart](https://www.trulens.org/trulens_eval/langchain_quickstart/)
- [Langchain Quickstart](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.17.0/trulens_eval/examples/quickstart/langchain_quickstart.ipynb), 实践出错，[详情](https://github.com/truera/trulens/issues/545)
- [Llama-Index](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.17.0/trulens_eval/examples/quickstart/llama_index_quickstart.ipynb)
- [text2text_quickstart](https://github.com/truera/trulens/blob/releases/rc-trulens-eval-0.17.0/trulens_eval/examples/quickstart/text2text_quickstart.ipynb)


## RAG 问题

RAG 面临的七大挑战及解决方案
- • **缺失内容**：数据清理和提示工程，确保输入数据的质量并引导模型更准确地回答问题。
- • **未识别出**的最高排名：调整检索参数和优化文件排序来解决，以确保向用户呈现最相关的信息。
- • **背景不足**：扩大处理范围和调整检索策略至关重要，以包含更广泛的相关信息。
- • **格式错误**：改进提示、使用输出解析器和 Pydantic 解析器实现，有助于按照用户期望的格式获取信息。
- • **不完整部分**：查询转换来解决，确保全面理解问题并作出回应。
- • **未提取部分**：数据清洗、消息压缩和 LongContextReorder 是有效的解决策略。
- • **特定性不正确**：可以通过更精细化的检索策略如 Auto Merging Retriever、元数据替换等技巧来解决问题，并进一步提高信息查找精度。

通过对 RAG 系统挑战的深入分析和优化，不仅可以提升LLM的准确性和可靠性，还能大幅提高用户对技术的信任度和满意度。
- 以上方案的[代码实现](https://www.53ai.com/news/qianyanjishu/2024060334169.html), [架构图](https://api.ibos.cn/v4/weapparticle/accesswximg?aid=81998&url=aHR0cHM6Ly9tbWJpei5xcGljLmNuL3N6X21tYml6X3BuZy9uVUQ1RE13cW8wNmdKYmlhdkhuQXlYVXRtQXFoelVuYUQ3eDJCWHNSYUFzQ0I1aWJ4b3JFbzdwaFpoNjN3c2pOcjBHcWljQWpLZDRqcE0zdVZ0aDQ0SGhVQS82NDA/d3hfZm10PXBuZyZhbXA=;from=appmsg)

RAG 常见问题及解法
- 模型在解析查询内容时可能存在理解不足的问题，其在通过检索文本并总结出答案的能力上还有提升空间。例如，当询问“张三和李四谁更高？”时，如果知识库中张三和李四的身高信息并未在同一数据块（chunk）中同时出现，那么检索返回的文本信息密度较低，意味着需要整合多个数据块的内容才能得出正确的答案。可以通过query的改写来缓解这个问题，即将复杂问题拆解为多个子问题，每个子问题会从提供部分答案的相关文件检索答案。然后，收集这些中间结果，并将所有部分结果合成为最终响应。
- **数据分块**导致数据**语义不完整性**：基于规则或长度的分块可能会将一个完整的语义单元切割成多个块，或者将多个不同的语义单元混合在一个块中，导致信息的不完整或混淆。信息冗余：如果分块大小设置不当，可能会导致一个块中包含大量与查询无关的信息，增加了信息处理的负担，并可能降低检索的效率。忽视语义边界：规则或长度的分块可能不遵循自然语言的语义边界，如句子、段落或主题的边界，这会影响检索的准确性和效率。对文本结构敏感：不同的文本结构（如列表、表格、段落）可能需要不同的分块策略。基于规则或长度的方法可能无法适应这些多样性。这些问题可能导致RAG效率低、耗时长、答案准确性和相关度低等问题。使用基于语义的分块方法，如阿里的SequenceModel等。


如何解决**多实体提问**问题？
- **子问题分解**: 将多实体问题分解为多个单实体的子问题，然后逐一回答子问题，最后对答案汇总，给出最终答案。子问题分解，通常对模型的推理能力有较高的要求。

如何确定合适的 embedding？
- 通过检索器的性能衡量Embedding 效果，选择被广泛接受的两个指标：`Hit Rate`和 `Mean Reciprocal Rank` (MRR)。
- `命中率`（Hit Rate）：命中率计算在前k个检索到的文档中找到正确答案的查询的百分比。简单地说，这是关于我们的系统在前几次猜测中正确的频率。
- `平均倒数排名`（MRR）：对于每个查询，MRR通过查看排名最高的相关文档的排名来评估系统的准确性。具体来说，它是所有查询中这些排名的倒数的平均值。因此，如果第一个相关文档是最高结果，则倒数为1；如果是第二个，则倒数为1/2，依此类推。

RAG 如何处理LLM漏答或者回答不完整的问题？
- 的一个好方法是添加查询理解层——在实际查询向量存储之前添加查询转换。

四种不同的查询转换:
- **路由**: 保留初始查询，同时确定它所属的工具的适当子集，将这些工具指定为合适的查询工作。
- **查询重写**: 但以多种方式重新表述查询，以便在同一组工具中应用查询。
- **子问题**: 将查询分解为几个较小的问题，每个问题针对不同的工具。
- `ReAct`: 根据原始查询，确定要使用哪个工具，并制定要在该工具上运行的特定查询。

生成增强检索（`GAR`）和检索增强生成（`RAG`）
- `GAR` 是 retrieval-generation 协作框架的一部分，迭代retrieval-generation协同框架在每次迭代中包含两个步骤：（1）生成增强检索（GAR）：利用上一次迭代的输出来扩展查询，以帮助检索更多相关文档；（2）检索增强生成（RAG）：利用检索到的文档来生成新文档来回答问题。
- `GAR` 思想与Query改写的子问题分解思想类似。


## RAG 工具

【2023-11-7】RAG 工具包 [tigerlab](https://www.tigerlab.ai/), 三大功能
- TigerRag: Enhanced Retrieval Capabilities customized for your own data
- TigerTune: Boost performance with swift and simple models
- TigerArmor: Ensuring Secure and Responsible AI Interactions
- ![](https://user-images.githubusercontent.com/148816206/279892794-6616f960-1dc0-4e70-b44e-b34e20730152.png)

github: [tiger](https://github.com/tigerlab-ai/tiger)

### RAGxplorer 可视化

【2024-1-26】[大模型RAG流程可视化的开源工具—RAGxplorer](https://mp.weixin.qq.com/s/gCuARWfFPg5mP9vYJEKgNg)

[RAGxplorer](https://github.com/gabrielchua/RAGxplorer) 是一个交互式的streamlit工具，用于支持构建基于检索增强生成（Retrieval Augmented Generation, RAG）的应用程序，通过可视化文档块和嵌入空间中的查询来实现。
- •文档上传：用户可以上传PDF文档。
- •块配置：配置块大小和重叠的选项。
- •嵌入模型选择：all-MiniLM-L6-v2或text-embedding-ada-002。
- •向量数据库创建：使用Chroma构建向量数据库。
- •查询扩展：生成子问题和假设答案，以增强检索过程。
- •交互式可视化：使用Plotly来可视化块。

```sh
pip install -r requirements-local-deployment.txt
# 设置OPENAI_API_KEY（必需）和NYSCALE_API_KEY（如果您需要anyscale）。复制.streamlit/secrets.example.toml文件到.streamlit/secrets.toml并填写值。
streamlit run app.py
```

```py
import('pysqlite3')
import sys

sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
```


## RAG 不足

向量数据库打天下的方案已经不够了，于是各种花式疗法，从**构建索引**到**回复生成**，眼花缭乱：
- 内容切片不够好，容易切碎，于是有了**段落智能划分**；
- 向量生成的**质量不可控**，于是根据不同QA场景动态生成向量的Instructor；
- **隐式动态向量**不够过瘾，再用`HyDE`做个中间层：
- 先生成一些**虚拟文档/假设文档**再做召回，提升召回率；
- 如果向量这一路召回不够，再上**关键词召回**，传统BM25+向量HNSW融合各召回通路；
- 召回太多容易干扰答案生成，探究一下 Lost in the Middle，搞一搞trick，或者用LLMLingua压缩；
- 嫌召回太麻烦？直接扩到100k窗口全量怼进大模型，`LongLoRA`横空出世；

刚才提到的各个环节需要改进的点太多，懒得手工做，直接交给大模型，用`Self-RAG`替你完成每个步骤……

作者：[瀚海方舟](https://www.zhihu.com/question/625481187/answer/3279041129)

对金融行业有限的查询需求的洞察（已脱敏），列举一些RAG存在的挑战和解决思路。


【2024-3-7】[大模型RAG行业适配过程中的一些思考](https://zhuanlan.zhihu.com/p/682625461?utm_psn=1748870868057579520)

回答准确性分析
- 优势：小范围描述式问答回答精准
- 劣势：
  - 1、不擅长**关系型推理**；
  - 2、不擅长**时间跨度长**的问题。如：分析一下二战的一共有多少次战役。

如何提升准确率
- prompt优化: 减少问题中的错别字，并尽量对于提问内容表达详细。
  - 原始用户查询: “跟我说说托尼”
  - 用bard重新表述：”托尼的政治背景是什么，最显著的成就是什么，政治观点是什么“
- 保持embedding模型在同一个平面上。
- chunk的创建策略：其中，chunk大小是优质的超参，微软通过实验给出了各超参尺寸下的recall值，可以进行参考。
- 文本分割的策略：根据微软的实验结果，多的overlapping对于召回也会有比较好的提升。
- embedding模型语义提取能力有限，多主体、多回合的语料库不如简单语料更加有效，微软分析，最小的chunk大小是512 tiokens，因此，要做好分块的数据实验，根据自身情况调整块大小与分块策略。

各超参尺寸下的recall值

|每个向量输入token数|Recall@50|
|---|---|
|512|42.4|
|1024|37.5|
|4096|36.4|
|8191|34.9|

文本分割策略

|chunk(512 tokens)划分策略|Recall@50|
|---|---|
|token边界断开|40.9|
|保留完整句子|42.4|
|10% chunk重叠|43.4|
|25% chunk重叠|43.9|

通用优化策略：
- 大量**重叠**chunk 减少信息的丢失，以此提高准确率；
- 引入**知识图谱**，RAG将关系存储到图数据库中，以此保留关系信息。


### 【一】**世界知识&私有知识混淆**

```json
提问：乙烯和丙烯的关系是什么？
```

大模型应该回答两者都属于有机化合物，还是根据近期产业资讯回答，两者的价格均在上涨？

### 【二】**召回结果混淆**

```text
提问：化学制品行业关注度排名第几？
```

如果我们召回的文档如下：

```text
（某日）……对化学制品行业的关注度近一周下降，目前降至第8……
（某日）……对化学制品行业的关注度近一周下降，目前降至第7……
（某日）……对化学制品行业的关注度近一周下降，目前降至第6……
（此处省略多条相似结果）
```

经试验，大模型见此状会神经错乱。

### 【三】**多条件约束失效**

```text
提问：昨天《独家新闻》统计的化学制品行业的关注度排名第几？
```

如果说【二】中的约束太少导致召回结果过于泛滥，那么我加上约束之后，如何让大模型读懂什么叫“昨天”，又有哪段内容属于《独家新闻》？

### 【四】**全文/多文类意图失效**

```text
提问：近期《独家新闻》系列文章对哪些行业关注度最高？
```

受限于文档切割，遇到横跨多篇文章，或全篇文章的提问，基本上**凉凉了** 。

### 【五】**复杂逻辑推理**

```text
提问：近期碳酸锂和硫酸镍同时下跌的时候，哪个在上涨？
```

除非原文中有显性且密集型相关内容，大模型可能能够直接回答正确，否则凉凉的概率极高。而用户在提问这类问题时，往往是无法在某一段落中直接找到答案的，需要深层次推理。

### 【六】**金融行业公式计算**

```text
提问：昨天哪些股票发生了涨停？
```

如何让大模型理解“涨停”意味着 (收盘价/昨日收盘价-1)≥10%


## RAG优化


【2024-6-5】[LlamaIndex团队技术报告：“RAG的尽头是Agent”](https://mp.weixin.qq.com/s/wuyMN7CLAT9HGYlmjLWUtA)

LlamaIndex 团队2024年Talk：
- 报告人：Jerry Liu, LlamaIndex co-founder/CEO，
- 报告主题：“超越RAG：构建高级上下文增强型大型语言模型（LLM）应用”，
- 主题原文：“Beyond RAG: Building Advanced Context-Augmented LLM Applications”。

看完报告的感受: “RAG的尽头是Agent”

概要内容如下：
- **RAG局限性**：
  - RAG最初是为**简单问题**和**小型文档集**设计的，包括数据解析、索引检索和简单的问答。
  - 然而，处理更复杂的问题时存在局限性，例如：**总结**整个年度报告、**比较**问题、**结构化分析**和**语义搜索**等。

Pain Points 痛点

There's certain questions we want to ask where naive RAG will fail.Examples:
- 总结 Summarization Questions: "Give me a summary of the entire \<company\>10K annual report”
- 对比 Comparison Questions: "Compare the open-source contributions ofcandidate A and candidate B”
- 结构分析+语义搜索 Structured Analytics + Semantic Search: "Tell me about the risk factors ofthe highest-performing rideshare company in the us"
- 多方观点 General Multi-part Questions:"Tell me about the pro-x arguments in articleA, and tell me about the pro-Y arguments in article B, make a table based onour internal style guide, then generate your own conclusion based on these facts.”

- **Agent引入**：为了解决RAG的局限性，文档提出了引入Agent的概念。
  - Agent是一种更高级的系统，它能够执行多轮对话、查询/任务规划、工具使用、反思和记忆维护等更复杂的功能。

从RAG到Agent的转变：提到了从RAG到Agent的转变，这涉及到增加以下几个层次的功能：
- 多轮对话：与用户进行更深入的互动。
- 查询/任务规划层：能够理解并规划复杂的查询和任务。
- 工具接口：与外部环境进行交互，使用工具来辅助任务执行。
- 反思：能够自我评估并改进执行过程。
- 记忆：维护用户交互的历史，以提供个性化服务。

Agent不同层次：从简单到高级Agent的不同层次，包括：
- 简单Agent：成本较低，延迟较低，但功能有限。
- 高级Agent：成本较高，延迟较高，但提供更复杂的功能，如动态规划和执行。
- ReAct：ReAct（Reasoning + Acting with LLMs），这是一个结合了推理和行动的LLM系统，它利用查询规划、工具使用和记忆来执行更复杂的任务。
- LLMCompiler：一个Agent编译器，用于并行多功能规划和执行，它通过生成步骤的有向无环图（DAG）来优化任务执行。

- 自我反思和可观察性：Agent能够通过自我反思和反馈来改进执行，同时提供可观察性，以便开发者能够追踪和理解Agent的行为。
- 多Agent系统：多Agent系统的概念，其中多个Agent可以同步或异步地交互，以执行更复杂的任务。




### Agentic RAG

【2024-6-19】[Agentic RAG 与图任务编排](https://zhuanlan.zhihu.com/p/704229450)

（1）朴素 RAG 流程
- 用户提出问题 -> 系统基于用户提问召回 -> 对召回结果进行重排序 -> 拼接提示词后送给 LLM 生成答案
- ![](https://pic2.zhimg.com/80/v2-67419924f4b4af90a2ca2a87a8df34c5_1440w.webp)
- 问题: 用户意图并不明确时，无法通过直接检索找到答案
  - 例如：针对多文档的总结类提问，需要进行**多步推理** (Reasoning) 等等。
- 解决方法: Agentic RAG


（2）Agentic RAG 是基于 Agent 的 RAG。

Agent 与 RAG 关系紧密，两者互为基石。

Agentic RAG 和简单 RAG 的最大区别
- Agentic RAG 引入了 Agent 的**动态编排**机制，可根据用户提问的不同意图，引入**反馈**和**查询改写**机制，并进行“多跳”知识推理，从而实现对复杂提问的回答。

Agentic RAG 方法
- 初级: `Self-RAG` 
- 高级: `Adaptive RAG `

`Self-RAG` 引入反思机制。
- 从知识库中检索出结果后，评估结果是否**与用户提问相关**。
  - 如果不相关，就要改写查询
- 然后重复 RAG 流程直到相关度评分达到要求。

![](https://pic4.zhimg.com/80/v2-063736cdf25e35ab24f86627708a1b8b_1440w.webp)

Self-RAG 实现两大组件：
- 一套基于 Graph 的**任务编排系统**。
- Graph 内执行的必要算子：比如在 Self-RAG 中，**评分算子**就至关重要。
  - 原始论文需要自己训练一个**打分模型**来针对检索结果评分；
  - 在实际实现中也可以采用 **LLM 进行评分**，简化系统开发并且减少对各类环节依赖。

`Self-RAG` 是相对初级的 Agentic RAG，RAGFlow 中也已提供了相关实现。

实践证明
- `Self-RAG` 对于较复杂多跳问答和多步推理可以明显提升性能。

`Adaptive RAG` 根据用户提问的不同意图，采用对应策略：
- 开放域问答：直接通过 LLM 产生答案而无需依赖 RAG 检索。
- 多跳问答：首先将多跳查询分解为更简单的单跳查询，重复访问 LLM 和 RAG 检索器来解决这些子查询，并合并它们的答案以形成完整答案。
- 自适应检索：适用于需要多步逻辑推理的复杂问题。复杂的问答往往需要从多个数据源综合信息并进行多步推理。自适应检索通过迭代地访问 RAG 检索器和 LLM，逐步构建起解决问题所需的信息链。

`Adaptive-RAG` 工作流程与 `Self-RAG` 类似，只是在前面增加了一个查询分类器，就提供了更多种对话的策略选择。
- ![](https://pic4.zhimg.com/80/v2-26eded60f4fe1605c4a3d6ce328b0603_1440w.webp)

高级 RAG 系统基于任务编排系统上提供以下功能：
- 复用已有的 Pipeline 或者子图。
- 与包含 Web Search 在内的外部工具协同工作。
- 可以规划查询任务，例如查询意图分类，查询反馈等等。

任务编排系统类似的实现: Langchain 的 ️LangGraph 和 llamaIndex；

Agent 开发框架包括: AgentKit、Databricks 最新发布的 Mosaic AI Agent Framework 等等。 

[RAGFlow](https://ragflow.io/) 是一款基于深度文档理解构建的开源 RAG（Retrieval-Augmented Generation）引擎。RAGFlow 可以为各种规模的企业及个人提供一套精简的 RAG 工作流程，结合大语言模型（LLM）针对用户各类不同的复杂格式数据提供可靠的问答以及有理有据的引用。

### RAG优化方向

【2023-12-11】RAG优化分为两个方向：RAG基础功能优化、RAG架构优化
- （1）**RAG基础功能优化**: 针对 文档块切分、文本嵌入模型、提示工程优化、大模型迭代
  - 文档块切分：设置适当的块间重叠、多粒度文档块切分、基于语义的文档切分、文档块摘要。
  - 文本嵌入模型：基于新语料微调嵌入模型、动态表征。
  - 提示工程优化：优化模板增加提示词约束、提示词改写。
  - 大模型迭代：基于正反馈微调模型、量化感知训练、提供大context window的推理模型。
  - query召回文档块集合：元数据过滤、重排序减少文档块数量。
- （2）**RAG架构优化**
  - Vector+**KG** RAG：
    - 问题：向量数据库无法获取长程关联知识、信息密度低（尤其当LLM context window较小时不友好）
    - 解法：增加与向量库平行的KG（知识图谱）上下文增强策略
  - **Self**-RAG：去掉无关内容
    - 问题：传统RAG对召回上下文无差别合并，但召回上下文与query无关或者矛盾时，应舍弃这个上下文，尤其当大模型上下文窗口较小时非常必要（目前4k的窗口比较常见）
    - 解法：判断query是否需要检索。如果需要才检索若干passage，然后经一系列处理生成若干next segment候选。最后，对这些候选segment进行排序，生成最终的next segment。
  - 多向量检索器**多模态**RAG：有三种工作模式
    - 半结构化RAG（文本+表格）
    - 多模态RAG（文本+表格+图片）
    - 私有化多模态RAG（文本+表格+图片）

图片详情见原文：[检索增强生成（RAG）有什么好的优化方案？ - AI pursuer的回答](https://www.zhihu.com/question/628651389/answer/3321989558)

RAG工作流
- ![](https://pica.zhimg.com/80/v2-1348c243eee5fbe59e890d56df8a5736_1440w.webp?source=2c26e567)

### 提升RAG性能的 10 种方法

【2023-10-1】[提升RAG性能的 10 种方法](https://mp.weixin.qq.com/s/WDV31S3C7YQKekwJTIYt5Q)

使用 LangChain 或 LlamaIndex 等框架的快速入门指南，任何人都可以使用大约五行代码构建一个简单的 RAG 系统，例如文档的聊天机器人。

但是，用这五行代码构建的机器人不会很好地工作。RAG 很容易制作原型，但很难达到用户满意的地步。基本教程可能会让 RAG 以 80% 的速度运行。但要弥补接下来的 20%，通常需要进行一些认真的实验。

提高RAG性能的 10 种方法
- 清理数据：提升数据质量，优化数据分布
- 探索不同索引类型：索引是LlamaIndex和LangChain的核心
  - RAG 标准方法涉及嵌入和相似性搜索，将上下文数据分块，嵌入所有内容，当查询到来时，从上下文中找到相似的部分。
  - 这种方法效果很好，但并不是适合每个用例的最佳方法。
- 尝试多种分块方法
  - 将上下文数据分块是构建 RAG 系统的核心
  - 块大小很重要。较小的块通常可以改善检索，但可能会导致生成过程缺乏周围的上下文；
  - 小、中、大块大小循环浏览了每一组，发现小是最好的
- 覆盖基本提示: 使用时覆盖基础提示
  - 示例： 你是一名客户支持代理。您的目的是在仅提供事实信息的同时尽可能提供帮助。你应该友好，但不要过于健谈。`上下文信息如下。给定上下文信息而不是先验知识，回答查询`。
- 元数据过滤
  - 元数据（如日期）添加到块中，然后用它来帮助处理结果
  - 构建 RAG 时要记住的一般概念：<span style='color:red'>相似 ≠ 相关</span>
- 查询路由
  - 适用场景：有多个索引，如摘要、敏感问题识别、日期相关，优化成一个索引不一定好
- 重排名
  - 重新排名是解决**相似性**和**相关性**之间差异问题的一种解决方案
  - 如 Cohere Rereanker，LangChain 和 LlamaIndex 都有抽象，可以轻松设置。
- 查询转换（改写）
  - 将用户查询放入基本提示中来更改它
  - 重新措辞：如果系统找不到查询的相关上下文，让LLM重新措辞查询并重试
  - HyDE 是一种策略，接受查询，生成假设的响应，然后将两者用于嵌入查找。研究发现这可以显着提高性能。
  - 将一个查询分解为多个问题（子查询），LLM在分解复杂查询时往往会工作得更好
- 微调embedding模型
  - 基于嵌入的相似性是 RAG 的标准检索机制
  - 预训练模型（如 OpenAI的ada）关于嵌入空间中相似内容的概念可能与场景上下文中相似内容不一致
    - 处理法律文件：希望嵌入更多地基于您的领域特定术语（例如“知识产权”或“违反合同”）对相似性的判断，而不是基于“特此”和“协议”等一般术语。
  - 微调可以将检索指标提高 5-10%，LlamaIndex 可以生成训练集
- LLM 开发工具
  - LlamaIndex 或 LangChain 这两个框架都提供调试工具，允许定义回调、查看使用的上下文、检索来自哪个文档等等。
  - Arize AI 有一个笔记本内工具，可探索如何检索哪些上下文及其原因。
  - Rivet 是一个提供可视化界面的工具，可帮助您构建复杂的代理，由法律技术公司 Ironclad 开源。

其它：【2023-9-27】[检索增强生成 (RAG):What, Why and How?](https://mp.weixin.qq.com/s?__biz=MzkzNDQxNDU1Ng==&mid=2247484067&idx=1&sn=1eafa47e700526ecd9862f7c39587738&chksm=c2bcd310f5cb5a06db8832792c4d810e1a53a26b4435195cba6adf5848e8a253b6150f102834&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect)
- 混合搜索：将语义搜索与关键词搜索结合起来，从向量存储中检索相关数据 —— 已被证明对大多数用例都能获得更好的结果
- 摘要：对块进行摘要并将摘要存储在向量存储中，而不是原始块
- 丢失问题：LLMs并不给予输入中所有标记**相同权重**。中间标记似乎比输入开头和结尾处的标记被赋予较低权重，中间丢失问题。
  - 可重新排列上下文片段，使最重要的片段位于输入**开头**和**结尾**，并将次要片段放置在中间位置。

### RAG 血泪史

【2023-11-13】[RAG探索之路的血泪史及曙光](https://zhuanlan.zhihu.com/p/664921095), 包含 RAG 工程落地经验，ppt截图和视频

- Embedding 召回方案及局限性分析
  - 朴素 RAG 通过向量召回的诸多局限，比如：**不精确**、**粒度粗**、不支持**条件查询/统计**、不能替代**信息提取**等。
  - 使用 LLM 做信息提取的几种方案和弊端。
- 意图识别优化：<span style='color:red'>传统NLP不是“破落户”</span>
  - 基于**词性标注**和成分**句法分析**（Constituency Parsing、CON），解决**并列**关系的**多实体**、**多条件**提取问题。
  - **意图识别**的重要性
  - 意图识别: **意图分类**和**槽位填充**。
    - 涉及：rule-based 、 BERT fine-tuning，DIET 等。
  - **上下文补全**解决方案
  - 复杂多轮对话中，如何与用户交互,直到其**补全**所有信息。
- 检索优化：从`向量`到`关系`
  - 知识库召回其实不仅仅是 vector store，还可以使用**关系型**数据库和**图数据库**
  - vector store 使用 embedding 召回的上下文补全解决方案
  - 关系型数据的查询方案：小型数据基于 pandas dataframe，大型数据基于 sql
  - 图数据库应用：主要解决多度关系和推荐问题。


## 【2022-8-29】HyDE

**假设文档嵌入**（Hypothetical Document Embeddings, `HyDE`）

2022 年, Gao 等人 论文《[Precise Zero-Shot Dense Retrieval without Relevance Labels]()》

Contriever 源自 2022 年 8 月Izacard 等人发表的早期论文
- 《[Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/pdf/2112.09118.pdf)》
- 神经网络已成为检索的词频方法的良好替代方案，但需要大量数据，而且并不总是能很好地迁移到新的应用领域。因此，设计了一种通过`对比学习`以`无监督`方式训练嵌入网络的方法。
- 该方法能够与 BM25 等无监督词频方法相匹配，并且在跨语言检索方面也表现良好，这是术语匹配方法无法实现的。


改进零样本密集检索的方法（即使用语义嵌入相似性）。

HyDE 不仅与问题进行**相似度搜索**，而是生成一个**假答案**并与之进行相似度搜索。

"假设"文档
> 通过 mask、重排等方式人为构造的虚拟文档。


HyDE 基于 RAG 模型
- 用 LLM 生成一个**假设回答文档**
- 将文档与知识库中的文档进行比较，从而找到与用户问题相关的信息。



### HyDE 原理

HyDE 核心思想
- 对比真实文档和"假设"文档的差异，学习出能够更好捕捉文本语义和上下文信息的文档向量表示。

模型需要学会区分真实文档和这些假设文档，从而得到蕴含深层语义的文档向量。

[img](https://www.4async.com/2024/04/learn-hyde-from-scratch-1/cover_hua9bada4ff4643b8c5d0bd42b9fa020ef_200642_1024x0_resize_box_3.png)
- ![](https://www.4async.com/2024/04/learn-hyde-from-scratch-1/cover_hua9bada4ff4643b8c5d0bd42b9fa020ef_200642_1024x0_resize_box_3.png)

相比原生 RAG，HyDE 和 RAG 合作，多了两步：
- 用 LLM 尝试回答问题
- 生成一个**假设文档**，这个假设的文档是通过 LLM 生成的。

HyDE 两步法
- 步骤1 指令提示语言模型（论文中用 GPT-3）根据**原始查询**生成**假设文档**。
- 步骤2 使用 Contriever 或 “无监督对比编码器”将该假设文档转换为**嵌入向量**，用于下游相似性搜索和检索。

![](https://pica.zhimg.com/80/v2-4d8598bdfbbef05aaf54ad9f91327d72_1440w.webp)

同时，embedding 时使用来源也不同：
- 原生 RAG 使用 用户问题
- HyDE 则用 LLM 生成文档。

HyDE 效果：
- 利用 HyDE 提升 RAG 生成性能：
  - HyDE 生成文档向量更好地捕捉文本语义和上下文信息，有助于 RAG 模型产生更加相关、连贯的生成内容。
- 增强 RAG 模型可靠性：
  - RAG 可以利用外部知识库增强自身知识，提高生成内容的准确性；而 HyDE 则能进一步**约束**生成过程，确保输出结果更加可靠。

参考
- [从零学习 Hypothetical Document Embeddings (HyDE) - 1](https://www.4async.com/2024/04/learn-hyde-from-scratch-1/)

### Langchain 实现

HyDE 实现：生成、嵌入、平均、检索。
- [LangChain 文档HyDE指南](https://python.langchain.com.cn/docs/modules/chains/additional/hyde)
- [Langchain 实现 Demo](https://github.com/ianhohoho/auto-hyde/blob/main/langchain-hyde-demo.ipynb)
- [说明](https://blog.csdn.net/wangjiansui/article/details/139751438)

```py
from langchain.chains import HypotheticalDocumentEmbedder, LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI, OpenAIEmbeddings

# 初始化 embedding 模型
base_embeddings = OpenAIEmbeddings()

# 初始化 LLM: 以下多种方式选其一即可
# ① 生成一个文档
llm = OpenAI() 
# ② 生成多个文档, 取平均
llm = OpenAI(n=4, best_of=4) 
# ③ 自定义 prompt
prompt_template = """请回答关于最近一次国情咨文的用户问题
问题$：{question}
回答$："""

prompt = PromptTemplate(input_variables=["question"], template=prompt_template)
llm_chain = LLMChain(llm=llm, prompt=prompt)

# HyDE 执行 
embeddings = HypotheticalDocumentEmbedder(
    llm_chain=llm_chain, base_embeddings=base_embeddings
)

result = embeddings.embed_query("Where is the Taj Mahal?")

# 使用 web_search 提示加载HyDE
embeddings = HypotheticalDocumentEmbedder.from_llm(llm, 
  base_embeddings, 
  "web_search"
)

# 使用
result = embeddings.embed_query("泰姬陵在哪里？")

# --------- 检索 ----------
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import CharacterTextSplitter

with open("data/state_of_the_union.txt") as f:
    state_of_the_union = f.read()

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_text(state_of_the_union)
docsearch = Chroma.from_texts(texts, embedding_fn, persist_directory="./data/.chroma_db")

query = "What did the president say about Ketanji Brown Jackson"
docs = docsearch.similarity_search(query)

```




### HyDE 局限

局限性
- 问答框架: 
  - 并非所有检索用例都用于问答, 所以 让LLM假设回答问答不一定都有意义
  - 文档分块检索时, 各分块在风格、语气和结构等方面并不一定同质, 难以用一个通用提示捕捉多样性
- 用户表达: 多数意图表达模糊


### AutoHyDE


【2024-9-12】[AutoHyDE：使HyDE更好地用于高级LLM RAG](https://zhuanlan.zhihu.com/p/714886313?utm_psn=1817475798434721793)

假设文档嵌入(HyDE) 已被证明是一种强大的查询重写形式，可提高检索文档的相关性。

但 现有HyDE实现并非开箱即用，不够灵活

AutoHyDE，提高 LLM RAG 假设文档嵌入（HyDE）的有效性、覆盖率和适用性的**半监督**框架。

AutoHyDE **自动**发现索引文档中的**潜在相关模式**，并生成直接表示这些相关性模式的假设文档。
- 直接调整了 LangChainHypotheticalDocumentEmbedder 类来实现 AutoHyDE，以便它仍然可以与 LangChain 的其他部分链接在一起。

AutoHyDE 主要改进: **假设文档生成方式**。
- 没有使用固定提示（无论是 LangChain 预定义, 还是用户自定义）
- 而是设计框架, 自动发现所有文档中可能被基线检索忽略的潜在相关模式
- 并为每一种模式生成假设文档。

通过这种方式，我们能够使 HyDE 适应各种各样的任务和环境，并适应相关模式异构的索引的检索。

AutoHyDE 主要目的
- 自动发现向量数据库中各种**相关模式**, 并生成各种文档, 以提高这些模式的**覆盖率**。

主要步骤
- 从query中提取关键词
- 初步检索
- 获取包含关键词的忽略文档
- 被忽略文档聚类
- 生成假设文档
- 嵌入
- 合并

技术上，采用 LangChain 中 HypotheticalDocumentEmbedder 类的现有实现，并创建了新函数来实现 AutoHyDE，这样它就可以立即作为任何 RAG 链的一部分运行


代码实现
- [auto-hyde-demo.ipynb](https://github.com/ianhohoho/auto-hyde/blob/main/auto-hyde-demo.ipynb)

```py
# ------- Define LLM and Embeddings -------
from langchain_openai import OpenAIEmbeddings
# embedding 定义
base_embeddings = OpenAIEmbeddings()

# LLM 定义
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4-1106-preview", temperature=1)
# ------- Split Documents -------
from langchain_community.document_loaders import TextLoader

loader = TextLoader("data/util.txt")
documents = loader.load()

from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(separator='.', chunk_size=1200, chunk_overlap=0)
docs = text_splitter.split_documents(documents)
# ------- Define Chroma Vector DB -------
from langchain_community.vectorstores import Chroma

db = Chroma.from_documents(docs, 
                           base_embeddings, 
                           collection_metadata={'hnsw:space': 'cosine'})

# ------- Instantiate improved HypotheticalDocumentEmbedder --------
from src.auto_hyde import HypotheticalDocumentEmbedder

hyde_embedder = HypotheticalDocumentEmbedder(
    llm_chain=llm, base_embeddings=base_embeddings
)
# Define Custom HyDE params
hypo_params = {
    'baseline_k': 20,
    'exploration_multiplier': 5,
    'verbose': True
}
# Get Embedding
query = 'what is the relationship between justice and happiness?'
hyde_embedding = hyde_embedder.embed_query(text=query, db=db, hypo_params=hypo_params)

hyde_embedding[:10]
```


## 【2023-11-15】CoN 承认不会


【2023-11-15】腾讯AI Lab

Chain-of-Note：大模型检索增强生成更加鲁棒
- 论文链接：[CoN](https://arxiv.org/pdf/2311.09210.pdf)

缺乏知识的情况下，基于检索增强的语言模型应当能够**承认自己的“无知”**。为了应对这些挑战，作者引入Chain-of-Noting（CON）的方法，提高基于检索增强的语言模型面对嘈杂、不相关文档以及处理未知场景的鲁棒性。

CON 核心思想
- 为检索到的文档生成**连续阅读笔记**，使其能够彻底评估与给定问题的相关性，并将这些信息整合以生成最终答案。

三种不同类型的CoN
- 语言模型根据检索到的信息生成最终答案
- 检索到的文档虽然没有直接回答query，但提供了上下文，使得语言模型能够将这些信息与其固有知识结合起来，从而推导出答案
- 语言模型遇到不相关文档并缺乏回应所需知识的情况，能够承认自己的“无知”


## 【2023-11-14】FILCO 提前过滤不相关


【2023-11-14】CMU 提出

FILCO：检索增强生成加一道“过滤”
- 论文链接：[Learning to Filter Context for Retrieval-Augmented Generation](https://arxiv.org/pdf/2311.08377.pdf)

在开放领域问答和事实验证等任务中，如何改进检索增强型生成系统的性能。
- 生成回答时，会检索相关知识，但由于检索系统并不完美，有时会提供**部分或完全不相关的信息**。这可能导致生成模型过度或不足地依赖于上下文信息，进而产生错误的输出，例如虚构信息。
- 论文提出一种名为 FILCO 的方法，它通过词汇和信息论方法来识别有用的上下文，并训练上下文过滤模型在测试时过滤检索到的上下文。

FILCO 方法通过两个步骤改善生成器提供的上下文质量：
- (1) 基于词汇和信息论方法识别有用的上下文；
- (2) 训练上下文过滤模型在测试时过滤检索到的上下文。

## 【2024-2-13】GraphRAG

【2023-8-15】GraphRAG 基于知识图谱的搜索增强

### GraphRAG 发布

【2024-2-13】微软 首次宣布推出 GraphRAG
- [GraphRAG: Unlocking LLM discovery on narrative private data](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)
- GraphRAG 对复杂信息进行文档分析, 用 LLM 生成知识图, 显着提高问答性能
- 便于在私有数据集上增强RAG能力, 如: 企业专有研究、商业文档或通信。

传统 RAG 不擅长 
- 通过共享属性遍历不同信息, 提供新的综合见解: 
- 全面理解大型数据集,甚至单个大型文档, 概括语义概念；
  - 传统 RAG 依赖于数据集中语义相似文本内容的矢量搜索。查询中没有任何内容可以将其定向到正确的信息。
  - 数据中排名前 5 位的主题是什么？

GraphRAG 用LLM基于**私有数据集**创建知识图谱。然后与`图机器学习`一起使用，查询时执行提示增强。 
- GraphRAG 在回答上述两类问题上，提升明显，展示了优于之前应用于私有数据集的其他方法的智能或掌握能力。

问题：
- Query: “What is Novorossiya?” 问题： “什么是新俄罗斯？”
- 基线RAG 和 GraphRAG 表现良好

|类型|示例|Baseline RAG|GraphRAG|分析|
|---|---|---|---|---|
|常规|“What is Novorossiya?” <br> “什么是新俄罗斯？”|所提供的上下文中未提及“Novorossiya”一词。然而，为了提供一些信息，Novorossiya（新俄罗斯）是一个历史术语，指的是乌克兰南部的一个地区，该地区在 18 世纪被俄罗斯帝国征服。近年来，俄罗斯支持的乌克兰东部分离主义者重新使用了这个词来指代他们声称拥有主权的领土，但国际机构并未正式承认它是一个独立的实体。|新俄罗斯，又称新俄罗斯，是一个具有历史和当代意义的术语。历史上，它指的是俄罗斯帝国南部的一个地区，现在是乌克兰的一部分。在当代背景下，它被亲俄分离主义者用来指代乌克兰的某些地区[数据：实体（6494, 912, 6493）]。<br>“Novorossiya”一词在乌克兰的政治运动中被使用，特别是俄罗斯民族主义者。这一运动与破坏乌克兰稳定和创建所谓的“Novorossiya”的努力有关 [ 数据：实体（912）；关系（15211、15206） ]。该术语还被用来表示乌克兰的一个分离主义国家，称为新俄罗斯联邦共和国 (FRN) [数据：实体 (7690)]。|二者表现都可以|
|需要连接|What has Novorossiya done?”<br>“新俄罗斯做了什么？”|该文本没有提供有关 Novorossiya 所做工作的具体信息。|乌克兰的政治运动 Novorossiya 参与了一系列破坏性活动，特别是针对乌克兰的各种实体 [实体 (6494, 912)]。该运动与摧毁多个乌克兰实体财产的计划有关，包括罗森、敖德萨罐头厂、敖德萨地区广播电视传输中心和乌克兰国家电视公司[ 关系（15207、15208、15209、15210）] 。除此之外，Novorossiya 还参与了摧毁 Roshen、敖德萨音乐学院工厂、敖德萨地区广播电视传输中心和乌克兰国家电信公司财产的计划 [关系 (8329、8330、8331、8332)] 。此外，Novorossiya 还参与了炸毁乌克兰最大国有银行 PrivatBank ATM 机的计划 [Relationships (8335, 8335)]。<br>乌克兰总检察长办公室报告了 Novorossiya 的成立，表明政府对该运动的活动的认识和潜在关注 [Relationships (8264, 8264)]。新俄罗斯的破坏活动对目标实体产生了重大影响，并可能加剧了该地区持续的紧张局势和冲突。<br>总之，“新俄罗斯”参与了一系列针对乌克兰境内各个实体的破坏性活动。这些活动对目标实体产生了重大影响，并可能加剧了该地区持续的紧张局势和冲突。乌克兰政府通过总检察长办公室承认了新俄罗斯组织的存在和活动，表明对该运动的行为表示一定程度的关注。|源文档没有任何文本段讨论 Novorossiya，导致此Baseline RAG失败;<br>而GraphRAG 方法在查询中发现了一个实体 Novorossiya, 据此进一步探索，LLM用来通过图中两个实体之间存在的关系来支持特定银行是 Novorossiya 目标的断言|
|综合理解|“What are the top 5 themes in the data?“<br>“数据中排名前 5 的主题是什么？ “|1. 提高城市生活质量和城市发展。<br>2. 弗拉基米尔·普京与阿拉伯联合酋长国总统穆罕默德·本·扎耶德·阿勒纳哈扬的会晤。<br>3. 俄罗斯各地区投资环境国家评级。<br>4. 俄罗斯经济状况和国内品牌的成长。<br>5. 关于生命意义和科学在理解生命中的作用的讨论。|数据呈现了广泛的主题，但最流行的五个主题可以确定如下：<br>1.冲突和军事活动：很大一部分数据围绕乌克兰持续的冲突，有大量报告详细介绍了各种实体的军事活动、占领和袭击。该主题包括乌克兰军队、俄罗斯军队以及其他各种军事单位和实体的行动[ 数据：报告（513、241、151、493、343，+更多）]。...(省略)...|Baseline RAG 主题都与两国之间的战争没有太大关系;<br>GraphRAG结果与整个数据集中发生的情况更加一致。答案提供了五个主要主题以及数据集中观察到的支持细节。|
||||||

用LLM生成的知识图，GraphRAG 极大地改进了 RAG 的“检索”部分，用更高相关性的内容填充上下文窗口，从而获得更好的答案并捕获证据来源。

### GraphRAG 开源+私人数据

【2024-7-4】[下一代 RAG 技术来了！微软正式开源 GraphRAG：大模型行业将迎来新的升级](https://mp.weixin.qq.com/s/UIUWdvSiBWFq6sqqsumbzw)

7月2日，微软开源 GraphRAG，一种基于图的检索增强生成 (RAG) 方法，可以对**私有或以前未见过**的数据集进行问答。
- [GraphRAG: New tool for complex data discovery now on GitHub](https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/)
- 开源地址：[graphrag](https://github.com/microsoft/graphrag)

GraphRAG 极大增强 LLM 在处理私有数据时的性能，同时具备连点成线的跨大型数据集的复杂语义问题推理能力。普通 RAG 技术在私有数据，如企业的专有研究、商业文档表现非常差，而 GraphRAG 则基于前置的知识图谱、社区分层和语义总结以及图机器学习技术可以大幅度提供此类场景的性能。

GraphRAG 方法：
- 利用大型语言模型 (LLMs) 从数据源中提取知识图谱；
- 将此图谱聚类成不同粒度级别的相关实体社区；
- 对于 RAG 操作，遍历所有社区以创建“社区答案”，并进行缩减以创建最终答案。

大规模播客以及新闻数据集上进行了测试，在全面性、多样性、赋权性方面，结果显示 GraphRAG 都优于朴素 RAG（70~80% 获胜率）。

查询语句： 对比 原始RAG 和 GraphRAG
- Novorossiya 是什么？ —— 两者差异不大
- Novorossiya 做了什么？—— GraphRAG > 原始RAG（回答失败）

缺陷
- token 使用和推理时间都会增加......


### GraphRAG 自动适配新领域

【2024-9-9】[GraphRAG auto-tuning provides rapid adaptation to new domains](https://www.microsoft.com/en-us/research/blog/graphrag-auto-tuning-provides-rapid-adaptation-to-new-domains/)

LLM 在一组特定于领域的提示的指导下，读取所有源内容并提取相关信息，包括**实体**和**关系**，然后将其用于构建图表。

例如，在分析新闻文章时，人物、地点和组织等实体很重要。在这里，关系类型可能包括“居住”、“领导”和“拥有”。
- 每个域都有一组不同的实体和关系类型。例如，在化学领域，实体类型包括分子、酶和反应，而关系类型包括“催化”和“还原”。
- 尽管 GraphRAG 中的默认新闻域提示在应用于化学时可以生成图表，但它们无法捕获化学家期望的特定内容。

新闻文章用的所有提示都是手动生成。为了简化这个过程，开发了一个**自动化工具**，生成特定于域的提示，这些提示经过调整并可供使用。

该工具遵循类似人类的方法；向LLM提供了文本数据样本（例如，10,000 篇化学论文中的 1%），并指示其生成它认为最适用于内容的提示。现在，借助这些自动生成和调整的提示，可立即将 GraphRAG 应用到选择的新领域，并相信将获得高质量的结果。

GraphRAG 三个主要索引提示：
- **实体和关系提取**：识别存在的所有实体并建立它们之间的关系。
- 实体和关系**摘要**：将实体实例及其关系合并为一个简洁的描述。
- 社区**报告**生成：为构建的知识图谱中的每个社区生成摘要报告。


提取提示包括：
- 提取说明：向LLM提供如何执行提取的指导。
- Few-shot Examples ：为LLM提供值得提取的实体和关系类型的真实例子。
- 真实数据：充当占位符，被源内容块替换。
- Gleanings ：鼓励LLM在多个回合中提取更多信息。

Auto-tuning architecture 自动调优架构

自动调整获取源内容并生成一组自动生成的特定于域的提示
- ![](https://www.microsoft.com/en-us/research/uploads/prod/2024/08/Fig1_Algorithm-conceptual-diagram.png)

源内容的样本发送给LLM ，LLM 首先识别域，然后创建适当的角色 - 与下游代理一起使用来调整提取过程。一旦建立了域和角色，就会并行发生多个过程来创建我们的自定义索引提示。这样，就可以根据实际领域数据并从角色的角度生成少量提示。



### 解决什么问题

查询：
> “告诉我所有关于苹果和乔布斯的事”

基于《乔布斯自传》这本书进行问答，而这个问题涉及到的上下文分布在自传这本书的 30 页（分块）时
- 传统方法 “**分割数据，Embedding 再向量搜索**”, 在多个文档块里用 top-k 去搜索,**很难**得到这种分散，细粒的完整信息。
- 而且，这种方法还很容易**遗漏**互相关联的文档块，从而导致信息检索不完整。

知识图谱可以减少基于**嵌入**语义搜索所导致的不准确性。
- 例子：“保温大棚”与“保温杯”，尽管在语义上两者是存在**相关性**的，但在大多数场景下，这种**通用语义**（Embedding）下的相关性常常并不希望产生，进而作为错误的上下文而引入“幻觉”。

这时候，保有**领域知识**的知识图谱则是非常直接可以缓解、消除这种幻觉的手段。

### 基本流程

增加一路与向量库平行的KG（知识图谱）上下文增强策略
- ![](https://picx.zhimg.com/80/v2-971de4e9e7562db93ba913111f0dfba5_1440w.webp?source=2c26e567)
- ![](https://pic2.zhimg.com/80/v2-85243e4a9ce90fd15ca948843dc28b19_1440w.webp)

query进行KG增强是通过NL2Cypher模块实现的。可用更简单的图采样技术来进行KG上下文增强。具体流程是：
- 根据query抽取**实体**
- 然后把实体作为种子节点对图进行**采样**（必要时，可把KG中节点和query中实体先向量化，通过向量相似度设置种子节点）
- 然后把获取的子图转换成文本片段，从而达到上下文增强的效果。

Graph RAG 可简单实现：
- 使用LLM(或其他)模型从问题中提取关键实体。
- 根据这些实体检索子图，深入到一定的深度（例如，2）。
- 利用获得的上下文利用LLM产生答案。

可用 Llama Index轻松搭建 Graph RAG，甚至整合更复杂的 RAG 逻辑，比如 [Graph+Vector RAG](https://gpt-index.readthedocs.io/en/latest/examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.html)。
- ![](https://github-production-user-asset-6210df.s3.amazonaws.com/1651790/260618959-f783b592-7a8f-4eab-bd61-cf0837e83870.png)

Llama Index 有两种方法实现 Graph RAG：
- `KnowledgeGraphIndex` 从任何私有数据只是**从零构建知识图谱**（基于 LLM 或者其他语言模型），然后 4 行代码进行 Graph RAG。
- `KnowledgeGraphRAGQueryEngine` 在任何**已经存在的知识图谱**上进行 Graph RAG，不过还没有完成这个 PR。


### 分析


GraphRAG 的优势主要体现在以下三个方面：
- 更高的准确度和更完整的答案（运行时间/生产效益）
  - 2023 年底，数据目录公司 Data.world 发表一项研究，GraphRAG 在 43 个业务问题中平均将 LLM 响应的准确性提高了 3 倍。
  - 2024年2月，微软：GraphRAG 极大地改进了 RAG 的‘检索’部分，用更高相关性的内容填充上下文窗口，从而得到更好的答案并捕获证据出处。 ” GraphRAG 所需 token 减少了 26% 到 97%，这不仅使其能够更好地提供答案，而且更便宜、更具可扩展性。
  - Linkedin: GraphRAG 提高了回答客户服务问题的正确性和丰富性（因此也提高了实用性），为其客户服务团队将每个问题的平均解决时间缩短了 28.6% 。
- 一旦创建了知识图谱，构建、维护 RAG 应用程序更加容易（开发时间优势）
- 更好的可解释性、可追溯性和访问控制（治理优势）

治理：可解释性、安全性等

### text2cypher

基于图谱的 LLM 的另一种有趣方法是text2cypher。
- 不依赖于实体的子图检索，而是将任务/问题翻译成一个面向答案的特定图查询，和 text2sql 方法本质是一样的。

`text2cypher` 根据 KG 的 Schema 和给定的任务生成图形模式查询，而 `SubGraph RAG`获取相关的子图以提供上下文。

得益于 LLM，实现 text2cypher 比传统的 ML 方法更为简单和便宜。
- LangChain: [NebulaGraphQAChain](https://python.langchain.com/docs/use_cases/more/graph/graph_nebula_qa)
- Llama Index: [KnowledgeGraphQueryEngine](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/knowledge_graph_query_engine.html) 

3 行代码就能跑起来 text2cypher。


### 示例

【2024-7-17】[GraphRAG实战](https://zhuanlan.zhihu.com/p/709216702?utm_psn=1796814936342667265)

【2024-7-15】 [GraphRAG制作的《凡人修仙传》知识图谱长什么样](https://www.youtube.com/watch?v=pWqQS0OX2Qs)
- 用 LLM 从小说中抽取 `节点`、`边`、`属性`信息
- 实体类别: `person`, `event`, `geo`, `organization`, `cencept`, `role`, `technology`
- 以 图方式检索、排序，附带权重

<iframe width="560" height="315" src="https://www.youtube.com/embed/pWqQS0OX2Qs?si=9N4QTnBquh7K3JdU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


### 结合 Neo4j


【2024-8-8】[将微软GraphRAG集成到Neo4j中](https://zhuanlan.zhihu.com/p/713201715)
- 将 GraphRAG 生成的 Parquet 文件导入到 Neo4j 中, [ms_graphrag_import.ipynb](https://github.com/tomasonjo/blogs/blob/master/msft_graphrag/ms_graphrag_import.ipynb)
- 用LangChain和LlamaIndex实现检索器。

```py
import pandas as pd
from neo4j import GraphDatabase
import time

GRAPHRAG_FOLDER="artifacts"

NEO4J_URI="bolt://localhost"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="password"
NEO4J_DATABASE="neo4j"

driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))

def batched_import(statement, df, batch_size=1000):
    """
    Import a dataframe into Neo4j using a batched approach.
    Parameters: statement is the Cypher query to execute, df is the dataframe to import, and batch_size is the number of rows to import in each batch.
    """
    total = len(df)
    start_s = time.time()
    for start in range(0,total, batch_size):
        batch = df.iloc[start: min(start+batch_size,total)]
        result = driver.execute_query("UNWIND $rows AS value " + statement, 
                                      rows=batch.to_dict('records'),
                                      database_=NEO4J_DATABASE)
        print(result.summary.counters)
    print(f'{total} rows in { time.time() - start_s} s.')    
    return total

statements = """
create constraint chunk_id if not exists for (c:__Chunk__) require c.id is unique;
create constraint document_id if not exists for (d:__Document__) require d.id is unique;
create constraint entity_id if not exists for (c:__Community__) require c.community is unique;
create constraint entity_id if not exists for (e:__Entity__) require e.id is unique;
create constraint entity_title if not exists for (e:__Entity__) require e.name is unique;
create constraint entity_title if not exists for (e:__Covariate__) require e.title is unique;
create constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique;
""".split(";")

for statement in statements:
    if len((statement or "").strip()) > 0:
        print(statement)
        driver.execute_query(statement)

doc_df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_documents.parquet', columns=["id", "title"])
doc_df.head(2)

# import documents
statement = """
MERGE (d:__Document__ {id:value.id})
SET d += value {.title}
"""

batched_import(statement, doc_df)
```

更多源码见笔记
- [ms_graphrag_import.ipynb](https://github.com/tomasonjo/blogs/blob/master/msft_graphrag/ms_graphrag_import.ipynb)

![](https://pic2.zhimg.com/80/v2-19c55b1ccd80ba395bd9f05a21326145_1440w.webp)


## Vector RAG

基于embedding的 RAG方法，常见


## 【2023-10-7】RAG-Fusion

【2023-10-7】[使用RAG-Fusion和RRF让RAG在意图搜索方面更进一步](https://mp.weixin.qq.com/s/N7HgjsqgCVf2i-xy05qZtA)
- 原文: [Forget RAG, the Future is RAG-Fusion](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1)
- [第三方来源](https://luxiangdong.com/2023/10/07/ragfusion/#/%E6%B7%B1%E5%85%A5%E7%A0%94%E7%A9%B6RAG-Fusion%E7%9A%84%E6%9C%BA%E5%88%B6)，包含图片

### 起因

RAG有许多优点:
- 向量搜索融合： RAG通过将向量搜索功能与生成模型集成，引入了一种新的范例。这种融合能够从大型语言模型(大语言模型)生成更丰富、更具上下文感知的输出。
- 减少幻觉： RAG显著降低了LLM的幻觉倾向，使生成的文本更基于数据。
- 个人和专业实用程序：从个人应用程序如筛选笔记到更专业的集成，RAG展示了在提高生产力和内容质量方面的多功能性，同时基于可信赖的数据源。
然而，我发现越来越多的“限制”:

当前搜索技术限制： 
- RAG受到基于检索的**词法**和**向量搜索技术**的相同限制。
- 人工搜索效率低下：人类并不擅长在搜索系统中输入他们想要的东西，比如打字错误、模糊的查询或有限的词汇，这通常会导致错过明显的顶级搜索结果之外的大量信息。虽然RAG有所帮助，但它并没有完全解决这个问题。
- 搜索的**过度简化**：流行的搜索模式将查询线性地映射到答案，缺乏深度来理解人类查询的多维本质。这种线性模型通常无法捕捉更复杂的用户查询的细微差别和上下文，从而导致相关性较低的结果。


### 为什么使用RAG-Fusion?

为什么使用RAG-Fusion
- 通过生成**多个**用户查询和**重新排序**结果来解决RAG固有的约束。
- 利用**倒数排序融合**（RRF）和自定义向量评分加权，生成全面准确的结果。

RAG-Fusion 弥合用户明确提出的问题和（原本的意图）打算提出的问题之间的差距，更接近于发现通常仍然隐藏的变革性知识。
- [RAG-Fusion代码](https://github.com/Raudaschl/rag-fusion)

### RAG-Fusion 机制

RAG Fusion的基本三要素与RAG相似，并在于相同的三个关键技术:
- 通用编程语言，通常是Python。
- 专用的向量搜索数据库，如Elasticsearch或Pinecone，指导文档检索。
- 强大的大型语言模型，如ChatGPT，制作文本。

然而，与RAG不同，RAG-Fusion通过几个额外步骤来区分自己——查询生成和结果的重新排序。

RAG-Fusion’s 工作步骤:
- 查询语句的相关性复制(多查询生成)：通过LLM将用户的查询转换为**相似但不同**的查询。
  - 单个查询可能无法捕获用户感兴趣的全部范围，或者它可能太窄而无法产生全面的结果。这就是从不同角度生成多个查询的原因。
- 并发的向量搜索：对原始查询及其新生成的同级查询执行并发的向量搜索。
- 智能重新排名：聚合和细化所有结果使用倒数排序融合(RRF)。
- 最后优中选优：将精心挑选的结果与新查询配对，引导LLM进行有针对性的查询语句输出，考虑所有查询和重新排序的结果列表。
- ![](https://simg.baai.ac.cn/hub-detail/9d417a2e8e269db73c441281edd48cd31696839601937.webp)

### 多查询生成

工作原理:
- 对语言模型的函数调用:函数调用语言模型(在本例中为chatGPT)。这种方法需要一个特定的指令集(通常被描述为“系统消息”)来指导模型。例如，这里的系统消息指示模型充当“AI助手”。
- 自然语言查询:然后模型根据原始查询生成多个查询。
- 多样性和覆盖范围:这些查询不仅仅是随机变化。它们是精心生成的，以提供对原始问题的不同观点。例如，如果最初的查询是关于“气候变化的影响”，生成的查询可能包括“气候变化的经济后果”、“气候变化和公共卫生”等角度。
- ![](https://luxiangdong.com/images/ragfusion/4.jpeg)

### 倒数排序融合 (RRF)

Why RRF?
- 倒数排序融合(RRF) 是一种将具有不同相关性指标的多个结果集组合成单个结果集的方法，不同的相关性指标也不必相互关联即可获得高质量的结果。

该方法的优势在于不利用相关分数，而仅靠排名计算。相关分数存在的问题在于不同模型的分数范围差。RRF是与滑铁卢大学(CAN)和谷歌(Google)合作开发
- [cormacksigir09-rrf](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)
- “比任何单独的系统产生更好的结果，比标准的重新排名方法产生更好的结果”。

在elasticsearch的8.8版本，已经引入了RRF。

RRF 想象成那种坚持在做决定之前征求每个人意见的人，这种意见是有帮助的，兼听则明，多多益善
- ![](https://luxiangdong.com/images/ragfusion/6.jpeg)

### 生成输出

用户意图保存
- 使用多个查询的挑战之一是可能会削弱用户的原始意图。为了缓解这种情况，我们指示模型在prompt工程中给予原始查询更多的权重。


### RAG-Fusion优缺点

优势
- 1、优质的原材料质量
  - 使用RAG Fusion时，搜索深度不仅仅是“增强”，并且其实搜索范围已经被放大了。相关文档的重新排序意味着你不仅仅是在抓取信息的字面意思，而是在深入这个搜索的意图，所以会涉及到更多的优质文档和待搜索内容。
- 2、增强用户意图对齐
  - RAG Fusion的设计理念中包含了自动提示，很多时候我们在搜索的时候并不知道应该怎么描述，像Google、百度就会进行输入框的自动补全提示。RAG Fusion可以捕获用户信息需求的多个方面，从而提供整体输出并与对用户意图进行增强。
- 3、自动为用户查询输入纠错
  - 该系统不仅可以解释用户的查询，还可以精炼用户的查询。通过生成多个查询变体，RAG Fusion执行隐式拼写和语法检查，从而提高搜索结果的准确性。
- 4、导航复杂查询（自动分解长句的意图）
  - 人类的语言在表达复杂或专门的思想时往往会结结巴巴。该系统就像一个语言催化剂，生成各种变体，这些变体可能包含更集中、更相关的搜索结果所需的行话或术语。它还可以处理更长的、更复杂的查询，并将它们分解成更小的、可理解的块，用于向量搜索。
- 5、搜索中的意外发现（关联推荐）
  - 以前在亚马逊买书的时候，总能因为相关推荐发现我更想要的书，RAG Fusion允许这个偶然的发现。通过使用更广泛的查询范围，系统有可能挖掘到信息，而这些信息虽然没有明确搜索，但却成为用户的“啊哈”时刻。这使得RAG Fusion有别于其他传统的搜索模型。

挑战
- 1、过于啰嗦的风险
  - RAG-Fusion的深度有时会导致信息泛滥。输出可能会详细到令人难以承受的程度，把RAG-Fusion想象成一个过度解释事物的朋友。
- 2、可能成本会比较昂贵
  - 多查询输入是需要LLM来做处理的，这时候，很有可能会引起更多的tokens消耗。


## 【2023-10-9】DeepMind: Step-Back Prompting 

【2023-10-27】[DeepMind新技术Step-Back Prompting，可提升RAG应用效果，让大模型学会抽象思考](https://mp.weixin.qq.com/s/3WKUrAI_MflqqdwKIyoDEg)
- [退一步，看得更远：通过抽象引发大型语言模型中的推理](https://zhuanlan.zhihu.com/p/663680218)

Zero-Shot-CoT领域，最近几天（10.3）刚刚有一个新的研究《Large Language Models as Analogical Reasoners》提出，通过类比推理提示（Analogical Prompting）可以让大模型自己生成相似问题做为例子，从而再根据例子步骤形成思维链来解决新问题。

观察：很多任务都很复杂，充满了细节，大语言模型（LLMs）很难找到解决问题所需的相关信息。
- 物理问题：“如果一个理想气体的温度翻了一番，体积增加了八倍，那么它的压强 P 会发生什么变化？” LLM 在直接解答这个问题时可能会忽略**理想气体定律**的基本原则。
- 询问“Estella Leopold 在 1954 年 8 月到 11 月期间就读于哪所学校？”的问题，由于**时间范围非常具体**，直接回答也是非常困难的。

思考
- 在这两种情况下，通过**退一步**提问，能够帮助模型更有效地解决问题。

谷歌DeepMind 10月9日提了一项新技术“Step-Back Prompting”，简称`后退提示`（STP），不是类比寻找相似示例，而是让LLMs**自己抽象问题**，得到更高维度概念和原理，再用这些知识推理并解决问题。这种思维模式非常类似于人类解决问题的方式，让大模型能够借鉴已有规律解决问题。
- [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/abs/2310.06117)

`退一步问题`为从原始问题中派生出来的、层级更高的**抽象问题**。
- 不直接问 “Estella Leopold 在特定时间段内的学校是哪所”，而是问一个更高层次的问题：“Estella Leopold 的教育历史是怎样的？”
- 通过回答这个更抽象的问题，获得解答原始问题所需的所有信息。

通常来说，`退一步问题`比`原始问题`更容易回答。基于这种抽象层次的推理有助于避免中间步骤的错误，链式思维提示的例子一样。总的来说，退一步提示法包含两个简单的步骤：
- 抽象：我们首先提示 LLM 提出一个关于更高层次概念或原则的通用问题，并检索与之相关的信息，而不是直接回答原始问题。
- 推理：在获取了关于高层次概念或原则的信息后，LLM 可以基于这些信息对原始问题进行推理。我们将这种方法称为基于抽象的推理。

`后退提示`（STP）可以和RAG相结合，利用`后退提示`获得的抽象问题，获得更多与最终答案需要的的上下文信息，然后，再将获得的上下文和原始问题一起提交给LLM，从而让LLM获得更好的回答质量。
- ![](https://pic1.zhimg.com/80/v2-54afd626bd6d05b95562deae90ecdcd4_1440w.webp)

PaLM-2L模型做了实验，发现这种Prompt技巧能显著提升推理任务（STEM、知识问答、多步推理）的性能表现。
- step-back prompting与rag配合使用的方式 相比较于baseline提升了39.9%，相较于单纯RAG应用，提升了21.6%的效果。

工程实现
- 该技术已经被langchain支持
- [colab尝试](https://github.com/langchain-ai/langchain/blob/master/cookbook/stepback-qa.ipynb)

```py
response_prompt_template = """You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.

{normal_context}

Original Question: {question}
Answer:"""
response_prompt = ChatPromptTemplate.from_template(response_prompt_template)

chain = (
    {
        # Retrieve context using the normal question (only the first 3 results)
        "normal_context": RunnableLambda(lambda x: x["question"]) | retriever,
        # Pass on the question
        "question": lambda x: x["question"],
    }
    | response_prompt
    | ChatOpenAI(temperature=0)
    | StrOutputParser()
)

chain.invoke({"question": question})
```

## 【2023-10-17】华盛顿大学: Self-RAG

【2023-10-17】华盛顿大学 [Self-RAG：通过自我反思实现检索增强生成](https://zhuanlan.zhihu.com/p/662969847)

检索增强生成（Retrieval-Augmented Generation，`RAG`）方法通过检索相关知识来减少这类问题，降低了LLMs在知识密集型任务中的事实错误率）。但是，会存在如下问题
- <span style='color:red'>不加区别地检索和合并一定数量的检索文段</span>，无论是否**需要检索**或文段**是否相关**，这会降低LLMs的**多功能性**或导致生成质量不佳（Shi等人，2023），因为不加区别地检索文段，无论事实支持是否有帮助。
- <span style='color:red'>生成结果与检索段落未必一致</span>（Gao等人，2023），因为这些模型没有明确训练以利用和遵循所提供文段的事实。
- ![](https://pic1.zhimg.com/80/v2-86a0ca09dc393444588990f487fdfa00_1440w.webp)

论文提出一种新框架：`自我反思检索增强生成`（[SELF-RAG](https://selfrag.github.io/)），通过**按需检索**和**自我反思**来提高LLM的生成质量，包括其事实准确性，而不损害其多功能性。
- [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/abs/2310.11511)
- [SELF-RAG](https://selfrag.github.io/) 主页包含代码、模型和数据

论文以端到端方式训练**任意LLM**来学习反思自身的生成过程，通过生成任务输出和间歇性的特殊token（即反思token）。
- 反思token分为**检索**和**评论**token，分别表示检索的需求和生成的质量（图中右侧）。

具体做法如下：
- 给定输入提示和先前的生成，[SELF-RAG](https://selfrag.github.io/)
- 首先，确定继续生成增加检索文段是否**有所帮助**。如果是,输出一个检索标记，以便按需调用一个检索模型（步骤1）。
- 随后，[SELF-RAG](https://selfrag.github.io/)同时处理**多个检索文段**，评估相关性，然后生成相应的任务输出（步骤2）。
- 然后，生成**评论标记**来评估输出，并选择在事实准确性和整体质量方面最好的生成（步骤3）。
  - 这个过程与传统RAG（图1左侧）不同，后者不管检索是否有必要（例如，底部示例不需要事实知识），都会一律检索固定数量的文档进行生成，并且从不第二次访问生成质量。
- 此外，[SELF-RAG](https://selfrag.github.io/)为每个部分提供**引文**，附带自我评估是否输出受文段支持，从而简化了事实验证。

实验证据表明
- SELF-RAG 在 6个任务上**明显优于**经过预训练或指令学习的LLMs，以及更高引用准确性的RAG方法，达到sota。
- SELF-RAG 在 4个任务上优于具有检索增强功能的`ChatGPT`，`Llama2-chat`和`Alpaca`, 在所有任务中的性能更好。

Self-RAG outperforms vanilla ChatGPT or LLama2-chat across six tasks, and outperforms those SOTA models with widely-used retrieval-augmentation methods in most tasks by large margin

论文分析证明了使用**反思标记**进行训练和推理对整体性能提升以及测试时模型自定义（例如，在引文预测和完整性之间的权衡）的有效性。

Connections to Prior Work
- v.s. **Retrieval-augmented Generation** 与传统RAG相比，`Self-RAG`针对多样性任务自适应检索,并评估相关性，更加灵活
  - `Standard RAG` only retrieves once or fixed number of time steps, while `Self-RAG` enables **Adaptive retrieval** for diverse task inputs, and can retrieve multiple times while generations, or completely skip retrieval, making it more suitable for diverse downstream queries (e.g., instruction-following).
  - `Self-RAG` carefully criticize retrieved passages or its own generations via reflection tokens and incorporate hard or soft constrained during decoding, while `standard RAG` does not assess relevance of passages or whether the output is indeed supported by the passages.
- v.s. **Learning from Critique** (Feedback) `Self-RAG`再多个参考结果中调整奖励权重，不需要训练
  - Reflection tokens are inserted offline by another Critic model trained on machine-generated feedback, making training much more memory efficient and stable than widely adopted RLHF methods (e.g., PPO).
  - `Self-RAG` enables tailored behaviors by simply adjusting reward weights across multiple preference aspects, while prior fine-grained feedback learning method requires training for different model behaviors.

【2024-7-11】[手把手代码复现Self RAG](https://mp.weixin.qq.com/s/_80ndeV9RVtyLdkQtGt96g)



## 【2023-10-28】Agent RAG 

- OpenAI community [Standard RAG + Agent Solution](https://community.openai.com/t/standard-rag-agent-solution/454605)

### AutoGen

【2023-10-28】[Retrieval-Augmented Generation (RAG) Applications with AutoGen](https://microsoft.github.io/autogen/blog/2023/10/18/RetrieveChat/)
- 基于 AtuoGen 的 RAG
- ![](https://microsoft.github.io/autogen/assets/images/retrievechat-arch-959e180405c99ceb3da88a441c02f45e.png)

AutoGen 构建一个能够进行智能搜索和信息检索的聊天应用。**用户**可以通过自然语言查询代理提出问题或请求信息，然后**搜索代理**会从互联网或其他数据源中检索相关信息，并通过自然语言代理将结果返回给用户。

AutoGen 的 RAG系统由两个代理组成，都从 AutoGen 的内置代理扩展而来
- `RetrieveUserProxyAgent`: 人类代理，也能执行代码、调用函数
  - `code_execution_config` 可关闭自动执行
  - 默认不启用 LLM，可以通过配置文件 `llm_config` 开启
  - 指定文档集合路径。随后下载文档，分割成特定大小的块，计算嵌入，并存储在矢量数据库中
- `RetrieveAssistantAgent`: 检索增强代理，与 LLM 交互，可以执行LLM生成的Python代码
  - 包含一个向量数据库

相关配置
- `llm_config` LLM 配置信息

- RAG 代理的定制
  - 定制嵌入功能、文本分割功能和矢量数据库。
- RAG 代理的两种高级用法，即
  - 与**群聊**集成
  - 使用 Gradio 构建聊天应用程序

Diverse Applications Implemented with AutoGen
- ![](https://microsoft.github.io/autogen/assets/images/app-b0acafd5e331fa9471ab6d0e7010a83d.png)


## 【2023-11-15】Chain-Of-Note

【2023-11-23】腾讯 AI Lab [Chain-Of-Note：解决噪声数据、不相关文档和域外场景来改进RAG的表现](https://zhuanlan.zhihu.com/p/667713224?utm_psn=1710784683930124288)
- [CHAIN-OF-NOTE: ENHANCING ROBUSTNESS IN RETRIEVAL-AUGMENTED LANGUAGE MODELS](https://arxiv.org/pdf/2311.09210.pdf)

RAG检索增强生成已经成为llm的重要推动者。最值得注意的是，随着RAG的引入，模型幻觉得到了很大程度的抑制，RAG也可以作为模型性能的均衡器。

RAG（或 RALMs） 面临的挑战是确保在推理时向LLM提供准确、高度简洁和上下文相关的数据。但是
- 不相关数据的检索可能导致错误响应，并可能导致模型忽略（overlook）其固有的知识，即使它拥有足够的信息来处理查询。
- 很难评估内在、检索的知识对于得出正确输出是否够用
- 知识不足时，系统应该输出“不知道”

CoN 作为一种新的方法，提高RAG的弹性。特别是在RAG数据不包含与查询上下文相关的明确信号的情况下。
- 提高 RAG 在噪声、不相关文档、未知情形的鲁棒性

### CoN 原理

CoN框架由三种不同的类型组成，阅读笔记。
- 类型(A)显示了检索到的数据或文档回答查询的位置。LLM仅使用NLG从提供的数据中格式化答案。
- 类型(B)中，检索到的文档不直接回答查询，但是上下文洞察足以使LLM将检索到的文档与它自己的知识结合起来，从而推断出答案。
- 类型(C)是指检索到的文档是不相关的，LLM没有相关的知识来响应，导致框架没有给出错误或错误的答案。
- ![](https://pic2.zhimg.com/80/v2-703d951c929356034224b87b59b976d5_1440w.webp)
- ![](https://pic4.zhimg.com/80/v2-0b067343e916dc504a47d47807b5b4e7_1440w.webp)

CoN框架为检索到的文档生成顺序的**阅读注释**，从而能够系统地评估从外部文档检索到的信息的相关性和准确性。

通过创建顺序阅读笔记，该模型不仅评估每个文档与查询的**相关性**，而且还确定这些文档中最**关键**和最**可靠**的信息片段。这个过程有助于过滤掉不相关或不可信的内容，从而产生更准确和上下文相关的响应。

CoN是一个自适应过程，或逻辑和推理层，其中直接信息与上下文推理和法学硕士知识识别相平衡。

为了使模型具有生成NoC阅读笔记的能力，需要进行微调。
- 论文训练了一个llama - 27b模型，将笔记能力整合到CON中。

CoN不仅是一个提示模板，而且还包含了一个经过微调的可以记笔记模型。

因此CoN 是 RAG 和 Fine-Tuning 的结合。

这又回到了数据人工智能的概念和数据的四个方面，即数据发现、数据设计、数据开发和数据交付。

一般来说，RAG和具体的CoN可以看作是数据交付过程的一部分。但是为了训练NoC模型，需要一个数据发现、数据设计和数据开发的过程。

使用 ChatGPT 生成训练数据，通过 LLama-2 7b 模型训练，在开放领域问答语料上实验，显著超过传统RAG

### CoN 示例

LangSmith的CoN模板。给定一个问题，查询Wikipedia并使用带有Chain-of-Note提示的OpenAI的API提取答案。
- ![](https://pic4.zhimg.com/80/v2-118883738949279770d0ef11ec4aedbf_1440w.webp)

标准RAG:
> Task Description: The primary objective is to briefly answer a specific question.

带CON的RALM:

```
Task Description: 

1. Read the given question and five Wikipedia passages to gather relevant information. 
2. Write reading notes summarizing the key points from these passages. 
3. Discuss the relevance of the given question and Wikipedia passages. 
4. If some passages are relevant to the given question, provide a brief answer based on the passages.  
5. If no passage is relevant, direcly provide answer without considering the passages.
```


## 【2024-2-23】文档分割

【2024-2-23】文档语义分割
- [一文掌握文本语义分割：从朴素切分、Cross-Segment到阿里SeqModel](https://blog.csdn.net/v_JULY_v/article/details/135386202)

RAG中，embedding 和 文档语义分割、段落分割 是绕不开的关键点，重点梳理下各类典型的语义分割模型

基于 CrossSegmentAttention 的模型（如 `Cross-segmentBERT` 和 `BERT+Bi-LSTM`）以及阿里巴巴开源的 `SeqModel`，对比了在处理文本分割任务时的上下文利用和效率

### CrossSegmentAttention


RAG 场景下，常用文本切块方法基于**策略**，例如大模型应用开发框架提供的 `RecursiveCharacterTextSplitter` 方法，定义**多级分割符**，用上一级切割符分割后的文本块, 如果还是超过最大长度限制，再用第二级切割符进一步切割

Lukasik 等人在论文《[Text Segmentation by Cross Segment Attention](https://blog.csdn.net/v_JULY_v/article/details/135386202)》提出了三种基于transformer的**分割模型**架构。
- 其中一种仅利用每个**候选断点**(candidate break)周围的局部上下文
- 而另外两种则利用来自输入的完整上下文(所谓候选断点指任何潜在的段边界，即 any potential segment boundary)

(1) `Cross-segment BERT`：确定某个句子是否作为下一个段落的开头

分割模型旨在完成**文档分割**任务，预测每个句子是否是文本分段边界
- 在 `Cross-segment BERT` 模型中，围绕潜在段落断点的局部上下文输入到模型中：左边k个标记和右边k个标记
- 其中与`[CLS]`对应的输出隐状态被传递给softmax分类器，以便对候选断点进行分段决策

(2) `BERT+Bi-LSTM`

在BERT+Bi-LSTM模型中，首先用 BERT 对每个句子进行编码，然后将句子表示输入到Bi-LSTM中
- 当用BERT编码每个句子时，所有序列都以`[CLS]`标记开始
- LSTM 负责处理具有**线性**计算复杂度的多样化和潜在的大型句子序列

(3) `Hierarchical BERT`

分层BERT模型中，首先用 BERT 对每个句子进行编码，然后将输出的句子表示输入到基于Transformer的另一个模型中

图见[原文](https://blog.csdn.net/v_JULY_v/article/details/135386202)


### SeqModel

阿里语义分割模型 SeqModel

SeqModel 核心原理
- Cross-Segment 提出基于**本地上下文**的**跨段BERT模型**和**分层BERT模型**（Hier.BERT），利用两个BERT模型对句子和文档进行编码，从而实现更长的上下文建模
- 2020年，论文《Two-level transformer and auxiliary coherence modeling for improved text segmentation》还采用了两个分层连接的Transformer结构(uses two hierarchically con-nected transformers)。然而，由于分层模型计算成本高且推理速度较慢


SeqModel：将文档分割建模为**句子级序列标记**任务

Zhang 等人在论文《Sequence Model with Self-Adaptive Sliding Window for Efficient Spoken Document Segmentation》中提出了SeqModel

SeqModel 利用BERT对**多个句子同时编码**，建模更长的上下文之间依赖关系之后再计算句向量，最后预测每个句子后边是否进行文本分割

此外，该模型还使用了**自适应滑动窗口**方法，在在不牺牲准确性的情况下进一步加快推理速度

图见[原文](https://blog.csdn.net/v_JULY_v/article/details/135386202)

步骤
- 首先，文档中每个句子经过 WordPiece 分词器进行分词
- 然后，通过由“token嵌入、位置嵌入和段落嵌入”组成的输入表示层来对这些句子进行编码
- 这些嵌入被送入Transformer编码器，输出与k个标记对应的隐状态，并使用均值池化方法得到句子编码
- 最后，将所有句子编码输入softmax二元分类器，以判断每个句子是否为段落边界，训练目标是最小化softmax交叉熵损失

自适应滑动窗口(Self-adaptive Sliding Window)

还提出了一种自适应滑动窗口方法，在不牺牲准确性的情况下进一步加快推理速度
- 传统分割推理滑动窗口使用**固定前向步长**。自适应滑动窗口方法，在推理过程中，从前一个窗口中的最后一句话开始，模型在最大后向步长内 向后查看，以找到来自前一个推理步骤的积极分割决策（模型对分割预测概率>0.5）
- 当在这个跨度内有积极的决策时，下一个滑动窗口将自动调整为：从最近预测片段边界之后的下一个句子开始
- 考虑到最后一段和历史对下一个分割决策影响已经降低，这种策略有助于丢弃滑动窗口内不相关的历史信息。因此，自适应滑动窗口既可以加快推理速度也可以提高分割准确性



## 【2024-6-26】UAR

【2024-6-26】[复旦+上海AI Lab提出统一主动检索RAG，减少延迟，提升响应](https://mp.weixin.qq.com/s/4i5lWgTkp1GpsCJzdq6PEg)
- 复旦大学和上海人工智能实验室Qinyuan Cheng等人发表《[Unified Active Retrieval for Retrieval Augmented Generation](https://arxiv.org/pdf/2406.12534)》
- [UAR](https://github.com/xiami2019/UAR) 代码尚未放出

在检索增强型生成（Retrieval-Augmented Generation, RAG）中，如何智能地决定何时使用检索来增强大型语言模型（LLMs）的输出。

RAG中并非所有情况下检索都是有益的，对每个指令都应用检索是次优的。

因此，确定是否进行检索对于RAG来说至关重要，这通常被称为**主动检索**（Active Retrieval）。

主动检索（Active Retrieval）相关的一些研究工作，以下是一些主要的相关研究：
- `Self-RAG` (Asai et al., 2023): 这是一种基于LLM自我意识的方法，只有在模型认为它不知道答案时才触发检索。
- `FLARE` (Jiang et al., 2023): 这种方法基于模型对其生成响应的不确定性来决定是否需要外部检索。
- `SKR` (Wang et al., 2023b): 首先确定模型是否知道问题的答案，如果不知道，则使用检索增强。
- `RA-ISF` (Liu et al., 2024): 与SKR类似，首先评估模型是否知道问题的答案，然后决定是否进行检索。
- `Self-DC` (Wang et al., 2024): 这种方法探讨了何时应该检索和何时应该生成答案，采用自划分和征服的方法处理组合未知问题。

其他一些与大型语言模型的**时间意识**（Time-awareness）和**自我意识**（Self-awareness）相关的研究，例如：
- `TimeQA` (Chen et al., 2021): 用于评估模型处理时效性问题的能力。
- `MULAN` (Fierro et al., 2024): 用于评估语言模型预测可变事实的能力。
- 关于自我意识的研究 (Kadavath et al., 2022; Lin et al., 2022a; Yin et al., 2023; Zhang et al., 2023; Cheng et al., 2024): 这些研究探讨了如何增强语言模型对自己知识范围的认识和表达。

这些相关研究为UAR提供了背景和对比，展示了在主动检索领域中不同方法的优缺点，以及UAR如何通过综合多个标准来改进检索时机的判断。

现有的主动检索方法面临两个挑战：
- 依赖单一标准，难以处理各种类型的指令；
- 依赖于专业化和高度差异化的程序，这使得将它们结合到RAG系统中更加复杂，并导致响应延迟增加。

为了解决这些挑战，提出`统一主动检索`（Unified Active Retrieval, `UAR`）的新框架。

UAR包含四个正交标准，转化为即插即用的分类任务，以最小额外推理成本实现多方面的检索时机判断。

UAR框架解决这个问题的关键步骤和方法：
- 多标准集成：UAR不依赖单一标准，而是将四个正交的检索时机判断标准集成到一个框架中。这些标准包括意图意识（Intent-aware）、知识意识（Knowledge-aware）、时效性意识（Time-aware）和自我意识（Self-aware）。
- 轻量级分类器：对于每个检索标准，UAR使用轻量级的二分类器（例如单层多层感知机，MLP），这些分类器基于固定大型语言模型（LLM）的最后一层隐藏状态来训练。这种方法避免了对整个LLM进行昂贵的微调，并减少了推理成本。
- 统一的检索判断流程：UAR引入了统一主动检索标准（UAR-Criteria），这是一个决策树，它根据多个检索标准设定的优先级顺序，统一地判断何时需要进行检索。
- 即插即用（Plug-and-play）：UAR作为一个即插即用的框架，不需要改变LLM的参数，可以轻松地集成到现有的RAG系统中。
- 标准化处理流程：UAR-Criteria通过标准化的流程处理各种用户指令，确保在不同场景下都能做出一致的检索决策。
- 实验验证：论文通过在四种代表性的用户指令类型上进行实验，验证了UAR在检索时机判断准确性和下游任务性能上的显著优势。
- 资源发布：为了促进未来的研究，作者还发布了UAR的代码、数据、模型和相关资源。
- 通过这些方法，UAR框架能够有效地解决何时应该使用检索来辅助LLM生成响应的问题，同时避免了不必要的检索，提高了系统效率和响应速度。

文章还引入了`统一主动检索标准`（UAR-Criteria），旨在通过标准化程序处理多种主动检索场景。

通过在四种代表性用户指令类型的实验中，UAR在检索时机判断和下游任务性能上显著优于现有工作，证明了UAR的有效性及其对下游任务的帮助。

贡献
- 提出了UAR这一主动检索框架，创建了评估检索时机准确性的Active Retrieval Benchmark（AR-Bench），并在AR-Bench和下游任务上进行了全面的实验，展示了UAR的显著性能提升。
- 此外，发布了代码、数据、模型和相关资源，以促进未来的研究。



## RankRAG

【2024-7-2】[RankRAG，英伟达RAG新思路](https://zhuanlan.zhihu.com/p/708546587)
- 论文 [RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://arxiv.org/abs/2407.02485)


RAG存在一些局限性：
- • 检索器容量的限制。考虑到处理效率，现有RAG一般采用**稀疏检索**（比如BM25）或中等规模的嵌入模型（比如Bert）作为检索器。
- • 只选择**前K个文档**。尽管最新的大语言模型扩大了上下文长度的限制，能够接受更多的上下文作为输入，但是实际性能会随着K的增加而迅速达到饱和。比如在长问答任务中，最佳的分块上下文数量大约是10。虽然更大的K可以提高召回率，但是同时也引入了更多无关的内容，干扰大语言模型生成准确回答。

RankRAG 利用单一大语言模型, 实现**高召回率**的上下文提取和高质量内容生成。通过对单一大语言模型进行指令调优，使其可以同时进行上下文排序和答案生成，进一步提升LLM在RAG检索和生成阶段排除不相关上下文的能力。

RankRAG整体包括两个阶段：指令调优阶段、排名与生成综合指令调优阶段。
- ![](https://pic2.zhimg.com/80/v2-3723a659aa92a0622aff5c81b93599d1_1440w.webp)



## 【2024-9-6】HybridRAG


【2024-9-6】[HybridRAG：VectorRAG+GraphRAG在时序向量图谱数据库AbutionGraph中的一体化实现](https://zhuanlan.zhihu.com/p/718613162?utm_psn=1816525270162743297)

`HybridRAG` (Hybrid Retrieval-Augmented Generation) 结合了两种检索增强 和生成式 (Generation) 方法的自然语言处理技术。
- Retrieval-Augmented，前有 `VectorRAG`-采用AI/向量数据库存算Embedding文本相似度，后有`GraphRAG`-采用KG/知识图谱数据库存查相关联内容

通常用于问答系统、对话系统或文档生成等领域，创新结合检索式方法的信息准确性和生成式方法的流畅性和创造性。

HybridRAG 实现可能涉及到复杂的算法和技术，包括但不限于自然语言理解、机器学习、知识表示和推理等。随着NLP领域的不断发展，HybridRAG有望成为提高语言模型性能的关键技术之一

HybridRAG 通过结合VectorRAG和GraphRAG的特点，旨在提供一种更全面和强大的解决方案。它不仅利用向量数据库检索与查询相关的文本信息，还利用知识图谱来理解和推理文本中描述的实体及其关系。

优势：
1. 信息准确性：通过检索式方法，HybridRAG能够提供准确的信息，确保生成的响应基于可靠的数据。
2. 流畅性和创造性：生成式方法的结合使得HybridRAG能够生成流畅、自然且具有创造性的文本。
3. 上下文理解：通过知识图谱，HybridRAG能够更好地理解文本的上下文和语义关系，从而生成更准确和相关的响应。

应用场景：
1. 问答系统：在问答系统中，HybridRAG可以提供更准确的答案，同时保持对话的自然流畅。
2. 对话系统：在对话系统中，HybridRAG可以生成更自然、更相关的回复，提高用户体验。
3. 文档生成：在文档生成中，HybridRAG可以利用检索到的信息和知识图谱的结构化数据来创建内容丰富、逻辑清晰的文档。

![](https://picx.zhimg.com/80/v2-8b3d4630b6007f7bc1ce0c17083d0097_1440w.webp)

尽管HybridRAG提高了LLM的生成效果，也会带来新的挑战：
1. 系统集成：将两种数据库技术有效地集成到一个系统中需要复杂的设计和实现。
2. 实现定制：不同的业务场景对Graph(Schema)和Vector(原文/摘要)的形式要求存在差异，比如纯文本的数据和代码文本及结构数据文本的问答知识库面向场景不同，则需要个性化定制以确保引用质量。
3. 性能优化：需要优化系统以确保检索和生成过程的效率和响应速度。
4. 数据一致性：确保从不同来源检索的数据在知识图谱中保持一致性和准确性是个挑战。
5. 全局问答/Global Search：架构上还是Local Search 友好型，Global Search支持困难。

AbutionGraph支持静态图谱、时序图谱、向量图谱3种存储方式，相比于向量数据库+图谱数据库的HybridRAG实现方式，我们只需采用AbutionGraph的向量图谱存储方式即可达到HybridRAG的效果，是一种更优化/落地难度/成本更低的选择。实现上，我们可以像增加一个字段的方式轻松保存向量，运用时序图谱的特性，向量会自动完成合并并更新保存。根据向量保存在实体中的层级不同，可以实现例如图谱级、文档级、文本块级、实体级的向量存储，通常，我们使用文档级别即可实现简单高效（分布式）的存用。此外，根据构建图谱中层级的不同，同样可以生成不同粒度的摘要描述，运用流式时序图谱的特性，摘要可以自动合并，或使用轻量级LLM作为聚合函数自动完成摘要更新，时刻保持高质量的图谱信息。特别的，摘要（有文档摘要、文本块摘要、实体和关系的描述）是向量检索中非常重要的数据，我们将摘要向量化后存到向量索引中，实现图谱摘要的模糊检索，在提问时即可一步获取摘要同时得到关联图谱。AbutionGraph具有数据直接Json化的能力，检索输出对接到Prompt即可供大语言模型使用，Pipeline一栈式的构建个性化GraphRAG应用。

HybridRAG（采用：向量图谱数据库）

（向量图谱数据库一体化的HybridRAG方案图）

优势：
1. 继承Vector+Graph RAG的优势：能力更集中的向Graph靠拢，丰富知识库能力。
2. 架构简洁：一个数据库支撑住一个系统，图谱化的过程也很简洁，把更多精力放在业务优化上。
3. 速度提升：图谱简洁-构建快速，整个过程全倚仗大模型的生成速度。
4. 查询简单：得益于向量和图谱的绑定，且是RDF+属性图的架构，相比于微软开源的GraphRAG，整个图谱变得很精炼，问答检索的GQL仅需要一条固定不变查询语句即可完成。
5. 信息准确：通过图向量的检索方法，一体化的HybridRAG能够提供准确的信息，确保生成的响应基于可靠的相关联的数据。
6. 实现定制：可针对不同的业务场景调整/优化数据的存取和快速/便捷的获取，例如结合结构化数据、算法指标数据（出度入读等）、时序数据（年月日维度）等设计分析型业务。
7. 知识融合：不同标签的相同实体通过AbutionGraph的预聚合能力可以实现新增属性、摘要、向量的自定义融合；
8. 全局问答：通过顶级节点的摘要信息可以快速得到全局性的答案，此外，我们在图谱中内置了出入读的实时计算，可以更方便的去做Leiden社区划分；再有，我们有了摘要聚合的信息，也可以通过节点及其关系的聚合摘要来计算相似度来得出文本相关的社区，这会是一种更简便快速的方法；最后，通过AbutionGraph的技术特性，我们可以优化定制出更多针对特定场景的方案。

挑战：
1. LLM工具化/函数化：随着轻量级/嵌入式LLM性能的提升，我们可以把其能力当作AbutionGraph数据库的一个聚合函数来使用，让动态知识图谱认知能力更强。
2. 实体消歧：我们将{实体名称,实体描述}向量化，并采用AbutionGraph的流式聚合能力实时更新实体名称对应TopN个相似名称的字典，采用时序监控能力实时/定时监控大于1个相似名称的values，最后采用LLM重新标准化实体名，并更新图谱的关联关系。

AbutionGraph-时序/向量/图谱数据库的一体化GraphRAG实现方案介绍，利用图向量&向量图谱存储，可以轻松地保存向量和摘要并自动完成合并更新，实现一键相似度检索知识库，简化HybridRAG（VectorRAG+GraphRAG）的实现，使其构建知识库更加高效简便和降低LLM行业垂直应用落地门槛
- ![](https://pic2.zhimg.com/80/v2-5d76065952e70cf262458edb124dac53_1440w.webp)

三种主流技术方案的实现对比

|方法|介绍|优势|问题|图解|
|---|---|---|---|---|
|`VectorRAG`|利用`向量数据库`来存储文本的Embedding形式数据来支持生成任务,通过检索与输入查询相关的文本信息来改进NLP任务，特别是在需要生成有意义和连贯响应的情况下。|确保 LLM 生成的响应与原始输入查询保持一致性和相关性|1. 使用**段落级**分块技术，不适合文档的层次结构性质，可能导致关键上下文信息的丢失。<br>2. 从大型异构语料库中检索的上下文质量可能不一致，可能导致分析的不准确和不完整|![](https://pic2.zhimg.com/80/v2-da83bc975921a6af9a50b25306002791_1440w.webp)|
|`GraphRAG`|GraphRAG 将文本转化为**实体**及其**关系**的三元组集合，再利用`图谱数据库`将三元组集合存储, 实现与输入相关元素的查询检索，是一种将非结构化数据结构化的方法。|知识图谱提供了一种结构化方式来表示和管理知识，有助于高效的查询和推理，已经在多个领域得到广泛应用|1. 构建和维护一个高质量知识图谱，并将不同来源的数据集成到一个连贯的知识图谱中是一个复杂且技术门槛较高的过程。<br>2. 通常在抽象的Q&A任务或提问中没有提到明确的实体时表现不佳|![](https://pic2.zhimg.com/80/v2-598c026d157ede008d7765fd8e809121_1440w.webp)|
|`HybridRAG`|结合VectorRAG和GraphRAG的特点, `向量数据库`+`图谱数据库`|||![](https://picx.zhimg.com/80/v2-8b3d4630b6007f7bc1ce0c17083d0097_1440w.webp)|
|`HybridRAG`|结合VectorRAG和GraphRAG的特点, `向量图谱数据库`||||


将 AbutionGraph 时序向量图谱数据库的 HybridRAG 实现方案与3种主流RAG技术方案进行了功能融合和对比，用于介绍该种创新的一体化RAG技术实现方式：

- 1）VectorRAG：
  - 优势：保持生成响应与输入查询的一致性和相关性。
  - 问题：可能丢失关键上下文信息；检索上下文质量不一致。
- 2）GraphRAG：
  - 优势：利用知识图谱进行高效的查询和推理。
  - 问题：构建和维护高质量知识图谱复杂；在抽象Q&A任务中表现不佳。
- 3）HybridRAG：
  - 优势：结合VectorRAG和GraphRAG，提供全面解决方案；信息准确性和流畅性；上下文理解。
  - 问题：系统架构复杂，定制化开发成本高。
- 4）基于AbutionGraph数据库的HybridRAG实现方案：

AbutionGraph数据库通过向量图谱存储方式，继承了Vector+Graph RAG的优势，简化了HybridRAG的实现，提供了一种构建存储和查询检索更优化、成本效益更高的选择。它支持静态图谱、时序图谱和向量图谱的存储，适用于需要高时效性和生成高质量响应的场景或通常一种数据库无法轻易完成的交叉性场景。


## 【2024-9-10】MemoRAG


RAG 的一个根本挑战: 处理**复杂、模糊查询**和**非结构化**知识。
- 传统 RAG 非常适合提供**明确信息**的**简单问答**任务，但在面对更细微的场景时就会失败。

突破性框架 MemoRAG 通过**集成长期记忆功能**将 RAG 推向新领域，实现更深入的上下文理解和更准确的信息检索。

【2024-9-10】`北京智源人工智能研究院` BAAI 与`中国人民大学` RUC 高瓴人工智能学院联合推出基于**长期记忆**的下一代检索增强大模型框架`MemoRAG`，推动RAG技术从仅能处理**简单QA**任务向应对**复杂一般性任务**拓展。
- 论文: [MemoRAG: Moving Towards Next-Gen RAG Via Memory-Inspired Knowledge Discovery](https://arxiv.org/pdf/2409.05591)
- 代码: [MemoRAG](https://github.com/qhjqhj00/MemoRAG)

`MemoRAG` 提出全新的 RAG模式: 
- “基于**记忆**的**线索生成**——基于**线索指引**的**信息获取**——基于**检索片段**的**内容生成**”
- 实现了复杂场景条件下(尤其是“模糊查询表述”、“高度非结构化知识”) 的精准信息获取。
- ![](https://github.com/qhjqhj00/MemoRAG/raw/main/asset/tech_case.jpg)


`MemoRAG` 是一个基于内存的 RAG 创新性框架，通过**高效**、**超长内存模型**支持各种应用场景。
- 与传统 RAG 不同，`MemoRAG` 利用**记忆模型**来实现对整个数据集的**全局理解记忆**，通过从记忆中生成查询特定线索来增强证据检索，还会从数据集的“记忆”中提取信息，从而生成更准确和上下文丰富的答案。

`MemoRAG` 社区开发非常活跃，此存储库中自9月4日不断发布资源和原型。

MemoRAG 改变游戏规则

MemoRAG 在传统 RAG 系统难以解决的领域表现出色，特别是处理：
- **模糊查询**：即使查询是隐式的或不完整的，MemoRAG 的记忆系统也可以推断用户意图。
- **分布式**信息检索：需要从数据集的多个部分收集信息的任务可以通过 MemoRAG 从记忆中回忆线索并获取相关详细信息的能力轻松处理。
- **复杂摘要**：MemoRAG 可以通过生成关键点和检索支持证据将大型非结构化数据集浓缩为连贯的摘要。

实际应用

`MemoRAG` 在需要**复杂信息检索**和**高级理解**的领域特别有效，例如：
- 法律文件分析：详细背景和精确度至关重要。
- 财务数据汇总：从大量数据中提取关键趋势至关重要。
- 对话式应用：MemoRAG 能够记忆并回顾之前的交流，这使其成为长期对话式 AI 应用的强大工具。


MemoRAG 对于司法、医疗、教育、代码等现实场景中的领域知识密集型任务的处理展示出了极高潜力。


### MemoRAG 工作原理

MemoRAG 是双模型系统架构，采用两种不同的模型：
- `记忆模型`：轻量级、远程语言模型创建了数据集的**全局记忆**。它充当知识库，在非常长的上下文（100万个token）中压缩和保留关键信息。该模型生成线索或部分答案，指导相关信息的检索。
- `检索-生成模型`：一个更强大、更具表现力的语言模型，它根据记忆模型生成的线索，从数据库中检索必要的证据，并生成最终的高质量答案。

这种双模型系统架构确保 MemoRAG 能够处理需要多跳推理或具有隐含信息需求的任务。通过从记忆中回忆线索并检索相关数据，MemoRAG 弥补了原始输入与有意义且符合语境的准确响应之间的差距。

###  MemoRAG 主要特点

主要特点
- 全局记忆：单个上下文能够处理多达100万个token，确保对大型数据集的更全面理解；
- 上下文线索：记忆模型从全局记忆中生成精确的线索，将原始输入连接到答案，解锁复杂数据中的隐藏语义信息；
- 高效缓存：通过支持缓存分块索引和编码，将上下文预填充速度提高多达30倍；
- 上下文重用：对长上下文进行一次编码，并支持重复使用；
- 可优化和灵活：只需几个小时的额外训练就可以轻松适应新任务，以优化性能；
- 多功能集成：适用于广泛的模型和应用，适合需要高效理解上下文的行业，例如：金融、法律和医疗保健。


# 结束
