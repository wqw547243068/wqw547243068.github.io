---
layout: post
title:  大模型LLM推理
date:   2023-08-07 22:46:00
categories: AIGC
tags: gpt ChatGPT 量化
excerpt: 如何提升LLM推理效率？如何将大模型LLM部署在边缘设备上? 端侧大模型是未来趋势
mathjax: true
permalink: /llm_end
---

* content
{:toc}


# 端侧 LLM


## 边缘计算

边缘计算就是未来！

ChagtGPT、GPT-4等AIGC大模型虽然它们很厉害，但是它们只解决了语言大模型在**服务端部署**的问题。大模型需要极高的算力和资本投入，并不是一般的企业所能承受的

## 本地 ML



### GGML

GGML是一个用于机器学习的张量库，它只是一个c++库，允许你在CPU或CPU + GPU上运行llm。它定义了用于分发大型语言模型(llm)的二进制格式。GGML使用了一种称为量化的技术，该技术允许大型语言模型在消费者硬件上运行。



## LLM推理框架

【2023-8-30】[LLM七种推理服务框架总结](https://zhuanlan.zhihu.com/p/653352979)

40GB 的A100 GPU上，用 LLaMA-1 13b模型进行七个部署框架的对比

LLM推理有很多框架，各有其特点，七个框架的关键点：
- [1] vLLM ：适用于**大批量**Prompt输入，并对推理速度要求高的场景；
  - vLLM 吞吐量比HuggingFace Transformers（HF）高**14x-24倍**，比 HuggingFace Text Generation Inference（TGI）高2.2x-2.5倍。
- [2] Text generation inference：依赖HuggingFace模型，并且不需要为核心模型增加多个adapter的场景；
- [3] CTranslate2：可在CPU上进行推理；
- [4] OpenLLM：为核心模型添加adapter并使用HuggingFace Agents，尤其是不完全依赖PyTorch；
- [5] Ray Serve：稳定的Pipeline和灵活的部署，它最适合更成熟的项目；
  - Ray Serve 是一个可扩展的模型服务库，用于构建在线推理API。Serve与框架无关，因此可以使用一个工具包来为深度学习模型的所有内容提供服务。
- [6] MLC LLM：可在客户端（边缘计算）（例如，在Android或iPhone平台上）本地部署LLM；
  - LLM的机器学习编译（MLC LLM）是一种通用的部署解决方案，它使LLM能够利用本机硬件加速在消费者设备上高效运行
- [7] DeepSpeed-MII：使用DeepSpeed库来部署LLM；

![](https://pic1.zhimg.com/80/v2-3f467171d76953de58e69faf1d802394_1440w.webp)


### 一、vLLM


![](https://pic1.zhimg.com/80/v2-63fe1b3f450eefe222f025fac5a6cb84_1440w.webp)

vLLM的吞吐量比HuggingFace Transformers（HF）高14x-24倍，比HuggingFace Text Generation Inference（TGI）高2.2x-2.5倍。

**离线批量推理**

```py
# pip install vllm
from vllm import LLM, SamplingParams

prompts = [
    "Funniest joke ever:",
    "The capital of France is",
    "The future of AI is",
]
sampling_params = SamplingParams(temperature=0.95, top_p=0.95, max_tokens=200)
llm = LLM(model="huggyllama/llama-13b")
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

**API Server**

```sh
# Start the server:
python -m vllm.entrypoints.api_server --env MODEL_NAME=huggyllama/llama-13b

# Query the model in shell:
curl http://localhost:8000/generate \
    -d '{
        "prompt": "Funniest joke ever:",
        "n": 1,
        "temperature": 0.95,
        "max_tokens": 200
    }'
```

功能：
-   **Continuous batching\[9\]**：有iteration-level的调度机制，每次迭代batch大小都有所变化，因此vLLM在大量查询下仍可以很好的工作。
-   **PagedAttention\[10\]**：受操作系统中虚拟内存和分页的经典思想启发的注意力算法，这就是模型加速的秘诀。

优点：
-   **文本生成的速度：**实验多次，发现vLLM的推理速度是最快的；
-   **高吞吐量服务：**支持各种解码算法，比如parallel sampling, beam search等；
-   **与OpenAI API兼容：**如果使用OpenAI API，只需要替换端点的URL即可；

缺点：
-   **添加自定义模型**：虽然可以合并自己的模型，但如果模型没有使用与vLLM中现有模型类似的架构，则过程会变得更加复杂。例如，增加Falcon的支持，这似乎很有挑战性；
-   **缺乏对适配器（LoRA、QLoRA等）的支持**：当针对特定任务进行微调时，开源LLM具有重要价值。然而，在当前的实现中，没有单独使用模型和适配器权重的选项，这限制了有效利用此类模型的灵活性。
-   **缺少权重量化**：有时，LLM可能不需要使用GPU内存，这对于减少GPU内存消耗至关重要。

这是LLM推理最快的库。得益于其内部优化，它显著优于竞争对手。尽管如此，它在支持有限范围的模型方面确实存在弱点。

使用vLLM的开发路线可以参考：[https://github.com/vllm-project/vllm/issues/244](https://github.com/vllm-project/vllm/issues/244)**

### 二、Text generation inference

![](https://pic3.zhimg.com/80/v2-5238573ef15a96e9fcafc28193a56d9a_1440w.webp)

Text generation inference是用于文本生成推断的Rust、Python和gRPC服务器，在HuggingFace中已有LLM 推理API使用。

**使用docker运行web server**

```sh
mkdir data
docker run --gpus all --shm-size 1g -p 8080:80 \
-v data:/data ghcr.io/huggingface/text-generation-inference:0.9 \
  --model-id huggyllama/llama-13b \
  --num-shard 1
```

**查询实例**

```sh
# pip install text-generation
from text_generation import Client

client = Client("http://127.0.0.1:8080")
prompt = "Funniest joke ever:"
print(client.generate(prompt, max_new_tokens=17 temperature=0.95).generated_text)
```

功能：
-   **内置服务评估：**可以监控服务器负载并深入了解其性能；
-   **使用flash attention（和v2）和Paged attention优化transformer推理代码：**并非所有模型都内置了对这些优化的支持，该技术可以对未使用该技术的模型可以进行优化；

优点：
-   **所有的依赖项都安装在Docker中：**会得到一个现成的环境；
-   **支持HuggingFace模型：**轻松运行自己的模型或使用任何HuggingFace模型中心；
-   **对模型推理的控制**：该框架提供了一系列管理模型推理的选项，包括精度调整、量化、张量并行性、重复惩罚等；

缺点：
-   **缺乏对适配器的支持：**需要注意的是，尽管可以使用适配器部署LLM（可以参考[https://www.youtube.com/watch?v=HI3cYN0c9ZU](https://www.youtube.com/watch%3Fv%3DHI3cYN0c9ZU)），但目前还没有官方支持或文档；
-   **从源代码（Rust+CUDA内核）编译：**对于不熟悉Rust的人，将客户化代码纳入库中变得很有挑战性；
-   **文档不完整**：所有信息都可以在项目的自述文件中找到。尽管它涵盖了基础知识，但必须在问题或源代码中搜索更多细节；

使用Text generation inference的开发路线可以[参考](https://github.com/huggingface/text-generation-inference/issues/232)

### 三、CTranslate2


![](https://pic1.zhimg.com/80/v2-dfc1a2808b8b04b5e99f81e8734ca6b0_1440w.webp)

CTranslate2是一个C++和Python库，用于使用Transformer模型进行高效推理。

转换模型

```sh
pip install -qqq transformers ctranslate2

# The model should be first converted into the CTranslate2 model format:
ct2-transformers-converter --model huggyllama/llama-13b --output_dir llama-13b-ct2 --force
```

**查询实例**

```py
import ctranslate2
import transformers

generator = ctranslate2.Generator("llama-13b-ct2", device="cuda", compute_type="float16")
tokenizer = transformers.AutoTokenizer.from_pretrained("huggyllama/llama-13b")

prompt = "Funniest joke ever:"
tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt))
results = generator.generate_batch(
    [tokens], 
    sampling_topk=1, 
    max_length=200, 
)
tokens = results[0].sequences_ids[0]
output = tokenizer.decode(tokens)
print(output)
```

**功能：**
-   **在CPU和GPU上快速高效地执行：**得益于内置的一系列优化：层融合、填充去除、批量重新排序、原位操作、缓存机制等。推理LLM更快，所需内存更少；
-   **动态内存使用率：**由于CPU和GPU上都有缓存分配器，内存使用率根据请求大小动态变化，同时仍能满足性能要求；
-   **支持多种CPU体系结构：**该项目支持x86–64和AArch64/ARM64处理器，并集成了针对这些平台优化的多个后端：英特尔MKL、oneDNN、OpenBLAS、Ruy和Apple Accelerate；

**优点：**
-   **并行和异步执行**\--可以使用多个GPU或CPU核心并行和异步处理多个批处理；
-   **Prompt缓存**——在静态提示下运行一次模型，缓存模型状态，并在将来使用相同的静态提示进行调用时重用；
-   **磁盘上的轻量级**\--量化可以使模型在磁盘上缩小4倍，而精度损失最小；

**缺点**：
-   **没有内置的REST服务器**——尽管仍然可以运行REST服务器，但没有具有日志记录和监控功能的现成服务
-   **缺乏对适配器（LoRA、QLoRA等）的支持**

### 四、DeepSpeed-MII

![](https://pic1.zhimg.com/80/v2-b871e7cc5ef3ac72aa067953011333c4_1440w.webp)

在DeepSpeed支持下，DeepSpeed-MII可以进行低延迟和高通量推理。

**运行web服务**

```py
# DON'T INSTALL USING pip install deepspeed-mii
# git clone https://github.com/microsoft/DeepSpeed-MII.git
# git reset --hard 60a85dc3da5bac3bcefa8824175f8646a0f12203
# cd DeepSpeed-MII && pip install .
# pip3 install -U deepspeed

# ... and make sure that you have same CUDA versions:
# python -c "import torch;print(torch.version.cuda)" == nvcc --version
import mii

mii_configs = {
    "dtype": "fp16",
    'max_tokens': 200,
    'tensor_parallel': 1,
    "enable_load_balancing": False
}
mii.deploy(task="text-generation",
           model="huggyllama/llama-13b",
           deployment_name="llama_13b_deployment",
           mii_config=mii_configs)
```

**查询实例**

```py
import mii

generator = mii.mii_query_handle("llama_13b_deployment")
result = generator.query(  
  {"query": ["Funniest joke ever:"]}, 
  do_sample=True,
  max_new_tokens=200
)
print(result)
```

**功能**：
-   **多个副本上的负载平衡：**这是一个非常有用的工具，可用于处理大量用户。负载均衡器在各种副本之间高效地分配传入请求，从而缩短了应用程序的响应时间。
-   **非持久部署：**目标环境的部署不是永久的，需要经常更新的，这在资源效率、安全性、一致性和易管理性至关重要的情况下，这是非常重要的。

**优点**：
-   **支持不同的模型库：**支持多个开源模型库，如Hugging Face、FairSeq、EluetherAI等；
-   **量化延迟和降低成本：**可以显著降低非常昂贵的语言模型的推理成本；
-   **Native和Azure集成：**微软开发的MII框架提供了与云系统的出色集成；

**缺点**：
-   **支持模型的数量有限：**不支持Falcon、LLaMA2和其他语言模型；
-   **缺乏对适配器（LoRA、QLoRA等）的支持；**

### **五、OpenLLM**

![](https://pic1.zhimg.com/80/v2-2028bebc2adf59968b7506ebb697190c_1440w.webp)

OpenLLM是一个用于在生产中操作大型语言模型（LLM）的开放平台。

**运行web服务**

```sh
pip install openllm scipy
openllm start llama --model-id huggyllama/llama-13b \
  --max-new-tokens 200 \
  --temperature 0.95 \
  --api-workers 1 \
  --workers-per-resource 1
```

**查询实例**

```py
import openllm

client = openllm.client.HTTPClient('http://localhost:3000')
print(client.query("Funniest joke ever:"))
```

**功能**：

-   **适配器支持：**可以将要部署的LLM连接多个适配器，这样可以只使用一个模型来执行几个特定的任务；
-   **支持不同的运行框架：**比如Pytorch（pt）、Tensorflow（tf）或Flax（亚麻）；
-   **HuggingFace Agents\[11\]：**连接HuggingFace上不同的模型，并使用LLM和自然语言进行管理；

**优点**：

-   **良好的社区支持：**不断开发和添加新功能；
-   **集成新模型：**可以添加用户自定义模型；
-   **量化：**OpenLLM支持使用bitsandbytes\[12\]和GPTQ\[13\]进行量化；
-   **LangChain集成：**可以使用LangChian与远程OpenLLM服务器进行交互；

**缺点**：
-   **缺乏批处理支持：**对于大量查询，这很可能会成为应用程序性能的瓶颈；
-   **缺乏内置的分布式推理**——如果你想在多个GPU设备上运行大型模型，你需要额外安装OpenLLM的服务组件Yatai\[14\]；

### 六、Ray Serve

![](https://pic4.zhimg.com/80/v2-7746a44a79ce1a591a01ee8193c4bbc3_1440w.webp)

Ray Serve是一个可扩展的模型服务库，用于构建在线推理API。Serve与框架无关，因此可以使用一个工具包来为深度学习模型的所有内容提供服务。

![](https://pic1.zhimg.com/80/v2-524968a7269229d43184de705b0644f0_1440w.webp)

**运行web服务**

```py
# pip install ray[serve] accelerate>=0.16.0 transformers>=4.26.0 torch starlette pandas
# ray_serve.py
import pandas as pd

import ray
from ray import serve
from starlette.requests import Request

@serve.deployment(ray_actor_options={"num_gpus": 1})
class PredictDeployment:
    def __init__(self, model_id: str):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch

        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)

    def generate(self, text: str) -> pd.DataFrame:
        input_ids = self.tokenizer(text, return_tensors="pt").input_ids.to(
            self.model.device
        )
        gen_tokens = self.model.generate(
            input_ids,
            temperature=0.9,
            max_length=200,
        )
        return pd.DataFrame(
            self.tokenizer.batch_decode(gen_tokens), columns=["responses"]
        )

    async def __call__(self, http_request: Request) -> str:
        json_request: str = await http_request.json()
        return self.generate(prompt["text"])

deployment = PredictDeployment.bind(model_id="huggyllama/llama-13b")

# then run from CLI command:
# serve run ray_serve:deployment
```

**查询实例**

```py
import requests

sample_input = {"text": "Funniest joke ever:"}
output = requests.post("http://localhost:8000/", json=[sample_input]).json()
print(output)
```

**功能**：

-   **监控仪表板和Prometheus度量：**可以使用Ray仪表板来获得Ray集群和Ray Serve应用程序状态；
-   **跨多个副本自动缩放：**Ray通过观察队列大小并做出添加或删除副本的缩放决策来调整流量峰值；
-   **动态请求批处理：**当模型使用成本很高，为最大限度地利用硬件，可以采用该策略；

**优点**：
-   **文档支持：**开发人员几乎为每个用例撰写了许多示例；
-   **支持生产环境部署：**这是本列表中所有框架中最成熟的；
-   **本地LangChain集成：**您可以使用LangChian与远程Ray Server进行交互；

**缺点**：
-   **缺乏内置的模型优化：**Ray Serve不专注于LLM，它是一个用于部署任何ML模型的更广泛的框架，必须自己进行优化；
-   **入门门槛高：**该库功能多，提高了初学者进入的门槛；

如果需要最适合生产的解决方案，而不仅仅是深度学习，Ray Serve是一个不错的选择。它最适合于可用性、可扩展性和可观察性非常重要的企业。此外，还可以使用其庞大的生态系统进行数据处理、模型训练、微调和服务。最后，从OpenAI到Shopify和Instacart等公司都在使用它。

### 七、MLC LLM


![](https://pic4.zhimg.com/80/v2-d8f91231ec8466212d557d2c15808ba3_1440w.webp)

LLM的机器学习编译（MLC LLM）是一种通用的部署解决方案，它使LLM能够利用本机硬件加速在消费者设备上高效运行。

![](https://pic2.zhimg.com/80/v2-7bf144efe4e37a0b3c9bb95893ea6d65_1440w.webp)

**运行web服务**

```sh
# 1. Make sure that you have python >= 3.9
# 2. You have to run it using conda:
conda create -n mlc-chat-venv -c mlc-ai -c conda-forge mlc-chat-nightly
conda activate mlc-chat-venv

# 3. Then install package:
pip install --pre --force-reinstall mlc-ai-nightly-cu118 \
  mlc-chat-nightly-cu118 \
  -f https://mlc.ai/wheels

# 4. Download the model weights from HuggingFace and binary libraries:
git lfs install && mkdir -p dist/prebuilt && \
  git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib && \
  cd dist/prebuilt && \  
  git clone https://huggingface.co/huggyllama/llama-13b dist/ && \
  cd ../..
  
  
# 5. Run server:
python -m mlc_chat.rest --device-name cuda --artifact-path dist
```

**查询实例**

```py
import requests

payload = {
   "model": "lama-30b",
   "messages": [{"role": "user", "content": "Funniest joke ever:"}],
   "stream": False
}
r = requests.post("http://127.0.0.1:8000/v1/chat/completions", json=payload)
print(r.json()['choices'][0]['message']['content'])
```

**功能**：

-   **平台本机运行时：**可以部署在用户设备的本机环境上，这些设备可能没有现成的Python或其他必要的依赖项。应用程序开发人员只需要将MLC编译的LLM集成到他们的项目中即可；
-   **内存优化：**可以使用不同的技术编译、压缩和优化模型，从而可以部署在不同的设备上；

**优点**：

-   **所有设置均可在JSON配置中完成：**在单个配置文件中定义每个编译模型的运行时配置；
-   **预置应用程序：**可以为不同的平台编译模型，比如C++用于命令行，JavaScript用于web，Swift用于iOS，Java/Kotlin用于Android；

**缺点**：

-   **使用LLM模型的功能有限**：不支持适配器，无法更改精度等，该库主要用于编译不同设备的模型；
-   **只支持分组量化\[15\]：**这种方法表现良好，但是在社区中更受欢迎的其他量化方法（bitsandbytes和GPTQ）不支持；
-   **复杂的安装：**安装需要花几个小时，不太适合初学者开发人员；

如果需要在iOS或Android设备上部署应用程序，这个库正是你所需要的。它将允许您快速地以本机方式编译模型并将其部署到设备上。但是，如果需要一个高负载的服务器，不建议选择这个框架。


#### MLC LLM -- 陈天奇

- 【2023-5-2】[陈天奇等人新作引爆AI界：手机原生跑大模型，算力不是问题了](https://mp.weixin.qq.com/s/uQGAu1v-6ApgZHVkZJsUdQ)
- 【2023-6-5】[陈天奇官宣新APP，让手机原生跑大模型，应用商店直接下载使用](https://www.toutiao.com/article/7241085086400233995), 陈天奇公布了一个好消息：MLC Chat app 已经在苹果的 App Store 上线了。

【2023-5-2】[端侧语言大模型部署时代已经悄悄到来！](https://zhuanlan.zhihu.com/p/626268783)

#### TVM 简介

TVM是一个深度学习编译器，初衷是让各种训练框架训练好的模型能够在不同硬件平台上面快速推理
- 支持Pytorch、AutoML、Tensorflow、Onnx、Kersa、Mxnet等多种前端训练框架；
- 支持ARM CPU、Intel CPU、NVIDIA显卡、FPGA、ASIC等多种硬件设备。

MLC-LLM 底层技术其实就是`TVM`编译器。

该框架的输入是一些训练框架训练好的**模型文件**；
- 然后, 利用Relay将其转换成 High-Level Differentiable IR，该阶段会执行一些图优化操作，包括：算子融合、常量折叠、内存重排、模型量化等；
- 接着会利用AutoTVM、Ansor或者Meta Scheduler等自动化优化技术来将这种IR转换为Tensor Expression IR这种更低级的IR表示。

TVM深度学习编译器中的一个亮点工作就是**自动优化技术**
- 第一代优化技术叫`AutoTVM`
- 第二代叫`Ansor`或者`Auto Scheduler`
- 第三代叫`Meta Scheduler`

![](https://pic3.zhimg.com/80/v2-a2aa328a18b3afb02f48816419c481c6_1440w.jpg)

AutoTVM
- ![](https://pic3.zhimg.com/80/v2-9e23067a8872309bdafadede6328e192_1440w.jpg)

Ansor/Auto Scheduler
- ![](https://pic4.zhimg.com/80/v2-7ccb0e4a6fbbecd305f68dfd573b1377_1440w.jpg)


#### MLC LLM 简介

MLC LLM 是一种通用解决方案，允许将任何语言模型**本地部署**在各种硬件后端和本地应用程序上。

此外还有一个高效框架，供每个人进一步优化自己用例的模型性能。<span style='color:red'>一切都在本地运行，无需服务器支持</span>，并通过手机和笔记本电脑上的本地 GPU 加速。

TVM是一个深度学习编译器，知名编译工具，国内很多大公司都在使用它，国内很多的芯片公司都在使用它构建自己的工具链。
- `AutoTVM`、`Ansor`是`TVM`中比较亮眼的工作，思想都是利用ML将算子优化的任务自动化，当前它已经可以很好的支持多种硬件设备。
- 语言大模型的轻量化核心是**Transformer的加速与优化**，TVM社区很早就开始探索Transformer的加速与优化。除此之外，TVM中的图优化技术、自动优化等技术为语言大模型的轻量化打下了坚实的基础。

MLC-LLM只是语言大模型轻量化的开端，语言大模型**轻量化**方向近期会变得异常火热, 很多大公司陆续都是开源自己的一些工作。
- 随着MIC-LLM等工具出现，**端侧大模型部署**热潮已经来临。OpenAI一家独大的情况也会慢慢得打缓解，随着语言大模型的赋能，越来越多的智能设备，尤其是机器人的智能程度会更上一层楼！
- 随着端侧语言大模型的部署难题逐步被解决，端侧模型的**数据隐私**问题可能成为了端侧部署的一个关键问题。不过，这个问题应该相对来说会比较容易一些。期待了端侧语言大模型时代的到来！

- [演示图](https://vdn6.vzuu.com/SD/da2d7036-e81f-11ed-a962-bacc53acff3b-v1_f4_t2_etMRzyS8.mp4?pkey=AAV2GKNHXbMr7W0DZWmaAKjmklpebDlDgvlJQN4ElgagtlxqcrYmaLNld20o3ymLMrOUseNg1m3gdavjpUBHj89Y&c=avc.1.1&f=mp4&pu=078babd7&bu=078babd7&expiration=1691478412&v=ks6)
- [参考](https://zhuanlan.zhihu.com/p/626189075)


[mlc-llm](https://mlc.ai/mlc-llm/) 部署汇总
- 亲测：华为mate 30下载后，启动即闪退；iOS正常

|设备|地址|示例|
|---|---|---|
|iOS|[iOS地址](https://testflight.apple.com/join/57zd7oxa)|![](https://mlc.ai/mlc-llm/gif/ios-demo.gif)|
|Android|[Android地址](https://mlc.ai/mlc-llm/gif/android-demo.gif)|![](https://mlc.ai/mlc-llm/gif/android-demo.gif)|
|PC|[Windows Linux Mac](https://mlc.ai/mlc-llm/#windows-linux-mac)|![](https://mlc.ai/mlc-llm/gif/linux-demo.gif)|
|Web|[WebLLM](https://mlc.ai/mlc-llm/#web-browser)||

让大模型变小这条路上，人们做了很多尝试
- 先是 Meta 开源了 LLaMA，让学界和小公司可以训练自己的模型。
- 随后斯坦福研究者启动了 Lamini，为每个开发者提供了从 GPT-3 到 ChatGPT 的快速调优方案。
- 最近 MLC LLM 的项目一步登天，因为它能在**任何设备**上编译运行大语言模型。

MLC LLM 在各类硬件上**原生部署任意大型语言模型**提供了解决方案，可将大模型应用于移动端（例如 iPhone）、消费级电脑端（例如 Mac）和 Web 浏览器。
-  TVM、MXNET、XGBoost 作者，CMU 助理教授，OctoML CTO 陈天奇等多位研究者共同开发的，参与者来自 CMU、华盛顿大学、上海交通大学、OctoML 等院校机构，同时也获得了开源社区的支持。
- [github](https://github.com/mlc-ai/mlc-llm)
- [Demo](https://mlc.ai/mlc-llm/)
- [MLC课程](https://mlc.ai/summer22-zh/schedule)：机器学习编译
- [知乎专题](https://www.zhihu.com/question/598610139)

曾经开源过XGBoost和TVM `陈天奇`大佬已经完成了这件事情，推出了一个叫`MLC-LLM` 工具，在一些低算力平台上面运行一些语言大模型！只要GPU显存大于6GB，都可以去尝试在本地部署一下属于语言大模型

MLC LLM 旨在让每个人都能在个人设备上本地开发、优化和部署 AI 模型，而无需服务器支持，并通过手机和笔记本电脑上的消费级 GPU 进行加速。具体来说，MLC LLM 支持的平台包括：
- iPhone
- Metal GPU 和英特尔 / ARM MacBook;
- 在 Windows 和 Linux 上支持通过 Vulkan 使用 AMD 和 NVIDIA GPU；
- 在 Windows 和 Linux 上 通过 CUDA 使用 NVIDIA GPU；
- 浏览器上的 WebGPU（借助 MLC LLM 的配套项目 Web LLM）。

为了实现在各类硬件设备上运行 AI 模型的目标，研究团队要解决计算设备和部署环境的多样性问题，主要挑战包括：
- 支持不同型号的 CPU、GPU 以及其他可能的协处理器和加速器；
- 部署在用户设备的**本地环境**中，这些环境可能没有 python 或其他可用的必要依赖项；
- 通过仔细规划分配和积极压缩模型参数来解决**内存限制**。
- MLC LLM 提供可重复、系统化和可定制的工作流，使开发人员和 AI 系统研究人员能够以 Python 优先的方法实现模型并进行优化。MLC LLM 可以让研究人员们快速试验新模型、新想法和新的编译器 pass，并进行本地部署。

为了实现原生部署，研究团队以**机器学习编译**（MLC）技术为基础来高效部署 AI 模型。
- [MLC技术](https://mlc.ai/)
- MLC LLM 借助一些开源生态系统，包括来自 HuggingFace 和 Google 的分词器，以及 LLaMA、Vicuna、Dolly 等开源 LLM。
- ![](https://pica.zhimg.com/80/v2-b23bb5806fa9c32e51773e06494b8f62_1440w.webp?source=1940ef5c)

MLC LLM 的主要工作流基于 Apache TVM Unity，通过扩展 TVM 后端使模型编译更加透明和高效。
- Dynamic shape：该研究将语言模型烘焙（bake）为具有原生 Dynamic shape 支持的 TVM IRModule，避免了对最大输入长度进行额外填充的需要，并减少了计算量和内存使用量。
- 可组合的 ML 编译优化：MLC LLM 可以执行许多模型部署优化，例如更好的编译代码转换、融合、内存规划和库卸载（library offloading），并且手动代码优化可以很容易地合并为 TVM 的 IRModule 转换，成为一个 Python API。
- 量化：MLC LLM 利用低位量化来压缩模型权重，并利用 TVM 的 loop-level TensorIR 为不同的压缩编码方案快速定制代码生成。
- 运行时（Runtime）：TVM 编译生成的库能够通过 TVM runtime 在设备的原生环境中运行，TVM runtime 支持 CUDA/Vulkan/Metal 等主流 GPU 驱动以及 C、JavaScript 等语言的绑定。

此外，MLC 还为 CUDA、Vulkan 和 Metal 生成了 GPU shader，并通过 LLVM 支持多种 CPU，包括 ARM 和 x86。通过改进 TVM 编译器和运行时，使用者可以添加更多支持，例如 OpenCL、sycl、webgpu-native。


#### MLC LLM 支持设备

支持的设备类型
- MLC-LLM工具支持多种设备类型，大到N卡、AMD GPU，小到Android、IOS、WebGPU等。具体测试的设备列表如下所示。建议在设备内存大于等于6GB的设备上面进行推理与测试。

- iPhone, iPad

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| iPhone 14 Pro | iOS 16.4.1 | 7.2 | [https://github.com/junrushao](https://github.com/junrushao) |
| iPad Pro 11 with M1 | iPadOS 16.1 | 10.6 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529377124](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529377124) |

-  Metal GPUs and Intel/ARM MacBooks

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| UHD Graphics 630 | macOS Ventura | 2.3 | [https://github.com/junrushao](https://github.com/junrushao) |
| 2020 MacBook Pro M1 (8G) | macOS | 11.4 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529148903](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529148903) |
| 2021 MacBook Pro M1Pro (16G) | macOS Ventura | 17.1 | [https://github.com/mlc-ai/mlcllm/issues/15#issuecomment-1529434801](https://github.com/mlc-ai/mlcllm/issues/15%23issuecomment-1529434801) |
| M1 Max Mac Studio (64G) |  | 18.6 | [https://github.com/mlc-ai/mlcllm/issues/15#issuecomment-1529714864](https://github.com/mlc-ai/mlcllm/issues/15%23issuecomment-1529714864) |

- AMD and NVIDIA GPUs via Vulkan on Windows and Linux

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| Raden Pro 5300M | macOS Venture | 12.6 | [https://github.com/junrushao](https://github.com/junrushao) |
| AMD GPU on Steam Deck | TBD (S macOS Ventura ome Linux) | TBD | [https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia8ux6/](https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia8ux6/) |
| RX 7900 xtx |  |  | [https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia691u/](https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia691u/) |
| RX6800 16G VRAM | macOS Ventura | 22.5 | [https://github.com/mlc-ai/mlc-llm/issues/15](https://github.com/mlc-ai/mlc-llm/issues/15) |

- NVIDIA GPUs via CUDA on Windows and Linux

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| GTX 1060 (6GB) | Windows 10 | 16.7 | [https://github.com/mlc-ai/mlc-llm/issues/13#issue-1689858446](https://github.com/mlc-ai/mlc-llm/issues/13%23issue-1689858446) |
| RTX 3080 | Windows 11 | 26.0 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529434801](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529434801) |
| RTX 3060 | Debian bookworm | 21.3 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529572646](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529572646) |

- WebGPU on browsers


#### MLC-LLM 核心技术


![](https://pic4.zhimg.com/80/v2-c95a2f2706f88094bd196bd4bf7da53b_1440w.webp)

-   **Dynamic shape**: 作者将**语言模型**转换为具有原生动态形状支持的 TVM IRModule，避免了对最大长度进行额外填充的需要，并减少了计算量和内存使用量。如图所示，为了优化动态形状输入
  - 首先应用**循环切分**技术，即将一个大循环切分成两个小循环操作；
  - 然后应用张量自动化技术，即TVM中的Ansor或者Meta Scheduler技术。
  - ![](https://pic4.zhimg.com/80/v2-1a89c78eba7b1228fff6dd08d41ff2bf_1440w.webp)
-   **Composable ML compilation optimization**s: 执行了许多模型部署优化，例如更好的编译代码转换、融合、内存规划、库卸载和手动代码优化可以很容易地合并为TVM 的 IRModule 转换，作为 Python API 公开。如上图所示，模型推理工具链中常用的几种优化技术包括：算子简化、算子融合、常量折叠、内存排布等。
  - ![](https://pic1.zhimg.com/80/v2-dccad206e27b4d485879c50d9033a0ec_1440w.webp)
-   **Quantization**: 利用**低位量化**来压缩模型权重，并利用 TVM 的循环级 TensorIR 为不同的压缩编码方案快速定制代码生成。如图所示，TVM中可以通过两种方式来进行量化：1）通过 relay.quantize 完成浮点模型的量化，该量化包含annotate、calibrate和relize三步；2）通过一种称为 qnn 的 relay方言([http://relay.qnn.xxx](https://link.zhihu.com/?target=http%3A//relay.qnn.xxx)) 直接执行已经量化过的模型。
  - ![](https://pic4.zhimg.com/80/v2-883e9de589cbcac6e1f9854b46b160b7_1440w.webp)
-   **Runtime**: 最终生成的库在原生环境中运行，TVM 运行时具有最小的依赖性，支持各种 GPU 驱动程序 API 和原生语言绑定（C、JavaScript等）。如图所示，TVM支持多种Runtime，包括：JS、Java、Python、C++、Android、IOS、Web等，正是这些Runtime支持，才使得MLC-LLM可以很快的支持很多端侧设备!


#### MLC-LLM部署流图


![](https://pic4.zhimg.com/80/v2-04249102c3061d4c5e436e990a42125f_1440w.webp)

1、**Python first** development

-   IRModule: 如上图所示，该模块存储着一个张量函数集合，每个函数附带首个形状符号，并支持跟踪形状依赖。 该模块包含着Transformer中的关键模块，encoding和step\_decoding，前者用来做输入数据的编码操作，后者用来做数据的解码操作。  
-   ML Compilation Optimization: 该模块主要在计算图上面执行一些优化操作，具体包括：算子融合（降低多次加载的带宽开销）、内存规划（提前在编译阶段分配一些内存，并对内存的排布进行调整）、循环优化（利用常用的tile、reoder、paritation等技术）和权重量化（利用int8、int16等数据类型进行模型压缩）。  
-   TensorIR Schedules: 该模块主要利用Ansor自动优化或者Meta Scheduler自动优化技术对LLM模型中的算子进行调度优化。这是TVM编译器的一个杀手锏！该技术的核心思想是利用ML的思路来解决循环优化问题。  

2、**Universal** development

-   **最底层是硬件驱动层**，该层主要完成一些硬件适配与驱动的工作。支持的硬件具体包括：NVIDIA的CUDA、AMD的Rocm、苹果的Vulkan和WebGPU等。  
-   **第三层是TVM Runtim层**，该层主要完成TVM Runtime库的适配与加载任务。用户需要做的是调用TVM的Runtime推理接口完成模型的推理操作。  
-   **第二层是模型与代码层**，该层主要完成模型的优化与业务逻辑码的开发。通过Python First Development可以导出一个model.dylib库，用户需要实现[http://llm\_chat.cc](https://link.zhihu.com/?target=http%3A//llm_chat.cc)文件，即语言大模型的业务逻辑代码。  
-   **第一层是应用层**，该层用来开发一些上层应用，具体包括Chat CLI命令行工具、MLCChat.App 安卓或者IOS端的上层应用、基于WebGPU的网页端应用等。


#### MLC-LLM环境搭建

1、**iphone平台**

[参考](https://testflight.apple.com/join/57zd7oxa)页面安装已经编译好的APP。

注意事项：
- 试用此页面（仅限前 9000 名用户）以安装和使用作者为 iPhone 构建的示例 iOS 聊天应用程序。应用程序本身需要大约 4GB的内存才能运行。考虑到 iOS 和其他正在运行的应用程序，我们将需要具有 6GB（或更多）内存的最新 iPhone 来运行该应用程序。作者仅在 iPhone 14 Pro Max 和 iPhone 12 Pro上测试了该应用程序。

2、**Windows/Linux/Mac**平台

![](https://pic4.zhimg.com/v2-583cb94d4f32afe60eebeb0dfacbb847_b.gif)

![动图封面](https://pic4.zhimg.com/v2-583cb94d4f32afe60eebeb0dfacbb847_b.jpg)

步骤1 - 安装环境依赖
- 安装 Miniconda 或 Miniforge
- windows与linux用户-安装Vulkan驱动；对于Nvidia用户-建议安装Vulkan驱动

步骤2-创建环境

```sh
# Create new conda environment and activate the environment.
conda create -n mlc-chat
conda activate mlc-chat
# Install Git and Git-LFS, which is used for downloading the model weights from Hugging Face.
conda install git git-lfs
# Install the chat CLI app from Conda.
conda install -c mlc-ai -c conda-forge mlc-chat-nightly
# Create a directory, download the model weights from HuggingFace, and download the binary libraries from GitHub.
mkdir -p dist
git lfs install
git clone https://huggingface.co/mlc-ai/demo-vicuna-v1-7b-int3 dist/vicuna-v1-7b
git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/lib
# Enter this line and enjoy chatting with the bot running natively on your machine!
mlc_chat_cli
```

3、**Web浏览器**平台

步骤
1. 安装 Chrome Canary，它是支持使用 WebGPU 的 Chrome 开发者版本。
2. 利用下面的命令行发起Chrome Canary

```sh
/Applications/Google\ Chrome\ Canary.app/Contents/MacOS/Google\ Chrome\ Canary --enable-dawn-features=disable_robustness
```

3. 在浏览器运行[demo](https://mlc.ai/web-llm/#chat-demo)

注意事项：
- WebGPU 刚刚发布到 Chrome 并且处于测试阶段。我们在 Chrome Canary 中进行实验。你也可以试试最新的Chrome 113。Chrome版本≤112是不支持的，如果你正在使用它，demo会报错 Find an error initializing the WebGPU device OperationError: Required limit (1073741824) is greater than the 支持的限制 (268435456)。
- 验证 maxBufferSize 时 
- 验证所需限制时。已经在 windows 和 mac 上测试过了，你需要一个 6.4G 内存的 gpu。


#### MLC-LLM效果展示

1、**web端**Demo

![](https://pic3.zhimg.com/80/v2-22762accdf3d48acd09f06b8a60e2eda_1440w.webp)

2、IOS端Demo

![](https://pic1.zhimg.com/v2-1f68e0fc385e8b67344f4e5c99fcc837.jpg?source=382ee89a)


3、Web Stable Diffusion

![](https://pic2.zhimg.com/80/v2-895ea13851a24908d5ec8fb6ef1ad775_1440w.webp)


## LLM 推理优化


【2023-8-17】[LLM 的推理优化技术纵览](https://zhuanlan.zhihu.com/p/642412124)

推理是 LLM 应用的重要一环，在部署服务环节影响重大，本文将讨论主流的 LLM 的推理优化技术。

模型小型化关注：模型**平均推理时间**和**功耗**
- 平均推理时间: 用 `latency` 或 `throughput` 来衡量
- 功耗: 用参考生成token过程中所用到GPU的功耗来近似(因为TP/PP等方法就会引入多个GPU). 

这两个指标都与**模型参数量**紧密相关, 特别是LLMs的参数量巨大, 导致部署消耗GPU量大(而且甚至会引起旧GPU, 如: 2080ti等消费级卡直接下线离场)及GPU的IO时间长(memory write/read 的cycles是要远大于 operations cycles, 印象中是百倍)

部署过程中如何使得模型变得更小更轻且保持智能尽可能不下降就成了一个重要的研究话题。

### 一、子图融合（subgraph fusion）

图融合技术即通过将多个 OP（算子）合并成一个 OP（算子），来减少`Kernel`的调用。因为每一个基本 OP 都会对应一次 GPU kernel 的调用，和多次显存读写，这些都会增加大量额外的开销。

#### 1.1 FasterTransformer

[FasterTransformer](https://github.com/NVIDIA/FasterTransformer) by NVIDIA

`FasterTransformer`(FT) 是一个用于实现基于`Transformer`的神经网络推理的加速引擎。`FT`框架是用`C++/CUDA`编写的，依赖于高度优化的 cuBLAS、cuBLASLt 和 cuSPARSELt 库，与 [NVIDIA TensorRT](https://link.juejin.cn/%3Ftarget%3Dhttps%253A%252F%252Fdeveloper.nvidia.com%252Fblog%252Foptimizing-t5-and-gpt-2-for-real-time-inference-with-tensorrt%252F) 等其他编译器相比，FT 的特点是它支持**以分布式方式推理 Transformer 大模型**。

图融合是`FT` 的一个重要特征，将多层神经网络组合成一个单一的神经网络，将使用一个单一的内核进行计算。 这种技术减少了数据传输并增加了数学密度，从而加速了推理阶段的计算。 例如， multi-head attention 块中的所有操作都可以合并到一个内核中。

![](https://pic2.zhimg.com/80/v2-5c12abec5f35a555b6378e342fd51639_1440w.webp)

除此之外，`FT`还对部分大模型分别支持：
-   `INT8` 低精度量化推理
-   Ampere 架构的 GPU 硬件部分支持稀疏化
-   Hopper 架构支持 FP8 推理
-   Tensor 并行
-   Pipeline 并行

#### 1.2 DeepSpeed Inference

[DeepSpeed Inference](https://arxiv.org/pdf/2207.00032.pdf) by Microsoft

对于 Transformer layer，可分为以下4个主要部分：

1.  Input Layer-Norm plus Query, Key, and Value GeMMs and their bias adds.
2.  Transform plus Attention.
3.  Intermediate FF, Layer-Norm, Bias-add, Residual, and Gaussian Error Linear Unit (GELU).
4.  Bias-add plus Residual.

如图所示，每一部分可分别进行融合，与未融合相比，以上几个部分的加速比可分别达到 1.5x, 2.9x, 3x, 1.2x 。

![](https://pic2.zhimg.com/80/v2-fe415109e5bd552485d1a42fbdd3d679_1440w.webp)

除此之外，DeepSpeed Inference 的优化点还有以下几点：
-   多 GPU 的并行优化
-   INT8 模型量化
-   推理的 pipeline 方案

更多详细介绍及实践可参考笔者之前的文章：

[紫气东来：NLP（十二）：DeepSpeed Inference 在 LLM 推理上的优化探究](https://zhuanlan.zhihu.com/p/629085568?)

#### 1.3 MLC LLM

[MLC LLM](https://github.com/mlc-ai/mlc-llm) by TVM

之前介绍的推理方案主要是基于GPU的优化，而 MLC LLM 提供了可应用于移动端（例如 iPhone）、消费级电脑端（例如 Mac）和 Web 浏览器的轻设备解决方案。

MLC LLM 的主要工作流基于 Apache TVM Unity，通过扩展 TVM 后端使模型编译更加透明和高效。其中以编译代码转换、融合、内存规划和库卸载（library offloading）为代表的可组合的 ML 编译优化是其中重要的优化特性。

![](https://pic2.zhimg.com/80/v2-942db4a7b53c01251a30cd9f79e83439_1440w.webp)

除此之外，MLC LLM 还具有以下特性：
-   Dynamic shape：避免了对最大输入长度进行额外填充的需要，并减少了计算量和内存使用量。
-   量化：MLC LLM 利用低位量化来压缩模型权重，并利用 TVM 的 loop-level TensorIR 为不同的压缩编码方案快速定制代码生成。
-   运行时（Runtime）：TVM 编译生成的库能够通过 TVM runtime 在设备的原生环境中运行，TVM runtime 支持 CUDA/Vulkan/Metal 等主流 GPU 驱动以及 C、JavaScript 等语言的绑定。

除了上述3种方案外，其他也支持图融合的方案还包括 [NVIDIA TensorRT](https://link.juejin.cn/%3Ftarget%3Dhttps%253A%252F%252Fdeveloper.nvidia.com%252Fblog%252Foptimizing-t5-and-gpt-2-for-real-time-inference-with-tensorrt%252F)， [Tencent TurboTransformers](https://github.com/Tencent/TurboTransformers) 等。




### 二、模型压缩（Model Compression）

模型压缩的基本动机在于当前的模型是冗余的，可以在精度损失很小的情况下实现模型小型化，主要包括3类方法：稀疏(Sparsity)、量化(Quantization)、蒸馏(Distillation)。

#### 2.1 稀疏(Sparsity)

实现稀疏(Sparsity)的一个重要方法是剪枝(Pruning)。剪枝是在保留模型容量的情况下，通过修剪不重要的模型权重或连接来减小模型大小。 它可能需要也可能不需要重新培训。 修剪可以是非结构化的或结构化的。
-   非结构化剪枝允许删除任何权重或连接，因此它不保留原始网络架构。 非结构化剪枝通常不适用于现代硬件，并且不会带来实际的推理加速。
-   结构化剪枝旨在维持某些元素为零的密集矩阵乘法形式。 他们可能需要遵循某些模式限制才能使用硬件内核支持的内容。 当前的主流方法关注结构化剪枝，以实现 Transformer 模型的高稀疏性。

关于剪枝稀疏的基本原理，可参考笔者之前的文章：
- [大语言模型的稀疏化技术](https://zhuanlan.zhihu.com/p/615399255)

除了上文介绍的稀疏方法外，还有其他的稀疏化方法，包括但不限于：
-   [SparseGPT](https://arxiv.org/pdf/2301.00774.pdf)：该方法的工作原理是将剪枝问题简化为大规模的稀疏回归实例。它基于新的近似稀疏回归求解器，用于解决分层压缩问题，其效率足以在几个小时内使用单个 GPU 在最大的 GPT 模型（175B 参数）上执行。同时，SparseGPT 准确率足够高，不需要任何微调，剪枝后所损耗的准确率也可以忽略不计。
-   [LLM-Pruner](https://arxiv.org/pdf/2305.11627.pdf)：遵循经典的“重要性估计-剪枝-微调”的策略，能够在有限资源下完成大语言模型的压缩，结果表明即使剪枝 20％ 的参数，压缩后的模型保留了 93.6％ 的性能。
-   [Wanda](https://arxiv.org/pdf/2306.11695.pdf): 该方法由两个简单但必不可少的组件构成——剪枝度量和剪枝粒度。剪枝度量用来评估权重的重要性，然后按照剪枝粒度进行裁剪。该方法在 65B 的模型上只需要 5.6 秒就可以完成剪枝，同时达到SparseGPT相近的效果。

以上主要实现了稀疏的方法，那么对于稀疏后的模型如何加速呢？NVIDIA Ampere 架构对与结构化稀疏做了专门的[稀疏加速单元](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/)，下图展示了结构化稀疏的物理表示：
- ![](https://pic3.zhimg.com/80/v2-69b4e98ed5c47ba98ac496598ce4a31a_1440w.webp)

2:4 结构化稀疏表示

下图展示了稀疏单元GEMM计算与标准GEMM计算的区别（详细解释参见[https://arxiv.org/pdf/2104.08378.pdf](https://arxiv.org/pdf/2104.08378.pdf)）
- ![](https://pic4.zhimg.com/80/v2-67533793fe25f97960c383c62c9ff62b_1440w.webp)

Sparse VS Dense GEMM

#### 2.2 量化(Quantization)

【2023-9-7】
- [关于大模型推理的量化算法总结](https://zhuanlan.zhihu.com/p/645308698)
- [大语言模型的模型量化(INT8/INT4)技术](https://zhuanlan.zhihu.com/p/627436535)

`量化`(Quantization)可以很好地通过将**float**模型表征为**低位宽模型**实现减小模型存储空间, 加速模型推理的目标. 

量化定义为: 
> a technique that mapping of a **k-bit integer** to a **float** element, which **saves space** and **speedup computation** by compressing the digital representation. 

##### 量化分类

量化可以按不同角度对其进行归类: 
- 按量化**执行阶段** 分为**训练中量化**(`QAT`, Quantization-Aware-Training) 和 **训练后量化**(`PTQ`, Post-Training-Quantization); 
- 按量化**间隔是否等距** 分为`均匀量化`和`非均匀量化`(如图所示). 
- ![](https://pic3.zhimg.com/80/v2-7ab0c2a2269d98f38a0b99ac8a19725e_1440w.webp)

|划分维度|类1|类2|
|---|---|---|
|执行阶段|训练中量化 `QAT`|训练后量化 `PTQ`|
|间隔是否等距|均匀量化|非均匀量化|

这里主要讨论`PTQ`, `均匀量化`. 因为LLMs背景下
- `QAT`目前仍未有机构做出靠谱研究, 主要受限于QAT需要引入**模拟量化**的操作, 会引起**显存&计算量进一步上涨**以及**梯度mismatch**的问题, 从而增加训练成本以及影响Scaling Laws. 
- `非均匀量化`除非有特殊硬件支持, 否则在GPU上目前多数只能通过 **Look-Up-Table** 或 **移位**等方式来实现, 速度和精度没法得到同时保证.

常见量化有两种常见方法：
-   **训练后量化**（Post-Training Quantization，`PTQ`）：模型首先经过训练以达到收敛，然后将其权重转换为较低的精度，而无需进行更多训练。
  - 与训练相比，实施起来通常相当便宜。
-   **量化感知训练**（Quantization-Aware Training，`QAT`）：在**预训练**或**微调**期间应用量化。 
  - QAT 能够获得更好的性能，但需要额外的计算资源和对代表性训练数据的访问。

##### 量化原理

模型大小由其**参数量**及其**精度**决定，精度通常为 `float32`、`float16` 或 `bfloat16`
-   **Float32 (FP32)** 。标准的 IEEE 32 位浮点表示，指数 8 位，尾数 23 位，符号 1 位，可以表示大范围的浮点数。大部分硬件都支持 FP32 运算指令。
-   **Float16 (FP16)** 。指数 5 位，尾数 10 位，符号 1 位。FP16 数字的数值范围远低于 FP32，存在上溢 (当用于表示非常大的数时) 和下溢 (当用于表示非常小的数时) 的风险，通过缩放损失 (loss scaling) 来缓解这个问题。
-   **Bfloat16 (BF16)** 。指数 8 位 (与 FP32 相同)，尾数 7 位，符号 1 位。这意味着 BF16 可以保留与 FP32 相同的动态范围。但是相对于 FP16，损失了 3 位精度。因此，在使用 BF16 精度时，大数值绝对没有问题，但是精度会比 FP16 差。
-   **TensorFloat-32(TF32)** 。使用 19 位表示，结合了 BF16 的范围和 FP16 的精度，是计算数据类型而不是存储数据类型。目前使用范围较小。
- ![](https://pic2.zhimg.com/80/v2-500915ea5b15c798bdc00d679fdeb229_1440w.webp)

模型训练
- 训练时为保证精度，主权重始终为 `FP32`。
- 而推理时，`FP16` 权重通常能提供与 `FP32` 相似的精度

推理时使用 `FP16` 权重，仅需一半 GPU 显存就能获得相同的结果。那么是否还能进一步减少显存消耗呢？答案是用`量化`技术，最常见的就是 `INT8` 量化。
- ![](https://pic3.zhimg.com/80/v2-df305ab18d1a433744f877264c3c3a5a_1440w.webp)

INT8 量化即将浮点数 xf 通过缩放因子 scale 映射到范围在 `[-128, 127]` 内的 8bit 表示 xq, 即: 
- $ x_{q}=\operatorname{Clip}\left(\operatorname{Round}\left(x_{f} / \text { scale }\right)\right) $
- $ scale = (2*max(\left | x_f \right | ))/254 $
- Round 表示四舍五入都整数，Clip 表示将离群值(Outlier) 截断到 [-128, 127] 范围内。

量化-反量化例子
- ![](https://pic4.zhimg.com/80/v2-af2eaf59e0e9409d1587fe9ba82dadcb_1440w.webp)

进行矩阵乘法时，可以通过组合各种技巧，例如逐行或逐向量量化，来获取更精确的结果。举个例子，对矩阵乘法，我们不会直接使用常规量化方式，即用整个张量的最大绝对值对张量进行归一化，而会转而使用向量量化方法，找到 A 的每一行和 B 的每一列的最大绝对值，然后逐行或逐列归一化 A 和 B 。最后将 A 与 B 相乘得到 C。最后，我们再计算与 A 和 B 的最大绝对值向量的外积，并将此与 C 求哈达玛积来反量化回 FP16。


由于 GPU 内核缺乏对某些类型的**矩阵乘法**（例如 INT4 x FP16）的支持，理论最优量化策略与硬件内核支持之间的差距，并非以下所有方法都能加速实际推理。

两个公式
- ![](https://pic4.zhimg.com/80/v2-1e445bb92a842afb930f8f03e2850d03_1440w.webp)
- 1式中, `Q(·)`表示量化操作, `X`代表输入tensor, `S`即为scale, `Z`即为zero-point, `b`为量化位宽。
- 1式称为`quantization`, 2式称为 `de-quantization`. 
- `S`和`Z`统称为量化参数, 多数的量化算法可以理解为找到更好的S和Z使得量化模型的结果尽可能逼近原模型的结果. 

LLMs模型推理大致分为两个stage: **context** and **generation**. 
- 在context阶段：causal attention 因果注意力, 其行为可以类比训练的**前向过程**; 
- generations阶段：sequence length恒等于1。

这就要求推理框架需要支持**两套**计算逻辑(在FasterTransformer中可以看出)以适配其不同的特点. 在多数情况下, context阶段是**compute bound**(这不一定, 需要seqlen大于计算强度), 而generation是**IO bound**. 

很多情况下, generation较context在应用中出现频率更高, 而量化模型由于其低位宽的权重表征, 可以大大缓解IO bound现象. (当然如果在服务时使得batch化技术来加大一次推理的batch的话, 量化的效果可能会退化为节约模型存储(功耗)下降).

关于量化的基本原理和实现细节，可参考笔者之前的文章：
- [大语言模型的模型量化(INT8/INT4)技术](https://zhuanlan.zhihu.com/p/627436535)

许多关于 Transformer 模型量化的研究都有相同的观察结果：简单的低精度（例如 8 bit）训练后量化会导致性能显着下降，这主要是由于动态的 activation 和静态的 weight 量化策略无法保持一致。
- ![](https://pic1.zhimg.com/80/v2-9824082fcbbcba958934a4a4f5eab918_1440w.webp)

为了不损失精度而提高性能，可以考虑 WeightOnly 量化技术，即只把 Weight 量化成 int8 格式，以降低访存压力。到实际 Kernel 内部再 Dequantize 回 fp16，进行矩阵乘计算。这种方法在 BS 较小是比较有效(因为此时的瓶颈在IO)，BS 较大时(因为此时的瓶颈在计算)效果变差。
- ![](https://pic4.zhimg.com/80/v2-58705e83db3886efa8206769eb4d657b_1440w.webp)

WeightOnly 量化的典型案例是 [AWQ: Activation-aware Weight Quantization](https://arxiv.org/pdf/2306.00978.pdf)，即只对 weight 进行量化以实现压缩和加速的效果。

##### LLMs的量化方法

常见方法
- LLM.in8
- SmoothQuant
- GPTQ

(1) [LLM.int8()](https://arxiv.org/abs/2208.07339)

由于input的outliers只会固定在几个特定的hidden-dim的特点(LLaMA模型中也有该现象, 且随着模型加深越发严重. RMSNorm引起), 且outliers占据的dims很少(不到1%). 故提出将Linear拆成两部分, 一部分为`int8`, 一部分为`fp16`, 分别计算后相加. 该方法得到广泛的应用, 有两个方面
- 一个是因为被huggingface集成
- 另一个是因为其几乎不掉点. 

但该方法的缺点也是比较明显: 
- 模型量化仅到8bit, 仍是4bit的2倍大; 
- Linear的latency大幅上升, 原因在于它拆成两个matmul kernel, 而且后续为了fp16相加引入外积操作等, 即计算流程更为复杂多步.
- ![](https://pic4.zhimg.com/v2-af08f0cdb101569e054d26af4d984e6f_b.jpg)


(2) ZeroQuant系列
- [v1](https://arxiv.org/abs/2206.01861)
- [v2](https://arxiv.org/abs/2303.08302))

首次对采用input token-wise quantization 并结合 weight group-wise quantization; 另外设计LKD(Layerwise Knowledge Distillation, 使用随机生成的数据); 同时, 还做了一些kernel fused的工作, 实现了一个适配于int8的backend. 这系列的工作都比较像technical report, 且适用的模型尺寸比较小, 均在20B以下. 方法的scaling效果较差, 建议follow其量化粒度的设计.

(3) [SmoothQuant](https://arxiv.org/abs/2211.10438)

同样是为了解决input outlier的问题, `韩松`团队提供将input的动态范围除上scale(该scale > 1即可以实现动态范围减小, 从而改善量化结果), 并将该scale吸到下一层的weight内, 利用weight的细粒度量化来承担该量化困难(因为input往往使用token-wise quantization, 而weight通常使用channel-wise quantization或group-wise quantization). 相较于LLM.int8(), **由于input和weight全都是int8**, 并不会出现复杂的计算逻辑, 可以调用CUTLASS默认实现的int8 gemm来加速. 其缺点为: 精度没有LLM.int8()有保证, 且容易受到calibration-set的影响), 同时一旦weight精度调至4bit, 则模型精度下滑严重)
- ![](https://pic2.zhimg.com/80/v2-95f07c67325401e2c64a3f93701db989_1440w.webp)

(3) **[GPTQ](https://arxiv.org/abs/2210.17323)**

经典之作, 目前几乎是4bit/3bit方案的**默认首选**, 但也仅限于开源世界的娱乐可用, 离落地认定的靠谱精度还是有比较大的距离. 源于同一团队在nips22的工作([Optimal Brain Compression](https://arxiv.org/abs/2208.11580))延伸, 其同样将方法泛化到剪枝领域(也是大模型剪枝领域的经典, SparseGPT). 该方法的思路大致为: 利用hessian信息作为准则判定每个权重量化后对输出loss(通常定义为MSE)造成的影响, 量化影响最大的权重(即最敏感)挑选出来先进行量化, 然后对其他权重进行更新来补偿该权重量化导致的影响, 如此往复, 直至全部量化结果. 当然, 在GPTQ中作了一些简化, 比如是基于列元素进行量化循环, 来减少算法的运行时间. 该方法的优点: 首次将4bit/3bit权重量化在176B的模型上做work, 同时也提出对应的kernel(但比较糙, 优化空间大, 有不少团队做了优化). 缺点: 4bit/3bit的方案原始kernel由于有unpack操作, 导致gemv操作的计算时间低于fp16), 且精度距离落地有明显距离. 注: 从它开始, 很多人只开始研究4w16f的方案(即weight-only quantization), 因为在batch=1的gemv计算中, 只需要控制权重的读入时间即可, 且input的动态范围过大, 量化掉点过大.

(4) **[AWQ, Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978)**

SmoothQuant的续作, 从源代码来看, 它对SmoothQuant中计算scale时需要的超参alpha, 增加 了一步通过grid search得到每个scale的最优参数, 但论文的故事包装得很好, 同时取得的效果也是十分显著的, 符合大道至简的准则. 该方案是也是4-bit weight-only quantization, 其kernel实现凭借对PTX的深刻理解和应用, 取得了目前这些weight-only quantization的方案的第一. 在此基础上稍加优化即可以得到一个不错的baseline.

(5) **[SqueezeLLM](https://arxiv.org/abs/2306.07629)**

通过观察到部分权重决定了最终模型的量化性能, 提出以非均匀量化的方式缩小这些敏感权重的量化误差. 即通过loss的二阶hessian信息来确定量化敏感的权重, 将量化点安置在这些敏感权重附近, 其它点以MSE最小来安置. 该方法以少量的存储空间换来了目前最优的4-bit weight精度, 但其缺点也是极其明显: 由于采用LUT来实现非均匀量化, 导致其kernel在batch > 1(文中的batch我均定义为 batch \* seqlen)的情况下, Linear的执行速度急剧下滑。
- ![](https://pic3.zhimg.com/80/v2-2d1dd16e6a853fb194a503e01562531e_1440w.webp)

(6) **QLoRA**

这里顺带简单介绍一下QLoRA. 该方法提出4-bit NormalFloat, 一种新的数制(属于非均匀量化), 从理论角度上证明是4bit最优数制。 利用该方法量化模型的backbone得到4-bit的backbone, 然后基于lora进行SFT, 在只需要4-bit模型权重的情况下完成SFT, 从而使得许多人可以实现在单张消费级卡(i.e. 3080)上玩LLaMA。但当时我跑它的时候, 其缺点就是明显的kernel速度慢, 原因同样是因为它需要通过LUT来实现, 不知道现在情况怎么样了.

**Summary & Future**

4-bit weight-only quantization是一个相对比较均衡的方案。 在这个setting下, 量化的研究工作应更多集中在模型的精度提升的层面上, 尽可能地减少对模型智能的影响. 但对于如果想进一步得到更轻更快更强的模型, 可以从其他小型化策略入手. 在这些策略中, 蒸馏是一个最值得往前走的方案. 在LLaMA-2的tecnical report中就有多处地方使用了蒸馏, 比如: 在RLHF阶段仅用70B的reject sampling dataset来fine-tuning其他几个小尺寸的模型, 以及很多人都会尝试去用GPT4的SFT数据来fine-tuning自己的模型. 剪枝不太推荐, 因为至少从SparseGPT的复现结果来看, 除了非结构化剪枝精度还算有保证外, 其余方案精度下滑明显, 包括NV的2:4和4:8方案, 距离落地还有些距离, 且和量化结合后并不能进一步拿到50%的压缩收益。最后, 再提几点我认为有可能的方向:
-   更加系统全面地推理优化,包括: 更深度更大粒度的kernel-fusion, 其他部件优化(i.e. long context 下kv-cache的存储和IO时间, attention计算优化), system2的推理路径的优化
-   在模型训练中引入量化友好的策略, 来使得模型的权重和激活可以变得对量化不敏感, 从而实现4w4f
-   尝试引入QAT方案, 达到所见即所得, 拥抱极限 -- 但这个有点太激进, 还是需要在模型有足够理解后去尝试.
-   端云推理的协同优化, 即手机端和GPU之间如何交互, 利用手机端训个人SFT, 分配算力等

#### 2.3 蒸馏(Distillation)

[知识蒸馏](https://arxiv.org/abs/2006.05525)是一种构建更小、更便宜的模型（“student 模型”）的直接方法，通过从预先训练的昂贵模型中转移技能来加速推理（“ teacher 模型”）融入 student。 除了与 teacher 匹配的输出空间以构建适当的学习目标之外，对于如何构建 student 架构没有太多限制。

![](https://pic2.zhimg.com/80/v2-9dfcd56236628ab5d5e81c8a88f9e081_1440w.webp)

知识蒸馏基本框架

给定数据集，训练 student 模型通过蒸馏损失来模仿 teacher 的输出。 通常神经网络有一个softmax层； 例如，LLM 输出 token 的概率分布。 将 softmax 之前的 logits 层表示为 $\mathbf{z}_t$$\mathbf{z}_t$\\mathbf{z}\_t 和 $\mathbf{z}_s$$\mathbf{z}_s$\\mathbf{z}\_s , 分别表示 teacher 和 student 模型。 蒸馏损失最小化两个 softmax 输出之间的差异（温度 $T$$T$T ）。 当标签 $y$$y$y 已知，可以将其与student 的 logits 之间计算交叉熵，最后将两个损失相加，如下：

$\mathcal{L}_{\mathrm{KD}}=\mathcal{L}_{\text {distll }}\left(\operatorname{softmax}\left(\mathbf{z}_t, T\right), \operatorname{softmax}\left(\mathbf{z}_s, T\right)\right)+\lambda \mathcal{L}_{\mathrm{CE}}\left(\mathbf{y}, \mathbf{z}_s\right)$$\mathcal{L}_{\mathrm{KD}}=\mathcal{L}_{\text {distll }}\left(\operatorname{softmax}\left(\mathbf{z}_t, T\right), \operatorname{softmax}\left(\mathbf{z}_s, T\right)\right)+\lambda \mathcal{L}_{\mathrm{CE}}\left(\mathbf{y}, \mathbf{z}_s\right)$\\mathcal{L}\_{\\mathrm{KD}}=\\mathcal{L}\_{\\text {distll }}\\left(\\operatorname{softmax}\\left(\\mathbf{z}\_t, T\\right), \\operatorname{softmax}\\left(\\mathbf{z}\_s, T\\right)\\right)+\\lambda \\mathcal{L}\_{\\mathrm{CE}}\\left(\\mathbf{y}, \\mathbf{z}\_s\\right)

在 Transformer 中一个典型案例是[DistilBERT](https://arxiv.org/abs/1910.01108)，模型参数减少 40%，速度提升71%。在大模型时代，蒸馏可以与量化、剪枝或稀疏化技术相结合，其中 teacher 模型是原始的全精度密集模型，而 student 模型则经过量化、剪枝或修剪以具有更高的稀疏级别，以实现模型的小型化。

### 三、并行化（Parallelism）

当前的推理的并行化技术主要体现在3个维度上，即 3D Parallelism:
-   Data Parallelism(DP)
-   Tensor Parallelism(TP)
-   Pipeline Parallelism(PP)

![](https://pic1.zhimg.com/80/v2-a2af7781f1545f571af334383f3d5994_1440w.webp)

3D Parallelism 的3个维度

#### 3.1 数据并行 (Data Parallelism, DP)

在推理中，DP 主要是增加设备数来增加系统整体 Throughput，其中最经典的即DeepSpeed的Zero系列

![](https://pic4.zhimg.com/80/v2-7eca670d33d4ac372507c02038970123_1440w.webp)

另外 FSDP 也比较高效和易用

![](https://pic1.zhimg.com/80/v2-cf1d706571abca543117f453e9289d20_1440w.webp)

#### 3.2 张量并行(Tensor Parallelism, TP)

在推理中，TP 主要是横向增加设备数通过并行计算来减少 latency，其实现原理及细节可参考笔者之前的文章
- [GPT 的张量并行化（tensor parallelism）方案](https://zhuanlan.zhihu.com/p/603908668)

当前也有一些方便易用的 TP 方案，如 [BlackSamorez/tensor\_parallel](https://github.com/BlackSamorez/tensor_parallel) ，使用起来非常简单：

```py
import transformers
import tensor_parallel as tp
tokenizer = transformers.AutoTokenizer.from_pretrained("facebook/opt-13b")
model = transformers.AutoModelForCausalLM.from_pretrained("facebook/opt-13b")  # use opt-125m for testing

model = tp.tensor_parallel(model, ["cuda:0", "cuda:1"])  # <- each GPU has half the weights

inputs = tokenizer("A cat sat", return_tensors="pt")["input_ids"].to("cuda:0")
outputs = model.generate(inputs, num_beams=5)
print(tokenizer.decode(outputs[0])) # A cat sat on my lap for a few minutes ...

model(input_ids=inputs, labels=inputs).loss.backward()  # training works as usual
```

当前主流的推理框架都支持 TP 的方式，包括但不限于：
-   [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf)
-   [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)
-   [DeepSpeed Inference](https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/inference)
-   [vLLM](https://github.com/vllm-project/vllm)
-   [Text Generation Inference](https://github.com/huggingface/text-generation-inference)
-   [ParallelFormers](https://github.com/tunib-ai/parallelformers)
-   [ColossalAI](https://github.com/hpcaitech/ColossalAI)
-   [FlexFlow](https://github.com/flexflow/FlexFlow)
-   [LiBai](https://github.com/Oneflow-Inc/libai)
-   [AlpaServe](https://arxiv.org/pdf/2302.11665.pdf)

#### 3.3 流水线并行(Pipeline Parallelism, PP)

在推理中，PP 主要是纵向增加设备数通过并行计算来支持更大模型，同时提高设备利用率。

![](https://pic2.zhimg.com/80/v2-53ac64265bc0ce890b783956741169b1_1440w.webp)

通常来说，PP 需要与 TP 结合以支持更大模型，并实现最佳效果

![](https://pic2.zhimg.com/80/v2-e8d2aad9d31b31e5f01ec9a71cf3eae5_1440w.webp)

### 四、Transformer 结构优化

该类方法主要通过优化 Transformer 的结构以实现推理性能的提升。

#### 4.1 FlashAttention

该部分的实现细节可参考笔者之前的文章，在此不予赘述
- [从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能](https://zhuanlan.zhihu.com/p/638468472)

[FlashAttention-v2](https://tridao.me/publications/flash2/flash2.pdf) 在原基础上做了改进，使其在算法、并行化和工作分区等方面都有了显著改进，对大模型的适用性也更强。在A100 上性能数据如下：

![](https://pic2.zhimg.com/80/v2-d393f3e1d664cff181026a7b754ffefd_1440w.webp)

#### 4.2 PagedAttention

可参考
- [从 FlashAttention 到 PagedAttention, 如何进一步优化 Attention 性能](https://zhuanlan.zhihu.com/p/638468472)

#### 4.3 FLAT Attention

[FLAT Attention](https://arxiv.org/pdf/2107.06419.pdf)

FLAT-Attention 与 FlashAttention 采取不同的路线来解决同一问题。 提出的解决方案有所不同，但关键思想是相同的（tiling 和 scheudling）。下面主要讨论二者不同之处：

![](https://pic1.zhimg.com/80/v2-4aaef09f88889bee0ea87af715d2e674_1440w.webp)

**4.3.1 Tiling 策略比较**

FlashAttention 使用块平铺和权重固定。 FLAT-Attention 使用行平铺（行粒度）和输出固定。

![](https://pic4.zhimg.com/80/v2-9288c9716e66141781431e6d3eedf713_1440w.webp)

**4.3.2 Scheduling 策略(数据流)比较**

FlashAttention 的 Scheduling 过程

![](https://pic2.zhimg.com/80/v2-2dcdce1109983263b1492f2823841bfd_1440w.webp)

![](https://pic3.zhimg.com/80/v2-ef4bd480264d54814de243b3ca2544f2_1440w.webp)

FLAT-Attention 的 Scheduling 过程

![](https://pic4.zhimg.com/80/v2-6c4bdb8f7742e1dd8bf7725189c1b7c7_1440w.webp)

![](https://pic1.zhimg.com/80/v2-9c707e5a5b342707478ef54ceaeaf744_1440w.webp)

### 五、动态批处理（Dynamic Batch, Continuous batch）

该类方法主要是针对多 Batch 的场景，通过对 Batch 的时序优化，以达到去除 padding、提高吞吐和设备利用率。传统的 Batch 处理方法是静态的，因为Batch size 的大小在推理完成之前保持不变。

如下图所示，使用静态 Batch 完成四个序列。 在第一次迭代（左）中，每个序列从prompt（黄色）生成一个token（蓝色）。 经过几次迭代（右）后，每个完成的序列都有不同的大小，因为每个序列在不同的迭代中发出其序列结束标记（红色）。 可见序列 3 在两次迭代后就已经结束，但仍然需要等待 Batch 中的最后一个序列完成生成（在本例中，序列 2 在六次迭代后）才能统一输出，这意味着 GPU 未被充分利用。

![](https://pic2.zhimg.com/80/v2-88d3d261993d2d8e282a615e9331b835_1440w.webp)

静态 Batch 的推理情况

下面我们来研究 Dynamic Batch 是如何优化这一过程的。

#### 5.1 ORCA

[ORCA](https://www.usenix.org/system/files/osdi22-yu.pdf)

Orca 不是等到 Batch 中的所有序列完成生成，而是实现 _iteration_ 级调度，其中Batch size由每次迭代确定。 结果是，一旦 Batch 中的序列完成生成，就可以在其位置插入新序列，从而比静态 Batch 产生更高的 GPU 利用率。

![](https://pic1.zhimg.com/80/v2-58ae1d63829f910e0cfc7219c9f8c730_1440w.webp)

下图展示了使用 Dynamic Batch 完成七个序列的过程。 左侧显示单次迭代后的批次，右侧显示多次迭代后的 Batch 。 一旦序列发出序列结束标记，就在其位置插入一个新序列（即序列 S5、S6 和 S7）。 这样可以实现更高的 GPU 利用率，因为 GPU 不会等待所有序列完成才开始新的序列。

![](https://pic2.zhimg.com/80/v2-e6dace0f5c6f874039f626eafcf8fa9d_1440w.webp)

结果显示在延时不变的情况下，其相对于FasterTransformer 可获得 36.9 倍的吞吐提升。

![](https://pic1.zhimg.com/80/v2-7fb66789eae51576b8841388f59d2a0c_1440w.webp)

### 5.2 FastServe

[FastServe](https://arxiv.org/pdf/2305.05920.pdf)

ORCA 使用first-come-first-served (FCFS) 处理推理作业, 计划任务持续运行直至完成。 由于 GPU 内存容量有限以及推理对延时敏感，无法通过任意数量的传入函数来增加处理，由此可能会导致队列阻塞。

FastServe 使用 preemptive scheduling，通过新颖的跳跃连接 Multi-Level Feedback Queue 程序来最小化延时。 基于 LLM 推理的长度无法确定，调度程序利用输入长度信息来分配适当的初始值每个到达作业要加入的队列。 较高优先级队列跳过加入的队列以减少降级。 设计高效的GPU内存管理机制主动下载和上传 GPU 内存和主机内存之间的中间状态，以进行 LLM 推理。

![](https://pic3.zhimg.com/80/v2-924abfec2b40e77cfc4420d0b8a16d72_1440w.webp)

实验表明，该方法比ORCA有明显的性能提升

![](https://pic1.zhimg.com/80/v2-e826eaa832d2c9b973ba86b6ceaa074c_1440w.webp)

### 5.3 [vLLM](https://vllm.ai/)

vLLM 的核心是 PagedAttention，其灵感来自传统操作系统概念，例如分页和虚拟内存。 它们通过在固定大小的“页面”或块中分配内存，允许 KV 缓存变得不连续。 然后可以重写 attention 机制以对块对齐的输入进行操作，从而允许在非连续的内存范围上执行 attention 。

这意味着 cache 分配可以 just-in-time，而不是 ahead-of-time：当启动一个新的生成任务时，框架不需要分配大小为 Maximum\_context\_length 的连续 cache。 每次迭代，调度程序都可以决定特定生成任务是否需要更多空间，并动态分配，而不会降低 PagedAttention 的性能。 这并不能保证内存的完美利用（浪费现在限制在 4% 以下，仅在最后一个块中），但它明显改善了当今业界广泛使用的提前分配方案的浪费 。

总而言之，PagedAttention + vLLM 可节省大量内存，因为大多数序列不会消耗整个上下文窗口。 这些内存节省直接转化为更高的 Batch 大小，这意味着更高的吞吐量和更便宜的服务。

实验表明，该方法相比于静态 Batch 与其他动态 Batch 的方法吞吐性能提升明显。

![](https://pic1.zhimg.com/80/v2-4d15f8b5575c5ac67cede615a335d3b0_1440w.webp)

### 5.4 Text Generation Inference

[Text Generation Inference](https://github.com/huggingface/text-generation-inference)

TGI 是 HuggingFace 开发的基于 Rust, Python 和 gRPC 的推理服务工具，其基本框架如下：
- ![](https://pic1.zhimg.com/80/v2-3ae33f707e939f4ff89cd4f1680960e0_1440w.webp)

关于 TGI 的用法，可参考笔者的文章，同时对比了和 vLLM 和 FasterTransformer 的性能。

[紫气东来：小记：主流推理框架在Llama 2 的上性能比较](https://zhuanlan.zhihu.com/p/646772063)

### 5.5 LMDeploy

[LMDeploy](https://github.com/InternLM/lmdeploy)

LMDeploy 是由 MMRazor 和 MMDeploy 团队开发的用于压缩、部署 LLM 服务的工具包。 它具有以下核心特点：
-   TurboMind：基于FasterTransformer 的高效推理引擎。
-   交互推理：通过缓存多轮对话过程中的 k/v，记住对话历史，以避免对历史会话的重复处理。
-   多GPU模型部署和量化
-   Dynamic Batch

### 六、硬件升级

以上主要介绍了在算法和模型层面的优化方法，除此之外，升级硬件系统可以进一步提升整体性能，下面将介绍几种可用于(和潜在的)推理加速的硬件产品。

#### 6.1 NVIDIA H100 PCIe

[NVIDIA H100 PCIe](https://www.nvidia.cn/data-center/h100/)

NVIDIA H100 核心架构与 Ampere 相似，数学运算部分布置在144组CUDA上，最高可拥有18432个FP32(单精度)、9216个FP64(双精度)CUDA核心，辅以576个第四代Tensor核心。H100核心采用台积电的N4工艺制造，内建800亿个晶体管，核心面积仅有814m㎡。其与A100 主要参数对比如下：
- ![](https://pic2.zhimg.com/80/v2-bf41cd47353338d3d6e9345fb84fc275_1440w.webp)

在性能方面，H100 较 A100 也有明显提升，其部分数据如下所示。
- ![](https://pic1.zhimg.com/80/v2-4c0e24b9d35250c471288cffa77764b8_1440w.webp)

#### 6.2 AMD MI300

AMD MI300 处理器集成了24个Zen 4架构CPU核心，以及CDNA 3架构GPU核心，周围还有着8颗HBM3高速缓存，容量高达128GB，总计拥有1460亿个晶体管。与上一代 MI250相比，MI300进行AI运算的速度将提高至8倍，能效方面也将提升5倍。

目前未找到公开的在 LLM 方面的推理性能数据。

#### 6.3 Apple M2 Ultra

[Apple M2 Ultra](https://www.apple.com/newsroom/2023/06/apple-introduces-m2-ultra/)

M2 Ultra 采用第二代 5 纳米工艺制造，并使用 Apple 突破性的 UltraFusion 技术连接两个 M2 Max 芯片的芯片，使性能提高一倍。 M2 Ultra 由 1340 亿个晶体管组成，比 M1 Ultra 多了 200 亿个。 其统一内存架构支持突破性的192GB内存容量，比M1 Ultra多出50%，并具有800GB/s的内存带宽，是M2 Max的两倍。 M2 Ultra 配备更强大的 CPU（比 M1 Ultra 快 20%）、更大的 GPU（快 30%）以及神经引擎（快 40%）。

目前未找到公开的在 LLM 方面的推理性能数据。

#### 6.4 Graphcore IPU

Graphcore C600 IPU处理器PCIe卡是针对机器学习推理应用的高性能加速卡。每个IPU具有1472个处理核心，能够并行运行8832个独立程序线程。每个IPU都有900MB的片上SRAM存储。用户可以在单个机箱中直接连接多达8块卡，通过高带宽的IPU-Links进行桥接。在训练和推理自然语言处理 (NLP) 模型（如 BERT 和 GPT、图神经网络 (GNN)、目标检测、语音等）时表现出色的结果。

目前未找到公开的在 LLM 方面的推理性能数据。

#### 6.5 Biren BR100

BR100是由壁仞科技发布自主研发的首款通用GPU芯片，其16位浮点算力达到1000T以上、8位定点算力达到2000T以上，单芯片峰值算力达到PFlops（1PFlops等于1000万亿次浮点指令/秒）级别。其与 H100 的参数对比如下所示：

![](https://pic3.zhimg.com/80/v2-f5414cf8ea9456a43fe38f49c4c8735e_1440w.webp)

目前未找到公开的在 LLM 方面的推理性能数据。


## 本地 LLM

### LLaMA

模型权重是浮点数。就像表示大整数(例如1000)比表示小整数(例如1)需要更多的空间一样，表示高精度浮点数(例如0.0001)比表示低精度浮点数(例如0.1)需要更多的空间。量化大型语言模型的过程涉及降低表示权重的精度，以减少使用模型所需的资源。GGML支持许多不同的量化策略(例如4位、5位和8位量化)，每种策略在效率和性能之间提供不同的权衡。

量化后模型大小的对比：
- ![](https://pic4.zhimg.com/80/v2-f21157c61f394b898fa7c23a11510bcb_1440w.webp)
- [Refer](https://zhuanlan.zhihu.com/p/639565332)

#### LLaMA.cpp

【2023-8-21】[研究完llama.cpp，我发现手机跑大模型竟这么简单](https://www.toutiao.com/article/7268183078362087976)
- [llama.cpp](https://github.com/ggerganov/llama.cpp) 项目用原始 C++ 重写了 LLaMa 的推理代码，效果极好

经过优化和量化权重，各种以前无法想象的硬件上本地运行 LLaMa 模型。其中：
- 谷歌 Pixel5 手机上，它能以 1 token/s 的速度运行 7B 参数模型。
- M2 芯片的 Macbook Pro 上，使用 7B 参数模型的速度约为 16 token/s
- 甚至于在 4GB RAM 的树莓派上运行 7B 模型，尽管速度只有 0.1 token/s

2023 年 6月，llama.cpp  作者 Georgi Gerganov 开始创业，宣布创立一家新公司 ggml.ai，旨在用纯 C 语言框架降低大模型运行成本。

GPU 对深度学习有两个主要好处：
- 很大的内存带宽（如 A100：1935 GB/s，RTX 4090：1008 GB/s）
  - 关系到数据从 HBM 内存（即 RAM）移动到片上内存需要花费的时间。片上内存相当小（A100 上为 40MB，而 RAM 为 40-80GB）内存带宽比计算性能小约 2 个数量级
  - 内存带宽几乎是与 transformer 采样相关的最大限制因素。任何降低这些模型内存需求的方法都会使它们更容易提供服务 —— 比如量化
- 很大的算力（A100：FP16 有 312 TFLOPS，RTX 4090：FP16 有 82.6 TFLOPS）

llama.cpp 使用深度学习推理中较为激进的 `int4` 格式，因此 KV 缓存的 RAM 需求减少到 1.33GB，模型参数的 VRAM 减少到 16.25GB。

## 端侧LLM实现

模型手机部署
- 【2023-9-7】[手机大模型也卷起来了](https://www.toutiao.com/article/7276004198729581115)

### 手机大模型

资讯
- [C-Eval](https://cevalbenchmark.com/static/leaderboard.html)全球大模型综合性考试评测榜上，也分别出现了 `vivo` 和`OPPO` 自研大模型云端方案
- 除了华为，小米、OPPO、vivo等厂商也在积极入局大模型，试图在手机大模型这一领域抢占先发优势。

手机厂商，All in大模型
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/203ad77492394051851ee1784159c00d~tplv-tt-origin-asy2:5aS05p2hQGnpu5Hpqaw=.image?_iz=58558&from=article.pc_detail&x-expires=1694759358&x-signature=Z27rZcowyVZg928D14%2F7YEZsLg4%3D)

国内手机厂商开卷大模型，背景是中国智能手机市场的持续低迷。为了在瓶颈期寻求新的差异化增长点，国产手机厂商已经在影像力、参数、产品形态等方面卷到了极致。

虽然手机厂商都将大模型作为未来业务发力点，但目前手机大模型尚未全面对用户开放，仍处于小范围内测阶段，用户对它的接受程度还不得而知。

而**手机大模型**不仅要面对**友商**之间的竞争，还要面临像`文心一言`APP这样的**云端大模型**竞争对手。
- 文心一言发布初期，参数已经高达**2600亿**，支持多模态的生成式AI内容，比如文生图、代码理解、生成Excel等功能，覆盖了办公、学习、情感、绘画、娱乐等应用场景。
- `小爱同学`在本地部署的大模型受限于**手机算力**，参数相对要更轻量化。

小爱同学13亿的参数量跟文心一言2600亿的参数量并不是同一个数量级，因此应用场景也有一定局限性，就目前内测版本的反馈来看，小爱同学还不支持文生图功能。

相比ChatGPT、文心一言等云端大模型，手机大模型也有其得天独厚的优势
- 因为完全**本地部署**，用户在使用它时的操作对话也在本地运行，在用户数据的**隐私和安全性方**面更有保障。
- 因为参数量较为**轻量化**，加载运行速度也会更快，**不受网络环境限制**。
- 这样轻量化的大模型训练**周期短**，且能根据用户需求进行快速的迭代更新。

另外，可定制化的手机大模型将能更灵活地响应用户的各种需求，用户可以根据自己的兴趣、工作性质、起居习惯定制个性化的大模型助手。比如大模型助手可以根据用户画像，为用户提供饮食起居建议等个性化服务，也可以根据用户喜好，扮演其喜爱的角色形象。

目前的手机大模型实用价值还有待挖掘，但它被认为是打造品牌形象和彰显产品算力的全新范式。

尽管从理论上来说，目前市面大多数智能手机都能达到要求(6GB内存)，但是能够**运行本地大模型**的手机会自然地被消费者认为算力和配置更高。
- 类似**无线充电**这种功能，消费者的使用频率很低，但缺少了无线充电的手机，一定不是一台合格的旗舰机。
- 手机大模型也是同理，未来消费者也可能更加倾向于优先选购那些部署大模型的智能手机。

端手机市场，**自研芯片**始终是彰显品牌能力的杀手锏，但在`Mate 60`系列回归前，华为`麒麟`被制裁、OPPO`哲库`解散，国产手机芯片的自主研发也进入了至暗时刻。因此手机厂商退而求其次，选择在影像方向构建自己的护城河，国产旗舰机型都在讲述自己的影像故事，他们力求通过影像层面的差异化打造品牌的高端形象。而手机大模型的出现也为品牌的高端化故事提供了一个新角度，因此布局大模型也成为国产手机厂商的共识。


### AX650N

【2023-6-1】[5分钟就能完成原版Swin Transformer端侧部署](https://zhuanlan.zhihu.com/p/633942783)

一款号称现实开源模型直接拿来用，还能让性能、功耗与自动驾驶领域基于GPU的端侧芯片有得一拼的平台诞生了。
- 爱芯元智推出AX650N，对Transformer架构支持效果尤甚。

Transformer 是当下最火的 ChatGPT、Stable Diffusion等大模型背后的基础架构。

AX650N 是 AI芯片公司爱芯元智发布的第三代端侧芯片。其构成包括CPU和NPU等，其中CPU采用的是八核A55处理器，NPU则采用了自研混合精度技术，可以做到43.2TOPs（INT4）或10.8TOPs（INT8）的高算力。

AX650N主要用于**端侧视觉感知**。
- ![](https://pic2.zhimg.com/80/v2-2c23c0da748d54750a4d683c8528a995_1440w.webp)
- 该领域业界主要还是基于CNN网络开发应用。
- 准确率和性能双佳的`Swin Transformer`并没有得到突出的大规模落地，还是多部署于云端服务器。

因为GPU对于MHA结构（Transformer中的多头注意力机制）计算支持更友好。

而目前大部分端侧AI芯片由于其**架构限制**为了保证CNN结构的模型效率更好，基本上对MHA结构没有过多性能优化，因此需要修改Swin Transformer的网络结构才能勉强将其部署在端侧—— 一旦修改网络结构，就意味着将出现一系列问题，例如精度下降，精度一降就得对模型进行重训，这个过程就要以星期甚至是月来计算了。

爱芯元智联合创始人、副总裁刘建伟介绍：
> 用AX650N在端侧部署原版Swin Transformer，从拿到测试板到demo复现，只需要5分钟，再到在自己的私有环境里跑起来私有模型，只要1个小时就能搞定。

不仅能跑起来，还跑得飞快、性能高且功耗低。AX650N端侧部署`Swin Transformer`性能可达361 FPS。
- AX650N 还支持低比特混合精度，遇到大规模参数的模型，我们就可以采用INT4来减少内存和带宽占用率，从而降低大模型在端侧边缘侧部署的成本。
- AX650N可以说是成为了目前对Transformer架构支持最好的一个端侧部署平台。
- AX650N还适配ViT/DeiT、DETR在内的Transformer模型，Meta最新发布的视觉模型DINOv2也达到了30帧以上的运行结果。

因此，有了AX650N，下游进行检测、分类、分割等操作也更加方便。
- ![](https://pic3.zhimg.com/80/v2-c1c280c25a868230fccbf3208e49a302_1440w.webp)

接下来，爱芯元智AX650N将会针对Transformer结构进行进一步优化，并且将探索多模态方向的Transformer模型。



### mlc

Github上开源的手机大模型`mlc-chat`为例，这个**60亿**参数的大模型可以在**6GB**内存以上配置的手机上运行

### 高通

高通认为
- 在`云端`和`PC`、`智能手机`等终端之间协同工作的**混合AI**才是AI的未来，将大模型从**云**到**端**部署，尤其进入**手机**，是必然的演进路线。
- 参数超过**10亿**的AI模型已经能够在搭载高通处理器的手机上运行，且性能和精确度水平达到与**云端**相似的水平。
- 在MWC上，高通演示了在**不联网**状态下用安卓手机运行 `Stable Diffusion`，15秒即可生成AI图像。

不过，高通中国区研发负责人`徐晧`曾提到过
- “大模型在手机上是可行的，但真正大规模地部署仍需要时间”。

- 【2023-8-8】[大模型在手机上运行的预言，被高通提前实现了](https://www.leiphone.com/category/industrynews/SPX5rXn2JIfuGELC.html)
- 【2023-7-5】[安卓手机上跑15亿参数大模型，12秒不到就推理完了](https://www.51cto.com/article/759615.html)

- 操作人员在一部没有联网的安卓手机上使用了Stable Diffusion 来生成 AI 图像，整个生成时间不超过 15 秒，整个过程完全在终端进行，但是生成效果却没打一点折扣。 
- [白皮书链接](https://www.qualcomm.cn)
- ![](https://static.leiphone.com/uploads/new/images/20230629/649d22502f28e.jpg?imageView2/2/w/740)
- ![](https://s2.51cto.com/oss/202307/05/4405326526ce7c92c19505e53b006d3fd4d863.gif)
- ![](https://static.leiphone.com/uploads/new/images/20230629/649d225ec79ab.jpg?imageView2/2/w/740)

如果模型大小、提示（prompt）和生成长度小于某个限定值，并且能够提供可接受的精确度，推理即可完全在终端侧进行。如果是更复杂的任务，模型则可以跨云端和终端运行。 

混合 AI 还能支持模型在终端侧和云端同时运行，也就是在终端侧运行轻量版模型时，在云端并行处理完整模型的多个标记（token），并在需要时更正终端侧的处理结果。这能极大限度地解决能耗和成本问题。 

直接从源头减少数据运输过程，隐私泄露的问题便不复存在。 高通指出，混合 AI 架构中有一个“隐私模式”，当用户利用终端侧 AI 向聊天机器人输入健康问题或创业想法等敏感话题时，这个模式会自动开启。


### 苹果 


#### 布局

【2023-8-8】[苹果年薪百万开招AIGC人才，目标：让iPhone本地跑上大模型](https://mp.weixin.qq.com/s/MuZxazt3VXiR0WuAAP8UdQ)
- 苹果想要将大模型压缩到终端，在未来让iPhone/iPad等核心产品直接跑上AIGC技术。苹果想要在核心产品上发力端侧大模型，一大部分原因就是为了**隐私**。就像离线版Siri那样，不经云端直接运行AI软件，不仅可以让程序跑得更快，也能更安全和私密地处理用户数据。
  - 2020年的时候，苹果就斥资近2亿美元收购了一家总部位于西雅图的人工智能初创公司：Xnor，该公司专门在移动设备上运行复杂的机器学习模型，还一度击败了微软、亚马逊和英特尔等大厂的产品。
- 机器智能与神经设计 (Machine Intelligence Neural Design，MIND，属于苹果AIML的一部分) 等团队，要求工程师能够“在苹果下一代推理引擎中定义和帮助实现加速和压缩大型语言模型 (LLM) 的功能”，这指的就是在移动端而非云端。将“最先进的基础模型带入我们口袋里的iPhone，以保护隐私的方式实现下一代基于ML的体验”


### 华为

- 7月, 华为开发者大会上发布了面向行业的`盘古大模型3.0`，最高版本高达**1000亿**参数，同时也将盘古大模型应用到手机终端，将智能助手`小艺`接入了盘古大模型能力。
- 8月底，华为`Mate 60`系列手机发布，除了支持`卫星通信`，另一舆论关注点就是其接入了`盘古`人工智能大模型。
- 9月1日，华为`小艺`大模型开启众测，首批支持机型为Mate 60系列手机。

【2023-8-11】[华为率先把大模型接入手机！小艺+大模型，智慧助手智商](https://www.toutiao.com/article/7265924630538519081)

只需一句中文指令，华为小艺
- 写出一封英文邮件：
- 把自己的照片用AI做成不同风格：

还能说一长串指令，让它自己创建复杂场景，大白话就能听得懂

华为盘古L0基座大模型的基础上，融入大量场景数据，对模型进行精调，最后炼成的一个L1层对话模型。能搞定文本生成、知识查找、资料总结、智能编排、模糊/复杂意图理解等任务。而且也可以调用各种APP服务，实现系统级的智能化体验。

小艺依托的底层模型是华为盘古系列。

今年7月，华为正式发布盘古大模型3.0，并提出3层模型架构。
- L0：基础大模型，包括自然语言、视觉、多模态、预测、科学计算；
- L1：N个行业大模型，比如政务、金融、制造、矿山、气象等；
- L2：更细化场景的模型，提供“开箱即用”的模型服务
其中L0层基础大模型最大版本包含1000亿参数，预训练使用了超3万亿tokens。

小艺正是在华为盘古L0基座大模型的基础上，针对终端消费者场景构建了大量的场景数据，并对模型进行精调，最后炼成的L1层对话模型。

部署方面，华为正在不断增强大模型端云协同的能力，**端侧大模型**先对用户请求和上下文信息做一层预处理，再将预处理后的request请求到云侧。
- 既能发挥**端侧模型**响应快的优势，又能通过云端模型来提升问答和响应质量，同时也能更进一步保护用户隐私数据。

而在降低推理时延上，华为小艺做了**系统性工程优化**，包含从底层芯片、推理框架、模型算子、输入输出长度等全链路。

通过对各个模块时延进行拆解，研发团队明确了各部分优化目标，利用算子融合、显存优化、pipeline优化等方式降低时延。

同时 prompt长度和输出长度也会影响大模型推理速度。
- 华为针对不同场景的prompt和输出格式做了**逐字分析和压缩**，最终实现推理时延减半。

![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/849e07ed945047e396eb8b231b0b0fd1~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692416407&x-signature=mia2XHzxNcQ9PFYg2NEcTkbB6o8%3D)


### 小米

- 4月，组建大模型团队，小米大模型技术的主力突破方向为**轻量化**、**本地部署**，小米考虑的是优先在手机上实现端侧跑通，让每个人都能更好在手机上使用大模型。
- [MiLM-6B](https://cevalbenchmark.com/static/model.html?method=MiLM-6B) 是由小米全自研的大规模预训练语言模型
- 2023年8月14日，[小米新品发布会上](https://www.yzwb.net/zncontent/3150588.html)，雷军正式宣布小米**13亿**参数大模型已经成功在手机本地跑通，部分场景可以媲美**60亿**参数模型在云端运行结果。小爱同学升级AI大模型能力，并开启内测，未来小米大模型技术方向是轻量化、本地部署。
- ![img](https://imgcdn.yzwb.net/7343_1692025618000.jpg?imageMogr2/thumbnail/1080x%3E/strip/ignore-error/1|imageslim)

| 模型 | 平均分 | STEM | 人文学科 | 社会科学 | 其他 | 中国特定主题 | 
| --- | --- | --- | --- | --- | --- | --- | 
| MILM-1.3B | 50.79 | 40.51 | 54.82 | 54.15 | 53.99 | 52.26 | 
| Baichuan-13B | 54.63 | 42.04 | 60.49 | 59.55 | 56.6 | 55.72 | 
| ChatGLM2-6B | - | 41.28 | 52.85 | 53.37 | 52.24 | 50.58 |

注
- 2023年8月10日数据，CMMLU中文多任务语言理解评估

截至目前，已经有通过部分内测的用户初步尝鲜小爱大模型，并分享了“调戏”小爱同学全新版本的体验。


### vivo

vivo 即将推出的OriginOS 4.0也内置了大模型。此前，在C-Eval全球大模型综合性考试评测榜上，也分别出现了vivo 和OPPO的自研大模型云端方案。
- `vivo_Agent_LM_7B` 是由 vivo AI 全球研究院自主研发的大规模预训练语言模型，从命名不难看出它可能有着70亿参数。


### Personal GPT

A private, on-device AI Chatbot for iOS and macOS
- [Personal GPT](personalgpt.dev) is an AI chatbot that runs fully offline without an internet connection on iPhone, iPad and Macs.

[Private LLM](https://privatellm.app/)

World’s First Private AI Chatbot
- Tired of subscription fees and worried about your privacy? Experience the power of AI at your fingertips with Private LLM, AI chatbot that runs on your iPhone®, iPad® and Mac without needing an internet connection. A one time purchase lets you download and use the app on all of your Apple devices. Share it with your family with family sharing.



【2023-8-8】
- [体验地址](https://www.personalgpt.dev/chat/pKEy3n8)
- [Mac OS App](https://apps.apple.com/us/app/personal-gpt/id6448106860?l=zh-Hans-CN)
- [github](https://github.com/satpalsr/personalgpt/tree/master), js 库


# 结束