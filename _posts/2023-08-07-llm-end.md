---
layout: post
title:  大模型端侧部署 LLM Deployment at End devices
date:   2023-08-07 22:46:00
categories: 大模型
tags: 边缘计算 aiot 手机
excerpt: 如何将大模型LLM部署在边缘设备上? 端侧大模型是未来趋势
mathjax: true
permalink: /llm_end
---

* content
{:toc}


# 端侧 LLM

## 边缘计算

`端`即物联网中的**任何终端**，`端侧AI`即AI在端侧的应用，也是`边缘计算`。这种模式可以更好的支持AIoT场景
- `AI` + `IoT` = `AIoT`

边缘计算就是未来

端侧AI（On-device AI)

传统计算处理多数是基于**云侧**，把所有图像、音频等数据通过网络传输到**云中心**进行处理后将结果反馈。

但是随着数据**指数式增长**，依靠云侧的计算已经显现了诸多不足，例如：数据处理的实时性、网络条件制约、数据安全等

**端侧推理**则愈发重要。

ChagtGPT、GPT-4等AIGC大模型虽然它们很厉害，但是它们只解决了语言大模型在**服务端部署**的问题。大模型需要极高的算力和资本投入，并不是一般的企业所能承受的


### 端侧AI

云端协同
- 端侧AI当前普遍应用方式：**终端**侧处理 + **云端**处理补充，即`协同合作`（`云端协同`）
- ![](https://pic3.zhimg.com/80/v2-7f24564a3d2f300713068392c762db5a_1440w.webp)
- [图源](https://www.qualcomm.cn/on-device-ai/details)
- 参考：[端侧AI（On-device AI)](https://zhuanlan.zhihu.com/p/151288031)

端侧 AI 优势：
- 隐私保护: 数据存储本地无需上传
- 可靠: 降低数据上传云端链路过长导致的错误率
- 低延时: 本地存储计算, 减少不必要流量,选择性占用网络宽带资源
- 不依赖网络
- 个性化: 持续学习,模型调整,隐私保护的分布式学习.带来个性化的互动与体验

端侧 AI 挑战：
- **硬件资源**限制: 存储、内存、计算资源
  - `处理器`算力有限，**远低于**云端计算能力，如何满足日益复杂的端侧 AI 性能的需求至关重要
  - `内存`大小和`带宽`有限，对性能影响至关重要
- 模型**版本管理**、生命周期
- 终端种类很多，不同硬件**架构适配和模型迁移成本过高**, 同时导致了端侧AI应用**复杂度**增加

与**云端AI**面临的挑战类似
- `边缘`和`端侧AI`软硬件的成熟度以及软硬的融合很大程度上阻碍了AI的普及，降低AI开发门槛，提升通用性成为了AI普及的当务之急。
- 商业层面也面临诸多挑战，比如 应用场景众多，如何在众多的应用场景中形成一个良好的商业闭环？如何让传统行业也能轻松用上AI技术？怎么提供打动消费者的使用体验？

曾经火爆的`智能音箱`已经证明，不佳的使用体验只会让消费者迅速对新技术感到失望。
- 要实现良好的AI产品用户体验，多传感器融合以及多种AI技术的应用成为了未来的发展方向。

智能音箱反映出`端侧AI`普及面临**硬件**、**算法**以及**应用需求多样**的三道槛。
- 2018年爆发的智能音箱并没有用于实现AI功能的专用芯片，AI的算法也并不成熟，再加上整个行业的低价恶性竞争，体验不佳的智能音箱让众多尝鲜者很快放弃了这一AI“爆品”。

边缘和端侧AI硬件和算法向前迭代遇到普及三道槛
- 硬件设备
- 运行于芯片上的算法
- 应用多样，需求不同。

参考：[端侧AI普及，到底卡在哪了？](https://zhuanlan.zhihu.com/p/574589354)

### 应用

应用
- **APP**: 扫福、卡片识别等图像影音识别
- **Web**: 前端机器学习, Tensorflow.js
- **Low code**:生成前端代码
  - D2C:Design to code
    - sketch2code：手稿->code(html)
    - [imgcook](https://www.imgcook.com/bloghttps://mp.weixin.qq.com/s/WYmhBBb1x2cbdFSweNooNQ)：设计稿->code(React等)
  - D2C:Data to code
    - 阿里AVA：智能可视分析框架，从数据直接生成图表, [详见](https://mp.weixin.qq.com/s/NB3mPLktt_LytvUrtf3vow)
- 生成form表单
  - 非AI，阿里Formily
  - Json Schema=>Code
  - JSX Schema=>Code
- WebAssembly -> **IoT**
  - [Wasm 在物联网、云和多媒体领域发展现状](https://mp.weixin.qq.com/s/FFuWbU0WyVAf3n2IoAVn5w)

|案例|说明|示例|
|---|---|---|
|手机淘宝-千人千面||![](https://pic1.zhimg.com/80/v2-a1c7cced7b0ee6f4dd9cba87bf22378f_1440w.webp?source=1940ef5c)|
|支付宝扫福|运营活动|![](https://pic4.zhimg.com/80/v2-df0f01137b02d4b947affe304e92b967_1440w.webp)|
|卡片识别|银行卡识别|![](https://pic1.zhimg.com/80/v2-7c2205c4c2ff6ecfab62a2f45716ee5c_1440w.webp)|
|阿玛尼AR小程序|基于Web的TensorFlow Lite|![](https://pic3.zhimg.com/80/v2-e88c0d7288b29ef43fc70c1f1e513d6e_1440w.webp)|
|阿里PipCook|基于Tensorflow.js，为 JavaScript 开发者提供的机器学习工具集。与Tensorflow的区别在于全流程JS环境，且专注于前端领域。||
|D2C:Design to code|||

### 边缘计算设备

计算设备
- **ARM 处理器**在智能设备中占主导地位，是端侧 AI 落地的主流平台。
- `NPU`、`DSP`、`GPU`` 可以提供更高的计算能力，在端侧 AI 上有一定的应用场景，但**生态环境较差**，距离成熟还需要时间。

端侧 AI 最耗时的计算为**全连接**(FC)和**卷积**计算，底层核心计算为矩阵乘，底层计算库的性能对端侧 AI 能否落地起决定性作用。

#### 深度学习工具库

常用框架：
- Caffe2、TensorFlow Lite、Core ML


##### TensorflowLite

TensorflowLite
- 谷歌的Tensorflow适用于移动和其他边缘设备的版本
- [设备端重新训练模型](https://www.tensorflow.org/lite/examples/on_device_training/overview?hl=zh-cn), [
TensorFlow Lite 设备端训练](https://discuss.tf.wiki/t/topic/2074)


阿里的MNN
- 轻量级的深度学习端侧推理引擎, 详见[端侧AI：从探索尝试到逐步展开](https://mp.weixin.qq.com/s?__biz=MzUxMzcxMzE5Ng==&mid=2247493983&idx=2&sn=69ba3a5f8794c83bc41161bfa157d457&scene=21#wechat_redirect)


#### ARM 第三方 BLAS 库  
  
各 ARM blas 库矩阵乘法特点
- ![](https://pic3.zhimg.com/80/v2-eef49bd3304ce7e13ab8a2e6caa4533a_1440w.webp)


##### Eigen

线性代数运算的 C++ 模板库，矩阵的运算可直接用符号做。

##### OpenBLAS

由中科院计算所维护的一个开源的高性能 BLAS 库，基于 Kazushige Goto 的 GotoBLAS，支持 Fortran BLAS 和 CBLAS 接口调用。  
  
##### ARM Compute Library  

ARM 官方推出的计算库，支持 AI 的常见运算，其中矩阵乘法运算以模型推理层的形式封装，需要先初始化后才能调用。


常规矩阵规模上的矩阵乘法进行了较好的优化，性能表现较好，然后在扁平矩阵上性能表现较差。端侧 AI 底层计算主要为扁平矩阵的乘法，第三方计算库性能表现较差，没有充分发挥硬件的性能，不利于 AI 应用在端侧平台上落地。
- ![](https://pic2.zhimg.com/80/v2-8fcc60ba5cabac955baae5dcfc8f016d_1440w.webp)
- 表 2 ARM cortex-A53 四核第三方库 GEMM 计算效率

注：$ C(M, N) = A(M, K) * B(K, N) $ ，以上值取全行主序和全列主序的最好值，测试在相同的矩阵上重复 128 次，计算效率由 GEMM 计算 FLOPS 值除以硬件理论 FLOPS 值得到。
- ![](https://pic4.zhimg.com/80/v2-fffe722ffe6919e1c77955173494e607_1440w.webp)


### 本地ML案例


#### GGML 谷歌

GGML是一个用于机器学习的张量库，它只是一个c++库，允许你在CPU或CPU + GPU上运行llm。它定义了用于分发大型语言模型(llm)的二进制格式。GGML使用了一种称为量化的技术，该技术允许大型语言模型在消费者硬件上运行。

#### EMLL 网易

【2021-6-16】[网易有道开源 EMLL：高性能端侧机器学习计算库，大幅提高计算性能](https://zhuanlan.zhihu.com/p/381189277)

网易有道 AI 团队开源高性能端侧机器学习计算库——[EMLL](https://github.com/netease-youdao/EMLL) (Edge ML Library)

`EMLL` 为加速端侧 AI 推理而设计，提供基于端侧处理器的高性能机器学习计算库，支持 fp32、fp16、int8 等数据类型，已在网易有道词典笔、翻译王和超级词典等智能硬件产品的 NMT、ASR、OCR 引擎中应用，大幅提高计算性能，提升用户体验。
- [中文介绍](https://github.com/netease-youdao/EMLL/blob/v1.0/ReadMe_ZH.md)
- EMLL提供基于 C 的接口，详情请见 [Usage_ZH.md](https://github.com/netease-youdao/EMLL/blob/v1.0/doc/Usage_ZH.md)
- 更多原理详见[原文](https://zhuanlan.zhihu.com/p/381189277)

特点
- 高性能
  - EMLL实现的矩阵乘法函数，为端侧人工智能中常见的扁平矩阵作了专门的优化，为各常见ARM处理器作了特定的优化。对于cortex-A35/A53/A55处理器，本库针对它们的流水线特点，使用了汇编级别的优化。
- 易用性
  - EMLL使用的函数接口在参数设计上力求简洁直接，矩阵乘法去掉了不常用的LD*参数，矩阵和向量的传递通过指针和整数维度分别传递。本库的构建和运行不依赖第三方计算库。
- 扩展性
  - 对于矩阵乘法和量化函数，EMLL 库提取了它们和架构无关的代码作为通用的宏，这些宏可以在支持新的CPU架构时大大节省所需的代码量。

各函数支持的数据类型

| 处理器 | 矩阵乘法 | 偏置 | 量化 | 重量化 |
| --- | --- | --- | --- | --- |
| ARMv7a 32-bit | fp32，(u)int8 | fp32，int32 | fp32 -> (u)int16/(u)int8 | int32 -> (u)int16/(u)int8，int16 -> (u)int8 |
| ARMv8a 64-bit | fp32，fp16，(u)int8 | fp32，int32 | fp32 -> (u)int16/(u)int8 | int32 -> (u)int16/(u)int8，int16 -> (u)int8 |

EMLL 支持
- 在 Linux 和安卓系统上运行。
- 用 GCC 和 Clang 编译。
- 未来会增加对端侧 GPU 和 NPU 的支持，并拓展支持的算子范围（卷积、激活函数等）

支持的架构
- armv7a, armv8a

支持的端侧操作系统
- Linux, Android

应用效果
- EMLL 高性能端侧机器学习计算库，已经在网易有道多款智能硬件产品中实际应用并取得显著的效果，大幅提升性能，给用户带来更好的产品体验。

网易有道词典笔，是网易有道打磨的一款学习型智能硬件，凭借高效、准确查词和丰富、权威内容，成为 AI 技术在学习领域应用落地的优秀产品。网易有道词典笔，具有“多行扫描翻译”功能，支持整段翻译的智能学习硬件。
- 网易有道`超级词典`打造高效的智能英语学习系统，强化端侧功能，提供了拍照学英语、查词翻译、背单词、听力练习、对话翻译、语音助手等功能。
- 网易有道`翻译王`支持 43 种语言互译，畅游全球 191 个国家和地区，支持 21 种语言在线、7 种语言端侧拍照翻译，指示牌、菜单等即拍即译。

网易有道`词典笔`、`超级词典`、`翻译王`均内嵌了网易有道自主研发的神经网络翻译 NMT、光学字符识别 OCR、语音识别 ASR、语音合成 TTS 等业内领先的 AI 技术，并且支持离线功能。  

网易有道自研端侧机器学习计算库已在网易有道词典笔、超级词典、翻译王等智能硬件产品中使用，带来以下好处：
-   端到端性能相对于使用 eigen 库加速 1.3 到 2.43 倍，效果显著，大大降低了端侧推理引擎的延迟。除了在有道智能硬件带来了较好的性能提升，在配置骁龙 855 的某款手机上也做了性能测试，端到端性能相对于 eigen 提升了 25%-55%，效果明显。
-   端侧推理引擎采用 EMLL 之后，可以上线更大的 AI 模型，提高质量，并保证实时性，如端侧 NMT 质量(BLEU)提升 2 个点，端侧 ASR 准确度提升 4.73%。
-   EMLL 可以保证在更低端芯片上实时性，如在 cortex-A7 上使用 Eigen 库无法达到实时性，使用 EMLL 之后延迟大幅降低，并保证实时性效果。EMLL 可以让智能硬件更多的芯片选择，从而降低成本，提高市场竞争力。

端侧 NMT、ASR、OCR 在不同平台上使用 EMLL 和 eigen 端到端性能加速比
- ![](https://pic4.zhimg.com/80/v2-dc2281fa6475498778443ac02880989f_1440w.webp)


#### 端上推荐系统 阿里EdgeRec

【2023-9-13】[EdgeRec：电商信息流的端上推荐系统](https://zhuanlan.zhihu.com/p/440920682)

更多见推荐专题：[推荐系统](rp) 端上推荐专题

## LLM推理框架

参考
- 【2023-9-21】[大语言模型推理性能优化综述](https://zhuanlan.zhihu.com/p/656485997)

### 总结

40GB 的A100 GPU上，用 LLaMA-1 13b模型进行七个部署框架的对比

LLM推理有很多框架，各有其特点，七个框架的关键点：
- [1] vLLM ：适用于**大批量**Prompt输入，并对推理速度要求高的场景；
  - vLLM 吞吐量比HuggingFace Transformers（HF）高**14x-24倍**，比 HuggingFace Text Generation Inference（TGI）高2.2x-2.5倍。
- [2] Text generation inference：依赖HuggingFace模型，并且不需要为核心模型增加多个adapter的场景；
- [3] CTranslate2：可在CPU上进行推理；
- [4] OpenLLM：为核心模型添加adapter并使用HuggingFace Agents，尤其是不完全依赖PyTorch；
- [5] Ray Serve：稳定的Pipeline和灵活的部署，它最适合更成熟的项目；
  - Ray Serve 是一个可扩展的模型服务库，用于构建在线推理API。Serve与框架无关，因此可以使用一个工具包来为深度学习模型的所有内容提供服务。
- [6] MLC LLM：可在客户端（边缘计算）（例如，在Android或iPhone平台上）本地部署LLM；
  - LLM的机器学习编译（MLC LLM）是一种通用的部署解决方案，它使LLM能够利用本机硬件加速在消费者设备上高效运行
- [7] DeepSpeed-MII：使用DeepSpeed库来部署LLM；

![](https://pic1.zhimg.com/80/v2-3f467171d76953de58e69faf1d802394_1440w.webp)


### 一、vLLM

6月，加州大学伯克利分校等机构开源了 `vLLM`（目前已有 6700 多个 star），其使用了一种新设计的注意力算法 `PagedAttention`，可让服务提供商轻松、快速且低成本地发布 LLM 服务。

【2023-9-25】[vLLM出论文，让每个人都能快速低成本地部署LLM服务](https://www.toutiao.com/article/7282671140630315581)
- ![](https://pic1.zhimg.com/80/v2-63fe1b3f450eefe222f025fac5a6cb84_1440w.webp)
- 论文：[Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
- 代码：[vllm](https://github.com/vllm-project/vllm)
- 文档：[vllm](https://vllm.readthedocs.io)

vLLM的吞吐量比HuggingFace Transformers（HF）高**14x-24**倍，比HuggingFace Text Generation Inference（TGI）高2.2x-2.5倍。


**离线批量推理**

```py
# pip install vllm
from vllm import LLM, SamplingParams

prompts = [
    "Funniest joke ever:",
    "The capital of France is",
    "The future of AI is",
]
sampling_params = SamplingParams(temperature=0.95, top_p=0.95, max_tokens=200)
llm = LLM(model="huggyllama/llama-13b")
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

**API Server**

```sh
# Start the server:
python -m vllm.entrypoints.api_server --env MODEL_NAME=huggyllama/llama-13b

# Query the model in shell:
curl http://localhost:8000/generate \
    -d '{
        "prompt": "Funniest joke ever:",
        "n": 1,
        "temperature": 0.95,
        "max_tokens": 200
    }'
```

功能：
-   **Continuous batching\[9\]**：有iteration-level的调度机制，每次迭代batch大小都有所变化，因此vLLM在大量查询下仍可以很好的工作。
-   **PagedAttention\[10\]**：受操作系统中虚拟内存和分页的经典思想启发的注意力算法，这就是模型加速的秘诀。

优点：
-   **文本生成的速度：**实验多次，发现vLLM的推理速度是最快的；
-   **高吞吐量服务：**支持各种解码算法，比如parallel sampling, beam search等；
-   **与OpenAI API兼容：**如果使用OpenAI API，只需要替换端点的URL即可；

缺点：
-   **添加自定义模型**：虽然可以合并自己的模型，但如果模型没有使用与vLLM中现有模型类似的架构，则过程会变得更加复杂。例如，增加Falcon的支持，这似乎很有挑战性；
-   **缺乏对适配器（LoRA、QLoRA等）的支持**：当针对特定任务进行微调时，开源LLM具有重要价值。然而，在当前的实现中，没有单独使用模型和适配器权重的选项，这限制了有效利用此类模型的灵活性。
-   **缺少权重量化**：有时，LLM可能不需要使用GPU内存，这对于减少GPU内存消耗至关重要。

这是LLM推理最快的库。得益于其内部优化，它显著优于竞争对手。尽管如此，它在支持有限范围的模型方面确实存在弱点。

使用vLLM的开发路线可以参考：[https://github.com/vllm-project/vllm/issues/244](https://github.com/vllm-project/vllm/issues/244)**

### 二、Text generation inference

![](https://pic3.zhimg.com/80/v2-5238573ef15a96e9fcafc28193a56d9a_1440w.webp)

Text generation inference是用于文本生成推断的Rust、Python和gRPC服务器，在HuggingFace中已有LLM 推理API使用。

**使用docker运行web server**

```sh
mkdir data
docker run --gpus all --shm-size 1g -p 8080:80 \
-v data:/data ghcr.io/huggingface/text-generation-inference:0.9 \
  --model-id huggyllama/llama-13b \
  --num-shard 1
```

**查询实例**

```sh
# pip install text-generation
from text_generation import Client

client = Client("http://127.0.0.1:8080")
prompt = "Funniest joke ever:"
print(client.generate(prompt, max_new_tokens=17 temperature=0.95).generated_text)
```

功能：
-   **内置服务评估：**可以监控服务器负载并深入了解其性能；
-   **使用flash attention（和v2）和Paged attention优化transformer推理代码：**并非所有模型都内置了对这些优化的支持，该技术可以对未使用该技术的模型可以进行优化；

优点：
-   **所有的依赖项都安装在Docker中：**会得到一个现成的环境；
-   **支持HuggingFace模型：**轻松运行自己的模型或使用任何HuggingFace模型中心；
-   **对模型推理的控制**：该框架提供了一系列管理模型推理的选项，包括精度调整、量化、张量并行性、重复惩罚等；

缺点：
-   **缺乏对适配器的支持：**需要注意的是，尽管可以使用适配器部署LLM（可以参考[https://www.youtube.com/watch?v=HI3cYN0c9ZU](https://www.youtube.com/watch%3Fv%3DHI3cYN0c9ZU)），但目前还没有官方支持或文档；
-   **从源代码（Rust+CUDA内核）编译：**对于不熟悉Rust的人，将客户化代码纳入库中变得很有挑战性；
-   **文档不完整**：所有信息都可以在项目的自述文件中找到。尽管它涵盖了基础知识，但必须在问题或源代码中搜索更多细节；

使用Text generation inference的开发路线可以[参考](https://github.com/huggingface/text-generation-inference/issues/232)

### 三、CTranslate2


![](https://pic1.zhimg.com/80/v2-dfc1a2808b8b04b5e99f81e8734ca6b0_1440w.webp)

CTranslate2是一个C++和Python库，用于使用Transformer模型进行高效推理。

转换模型

```sh
pip install -qqq transformers ctranslate2

# The model should be first converted into the CTranslate2 model format:
ct2-transformers-converter --model huggyllama/llama-13b --output_dir llama-13b-ct2 --force
```

**查询实例**

```py
import ctranslate2
import transformers

generator = ctranslate2.Generator("llama-13b-ct2", device="cuda", compute_type="float16")
tokenizer = transformers.AutoTokenizer.from_pretrained("huggyllama/llama-13b")

prompt = "Funniest joke ever:"
tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt))
results = generator.generate_batch(
    [tokens], 
    sampling_topk=1, 
    max_length=200, 
)
tokens = results[0].sequences_ids[0]
output = tokenizer.decode(tokens)
print(output)
```

**功能：**
-   **在CPU和GPU上快速高效地执行：**得益于内置的一系列优化：层融合、填充去除、批量重新排序、原位操作、缓存机制等。推理LLM更快，所需内存更少；
-   **动态内存使用率：**由于CPU和GPU上都有缓存分配器，内存使用率根据请求大小动态变化，同时仍能满足性能要求；
-   **支持多种CPU体系结构：**该项目支持x86–64和AArch64/ARM64处理器，并集成了针对这些平台优化的多个后端：英特尔MKL、oneDNN、OpenBLAS、Ruy和Apple Accelerate；

**优点：**
-   **并行和异步执行**\--可以使用多个GPU或CPU核心并行和异步处理多个批处理；
-   **Prompt缓存**——在静态提示下运行一次模型，缓存模型状态，并在将来使用相同的静态提示进行调用时重用；
-   **磁盘上的轻量级**\--量化可以使模型在磁盘上缩小4倍，而精度损失最小；

**缺点**：
-   **没有内置的REST服务器**——尽管仍然可以运行REST服务器，但没有具有日志记录和监控功能的现成服务
-   **缺乏对适配器（LoRA、QLoRA等）的支持**

### 四、DeepSpeed-MII

![](https://pic1.zhimg.com/80/v2-b871e7cc5ef3ac72aa067953011333c4_1440w.webp)

在DeepSpeed支持下，DeepSpeed-MII可以进行低延迟和高通量推理。

**运行web服务**

```py
# DON'T INSTALL USING pip install deepspeed-mii
# git clone https://github.com/microsoft/DeepSpeed-MII.git
# git reset --hard 60a85dc3da5bac3bcefa8824175f8646a0f12203
# cd DeepSpeed-MII && pip install .
# pip3 install -U deepspeed

# ... and make sure that you have same CUDA versions:
# python -c "import torch;print(torch.version.cuda)" == nvcc --version
import mii

mii_configs = {
    "dtype": "fp16",
    'max_tokens': 200,
    'tensor_parallel': 1,
    "enable_load_balancing": False
}
mii.deploy(task="text-generation",
           model="huggyllama/llama-13b",
           deployment_name="llama_13b_deployment",
           mii_config=mii_configs)
```

**查询实例**

```py
import mii

generator = mii.mii_query_handle("llama_13b_deployment")
result = generator.query(  
  {"query": ["Funniest joke ever:"]}, 
  do_sample=True,
  max_new_tokens=200
)
print(result)
```

**功能**：
-   **多个副本上的负载平衡：**这是一个非常有用的工具，可用于处理大量用户。负载均衡器在各种副本之间高效地分配传入请求，从而缩短了应用程序的响应时间。
-   **非持久部署：**目标环境的部署不是永久的，需要经常更新的，这在资源效率、安全性、一致性和易管理性至关重要的情况下，这是非常重要的。

**优点**：
-   **支持不同的模型库：**支持多个开源模型库，如Hugging Face、FairSeq、EluetherAI等；
-   **量化延迟和降低成本：**可以显著降低非常昂贵的语言模型的推理成本；
-   **Native和Azure集成：**微软开发的MII框架提供了与云系统的出色集成；

**缺点**：
-   **支持模型的数量有限：**不支持Falcon、LLaMA2和其他语言模型；
-   **缺乏对适配器（LoRA、QLoRA等）的支持；**

### 五、OpenLLM

![](https://pic1.zhimg.com/80/v2-2028bebc2adf59968b7506ebb697190c_1440w.webp)

OpenLLM是一个用于在生产中操作大型语言模型（LLM）的开放平台。

**运行web服务**

```sh
pip install openllm scipy
openllm start llama --model-id huggyllama/llama-13b \
  --max-new-tokens 200 \
  --temperature 0.95 \
  --api-workers 1 \
  --workers-per-resource 1
```

**查询实例**

```py
import openllm

client = openllm.client.HTTPClient('http://localhost:3000')
print(client.query("Funniest joke ever:"))
```

**功能**：

-   **适配器支持：**可以将要部署的LLM连接多个适配器，这样可以只使用一个模型来执行几个特定的任务；
-   **支持不同的运行框架：**比如Pytorch（pt）、Tensorflow（tf）或Flax（亚麻）；
-   **HuggingFace Agents\[11\]：**连接HuggingFace上不同的模型，并使用LLM和自然语言进行管理；

**优点**：

-   **良好的社区支持：**不断开发和添加新功能；
-   **集成新模型：**可以添加用户自定义模型；
-   **量化：**OpenLLM支持使用bitsandbytes\[12\]和GPTQ\[13\]进行量化；
-   **LangChain集成：**可以使用LangChian与远程OpenLLM服务器进行交互；

**缺点**：
-   **缺乏批处理支持：**对于大量查询，这很可能会成为应用程序性能的瓶颈；
-   **缺乏内置的分布式推理**——如果你想在多个GPU设备上运行大型模型，你需要额外安装OpenLLM的服务组件Yatai\[14\]；

### 六、Ray Serve

![](https://pic4.zhimg.com/80/v2-7746a44a79ce1a591a01ee8193c4bbc3_1440w.webp)

Ray Serve是一个可扩展的模型服务库，用于构建在线推理API。Serve与框架无关，因此可以使用一个工具包来为深度学习模型的所有内容提供服务。

![](https://pic1.zhimg.com/80/v2-524968a7269229d43184de705b0644f0_1440w.webp)

**运行web服务**

```py
# pip install ray[serve] accelerate>=0.16.0 transformers>=4.26.0 torch starlette pandas
# ray_serve.py
import pandas as pd

import ray
from ray import serve
from starlette.requests import Request

@serve.deployment(ray_actor_options={"num_gpus": 1})
class PredictDeployment:
    def __init__(self, model_id: str):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        import torch

        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)

    def generate(self, text: str) -> pd.DataFrame:
        input_ids = self.tokenizer(text, return_tensors="pt").input_ids.to(
            self.model.device
        )
        gen_tokens = self.model.generate(
            input_ids,
            temperature=0.9,
            max_length=200,
        )
        return pd.DataFrame(
            self.tokenizer.batch_decode(gen_tokens), columns=["responses"]
        )

    async def __call__(self, http_request: Request) -> str:
        json_request: str = await http_request.json()
        return self.generate(prompt["text"])

deployment = PredictDeployment.bind(model_id="huggyllama/llama-13b")

# then run from CLI command:
# serve run ray_serve:deployment
```

**查询实例**

```py
import requests

sample_input = {"text": "Funniest joke ever:"}
output = requests.post("http://localhost:8000/", json=[sample_input]).json()
print(output)
```

**功能**：

-   **监控仪表板和Prometheus度量：**可以使用Ray仪表板来获得Ray集群和Ray Serve应用程序状态；
-   **跨多个副本自动缩放：**Ray通过观察队列大小并做出添加或删除副本的缩放决策来调整流量峰值；
-   **动态请求批处理：**当模型使用成本很高，为最大限度地利用硬件，可以采用该策略；

**优点**：
-   **文档支持：**开发人员几乎为每个用例撰写了许多示例；
-   **支持生产环境部署：**这是本列表中所有框架中最成熟的；
-   **本地LangChain集成：**您可以使用LangChian与远程Ray Server进行交互；

**缺点**：
-   **缺乏内置的模型优化：**Ray Serve不专注于LLM，它是一个用于部署任何ML模型的更广泛的框架，必须自己进行优化；
-   **入门门槛高：**该库功能多，提高了初学者进入的门槛；

如果需要最适合生产的解决方案，而不仅仅是深度学习，Ray Serve是一个不错的选择。它最适合于可用性、可扩展性和可观察性非常重要的企业。此外，还可以使用其庞大的生态系统进行数据处理、模型训练、微调和服务。最后，从OpenAI到Shopify和Instacart等公司都在使用它。

### 七、MLC LLM


![](https://pic4.zhimg.com/80/v2-d8f91231ec8466212d557d2c15808ba3_1440w.webp)

LLM的机器学习编译（MLC LLM）是一种通用的部署解决方案，它使LLM能够利用本机硬件加速在消费者设备上高效运行。

![](https://pic2.zhimg.com/80/v2-7bf144efe4e37a0b3c9bb95893ea6d65_1440w.webp)

**运行web服务**

```sh
# 1. Make sure that you have python >= 3.9
# 2. You have to run it using conda:
conda create -n mlc-chat-venv -c mlc-ai -c conda-forge mlc-chat-nightly
conda activate mlc-chat-venv

# 3. Then install package:
pip install --pre --force-reinstall mlc-ai-nightly-cu118 \
  mlc-chat-nightly-cu118 \
  -f https://mlc.ai/wheels

# 4. Download the model weights from HuggingFace and binary libraries:
git lfs install && mkdir -p dist/prebuilt && \
  git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib && \
  cd dist/prebuilt && \  
  git clone https://huggingface.co/huggyllama/llama-13b dist/ && \
  cd ../..
  
  
# 5. Run server:
python -m mlc_chat.rest --device-name cuda --artifact-path dist
```

**查询实例**

```py
import requests

payload = {
   "model": "lama-30b",
   "messages": [{"role": "user", "content": "Funniest joke ever:"}],
   "stream": False
}
r = requests.post("http://127.0.0.1:8000/v1/chat/completions", json=payload)
print(r.json()['choices'][0]['message']['content'])
```

**功能**：

-   **平台本机运行时：**可以部署在用户设备的本机环境上，这些设备可能没有现成的Python或其他必要的依赖项。应用程序开发人员只需要将MLC编译的LLM集成到他们的项目中即可；
-   **内存优化：**可以使用不同的技术编译、压缩和优化模型，从而可以部署在不同的设备上；

**优点**：

-   **所有设置均可在JSON配置中完成：**在单个配置文件中定义每个编译模型的运行时配置；
-   **预置应用程序：**可以为不同的平台编译模型，比如C++用于命令行，JavaScript用于web，Swift用于iOS，Java/Kotlin用于Android；

**缺点**：

-   **使用LLM模型的功能有限**：不支持适配器，无法更改精度等，该库主要用于编译不同设备的模型；
-   **只支持分组量化\[15\]：**这种方法表现良好，但是在社区中更受欢迎的其他量化方法（bitsandbytes和GPTQ）不支持；
-   **复杂的安装：**安装需要花几个小时，不太适合初学者开发人员；

如果需要在iOS或Android设备上部署应用程序，这个库正是你所需要的。它将允许您快速地以本机方式编译模型并将其部署到设备上。但是，如果需要一个高负载的服务器，不建议选择这个框架。


#### MLC LLM -- 陈天奇

- 【2023-5-2】[陈天奇等人新作引爆AI界：手机原生跑大模型，算力不是问题了](https://mp.weixin.qq.com/s/uQGAu1v-6ApgZHVkZJsUdQ)
- 【2023-6-5】[陈天奇官宣新APP，让手机原生跑大模型，应用商店直接下载使用](https://www.toutiao.com/article/7241085086400233995), 陈天奇公布了一个好消息：MLC Chat app 已经在苹果的 App Store 上线了。

【2023-5-2】[端侧语言大模型部署时代已经悄悄到来！](https://zhuanlan.zhihu.com/p/626268783)

#### TVM 简介

TVM是一个深度学习编译器，初衷是让各种训练框架训练好的模型能够在不同硬件平台上面快速推理
- 支持Pytorch、AutoML、Tensorflow、Onnx、Kersa、Mxnet等多种前端训练框架；
- 支持ARM CPU、Intel CPU、NVIDIA显卡、FPGA、ASIC等多种硬件设备。

MLC-LLM 底层技术其实就是`TVM`编译器。

该框架的输入是一些训练框架训练好的**模型文件**；
- 然后, 利用Relay将其转换成 High-Level Differentiable IR，该阶段会执行一些图优化操作，包括：算子融合、常量折叠、内存重排、模型量化等；
- 接着会利用AutoTVM、Ansor或者Meta Scheduler等自动化优化技术来将这种IR转换为Tensor Expression IR这种更低级的IR表示。

TVM深度学习编译器中的一个亮点工作就是**自动优化技术**
- 第一代优化技术叫`AutoTVM`
- 第二代叫`Ansor`或者`Auto Scheduler`
- 第三代叫`Meta Scheduler`

![](https://pic3.zhimg.com/80/v2-a2aa328a18b3afb02f48816419c481c6_1440w.jpg)

AutoTVM
- ![](https://pic3.zhimg.com/80/v2-9e23067a8872309bdafadede6328e192_1440w.jpg)

Ansor/Auto Scheduler
- ![](https://pic4.zhimg.com/80/v2-7ccb0e4a6fbbecd305f68dfd573b1377_1440w.jpg)


#### MLC LLM 简介

MLC LLM 是一种通用解决方案，允许将任何语言模型**本地部署**在各种硬件后端和本地应用程序上。

此外还有一个高效框架，供每个人进一步优化自己用例的模型性能。<span style='color:red'>一切都在本地运行，无需服务器支持</span>，并通过手机和笔记本电脑上的本地 GPU 加速。

TVM是一个深度学习编译器，知名编译工具，国内很多大公司都在使用它，国内很多的芯片公司都在使用它构建自己的工具链。
- `AutoTVM`、`Ansor`是`TVM`中比较亮眼的工作，思想都是利用ML将算子优化的任务自动化，当前它已经可以很好的支持多种硬件设备。
- 语言大模型的轻量化核心是**Transformer的加速与优化**，TVM社区很早就开始探索Transformer的加速与优化。除此之外，TVM中的图优化技术、自动优化等技术为语言大模型的轻量化打下了坚实的基础。

MLC-LLM只是语言大模型轻量化的开端，语言大模型**轻量化**方向近期会变得异常火热, 很多大公司陆续都是开源自己的一些工作。
- 随着MIC-LLM等工具出现，**端侧大模型部署**热潮已经来临。OpenAI一家独大的情况也会慢慢得打缓解，随着语言大模型的赋能，越来越多的智能设备，尤其是机器人的智能程度会更上一层楼！
- 随着端侧语言大模型的部署难题逐步被解决，端侧模型的**数据隐私**问题可能成为了端侧部署的一个关键问题。不过，这个问题应该相对来说会比较容易一些。期待了端侧语言大模型时代的到来！

- [演示图](https://vdn6.vzuu.com/SD/da2d7036-e81f-11ed-a962-bacc53acff3b-v1_f4_t2_etMRzyS8.mp4?pkey=AAV2GKNHXbMr7W0DZWmaAKjmklpebDlDgvlJQN4ElgagtlxqcrYmaLNld20o3ymLMrOUseNg1m3gdavjpUBHj89Y&c=avc.1.1&f=mp4&pu=078babd7&bu=078babd7&expiration=1691478412&v=ks6)
- [参考](https://zhuanlan.zhihu.com/p/626189075)


[mlc-llm](https://mlc.ai/mlc-llm/) 部署汇总
- 亲测：华为mate 30下载后，启动即闪退；iOS正常

|设备|地址|示例|
|---|---|---|
|iOS|[iOS地址](https://testflight.apple.com/join/57zd7oxa)|![](https://mlc.ai/mlc-llm/gif/ios-demo.gif)|
|Android|[Android地址](https://mlc.ai/mlc-llm/gif/android-demo.gif)|![](https://mlc.ai/mlc-llm/gif/android-demo.gif)|
|PC|[Windows Linux Mac](https://mlc.ai/mlc-llm/#windows-linux-mac)|![](https://mlc.ai/mlc-llm/gif/linux-demo.gif)|
|Web|[WebLLM](https://mlc.ai/mlc-llm/#web-browser)||

让大模型变小这条路上，人们做了很多尝试
- 先是 Meta 开源了 LLaMA，让学界和小公司可以训练自己的模型。
- 随后斯坦福研究者启动了 Lamini，为每个开发者提供了从 GPT-3 到 ChatGPT 的快速调优方案。
- 最近 MLC LLM 的项目一步登天，因为它能在**任何设备**上编译运行大语言模型。

MLC LLM 在各类硬件上**原生部署任意大型语言模型**提供了解决方案，可将大模型应用于移动端（例如 iPhone）、消费级电脑端（例如 Mac）和 Web 浏览器。
-  TVM、MXNET、XGBoost 作者，CMU 助理教授，OctoML CTO 陈天奇等多位研究者共同开发的，参与者来自 CMU、华盛顿大学、上海交通大学、OctoML 等院校机构，同时也获得了开源社区的支持。
- [github](https://github.com/mlc-ai/mlc-llm)
- [Demo](https://mlc.ai/mlc-llm/)
- [MLC课程](https://mlc.ai/summer22-zh/schedule)：机器学习编译
- [知乎专题](https://www.zhihu.com/question/598610139)

曾经开源过XGBoost和TVM `陈天奇`大佬已经完成了这件事情，推出了一个叫`MLC-LLM` 工具，在一些低算力平台上面运行一些语言大模型！只要GPU显存大于6GB，都可以去尝试在本地部署一下属于语言大模型

MLC LLM 旨在让每个人都能在个人设备上本地开发、优化和部署 AI 模型，而无需服务器支持，并通过手机和笔记本电脑上的消费级 GPU 进行加速。具体来说，MLC LLM 支持的平台包括：
- iPhone
- Metal GPU 和英特尔 / ARM MacBook;
- 在 Windows 和 Linux 上支持通过 Vulkan 使用 AMD 和 NVIDIA GPU；
- 在 Windows 和 Linux 上 通过 CUDA 使用 NVIDIA GPU；
- 浏览器上的 WebGPU（借助 MLC LLM 的配套项目 Web LLM）。

为了实现在各类硬件设备上运行 AI 模型的目标，研究团队要解决计算设备和部署环境的多样性问题，主要挑战包括：
- 支持不同型号的 CPU、GPU 以及其他可能的协处理器和加速器；
- 部署在用户设备的**本地环境**中，这些环境可能没有 python 或其他可用的必要依赖项；
- 通过仔细规划分配和积极压缩模型参数来解决**内存限制**。
- MLC LLM 提供可重复、系统化和可定制的工作流，使开发人员和 AI 系统研究人员能够以 Python 优先的方法实现模型并进行优化。MLC LLM 可以让研究人员们快速试验新模型、新想法和新的编译器 pass，并进行本地部署。

为了实现原生部署，研究团队以**机器学习编译**（MLC）技术为基础来高效部署 AI 模型。
- [MLC技术](https://mlc.ai/)
- MLC LLM 借助一些开源生态系统，包括来自 HuggingFace 和 Google 的分词器，以及 LLaMA、Vicuna、Dolly 等开源 LLM。
- ![](https://pica.zhimg.com/80/v2-b23bb5806fa9c32e51773e06494b8f62_1440w.webp?source=1940ef5c)

MLC LLM 的主要工作流基于 Apache TVM Unity，通过扩展 TVM 后端使模型编译更加透明和高效。
- Dynamic shape：该研究将语言模型烘焙（bake）为具有原生 Dynamic shape 支持的 TVM IRModule，避免了对最大输入长度进行额外填充的需要，并减少了计算量和内存使用量。
- 可组合的 ML 编译优化：MLC LLM 可以执行许多模型部署优化，例如更好的编译代码转换、融合、内存规划和库卸载（library offloading），并且手动代码优化可以很容易地合并为 TVM 的 IRModule 转换，成为一个 Python API。
- 量化：MLC LLM 利用低位量化来压缩模型权重，并利用 TVM 的 loop-level TensorIR 为不同的压缩编码方案快速定制代码生成。
- 运行时（Runtime）：TVM 编译生成的库能够通过 TVM runtime 在设备的原生环境中运行，TVM runtime 支持 CUDA/Vulkan/Metal 等主流 GPU 驱动以及 C、JavaScript 等语言的绑定。

此外，MLC 还为 CUDA、Vulkan 和 Metal 生成了 GPU shader，并通过 LLVM 支持多种 CPU，包括 ARM 和 x86。通过改进 TVM 编译器和运行时，使用者可以添加更多支持，例如 OpenCL、sycl、webgpu-native。


#### MLC LLM 支持设备

支持的设备类型
- MLC-LLM工具支持多种设备类型，大到N卡、AMD GPU，小到Android、IOS、WebGPU等。具体测试的设备列表如下所示。建议在设备内存大于等于6GB的设备上面进行推理与测试。

- iPhone, iPad

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| iPhone 14 Pro | iOS 16.4.1 | 7.2 | [https://github.com/junrushao](https://github.com/junrushao) |
| iPad Pro 11 with M1 | iPadOS 16.1 | 10.6 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529377124](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529377124) |

-  Metal GPUs and Intel/ARM MacBooks

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| UHD Graphics 630 | macOS Ventura | 2.3 | [https://github.com/junrushao](https://github.com/junrushao) |
| 2020 MacBook Pro M1 (8G) | macOS | 11.4 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529148903](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529148903) |
| 2021 MacBook Pro M1Pro (16G) | macOS Ventura | 17.1 | [https://github.com/mlc-ai/mlcllm/issues/15#issuecomment-1529434801](https://github.com/mlc-ai/mlcllm/issues/15%23issuecomment-1529434801) |
| M1 Max Mac Studio (64G) |  | 18.6 | [https://github.com/mlc-ai/mlcllm/issues/15#issuecomment-1529714864](https://github.com/mlc-ai/mlcllm/issues/15%23issuecomment-1529714864) |

- AMD and NVIDIA GPUs via Vulkan on Windows and Linux

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| Raden Pro 5300M | macOS Venture | 12.6 | [https://github.com/junrushao](https://github.com/junrushao) |
| AMD GPU on Steam Deck | TBD (S macOS Ventura ome Linux) | TBD | [https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia8ux6/](https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia8ux6/) |
| RX 7900 xtx |  |  | [https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia691u/](https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia691u/) |
| RX6800 16G VRAM | macOS Ventura | 22.5 | [https://github.com/mlc-ai/mlc-llm/issues/15](https://github.com/mlc-ai/mlc-llm/issues/15) |

- NVIDIA GPUs via CUDA on Windows and Linux

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| GTX 1060 (6GB) | Windows 10 | 16.7 | [https://github.com/mlc-ai/mlc-llm/issues/13#issue-1689858446](https://github.com/mlc-ai/mlc-llm/issues/13%23issue-1689858446) |
| RTX 3080 | Windows 11 | 26.0 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529434801](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529434801) |
| RTX 3060 | Debian bookworm | 21.3 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529572646](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529572646) |

- WebGPU on browsers


#### MLC-LLM 核心技术


![](https://pic4.zhimg.com/80/v2-c95a2f2706f88094bd196bd4bf7da53b_1440w.webp)

-   **Dynamic shape**: 作者将**语言模型**转换为具有原生动态形状支持的 TVM IRModule，避免了对最大长度进行额外填充的需要，并减少了计算量和内存使用量。如图所示，为了优化动态形状输入
  - 首先应用**循环切分**技术，即将一个大循环切分成两个小循环操作；
  - 然后应用张量自动化技术，即TVM中的Ansor或者Meta Scheduler技术。
  - ![](https://pic4.zhimg.com/80/v2-1a89c78eba7b1228fff6dd08d41ff2bf_1440w.webp)
-   **Composable ML compilation optimization**s: 执行了许多模型部署优化，例如更好的编译代码转换、融合、内存规划、库卸载和手动代码优化可以很容易地合并为TVM 的 IRModule 转换，作为 Python API 公开。如上图所示，模型推理工具链中常用的几种优化技术包括：算子简化、算子融合、常量折叠、内存排布等。
  - ![](https://pic1.zhimg.com/80/v2-dccad206e27b4d485879c50d9033a0ec_1440w.webp)
-   **Quantization**: 利用**低位量化**来压缩模型权重，并利用 TVM 的循环级 TensorIR 为不同的压缩编码方案快速定制代码生成。如图所示，TVM中可以通过两种方式来进行量化：1）通过 relay.quantize 完成浮点模型的量化，该量化包含annotate、calibrate和relize三步；2）通过一种称为 qnn 的 relay方言([http://relay.qnn.xxx](https://link.zhihu.com/?target=http%3A//relay.qnn.xxx)) 直接执行已经量化过的模型。
  - ![](https://pic4.zhimg.com/80/v2-883e9de589cbcac6e1f9854b46b160b7_1440w.webp)
-   **Runtime**: 最终生成的库在原生环境中运行，TVM 运行时具有最小的依赖性，支持各种 GPU 驱动程序 API 和原生语言绑定（C、JavaScript等）。如图所示，TVM支持多种Runtime，包括：JS、Java、Python、C++、Android、IOS、Web等，正是这些Runtime支持，才使得MLC-LLM可以很快的支持很多端侧设备!


#### MLC-LLM部署流图


![](https://pic4.zhimg.com/80/v2-04249102c3061d4c5e436e990a42125f_1440w.webp)

1、**Python first** development

-   IRModule: 如上图所示，该模块存储着一个张量函数集合，每个函数附带首个形状符号，并支持跟踪形状依赖。 该模块包含着Transformer中的关键模块，encoding和step\_decoding，前者用来做输入数据的编码操作，后者用来做数据的解码操作。  
-   ML Compilation Optimization: 该模块主要在计算图上面执行一些优化操作，具体包括：算子融合（降低多次加载的带宽开销）、内存规划（提前在编译阶段分配一些内存，并对内存的排布进行调整）、循环优化（利用常用的tile、reoder、paritation等技术）和权重量化（利用int8、int16等数据类型进行模型压缩）。  
-   TensorIR Schedules: 该模块主要利用Ansor自动优化或者Meta Scheduler自动优化技术对LLM模型中的算子进行调度优化。这是TVM编译器的一个杀手锏！该技术的核心思想是利用ML的思路来解决循环优化问题。  

2、**Universal** development

-   **最底层是硬件驱动层**，该层主要完成一些硬件适配与驱动的工作。支持的硬件具体包括：NVIDIA的CUDA、AMD的Rocm、苹果的Vulkan和WebGPU等。  
-   **第三层是TVM Runtim层**，该层主要完成TVM Runtime库的适配与加载任务。用户需要做的是调用TVM的Runtime推理接口完成模型的推理操作。  
-   **第二层是模型与代码层**，该层主要完成模型的优化与业务逻辑码的开发。通过Python First Development可以导出一个model.dylib库，用户需要实现[http://llm\_chat.cc](https://link.zhihu.com/?target=http%3A//llm_chat.cc)文件，即语言大模型的业务逻辑代码。  
-   **第一层是应用层**，该层用来开发一些上层应用，具体包括Chat CLI命令行工具、MLCChat.App 安卓或者IOS端的上层应用、基于WebGPU的网页端应用等。


#### MLC-LLM环境搭建

1、**iphone平台**

[参考](https://testflight.apple.com/join/57zd7oxa)页面安装已经编译好的APP。

注意事项：
- 试用此页面（仅限前 9000 名用户）以安装和使用作者为 iPhone 构建的示例 iOS 聊天应用程序。应用程序本身需要大约 4GB的内存才能运行。考虑到 iOS 和其他正在运行的应用程序，我们将需要具有 6GB（或更多）内存的最新 iPhone 来运行该应用程序。作者仅在 iPhone 14 Pro Max 和 iPhone 12 Pro上测试了该应用程序。

2、**Windows/Linux/Mac**平台

![](https://pic4.zhimg.com/v2-583cb94d4f32afe60eebeb0dfacbb847_b.gif)

![动图封面](https://pic4.zhimg.com/v2-583cb94d4f32afe60eebeb0dfacbb847_b.jpg)

步骤1 - 安装环境依赖
- 安装 Miniconda 或 Miniforge
- windows与linux用户-安装Vulkan驱动；对于Nvidia用户-建议安装Vulkan驱动

步骤2-创建环境

```sh
# Create new conda environment and activate the environment.
conda create -n mlc-chat
conda activate mlc-chat
# Install Git and Git-LFS, which is used for downloading the model weights from Hugging Face.
conda install git git-lfs
# Install the chat CLI app from Conda.
conda install -c mlc-ai -c conda-forge mlc-chat-nightly
# Create a directory, download the model weights from HuggingFace, and download the binary libraries from GitHub.
mkdir -p dist
git lfs install
git clone https://huggingface.co/mlc-ai/demo-vicuna-v1-7b-int3 dist/vicuna-v1-7b
git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/lib
# Enter this line and enjoy chatting with the bot running natively on your machine!
mlc_chat_cli
```

3、**Web浏览器**平台

步骤
1. 安装 Chrome Canary，它是支持使用 WebGPU 的 Chrome 开发者版本。
2. 利用下面的命令行发起Chrome Canary

```sh
/Applications/Google\ Chrome\ Canary.app/Contents/MacOS/Google\ Chrome\ Canary --enable-dawn-features=disable_robustness
```

3. 在浏览器运行[demo](https://mlc.ai/web-llm/#chat-demo)

注意事项：
- WebGPU 刚刚发布到 Chrome 并且处于测试阶段。我们在 Chrome Canary 中进行实验。你也可以试试最新的Chrome 113。Chrome版本≤112是不支持的，如果你正在使用它，demo会报错 Find an error initializing the WebGPU device OperationError: Required limit (1073741824) is greater than the 支持的限制 (268435456)。
- 验证 maxBufferSize 时 
- 验证所需限制时。已经在 windows 和 mac 上测试过了，你需要一个 6.4G 内存的 gpu。


#### MLC-LLM效果展示

1、**web端**Demo
- ![](https://pic3.zhimg.com/80/v2-22762accdf3d48acd09f06b8a60e2eda_1440w.webp)

2、IOS端Demo
- ![](https://pic1.zhimg.com/v2-1f68e0fc385e8b67344f4e5c99fcc837.jpg?source=382ee89a)


3、Web Stable Diffusion
- ![](https://pic2.zhimg.com/80/v2-895ea13851a24908d5ec8fb6ef1ad775_1440w.webp)


## 本地 LLM

### LLaMA

模型权重是浮点数。就像表示大整数(例如1000)比表示小整数(例如1)需要更多的空间一样，表示高精度浮点数(例如0.0001)比表示低精度浮点数(例如0.1)需要更多的空间。量化大型语言模型的过程涉及降低表示权重的精度，以减少使用模型所需的资源。GGML支持许多不同的量化策略(例如4位、5位和8位量化)，每种策略在效率和性能之间提供不同的权衡。

量化后模型大小的对比：
- ![](https://pic4.zhimg.com/80/v2-f21157c61f394b898fa7c23a11510bcb_1440w.webp)
- [Refer](https://zhuanlan.zhihu.com/p/639565332)

#### LLaMA.cpp

【2023-8-21】[研究完llama.cpp，我发现手机跑大模型竟这么简单](https://www.toutiao.com/article/7268183078362087976)
- [llama.cpp](https://github.com/ggerganov/llama.cpp) 项目用原始 C++ 重写了 LLaMa 的推理代码，效果极好

经过优化和量化权重，各种以前无法想象的硬件上本地运行 LLaMa 模型。其中：
- 谷歌 Pixel5 手机上，它能以 1 token/s 的速度运行 7B 参数模型。
- M2 芯片的 Macbook Pro 上，使用 7B 参数模型的速度约为 16 token/s
- 甚至于在 4GB RAM 的树莓派上运行 7B 模型，尽管速度只有 0.1 token/s

2023 年 6月，llama.cpp  作者 Georgi Gerganov 开始创业，宣布创立一家新公司 ggml.ai，旨在用纯 C 语言框架降低大模型运行成本。

GPU 对深度学习有两个主要好处：
- 很大的内存带宽（如 A100：1935 GB/s，RTX 4090：1008 GB/s）
  - 关系到数据从 HBM 内存（即 RAM）移动到片上内存需要花费的时间。片上内存相当小（A100 上为 40MB，而 RAM 为 40-80GB）内存带宽比计算性能小约 2 个数量级
  - 内存带宽几乎是与 transformer 采样相关的最大限制因素。任何降低这些模型内存需求的方法都会使它们更容易提供服务 —— 比如量化
- 很大的算力（A100：FP16 有 312 TFLOPS，RTX 4090：FP16 有 82.6 TFLOPS）

llama.cpp 使用深度学习推理中较为激进的 `int4` 格式，因此 KV 缓存的 RAM 需求减少到 1.33GB，模型参数的 VRAM 减少到 16.25GB。

#### TinyLlama-1.1B

小模型在边缘设备上有着广泛的应用，如智能手机、物联网设备和嵌入式系统，这些边缘设备通常具有有限的计算能力和存储空间，它们无法有效地运行大型语言模型。

场景：
- 对大型模型进行speculative decoding。
- 边缘装置上运行，比如离线的实时机器翻译
  - TinyLlama的4比特量化版本的模型权重只需要**550MB**的内存
- 游戏中实现实时对话生成(因为还得给游戏本身留显存所以模型要小)。

【2024-1-8】新加坡科技设计大学（SUTD）的研究者[近日](https://www.jiqizhixin.com/articles/2024-01-08-20)推出了 TinyLlama，该语言模型的参数量为 11 亿，在大约 3 万亿个 token 上预训练而成。
- 论文地址：[TinyLlama](https://arxiv.org/pdf/2401.02385.pdf)
- 项目地址：[TinyLlama](https://github.com/jzhang38/TinyLlama/blob/main/README_zh-CN.md)

TinyLlama 以 Llama 2 架构和分词器（tokenizer）为基础，TinyLlama 可以在许多基于 Llama 的开源项目中即插即用。此外，TinyLlama 只有 **11 亿**的参数，体积小巧，适用于需要限制计算和内存占用的多种应用。

该研究表示仅需 16 块 A100-40G 的 GPU，便可在 90 天内完成 TinyLlama 的训练。

尽管规模相对较小，但 TinyLlama 在一系列下游任务中表现相当出色，它的性能显著优于同等大小的现有开源语言模型。具体来说，TinyLlama 在各种下游任务中都超越了 OPT-1.3B 和 Pythia1.4B 。

此外，TinyLlama 还用到了各种优化方法，如 flash attention 2、FSDP（ Fully Sharded Data Parallel ）、 xFormers 等。

在这些技术的加持下，TinyLlama 训练吞吐量达到了每 A100-40G GPU 每秒 24000 个 token。例如，TinyLlama-1.1B 模型对于 300B token 仅需要 3,456 A100 GPU 小时，而 Pythia 为 4,830 小时，MPT 为 7,920 小时。这显示了该研究优化的有效性以及在大规模模型训练中节省大量时间和资源的潜力。

TinyLlama 实现了 24k tokens / 秒 / A100 的训练速度，这个速度好比用户可以在 8 个 A100 上用 32 小时训练一个具有 11 亿参数、220 亿 token 的 chinchilla-optimial 的模型。同时，这些优化也大大减少了显存占用，用户可以把 11 亿参数的模型塞入 40GB 的 GPU 里面还能同时维持 16k tokens 的 per-gpu batch size。只需要把 batch size 改小一点， 你就可以在 RTX 3090/4090 上面训练 TinyLlama。 


| Model	| A100 GPU hours on 300 tokens |
|---|---|
| TinyLlama-1.1B |	3456 |
| Pythia-1.0B	| 4830 |
| MPT-1.3B	| 7920 |



#### LiteLlama

德克萨斯工农大学的 Xiaotian Han 发布了 SLM-LiteLlama。它有 460M 参数，由 1T token 进行训练。这是对 Meta AI 的 LLaMa 2 的开源复刻版本，但模型规模显著缩小。
- 项目地址：[LiteLlama-460M-1T](https://huggingface.co/ahxt/LiteLlama-460M-1T)

LiteLlama-460M-1T 在 RedPajama 数据集上进行训练，并使用 GPT2Tokenizer 对文本进行 token 化。作者在 MMLU 任务上对该模型进行评估，结果如下图所示，在参数量大幅减少的情况下，LiteLlama-460M-1T 仍能取得与其他模型相媲美或更好的成绩。


### Ollama

【2023-12-5】[Ollama](https://ollama.ai/download), [GitHub](https://github.com/jmorganca/ollama)

Get up and running with Llama 2 and other large language models locally
- Model 集合: [library](https://ollama.ai/library)
- 提供 REST API 
- 可定制自己的模型

```sh
curl https://ollama.ai/install.sh | sh # linux 下载
# 启动llm
ollama run llama2
ollama run mistral # mistral 模型
```


## 端侧LLM实现


### 手机大模型

模型手机部署
- 【2023-9-7】[手机大模型也卷起来了](https://www.toutiao.com/article/7276004198729581115)
- 【2023-11-16】[掰开安卓手机，满屏都是三个字：大模型](https://mp.weixin.qq.com/s/VspBknaoChlcNwt6v0bzwg)
- [C-Eval](https://cevalbenchmark.com/static/leaderboard.html)全球大模型综合性考试评测榜上，也分别出现了 `vivo` 和`OPPO` 自研大模型云端方案
- 除了华为，小米、OPPO、vivo等厂商也在积极入局大模型，试图在手机大模型这一领域抢占先发优势。

这年头，安卓厂商没个大模型，都不敢开手机发布会了。
- 前脚OPPO刚用大模型升级了语音助手，后脚vivo就官宣自研手机AI大模型；
- 小米发布会则直接将大模型当场塞进手机系统……其竞争激烈程度，不亚于抢芯片首发。

智能终端已经成为了各类AIGC应用的落地“新滩头”
- 图像生成大模型，接二连三地被塞进手机
  - 十亿参数的`Stable Diffusion`，在手机上快速生成一只金毛小狗：
  - 手机上运行十五亿参数的`ControlNet`，快速生成一张限定图像结构的AI风景照
- 文本生成大模型们也争先恐后地推出了手机新应用——
  - 国内有文心一言、智谱清言APP
  - 国外则有OpenAI的移动版ChatGPT，Llama 2手机版也在加急准备中。

最底层的软硬件技术齿轮开始转动。从高通到苹果，最新的芯片厂商发布会，无一不在强调软硬件对机器学习和大模型的支持——
- 苹果M3能运行“数十亿参数”机器学习模型
- 高通的骁龙X Elite和骁龙8 Gen 3更是已经分别实现将130亿和100亿参数大模型装进电脑和手机。高通现场演示和手机中的百亿大模型对话



手机厂商，All in大模型
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/203ad77492394051851ee1784159c00d~tplv-tt-origin-asy2:5aS05p2hQGnpu5Hpqaw=.image?_iz=58558&from=article.pc_detail&x-expires=1694759358&x-signature=Z27rZcowyVZg928D14%2F7YEZsLg4%3D)

国内手机厂商开卷大模型，背景是中国智能手机市场的持续低迷。为了在瓶颈期寻求新的差异化增长点，国产手机厂商已经在影像力、参数、产品形态等方面卷到了极致。

虽然手机厂商都将大模型作为未来业务发力点，但目前手机大模型尚未全面对用户开放，仍处于小范围内测阶段，用户对它的接受程度还不得而知。

而**手机大模型**不仅要面对**友商**之间的竞争，还要面临像`文心一言`APP这样的**云端大模型**竞争对手。
- 文心一言发布初期，参数已经高达**2600亿**，支持多模态的生成式AI内容，比如文生图、代码理解、生成Excel等功能，覆盖了办公、学习、情感、绘画、娱乐等应用场景。
- `小爱同学`在本地部署的大模型受限于**手机算力**，参数相对要更轻量化。

小爱同学13亿的参数量跟文心一言2600亿的参数量并不是同一个数量级，因此应用场景也有一定局限性，就目前内测版本的反馈来看，小爱同学还不支持文生图功能。

相比ChatGPT、文心一言等云端大模型，手机大模型也有其得天独厚的优势
- 因为完全**本地部署**，用户在使用它时的操作对话也在本地运行，在用户数据的**隐私和安全性方**面更有保障。
- 因为参数量较为**轻量化**，加载运行速度也会更快，**不受网络环境限制**。
- 这样轻量化的大模型训练**周期短**，且能根据用户需求进行快速的迭代更新。

另外，可定制化的手机大模型将能更灵活地响应用户的各种需求，用户可以根据自己的兴趣、工作性质、起居习惯定制个性化的大模型助手。比如大模型助手可以根据用户画像，为用户提供饮食起居建议等个性化服务，也可以根据用户喜好，扮演其喜爱的角色形象。

目前的手机大模型实用价值还有待挖掘，但它被认为是打造品牌形象和彰显产品算力的全新范式。

尽管从理论上来说，目前市面大多数智能手机都能达到要求(6GB内存)，但是能够**运行本地大模型**的手机会自然地被消费者认为算力和配置更高。
- 类似**无线充电**这种功能，消费者的使用频率很低，但缺少了无线充电的手机，一定不是一台合格的旗舰机。
- 手机大模型也是同理，未来消费者也可能更加倾向于优先选购那些部署大模型的智能手机。

端手机市场，**自研芯片**始终是彰显品牌能力的杀手锏，但在`Mate 60`系列回归前，华为`麒麟`被制裁、OPPO`哲库`解散，国产手机芯片的自主研发也进入了至暗时刻。因此手机厂商退而求其次，选择在影像方向构建自己的护城河，国产旗舰机型都在讲述自己的影像故事，他们力求通过影像层面的差异化打造品牌的高端形象。而手机大模型的出现也为品牌的高端化故事提供了一个新角度，因此布局大模型也成为国产手机厂商的共识。


### AX650N

【2023-6-1】[5分钟就能完成原版Swin Transformer端侧部署](https://zhuanlan.zhihu.com/p/633942783)

一款号称现实开源模型直接拿来用，还能让性能、功耗与自动驾驶领域基于GPU的端侧芯片有得一拼的平台诞生了。
- 爱芯元智推出AX650N，对Transformer架构支持效果尤甚。

Transformer 是当下最火的 ChatGPT、Stable Diffusion等大模型背后的基础架构。

AX650N 是 AI芯片公司爱芯元智发布的第三代端侧芯片。其构成包括CPU和NPU等，其中CPU采用的是八核A55处理器，NPU则采用了自研混合精度技术，可以做到43.2TOPs（INT4）或10.8TOPs（INT8）的高算力。

AX650N主要用于**端侧视觉感知**。
- ![](https://pic2.zhimg.com/80/v2-2c23c0da748d54750a4d683c8528a995_1440w.webp)
- 该领域业界主要还是基于CNN网络开发应用。
- 准确率和性能双佳的`Swin Transformer`并没有得到突出的大规模落地，还是多部署于云端服务器。

因为GPU对于MHA结构（Transformer中的多头注意力机制）计算支持更友好。

而目前大部分端侧AI芯片由于其**架构限制**为了保证CNN结构的模型效率更好，基本上对MHA结构没有过多性能优化，因此需要修改Swin Transformer的网络结构才能勉强将其部署在端侧—— 一旦修改网络结构，就意味着将出现一系列问题，例如精度下降，精度一降就得对模型进行重训，这个过程就要以星期甚至是月来计算了。

爱芯元智联合创始人、副总裁刘建伟介绍：
> 用AX650N在端侧部署原版Swin Transformer，从拿到测试板到demo复现，只需要5分钟，再到在自己的私有环境里跑起来私有模型，只要1个小时就能搞定。

不仅能跑起来，还跑得飞快、性能高且功耗低。AX650N端侧部署`Swin Transformer`性能可达361 FPS。
- AX650N 还支持低比特混合精度，遇到大规模参数的模型，我们就可以采用INT4来减少内存和带宽占用率，从而降低大模型在端侧边缘侧部署的成本。
- AX650N可以说是成为了目前对Transformer架构支持最好的一个端侧部署平台。
- AX650N还适配ViT/DeiT、DETR在内的Transformer模型，Meta最新发布的视觉模型DINOv2也达到了30帧以上的运行结果。

因此，有了AX650N，下游进行检测、分类、分割等操作也更加方便。
- ![](https://pic3.zhimg.com/80/v2-c1c280c25a868230fccbf3208e49a302_1440w.webp)

接下来，爱芯元智AX650N将会针对Transformer结构进行进一步优化，并且将探索多模态方向的Transformer模型。



### mlc

Github上开源的手机大模型`mlc-chat`为例，这个**60亿**参数的大模型可以在**6GB**内存以上配置的手机上运行

### 高通

高通认为
- 在`云端`和`PC`、`智能手机`等终端之间协同工作的**混合AI**才是AI的未来，将大模型从**云**到**端**部署，尤其进入**手机**，是必然的演进路线。
- 参数超过**10亿**的AI模型已经能够在搭载高通处理器的手机上运行，且性能和精确度水平达到与**云端**相似的水平。
- 在MWC上，高通演示了在**不联网**状态下用安卓手机运行 `Stable Diffusion`，15秒即可生成AI图像。

不过，高通中国区研发负责人`徐晧`曾提到过
- “大模型在手机上是可行的，但真正大规模地部署仍需要时间”。

- 【2023-8-8】[大模型在手机上运行的预言，被高通提前实现了](https://www.leiphone.com/category/industrynews/SPX5rXn2JIfuGELC.html)
- 【2023-7-5】[安卓手机上跑15亿参数大模型，12秒不到就推理完了](https://www.51cto.com/article/759615.html)
- 操作人员在一部没有联网的安卓手机上使用了Stable Diffusion 来生成 AI 图像，整个生成时间不超过 15 秒，整个过程完全在终端进行，但是生成效果却没打一点折扣。 
- [白皮书链接](https://www.qualcomm.cn)
- ![](https://static.leiphone.com/uploads/new/images/20230629/649d22502f28e.jpg?imageView2/2/w/740)
- ![](https://s2.51cto.com/oss/202307/05/4405326526ce7c92c19505e53b006d3fd4d863.gif)
- ![](https://static.leiphone.com/uploads/new/images/20230629/649d225ec79ab.jpg?imageView2/2/w/740)

如果模型大小、提示（prompt）和生成长度小于某个限定值，并且能够提供可接受的精确度，推理即可完全在终端侧进行。如果是更复杂的任务，模型则可以跨云端和终端运行。 

混合 AI 还能支持模型在终端侧和云端同时运行，也就是在终端侧运行轻量版模型时，在云端并行处理完整模型的多个标记（token），并在需要时更正终端侧的处理结果。这能极大限度地解决能耗和成本问题。 

直接从源头减少数据运输过程，隐私泄露的问题便不复存在。 高通指出，混合 AI 架构中有一个“隐私模式”，当用户利用终端侧 AI 向聊天机器人输入健康问题或创业想法等敏感话题时，这个模式会自动开启。

【2023-11-16】[掰开安卓手机，满屏都是三个字：大模型](https://mp.weixin.qq.com/s/VspBknaoChlcNwt6v0bzwg)

高通最新推出的第三代骁龙8移动平台，就被定位为高通“首个专门为生成式AI打造的移动平台”：
- 能够在终端侧运行100亿参数大模型，面向70亿参数大语言模型，每秒能生成20个token。

较之前代产品，第三代骁龙8最重要的变化，就是驱动终端侧AI推理加速的高通AI引擎。

这个AI引擎由多个硬件和软件组成，包括高通Hexagon NPU、Adreno GPU、Kryo CPU和传感器中枢。其中最核心、与AI最密切相关的，是Hexagon NPU。
- 高通公布的数据显示，Hexagon NPU在性能表现上，比前代产品快98%，同时功耗降低了40%。
- Hexagon NPU升级了全新的微架构。更快的矢量加速器时钟速度、更强的推理技术和对更多更快的Transformer网络的支持等等，全面提升了Hexgon NPU对生成式AI的响应能力，使得手机上的大模型“秒答”用户提问成为可能。
- Hexagon NPU之外，第三代骁龙8在 Sensing Hub（传感器中枢）上也下了功夫：增加下一代微型NPU，AI性能提高3.5倍，内存增加30%。Sensing Hub有助于大模型在手机端的“定制化”。随时保持感知的Sensing Hub与大模型协同合作，可以让用户的位置、活动等个性化数据更好地为生成式AI所用。

- 内存方面，第三代骁龙8支持LPDDR5X，频率从4.2GHz提高到了4.8GHz，带宽77GB/s，最大容量为24GB。
  - 更快的数据传输速度，更大的带宽，也就意味着第三代骁龙8能够支持更大更复杂的AI模型。
- CPU方面，第三代骁龙8采用“**1+5+2**”架构（1个主核心、5个性能核心和2个能效核心），相较于前代的“**1+4+3**”，将1个能效核心转换为性能核心。其中超大核频率提升到3.3GHz，性能核心频率提升到最高3.2GHz，能效核心频率提升到2.3GHz。

新架构下，Kryo CPU性能提高了30%，功耗降低了20%。

### 苹果 


#### 布局

【2023-8-8】[苹果年薪百万开招AIGC人才，目标：让iPhone本地跑上大模型](https://mp.weixin.qq.com/s/MuZxazt3VXiR0WuAAP8UdQ)
- 苹果想要将大模型压缩到终端，在未来让iPhone/iPad等核心产品直接跑上AIGC技术。苹果想要在核心产品上发力端侧大模型，一大部分原因就是为了**隐私**。就像离线版Siri那样，不经云端直接运行AI软件，不仅可以让程序跑得更快，也能更安全和私密地处理用户数据。
  - 2020年的时候，苹果就斥资近2亿美元收购了一家总部位于西雅图的人工智能初创公司：Xnor，该公司专门在移动设备上运行复杂的机器学习模型，还一度击败了微软、亚马逊和英特尔等大厂的产品。
- 机器智能与神经设计 (Machine Intelligence Neural Design，MIND，属于苹果AIML的一部分) 等团队，要求工程师能够“在苹果下一代推理引擎中定义和帮助实现加速和压缩大型语言模型 (LLM) 的功能”，这指的就是在移动端而非云端。将“最先进的基础模型带入我们口袋里的iPhone，以保护隐私的方式实现下一代基于ML的体验”


### 华为

- 7月, 华为开发者大会上发布了面向行业的`盘古大模型3.0`，最高版本高达**1000亿**参数，同时也将盘古大模型应用到手机终端，将智能助手`小艺`接入了盘古大模型能力。
- 8月底，华为`Mate 60`系列手机发布，除了支持`卫星通信`，另一舆论关注点就是其接入了`盘古`人工智能大模型。
  - 全新小艺将在今年8月底开放邀请测试，并于晚些时候在搭载HarmonyOS 4.0及以上的部分机型通过OTA升级体验，具体升级计划稍晚公布。
- 9月1日，华为`小艺`大模型开启众测，首批支持机型为Mate 60系列手机。

【2023-8-11】[华为率先把大模型接入手机！小艺+大模型，智慧助手智商](https://www.toutiao.com/article/7265924630538519081)

只需一句中文指令，华为小艺
- 写出一封英文邮件：
- 把自己的照片用AI做成不同风格：

还能说一长串指令，让它自己创建复杂场景，大白话就能听得懂

华为盘古L0基座大模型的基础上，融入大量场景数据，对模型进行精调，最后炼成的一个L1层对话模型。
- 文本生成、知识查找、资料总结、智能编排、模糊/复杂意图理解等任务。
- 而且可调用各种APP服务，实现系统级的智能化体验。

小艺依托的底层模型是华为盘古系列。

今年7月，华为正式发布盘古大模型3.0，并提出3层模型架构。
- `L0`：**基础**大模型，包括自然语言、视觉、多模态、预测、科学计算；
- `L1`：N个**行业**大模型，比如政务、金融、制造、矿山、气象等；
- `L2`：更细化场景的模型，提供“开箱即用”的模型服务

其中L0层基础大模型最大版本包含1000亿参数，预训练使用了超3万亿tokens。

小艺正是在华为盘古L0基座大模型的基础上，针对终端消费者场景构建了大量的场景数据，并对模型进行精调，最后炼成的`L1`层对话模型。
- 精调中，小艺加入了覆盖终端消费者的主流数据类型，如：对话、旅游攻略、设备操控、吃穿住行等。
- 这能很好覆盖普通用户日常对话的知识范围，并且增强模型对话过程中的**事实性**、**实时性**以及**安全合规**等。

基于大模型能力，华为小艺这一次主要在三方面做了升级：
- （1）**智慧交互**: 让对话、交互更自然流畅, 听懂大白话，理解模糊意图和复杂命令。
  - 找不到最新的壁纸设置功能、也不知道功能名称，可以直接问：那个可以根据天气实时变化的壁纸怎么换？
  - 复杂的命令，包含多个要求的那种：找一家松山湖附近，评分高的海鲜餐厅，最好有适合四个人的优惠套餐。
  - 多模态能力，能理解图像内容, 看一张邀请函图片，然后说：
    - 导航去图上的地址。然后就可以提取出图上地址信息，并调用地图服务导航
    - 把邀请函中的联系信息保存，很好理解图像中的文本信息
  - 小艺进行复杂任务编排：
    - 设置一个晨跑场景：帮我创建晨跑场景。每周一到周五早上6点半为我播报当天天气。当我戴上蓝牙耳机的时候，就播放收藏的歌曲，并把手机设为静音模式。
- （2）**高效生产力**： 得益于大模型等能力加持，现在小艺可提供更高效的生产力工具。帮你看、读、写都没问题。
  - 看一篇英文文章，然后提问这篇文章中讲了什么？
  - 记住过一些信息，也能调用出来生成相应内容：过几天就要约David见面聊项目了，结合上次会议记的信息，写一份英文会议预约邮件。
  - 小艺也能利用AI视觉能力，将照片创作成多种风格
- （3）**个性化服务**： 小艺现在支持更加个性化服务，也能更懂你。
  - 所有记忆内容都是在用户授权下完成，会充分保护用户隐私。
  - 感知到更多用户的高频场景，能主动提供一站式的智慧组合建议，省去很多自己手动查找的过程。
    - 出境旅游场景下，出发前小艺能**实时**提醒最新汇率、兑换外币、帮助用户**即时**获取目的地游玩攻略；到达目的地后，还能提醒行李转盘信息、一键开启境外流量、快速获取实时翻译工具等。
  - 全新小艺智慧场景增加**3倍**，POI数量提升了**7倍**，能够覆盖核心餐饮购物门店、商圈、机场高铁站等场景。

全新小艺不仅获得了最新的AIGC能力，还改善了手机语音助手过去经常被诟病的一些短板。
- 如没有记忆力、对话呆板、听不懂大白话等……

具体能力提升
- 更自然语言对话、玩机知识问答、查找生活服务、对话识别屏幕内容、生成摘要文案图片等。

其它方面
- 部署方面，华为正在不断增强大模型**端云协同**能力
  - **端侧大模型**先对用户请求和上下文信息做一层预处理，再将预处理后的request请求到云侧。
  - 既能发挥**端侧模型**响应**快**的优势，又能通过云端模型来提升问答和响应**质量**，同时也能更进一步保护用户隐私数据。
- 降低推理时延上，华为小艺做了**系统性工程优化**，包含从底层芯片、推理框架、模型算子、输入输出长度等全链路。
  - 通过对各个模块时延进行拆解，研发团队明确了各部分优化目标，利用算子融合、显存优化、pipeline优化等方式降低时延。
- 同时 **prompt长度**和**输出长度**也会影响大模型推理速度。
  - 华为针对不同场景的prompt和输出格式做了**逐字分析和压缩**，最终实现推理时延**减半**。

华为小艺和大模型的融合，不是简单对聊天、AIGC、回复等任务进行增强，而是以大模型为核心，进行了系统级增强。让大模型成为系统的“大脑”
- ![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/849e07ed945047e396eb8b231b0b0fd1~noop.image?_iz=58558&from=article.pc_detail&x-expires=1692416407&x-signature=mia2XHzxNcQ9PFYg2NEcTkbB6o8%3D)

底层逻辑是：
> 将用户任务分配给合适的系统，各个系统各司其职，同时在复杂场景上增强体验。

具体来看小艺的典型对话流程，一共可分为三步：
- 第一步，接收用户问题，基于上下文理解/小艺记忆的能力，分析问题该如何处理。
- 第二步，根据请求类型调用不同能力，包括: 元服务检索、创意生成、知识检索。
  - 如果用户发起的请求涉及到**元服务**，比如他询问附近有哪些可以聚会的餐厅，这就涉及到了美食APP服务的调用，系统需要API生成，最后由服务方基于推荐机制给出响应。
  - 如果用户询问的是**知识问题**，比如问盘古大模型有多少参数。这时系统会调用搜索引擎、对应领域知识、向量知识进行查询，然后融合生成答案。
  - 如果用户的请求是**生成式任务**，那么大模型自身能力即可给出回复。
- 最后一步，所有生成的回答会经过风控评估，再返还给用户。
- ![arch](https://www.qbitai.com/wp-content/uploads/replace/e2e0633be09452ebb58e22e796ce2aba.png)

以上诸多体验都来自于华为研发团队成员日常感知到的场景。
- 有人习惯**上下班开车路上**获取新闻，对于**太长资讯**只能看不方便听，所以在华为小艺中出现了**资讯总结**的功能。
- 在写购物评论、生日祝福的时候总是词穷，所以华为小艺提供了**文案生成**功能。

而这种对场景体验的关注，是HarmonyOS的天生优势。

从诞生起，HarmonyOS便没有局限于手机端，而是面向**多种终端、全场景**。如今已经打造出“1+8+N”全场景生态。

华为小艺现在也已部署在了1+8设备上，未来将结合全场景设备的业务形态，逐步把拥有大模型能力的小艺部署到消费者全场景体验之上。

小艺作为一个AI驱动的智慧助手，从诞生起也在不断集成各种AI能力，如AI字幕、小艺朗读等

### 小米

- 4月，组建大模型团队，小米大模型技术的主力突破方向为**轻量化**、**本地部署**，小米考虑的是优先在手机上实现端侧跑通，让每个人都能更好在手机上使用大模型。
- [MiLM-6B](https://cevalbenchmark.com/static/model.html?method=MiLM-6B) 是由小米全自研的大规模预训练语言模型


#### 1.3b

- 2023年8月14日，[小米新品发布会上](https://www.yzwb.net/zncontent/3150588.html)，雷军正式宣布小米**13亿**参数大模型已经成功在手机本地跑通，部分场景可以媲美**60亿**参数模型在云端运行结果。
- ![img](https://imgcdn.yzwb.net/7343_1692025618000.jpg?imageMogr2/thumbnail/1080x%3E/strip/ignore-error/1|imageslim)

| 模型 | 平均分 | STEM | 人文学科 | 社会科学 | 其他 | 中国特定主题 | 
| --- | --- | --- | --- | --- | --- | --- | 
| MILM-1.3B | 50.79 | 40.51 | 54.82 | 54.15 | 53.99 | 52.26 | 
| Baichuan-13B | 54.63 | 42.04 | 60.49 | 59.55 | 56.6 | 55.72 | 
| ChatGLM2-6B | - | 41.28 | 52.85 | 53.37 | 52.24 | 50.58 |

注
- 2023年8月10日数据，CMMLU中文多任务语言理解评估
- ![](https://img.ithome.com/newsuploadfiles/2023/8/12597b8c-00a0-44c9-971a-1f37ef6d8b9d.jpg?x-bce-process=image/format,f_auto)
- [新闻源](https://www.ithome.com/0/712/321.htm)

#### 小爱同学升级

【2023-8-14】小爱同学升级AI大模型能力，并开启内测，未来小米大模型技术方向是轻量化、本地部署。

截至目前，已经有通过部分内测的用户初步尝鲜小爱大模型，并分享了“调戏”小爱同学全新版本的体验。

小爱同学功能解读[视频](https://www.toutiao.com/video/7269590168980587063)
- 生成创作：写文案、策划能力、写代码、画图
- 改写能力
- 闲聊、知识问答：孔子隔空对话、小米商品咨询

问题
- 稳定性不高，有时卡顿
- 理解能力有待提高
- 目前仅文本对话，不能传文档、表格、图片
- 无法验证知识、版权是否正确


### vivo 

- 【2023-12-19】[没绷住，vivo提前“泄密”大模型能力](https://mp.weixin.qq.com/s/ad-HXUIWEE_QPA2HmfnJiQ)
- 【2023-11-1】[vivo自研大模型/操作系统齐面世！蓝心大模型加持最新OriginOS4](https://mp.weixin.qq.com/s/gWrzWRRg8wBROqXRRvNFIA)

vivo自研通用大模型矩阵正式发布！

蓝心大模型矩阵一共5款，兼顾端云：
- 1B **端侧**大模型
- 7B **端云**两用模型
- 70B **云端**主力模型
- 130B 云端大模型
- 175B 云端大模型

7B版本将对外开源，vivo成为首家开源大模型的手机厂商

最卖座的安卓手机，竟然要实 装 大 模 型 了？还是发布即可用那种 —— 新版手机系统直接搭载，不整虚的。

虽然国产大模型百花齐放，但手机端“百模大战”，可以说是才刚进入热身阶段。自研大模型的手机厂商已有不少，但真正装进手机系统中的，还几乎没有

手机端实际上是大模型**最难落地**的场景之一。
- 受体积、耗电量所限，手机端侧算力相比云端算力“少得可怜”。
  - 大模型，如果直接部署在端侧，往往难以取得较好的使用效果，即使能运行起来，推理速度也不及预期
  - 如果做输入法的出词推荐，2秒才能出一个词, 但缩小模型体积，效果肯定会打折扣
- 大模型直接上传到云端联网使用，又会失去端侧部署的优势
  - 大模型原本可以根据用户信息，在手机上个性化定制手机助理，且确保信息不上传到云端；但如果大模型在云端加载，势必要将个人信息通过网络上传，隐私安全无法保障。
  - 云端运行大模型的成本非常高: vivo 有3亿中国大陆用户，如果每天用10次，一天的运算成本大概是3000万元，一年需要花费约90-100亿元。

vivo究竟是怎么将大模型部署到手机端的？一些“技巧”
- (1) 大模型的参数设计: 不同的参数量级，分别用于处理不同的任务。
  - 最小的大模型，包括10亿和70亿参数的模型，直接部署在端侧，确保耗电量不高。涉及用户信息等个性化任务需求时，可以用这类大模型来完成，例如一键将备忘录内容加入日历、并设置闹钟提醒。
  - 更大的大模型，如660亿、1300亿和1750亿参数的大模型，则根据任务难度来决定调用情况。大模型“智力涌现”所需的参数量级，几百亿足矣。
  - 像用超大模型如GPT-4来总结电子邮件的行为，就一直被调侃为“开兰博基尼送披萨”。
  - 遇上“难度系数较低”的任务时，可以切换更小的模型来进行，更复杂的如对上下文长度和输出效果要求更高的任务，再调用千亿参数大模型来完成。
- (2) 大模型的运行方式: 不依靠单一算力，而是**云端协同**的方式兼顾运行速度和体验。
  - 上千亿的大模型尚难以部署到手机端，即使能部署，运行速度和耗电量也无法接受。
  - 以谷歌和DeepMind同时推出的投机采样（speculative sampling/decoding）为例，这项技术就能在提升大模型推理速度的同时，确保生成效果。将一个大模型和一个较小的大模型（draft模型）进行组合，来解决大模型推理时的“内存限制”问题。然而，这个较小的模型并非“随便就能找到”，它必须和大模型“配套”，例如接口要统一、概率分布也要接近等。
  - vivoLM这5个大模型如果相互“配套”，就能运用类似技术来实现端云协同的效果：大模型在云端进行计算，更小的模型则放在端侧运行，能节省相当的推理成本。

搭载大模型的vivo手机新系统会拥有什么新功能
- 使用方法上，vivoLM 目测会以**语音助手**的形式作为入口，作为全机的“智能助理”随叫随到；
- 具体功能上，又主要可能分为三大类：
  - 生成类任务，如邮件智能撰写、AI头像生成等；
  - 复杂任务调度，如一键总结通话内容、设置特定使用场景等；
  - 意图理解，如根据模糊需求定制差旅等。

OriginOS 让系统交互和设计更加人性化。比如
- 点赞颇多的原子通知和原子组件就体现了OriginOS更直观的交互逻辑，通过点、触、滑动能直接使用组件功能。
- OriginOS 3中的侧边栏，具备场景识别能力，能根据用户正在浏览的界面，在侧边栏中匹配所需的应用

#### 7b

vivo 即将推出的 OriginOS 4.0也内置了大模型。此前，在C-Eval全球大模型综合性考试评测榜上，也分别出现了vivo 和OPPO的自研大模型云端方案。
- `vivo_Agent_LM_7B` 是由 vivo AI 全球研究院自主研发的大规模预训练语言模型，从命名不难看出它可能有着70亿参数。

【2023-10-19】MediaTek 与 @vivo @OriginOS 在AI领域深度合作和联调,率先实现了10亿 和 70亿AI大语言模型以及10亿AI视觉大模型在手机端侧的的落地,共同为消费者带来行业领先的端侧生成式AI应用创新体验。

天现移动芯片赋能vivo最新旗舰手机率先搭载端侧AI大模型。在终端侧部署的生成式AI在保护用户信息安全、提升实时性和实现个个性化用户体验等方面具备明显优势。MediaTek的新一代旗舰级AI处理器APU与AI开发平台 NeuroPilot,能显著提高大模型在终端侧的运行效率,为为vivo的端侧生成式AI应用提供强大的AI算力和性能。


vivo 悄悄自研手机AI大模型的消息传得沸沸扬扬，如今靴子落地，官宣定档11月1日vivo开发者大会，上机新版系统OriginOS 4。
- vivo新版自研大模型，取名 `vivoLM`
- vivoLM都已经提前在两大中文大模型评测榜单C-Eval和CMMLU上“刷榜”了一波。
- C-Eval榜单上，vivoLM取得了平均82.3分的榜一成绩，尤其在STEM、人文学科上表现突出；
- CMMLU榜单上，无论是 Five-shot（仅给5个样本示例）还是 Zero-shot（0样本示例），vivoLM-7B 版本都占据了 TOP 1，并同样在人文学科上“一骑绝尘”。
- 登顶CMMLU榜单的vivoLM-7B即70亿版本大模型，正是vivoLM将对外开放的版本

从vivo负责人剧透的消息中，可以窥见三个要点：
- 一口气发布5个大模型, 参数量分成: 十亿（1B/7B）、百亿（66B）和千亿（130B/175B）三个级别。
- 大模型嵌入手机，当助理还会画画
- 70亿版本大模型，对行业开放可用



### Personal GPT

A private, on-device AI Chatbot for iOS and macOS
- [Personal GPT](personalgpt.dev) is an AI chatbot that runs fully offline without an internet connection on iPhone, iPad and Macs.

[Private LLM](https://privatellm.app/)

World’s First Private AI Chatbot
- Tired of subscription fees and worried about your privacy? Experience the power of AI at your fingertips with Private LLM, AI chatbot that runs on your iPhone®, iPad® and Mac without needing an internet connection. A one time purchase lets you download and use the app on all of your Apple devices. Share it with your family with family sharing.



【2023-8-8】
- [体验地址](https://www.personalgpt.dev/chat/pKEy3n8)
- [Mac OS App](https://apps.apple.com/us/app/personal-gpt/id6448106860?l=zh-Hans-CN)
- [github](https://github.com/satpalsr/personalgpt/tree/master), js 库


### 微软 

small language models (SLMs) **Phi**

#### phi-1

【2023-9-15】[微软超强小模型引热议](https://www.toutiao.com/article/7278564747711595042)

2023年6月，微软发了一篇题为《[Textbooks Are All You Need](https://arxiv.org/abs/2309.05463)》的论文，用规模仅为 7B token 的「教科书质量」数据训练了一个 1.3B 参数的模型 ——`phi-1`。
- 尽管在数据集和模型大小方面比竞品模型小几个数量级，但 `phi-1` 在 HumanEval 的 pass@1 上达到了 50.6% 的准确率，在 MBPP 上达到了 55.5%。

phi-1 证明高质量的「小数据」能够让模型具备良好的性能。

#### phi-1.5

最近，微软又发表了论文《[Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)》，对高质量「小数据」的潜力做了进一步研究。

用 phi-1 的研究方法，并将研究重点放在自然语言常识推理任务上，创建了拥有 1.3B 参数的 Transformer 架构语言模型 phi-1.5。phi-1.5 的架构与 phi-1 完全相同，有 24 层，32 个头，每个头的维度为 64，并使用旋转维度为 32 的旋转嵌入，上下文长度为 2048。

Susan Zhang 进行了一系列验证，并指出：
- 「phi-1.5 能够对 GSM8K 数据集中的原问题给出完全正确的回答，但只要稍微修改一下格式（例如换行），phi-1.5 就不会回答了。」
- 修改问题中的数据，phi-1.5 在解答问题的过程中就会出现「幻觉」。例如，在一个点餐问题中，只修改了「披萨的价格」，phi-1.5 的解答就出现了错误。


#### phi-2

【2023-12-12】[Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)
- 2.7b 

效果比 `Gemini Nano 2` 好

| Model | Size | BBH | BoolQ | MBPP | MMLU |
| --- | --- | --- | --- | --- | --- |
| `Gemini Nano 2` | 3.2B | 42.4 | 79.3 | 27.2 | 55.8 |
| `Phi-2` | 2.7B | 59.3 | 83.3 | 59.1 | 56.7 |


### Google

`Gemini Nano 2` 3.2b


### Amazon

【2023-9-25】Amazon's Alexa LLM

Amazon has officially joined the commercial language model race with the announcement of the Alexa LLM.

There is no detail on the **model architecture** or **training data** and **training process**, which is unfortunately slowly becoming the norm in the industry.

However, based on a blog post by Alexa's Vice President, `Daniel Rausch`, and a product demo video, we can make speculations on how the model works.
- [What to know about Amazon’s Alexa LLM](https://bdtechtalks.com/2023/09/25/alexa-llm/?utm_source=substack&utm_medium=email)

Key findings:
- Alexa LLM will be **multimodal**, embedding commands, different **voice characteristics**, **visual** features, and more.
- The model has been designed to work with external APIs like ChatGPT **plugins** or the **Toolformer** approach
- The model adapts to users though the details are not clear—we can assume that Alexa LLM uses an advanced form of retrieval augmented generation (`RAG`)
- The model uses **in-context learning** to maintain coherence over long sequences of interactions between the user and the assistant
- Amazon aims to give the model personality, so we can expect it to be opinionated on some topics that might be controversial
- The model will be available as a “free preview,” but don’t expect Amazon to provide it as free as a final product (Alexa was losing money already)

Read the full article on TechTalks.






# 结束