---
layout: post
title:  端侧大模型LLM
date:   2023-08-07 22:46:00
categories: AIGC
tags: gpt ChatGPT  
excerpt: 如何将大模型LLM部署在边缘设备上? 端侧大模型是未来趋势
mathjax: true
permalink: /llm_end
---

* content
{:toc}


# 端侧 LLM


## 边缘计算

边缘计算就是未来！

ChagtGPT、GPT-4等AIGC大模型虽然它们很厉害，但是它们只解决了语言大模型在**服务端部署**的问题。大模型需要极高的算力和资本投入，并不是一般的企业所能承受的

## 本地 ML

### GGML

GGML是一个用于机器学习的张量库，它只是一个c++库，允许你在CPU或CPU + GPU上运行llm。它定义了用于分发大型语言模型(llm)的二进制格式。GGML使用了一种称为量化的技术，该技术允许大型语言模型在消费者硬件上运行。

## 本地 LLM


### LLaMA

模型权重是浮点数。就像表示大整数(例如1000)比表示小整数(例如1)需要更多的空间一样，表示高精度浮点数(例如0.0001)比表示低精度浮点数(例如0.1)需要更多的空间。量化大型语言模型的过程涉及降低表示权重的精度，以减少使用模型所需的资源。GGML支持许多不同的量化策略(例如4位、5位和8位量化)，每种策略在效率和性能之间提供不同的权衡。

量化后模型大小的对比：
- ![](https://pic4.zhimg.com/80/v2-f21157c61f394b898fa7c23a11510bcb_1440w.webp)
- [Refer](https://zhuanlan.zhihu.com/p/639565332)


## 端侧LLM实现

模型手机部署

### AX650N

【2023-6-1】[5分钟就能完成原版Swin Transformer端侧部署](https://zhuanlan.zhihu.com/p/633942783)

一款号称现实开源模型直接拿来用，还能让性能、功耗与自动驾驶领域基于GPU的端侧芯片有得一拼的平台诞生了。
- 爱芯元智推出AX650N，对Transformer架构支持效果尤甚。

Transformer 是当下最火的 ChatGPT、Stable Diffusion等大模型背后的基础架构。

AX650N 是 AI芯片公司爱芯元智发布的第三代端侧芯片。其构成包括CPU和NPU等，其中CPU采用的是八核A55处理器，NPU则采用了自研混合精度技术，可以做到43.2TOPs（INT4）或10.8TOPs（INT8）的高算力。

AX650N主要用于**端侧视觉感知**。
- ![](https://pic2.zhimg.com/80/v2-2c23c0da748d54750a4d683c8528a995_1440w.webp)
- 该领域业界主要还是基于CNN网络开发应用。
- 准确率和性能双佳的`Swin Transformer`并没有得到突出的大规模落地，还是多部署于云端服务器。

因为GPU对于MHA结构（Transformer中的多头注意力机制）计算支持更友好。

而目前大部分端侧AI芯片由于其**架构限制**为了保证CNN结构的模型效率更好，基本上对MHA结构没有过多性能优化，因此需要修改Swin Transformer的网络结构才能勉强将其部署在端侧—— 一旦修改网络结构，就意味着将出现一系列问题，例如精度下降，精度一降就得对模型进行重训，这个过程就要以星期甚至是月来计算了。

爱芯元智联合创始人、副总裁刘建伟介绍：
> 用AX650N在端侧部署原版Swin Transformer，从拿到测试板到demo复现，只需要5分钟，再到在自己的私有环境里跑起来私有模型，只要1个小时就能搞定。

不仅能跑起来，还跑得飞快、性能高且功耗低。AX650N端侧部署`Swin Transformer`性能可达361 FPS。
- AX650N 还支持低比特混合精度，遇到大规模参数的模型，我们就可以采用INT4来减少内存和带宽占用率，从而降低大模型在端侧边缘侧部署的成本。
- AX650N可以说是成为了目前对Transformer架构支持最好的一个端侧部署平台。
- AX650N还适配ViT/DeiT、DETR在内的Transformer模型，Meta最新发布的视觉模型DINOv2也达到了30帧以上的运行结果。

因此，有了AX650N，下游进行检测、分类、分割等操作也更加方便。
- ![](https://pic3.zhimg.com/80/v2-c1c280c25a868230fccbf3208e49a302_1440w.webp)

接下来，爱芯元智AX650N将会针对Transformer结构进行进一步优化，并且将探索多模态方向的Transformer模型。



### MLC LLM -- 陈天奇

- 【2023-5-2】[陈天奇等人新作引爆AI界：手机原生跑大模型，算力不是问题了](https://mp.weixin.qq.com/s/uQGAu1v-6ApgZHVkZJsUdQ)
- 【2023-6-5】[陈天奇官宣新APP，让手机原生跑大模型，应用商店直接下载使用](https://www.toutiao.com/article/7241085086400233995), 陈天奇公布了一个好消息：MLC Chat app 已经在苹果的 App Store 上线了。

【2023-5-2】[端侧语言大模型部署时代已经悄悄到来！](https://zhuanlan.zhihu.com/p/626268783)

#### TVM 简介

TVM是一个深度学习编译器，初衷是让各种训练框架训练好的模型能够在不同硬件平台上面快速推理
- 支持Pytorch、AutoML、Tensorflow、Onnx、Kersa、Mxnet等多种前端训练框架；
- 支持ARM CPU、Intel CPU、NVIDIA显卡、FPGA、ASIC等多种硬件设备。

MLC-LLM 底层技术其实就是`TVM`编译器。

该框架的输入是一些训练框架训练好的**模型文件**；
- 然后, 利用Relay将其转换成 High-Level Differentiable IR，该阶段会执行一些图优化操作，包括：算子融合、常量折叠、内存重排、模型量化等；
- 接着会利用AutoTVM、Ansor或者Meta Scheduler等自动化优化技术来将这种IR转换为Tensor Expression IR这种更低级的IR表示。

TVM深度学习编译器中的一个亮点工作就是**自动优化技术**
- 第一代优化技术叫`AutoTVM`
- 第二代叫`Ansor`或者`Auto Scheduler`
- 第三代叫`Meta Scheduler`

![](https://pic3.zhimg.com/80/v2-a2aa328a18b3afb02f48816419c481c6_1440w.jpg)

AutoTVM
- ![](https://pic3.zhimg.com/80/v2-9e23067a8872309bdafadede6328e192_1440w.jpg)

Ansor/Auto Scheduler
- ![](https://pic4.zhimg.com/80/v2-7ccb0e4a6fbbecd305f68dfd573b1377_1440w.jpg)


#### MLC LLM 简介

MLC LLM 是一种通用解决方案，允许将任何语言模型**本地部署**在各种硬件后端和本地应用程序上。

此外还有一个高效框架，供每个人进一步优化自己用例的模型性能。<span style='color:red'>一切都在本地运行，无需服务器支持</span>，并通过手机和笔记本电脑上的本地 GPU 加速。

TVM是一个深度学习编译器，知名编译工具，国内很多大公司都在使用它，国内很多的芯片公司都在使用它构建自己的工具链。
- `AutoTVM`、`Ansor`是`TVM`中比较亮眼的工作，思想都是利用ML将算子优化的任务自动化，当前它已经可以很好的支持多种硬件设备。
- 语言大模型的轻量化核心是**Transformer的加速与优化**，TVM社区很早就开始探索Transformer的加速与优化。除此之外，TVM中的图优化技术、自动优化等技术为语言大模型的轻量化打下了坚实的基础。

MLC-LLM只是语言大模型轻量化的开端，语言大模型**轻量化**方向近期会变得异常火热, 很多大公司陆续都是开源自己的一些工作。
- 随着MIC-LLM等工具出现，**端侧大模型部署**热潮已经来临。OpenAI一家独大的情况也会慢慢得打缓解，随着语言大模型的赋能，越来越多的智能设备，尤其是机器人的智能程度会更上一层楼！
- 随着端侧语言大模型的部署难题逐步被解决，端侧模型的**数据隐私**问题可能成为了端侧部署的一个关键问题。不过，这个问题应该相对来说会比较容易一些。期待了端侧语言大模型时代的到来！

- [演示图](https://vdn6.vzuu.com/SD/da2d7036-e81f-11ed-a962-bacc53acff3b-v1_f4_t2_etMRzyS8.mp4?pkey=AAV2GKNHXbMr7W0DZWmaAKjmklpebDlDgvlJQN4ElgagtlxqcrYmaLNld20o3ymLMrOUseNg1m3gdavjpUBHj89Y&c=avc.1.1&f=mp4&pu=078babd7&bu=078babd7&expiration=1691478412&v=ks6)
- [参考](https://zhuanlan.zhihu.com/p/626189075)


[mlc-llm](https://mlc.ai/mlc-llm/) 部署汇总
- 亲测：华为mate 30下载后，启动即闪退；iOS正常

|设备|地址|示例|
|---|---|---|
|iOS|[iOS地址](https://testflight.apple.com/join/57zd7oxa)|![](https://mlc.ai/mlc-llm/gif/ios-demo.gif)|
|Android|[Android地址](https://mlc.ai/mlc-llm/gif/android-demo.gif)|![](https://mlc.ai/mlc-llm/gif/android-demo.gif)|
|PC|[Windows Linux Mac](https://mlc.ai/mlc-llm/#windows-linux-mac)|![](https://mlc.ai/mlc-llm/gif/linux-demo.gif)|
|Web|[WebLLM](https://mlc.ai/mlc-llm/#web-browser)||

让大模型变小这条路上，人们做了很多尝试
- 先是 Meta 开源了 LLaMA，让学界和小公司可以训练自己的模型。
- 随后斯坦福研究者启动了 Lamini，为每个开发者提供了从 GPT-3 到 ChatGPT 的快速调优方案。
- 最近 MLC LLM 的项目一步登天，因为它能在**任何设备**上编译运行大语言模型。

MLC LLM 在各类硬件上**原生部署任意大型语言模型**提供了解决方案，可将大模型应用于移动端（例如 iPhone）、消费级电脑端（例如 Mac）和 Web 浏览器。
-  TVM、MXNET、XGBoost 作者，CMU 助理教授，OctoML CTO 陈天奇等多位研究者共同开发的，参与者来自 CMU、华盛顿大学、上海交通大学、OctoML 等院校机构，同时也获得了开源社区的支持。
- [github](https://github.com/mlc-ai/mlc-llm)
- [Demo](https://mlc.ai/mlc-llm/)
- [MLC课程](https://mlc.ai/summer22-zh/schedule)：机器学习编译
- [知乎专题](https://www.zhihu.com/question/598610139)

曾经开源过XGBoost和TVM `陈天奇`大佬已经完成了这件事情，推出了一个叫`MLC-LLM` 工具，在一些低算力平台上面运行一些语言大模型！只要GPU显存大于6GB，都可以去尝试在本地部署一下属于语言大模型

MLC LLM 旨在让每个人都能在个人设备上本地开发、优化和部署 AI 模型，而无需服务器支持，并通过手机和笔记本电脑上的消费级 GPU 进行加速。具体来说，MLC LLM 支持的平台包括：
- iPhone
- Metal GPU 和英特尔 / ARM MacBook;
- 在 Windows 和 Linux 上支持通过 Vulkan 使用 AMD 和 NVIDIA GPU；
- 在 Windows 和 Linux 上 通过 CUDA 使用 NVIDIA GPU；
- 浏览器上的 WebGPU（借助 MLC LLM 的配套项目 Web LLM）。

为了实现在各类硬件设备上运行 AI 模型的目标，研究团队要解决计算设备和部署环境的多样性问题，主要挑战包括：
- 支持不同型号的 CPU、GPU 以及其他可能的协处理器和加速器；
- 部署在用户设备的**本地环境**中，这些环境可能没有 python 或其他可用的必要依赖项；
- 通过仔细规划分配和积极压缩模型参数来解决**内存限制**。
- MLC LLM 提供可重复、系统化和可定制的工作流，使开发人员和 AI 系统研究人员能够以 Python 优先的方法实现模型并进行优化。MLC LLM 可以让研究人员们快速试验新模型、新想法和新的编译器 pass，并进行本地部署。

为了实现原生部署，研究团队以**机器学习编译**（MLC）技术为基础来高效部署 AI 模型。
- [MLC技术](https://mlc.ai/)
- MLC LLM 借助一些开源生态系统，包括来自 HuggingFace 和 Google 的分词器，以及 LLaMA、Vicuna、Dolly 等开源 LLM。
- ![](https://pica.zhimg.com/80/v2-b23bb5806fa9c32e51773e06494b8f62_1440w.webp?source=1940ef5c)

MLC LLM 的主要工作流基于 Apache TVM Unity，通过扩展 TVM 后端使模型编译更加透明和高效。
- Dynamic shape：该研究将语言模型烘焙（bake）为具有原生 Dynamic shape 支持的 TVM IRModule，避免了对最大输入长度进行额外填充的需要，并减少了计算量和内存使用量。
- 可组合的 ML 编译优化：MLC LLM 可以执行许多模型部署优化，例如更好的编译代码转换、融合、内存规划和库卸载（library offloading），并且手动代码优化可以很容易地合并为 TVM 的 IRModule 转换，成为一个 Python API。
- 量化：MLC LLM 利用低位量化来压缩模型权重，并利用 TVM 的 loop-level TensorIR 为不同的压缩编码方案快速定制代码生成。
- 运行时（Runtime）：TVM 编译生成的库能够通过 TVM runtime 在设备的原生环境中运行，TVM runtime 支持 CUDA/Vulkan/Metal 等主流 GPU 驱动以及 C、JavaScript 等语言的绑定。

此外，MLC 还为 CUDA、Vulkan 和 Metal 生成了 GPU shader，并通过 LLVM 支持多种 CPU，包括 ARM 和 x86。通过改进 TVM 编译器和运行时，使用者可以添加更多支持，例如 OpenCL、sycl、webgpu-native。


#### MLC LLM 支持设备

支持的设备类型
- MLC-LLM工具支持多种设备类型，大到N卡、AMD GPU，小到Android、IOS、WebGPU等。具体测试的设备列表如下所示。建议在设备内存大于等于6GB的设备上面进行推理与测试。

- iPhone, iPad

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| iPhone 14 Pro | iOS 16.4.1 | 7.2 | [https://github.com/junrushao](https://github.com/junrushao) |
| iPad Pro 11 with M1 | iPadOS 16.1 | 10.6 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529377124](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529377124) |

-  Metal GPUs and Intel/ARM MacBooks

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| UHD Graphics 630 | macOS Ventura | 2.3 | [https://github.com/junrushao](https://github.com/junrushao) |
| 2020 MacBook Pro M1 (8G) | macOS | 11.4 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529148903](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529148903) |
| 2021 MacBook Pro M1Pro (16G) | macOS Ventura | 17.1 | [https://github.com/mlc-ai/mlcllm/issues/15#issuecomment-1529434801](https://github.com/mlc-ai/mlcllm/issues/15%23issuecomment-1529434801) |
| M1 Max Mac Studio (64G) |  | 18.6 | [https://github.com/mlc-ai/mlcllm/issues/15#issuecomment-1529714864](https://github.com/mlc-ai/mlcllm/issues/15%23issuecomment-1529714864) |

- AMD and NVIDIA GPUs via Vulkan on Windows and Linux

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| Raden Pro 5300M | macOS Venture | 12.6 | [https://github.com/junrushao](https://github.com/junrushao) |
| AMD GPU on Steam Deck | TBD (S macOS Ventura ome Linux) | TBD | [https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia8ux6/](https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia8ux6/) |
| RX 7900 xtx |  |  | [https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia691u/](https://www.reddit.com/r/LocalLLaMA/comments/132igcy/comment/jia691u/) |
| RX6800 16G VRAM | macOS Ventura | 22.5 | [https://github.com/mlc-ai/mlc-llm/issues/15](https://github.com/mlc-ai/mlc-llm/issues/15) |

- NVIDIA GPUs via CUDA on Windows and Linux

| 硬件/GPU | 操作系统 | Tokens/sec | 链接 |
| --- | --- | --- | --- |
| GTX 1060 (6GB) | Windows 10 | 16.7 | [https://github.com/mlc-ai/mlc-llm/issues/13#issue-1689858446](https://github.com/mlc-ai/mlc-llm/issues/13%23issue-1689858446) |
| RTX 3080 | Windows 11 | 26.0 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529434801](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529434801) |
| RTX 3060 | Debian bookworm | 21.3 | [https://github.com/mlc-ai/mlc-llm/issues/15#issuecomment-1529572646](https://github.com/mlc-ai/mlc-llm/issues/15%23issuecomment-1529572646) |

- WebGPU on browsers


#### MLC-LLM 核心技术


![](https://pic4.zhimg.com/80/v2-c95a2f2706f88094bd196bd4bf7da53b_1440w.webp)

-   **Dynamic shape**: 作者将**语言模型**转换为具有原生动态形状支持的 TVM IRModule，避免了对最大长度进行额外填充的需要，并减少了计算量和内存使用量。如图所示，为了优化动态形状输入
  - 首先应用**循环切分**技术，即将一个大循环切分成两个小循环操作；
  - 然后应用张量自动化技术，即TVM中的Ansor或者Meta Scheduler技术。
  - ![](https://pic4.zhimg.com/80/v2-1a89c78eba7b1228fff6dd08d41ff2bf_1440w.webp)
-   **Composable ML compilation optimization**s: 执行了许多模型部署优化，例如更好的编译代码转换、融合、内存规划、库卸载和手动代码优化可以很容易地合并为TVM 的 IRModule 转换，作为 Python API 公开。如上图所示，模型推理工具链中常用的几种优化技术包括：算子简化、算子融合、常量折叠、内存排布等。
  - ![](https://pic1.zhimg.com/80/v2-dccad206e27b4d485879c50d9033a0ec_1440w.webp)
-   **Quantization**: 利用**低位量化**来压缩模型权重，并利用 TVM 的循环级 TensorIR 为不同的压缩编码方案快速定制代码生成。如图所示，TVM中可以通过两种方式来进行量化：1）通过 relay.quantize 完成浮点模型的量化，该量化包含annotate、calibrate和relize三步；2）通过一种称为 qnn 的 relay方言([http://relay.qnn.xxx](https://link.zhihu.com/?target=http%3A//relay.qnn.xxx)) 直接执行已经量化过的模型。
  - ![](https://pic4.zhimg.com/80/v2-883e9de589cbcac6e1f9854b46b160b7_1440w.webp)
-   **Runtime**: 最终生成的库在原生环境中运行，TVM 运行时具有最小的依赖性，支持各种 GPU 驱动程序 API 和原生语言绑定（C、JavaScript等）。如图所示，TVM支持多种Runtime，包括：JS、Java、Python、C++、Android、IOS、Web等，正是这些Runtime支持，才使得MLC-LLM可以很快的支持很多端侧设备!


#### MLC-LLM部署流图


![](https://pic4.zhimg.com/80/v2-04249102c3061d4c5e436e990a42125f_1440w.webp)

1、**Python first** development

-   IRModule: 如上图所示，该模块存储着一个张量函数集合，每个函数附带首个形状符号，并支持跟踪形状依赖。 该模块包含着Transformer中的关键模块，encoding和step\_decoding，前者用来做输入数据的编码操作，后者用来做数据的解码操作。  
-   ML Compilation Optimization: 该模块主要在计算图上面执行一些优化操作，具体包括：算子融合（降低多次加载的带宽开销）、内存规划（提前在编译阶段分配一些内存，并对内存的排布进行调整）、循环优化（利用常用的tile、reoder、paritation等技术）和权重量化（利用int8、int16等数据类型进行模型压缩）。  
-   TensorIR Schedules: 该模块主要利用Ansor自动优化或者Meta Scheduler自动优化技术对LLM模型中的算子进行调度优化。这是TVM编译器的一个杀手锏！该技术的核心思想是利用ML的思路来解决循环优化问题。  

2、**Universal** development

-   **最底层是硬件驱动层**，该层主要完成一些硬件适配与驱动的工作。支持的硬件具体包括：NVIDIA的CUDA、AMD的Rocm、苹果的Vulkan和WebGPU等。  
-   **第三层是TVM Runtim层**，该层主要完成TVM Runtime库的适配与加载任务。用户需要做的是调用TVM的Runtime推理接口完成模型的推理操作。  
-   **第二层是模型与代码层**，该层主要完成模型的优化与业务逻辑码的开发。通过Python First Development可以导出一个model.dylib库，用户需要实现[http://llm\_chat.cc](https://link.zhihu.com/?target=http%3A//llm_chat.cc)文件，即语言大模型的业务逻辑代码。  
-   **第一层是应用层**，该层用来开发一些上层应用，具体包括Chat CLI命令行工具、MLCChat.App 安卓或者IOS端的上层应用、基于WebGPU的网页端应用等。


#### MLC-LLM环境搭建

1、**iphone平台**

[参考](https://testflight.apple.com/join/57zd7oxa)页面安装已经编译好的APP。

注意事项：
- 试用此页面（仅限前 9000 名用户）以安装和使用作者为 iPhone 构建的示例 iOS 聊天应用程序。应用程序本身需要大约 4GB的内存才能运行。考虑到 iOS 和其他正在运行的应用程序，我们将需要具有 6GB（或更多）内存的最新 iPhone 来运行该应用程序。作者仅在 iPhone 14 Pro Max 和 iPhone 12 Pro上测试了该应用程序。

2、**Windows/Linux/Mac**平台

![](https://pic4.zhimg.com/v2-583cb94d4f32afe60eebeb0dfacbb847_b.gif)

![动图封面](https://pic4.zhimg.com/v2-583cb94d4f32afe60eebeb0dfacbb847_b.jpg)

步骤1 - 安装环境依赖
- 安装 Miniconda 或 Miniforge
- windows与linux用户-安装Vulkan驱动；对于Nvidia用户-建议安装Vulkan驱动

步骤2-创建环境

```sh
# Create new conda environment and activate the environment.
conda create -n mlc-chat
conda activate mlc-chat
# Install Git and Git-LFS, which is used for downloading the model weights from Hugging Face.
conda install git git-lfs
# Install the chat CLI app from Conda.
conda install -c mlc-ai -c conda-forge mlc-chat-nightly
# Create a directory, download the model weights from HuggingFace, and download the binary libraries from GitHub.
mkdir -p dist
git lfs install
git clone https://huggingface.co/mlc-ai/demo-vicuna-v1-7b-int3 dist/vicuna-v1-7b
git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/lib
# Enter this line and enjoy chatting with the bot running natively on your machine!
mlc_chat_cli
```

3、**Web浏览器**平台

步骤
1. 安装 Chrome Canary，它是支持使用 WebGPU 的 Chrome 开发者版本。
2. 利用下面的命令行发起Chrome Canary

```sh
/Applications/Google\ Chrome\ Canary.app/Contents/MacOS/Google\ Chrome\ Canary --enable-dawn-features=disable_robustness
```

3. 在浏览器运行[demo](https://mlc.ai/web-llm/#chat-demo)

注意事项：
- WebGPU 刚刚发布到 Chrome 并且处于测试阶段。我们在 Chrome Canary 中进行实验。你也可以试试最新的Chrome 113。Chrome版本≤112是不支持的，如果你正在使用它，demo会报错 Find an error initializing the WebGPU device OperationError: Required limit (1073741824) is greater than the 支持的限制 (268435456)。
- 验证 maxBufferSize 时 
- 验证所需限制时。已经在 windows 和 mac 上测试过了，你需要一个 6.4G 内存的 gpu。


#### MLC-LLM效果展示

1、**web端**Demo

![](https://pic3.zhimg.com/80/v2-22762accdf3d48acd09f06b8a60e2eda_1440w.webp)

2、IOS端Demo

![](https://pic1.zhimg.com/v2-1f68e0fc385e8b67344f4e5c99fcc837.jpg?source=382ee89a)


3、Web Stable Diffusion

![](https://pic2.zhimg.com/80/v2-895ea13851a24908d5ec8fb6ef1ad775_1440w.webp)



### 高通

- 【2023-8-8】[大模型在手机上运行的预言，被高通提前实现了](https://www.leiphone.com/category/industrynews/SPX5rXn2JIfuGELC.html)
- 【2023-7-5】[安卓手机上跑15亿参数大模型，12秒不到就推理完了](https://www.51cto.com/article/759615.html)

- 操作人员在一部没有联网的安卓手机上使用了Stable Diffusion 来生成 AI 图像，整个生成时间不超过 15 秒，整个过程完全在终端进行，但是生成效果却没打一点折扣。 
- [白皮书链接](https://www.qualcomm.cn)
- ![](https://static.leiphone.com/uploads/new/images/20230629/649d22502f28e.jpg?imageView2/2/w/740)
- ![](https://s2.51cto.com/oss/202307/05/4405326526ce7c92c19505e53b006d3fd4d863.gif)
- ![](https://static.leiphone.com/uploads/new/images/20230629/649d225ec79ab.jpg?imageView2/2/w/740)

如果模型大小、提示（prompt）和生成长度小于某个限定值，并且能够提供可接受的精确度，推理即可完全在终端侧进行。如果是更复杂的任务，模型则可以跨云端和终端运行。 

混合 AI 还能支持模型在终端侧和云端同时运行，也就是在终端侧运行轻量版模型时，在云端并行处理完整模型的多个标记（token），并在需要时更正终端侧的处理结果。这能极大限度地解决能耗和成本问题。 

直接从源头减少数据运输过程，隐私泄露的问题便不复存在。 高通指出，混合 AI 架构中有一个“隐私模式”，当用户利用终端侧 AI 向聊天机器人输入健康问题或创业想法等敏感话题时，这个模式会自动开启。


### 苹果 


#### 布局

【2023-8-8】[苹果年薪百万开招AIGC人才，目标：让iPhone本地跑上大模型](https://mp.weixin.qq.com/s/MuZxazt3VXiR0WuAAP8UdQ)
- 苹果想要将大模型压缩到终端，在未来让iPhone/iPad等核心产品直接跑上AIGC技术。苹果想要在核心产品上发力端侧大模型，一大部分原因就是为了**隐私**。就像离线版Siri那样，不经云端直接运行AI软件，不仅可以让程序跑得更快，也能更安全和私密地处理用户数据。
  - 2020年的时候，苹果就斥资近2亿美元收购了一家总部位于西雅图的人工智能初创公司：Xnor，该公司专门在移动设备上运行复杂的机器学习模型，还一度击败了微软、亚马逊和英特尔等大厂的产品。
- 机器智能与神经设计 (Machine Intelligence Neural Design，MIND，属于苹果AIML的一部分) 等团队，要求工程师能够“在苹果下一代推理引擎中定义和帮助实现加速和压缩大型语言模型 (LLM) 的功能”，这指的就是在移动端而非云端。将“最先进的基础模型带入我们口袋里的iPhone，以保护隐私的方式实现下一代基于ML的体验”


### Personal GPT

【2023-8-8】
- [体验地址](https://www.personalgpt.dev/chat/pKEy3n8)
- [Mac OS App](https://apps.apple.com/us/app/personal-gpt/id6448106860?l=zh-Hans-CN)


# 结束