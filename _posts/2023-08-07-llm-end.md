---
layout: post
title:  端侧大模型LLM
date:   2023-08-07 22:46:00
categories: AIGC
tags: gpt ChatGPT  
excerpt: 如何将大模型LLM部署在边缘设备上? 端侧大模型是未来趋势
mathjax: true
permalink: /agent
---

* content
{:toc}



# 端侧 LLM


## 边缘计算

边缘计算就是未来！

ChagtGPT、GPT-4等AIGC大模型虽然它们很厉害，但是它们只解决了语言大模型在**服务端部署**的问题。大模型需要极高的算力和资本投入，并不是一般的企业所能承受的


## 端侧LLM实现

模型手机部署

### MLC LLM -- 陈天奇

- 【2023-5-2】[陈天奇等人新作引爆AI界：手机原生跑大模型，算力不是问题了](https://mp.weixin.qq.com/s/uQGAu1v-6ApgZHVkZJsUdQ)
- 【2023-6-5】[陈天奇官宣新APP，让手机原生跑大模型，应用商店直接下载使用](https://www.toutiao.com/article/7241085086400233995), 陈天奇公布了一个好消息：MLC Chat app 已经在苹果的 App Store 上线了。

[端侧语言大模型部署时代已经悄悄到来！](https://zhuanlan.zhihu.com/p/626268783)


#### MLC LLM 背景

MLC LLM 是通用解决方案，允许将任何语言模型**本地部署**在各种硬件后端和本地应用程序上。
- [演示图](https://vdn6.vzuu.com/SD/da2d7036-e81f-11ed-a962-bacc53acff3b-v1_f4_t2_etMRzyS8.mp4?pkey=AAV2GKNHXbMr7W0DZWmaAKjmklpebDlDgvlJQN4ElgagtlxqcrYmaLNld20o3ymLMrOUseNg1m3gdavjpUBHj89Y&c=avc.1.1&f=mp4&pu=078babd7&bu=078babd7&expiration=1691478412&v=ks6)
- [参考](https://zhuanlan.zhihu.com/p/626189075)


[mlc-llm](https://mlc.ai/mlc-llm/) 部署汇总
- 亲测：华为mate 30下载后，启动即闪退；iOS正常

|设备|地址|示例|
|---|---|---|
|iOS|[iOS地址](https://testflight.apple.com/join/57zd7oxa)|![](https://mlc.ai/mlc-llm/gif/ios-demo.gif)|
|Android|[Android地址](https://mlc.ai/mlc-llm/gif/android-demo.gif)|![](https://mlc.ai/mlc-llm/gif/android-demo.gif)|
|PC|[Windows Linux Mac](https://mlc.ai/mlc-llm/#windows-linux-mac)|![](https://mlc.ai/mlc-llm/gif/linux-demo.gif)|
|Web|[WebLLM](https://mlc.ai/mlc-llm/#web-browser)||

让大模型变小这条路上，人们做了很多尝试
- 先是 Meta 开源了 LLaMA，让学界和小公司可以训练自己的模型。
- 随后斯坦福研究者启动了 Lamini，为每个开发者提供了从 GPT-3 到 ChatGPT 的快速调优方案。
- 最近 MLC LLM 的项目一步登天，因为它能在**任何设备**上编译运行大语言模型。

MLC LLM 在各类硬件上**原生部署任意大型语言模型**提供了解决方案，可将大模型应用于移动端（例如 iPhone）、消费级电脑端（例如 Mac）和 Web 浏览器。
-  TVM、MXNET、XGBoost 作者，CMU 助理教授，OctoML CTO 陈天奇等多位研究者共同开发的，参与者来自 CMU、华盛顿大学、上海交通大学、OctoML 等院校机构，同时也获得了开源社区的支持。
- [github](https://github.com/mlc-ai/mlc-llm)
- [Demo](https://mlc.ai/mlc-llm/)
- [MLC课程](https://mlc.ai/summer22-zh/schedule)：机器学习编译
- [知乎专题](https://www.zhihu.com/question/598610139)

曾经开源过XGBoost和TVM `陈天奇`大佬已经完成了这件事情，推出了一个叫`MLC-LLM` 工具，在一些低算力平台上面运行一些语言大模型！只要GPU显存大于6GB，都可以去尝试在本地部署一下属于语言大模型

MLC LLM 旨在让每个人都能在个人设备上本地开发、优化和部署 AI 模型，而无需服务器支持，并通过手机和笔记本电脑上的消费级 GPU 进行加速。具体来说，MLC LLM 支持的平台包括：
- iPhone
- Metal GPU 和英特尔 / ARM MacBook;
- 在 Windows 和 Linux 上支持通过 Vulkan 使用 AMD 和 NVIDIA GPU；
- 在 Windows 和 Linux 上 通过 CUDA 使用 NVIDIA GPU；
- 浏览器上的 WebGPU（借助 MLC LLM 的配套项目 Web LLM）。

为了实现在各类硬件设备上运行 AI 模型的目标，研究团队要解决计算设备和部署环境的多样性问题，主要挑战包括：
- 支持不同型号的 CPU、GPU 以及其他可能的协处理器和加速器；
- 部署在用户设备的**本地环境**中，这些环境可能没有 python 或其他可用的必要依赖项；
- 通过仔细规划分配和积极压缩模型参数来解决**内存限制**。
- MLC LLM 提供可重复、系统化和可定制的工作流，使开发人员和 AI 系统研究人员能够以 Python 优先的方法实现模型并进行优化。MLC LLM 可以让研究人员们快速试验新模型、新想法和新的编译器 pass，并进行本地部署。

为了实现原生部署，研究团队以**机器学习编译**（MLC）技术为基础来高效部署 AI 模型。
- [MLC技术](https://mlc.ai/)
- MLC LLM 借助一些开源生态系统，包括来自 HuggingFace 和 Google 的分词器，以及 LLaMA、Vicuna、Dolly 等开源 LLM。
- ![](https://pica.zhimg.com/80/v2-b23bb5806fa9c32e51773e06494b8f62_1440w.webp?source=1940ef5c)

MLC LLM 的主要工作流基于 Apache TVM Unity，通过扩展 TVM 后端使模型编译更加透明和高效。
- Dynamic shape：该研究将语言模型烘焙（bake）为具有原生 Dynamic shape 支持的 TVM IRModule，避免了对最大输入长度进行额外填充的需要，并减少了计算量和内存使用量。
- 可组合的 ML 编译优化：MLC LLM 可以执行许多模型部署优化，例如更好的编译代码转换、融合、内存规划和库卸载（library offloading），并且手动代码优化可以很容易地合并为 TVM 的 IRModule 转换，成为一个 Python API。
- 量化：MLC LLM 利用低位量化来压缩模型权重，并利用 TVM 的 loop-level TensorIR 为不同的压缩编码方案快速定制代码生成。
- 运行时（Runtime）：TVM 编译生成的库能够通过 TVM runtime 在设备的原生环境中运行，TVM runtime 支持 CUDA/Vulkan/Metal 等主流 GPU 驱动以及 C、JavaScript 等语言的绑定。

此外，MLC 还为 CUDA、Vulkan 和 Metal 生成了 GPU shader，并通过 LLVM 支持多种 CPU，包括 ARM 和 x86。通过改进 TVM 编译器和运行时，使用者可以添加更多支持，例如 OpenCL、sycl、webgpu-native。


### 高通

- 【2023-8-8】[大模型在手机上运行的预言，被高通提前实现了](https://www.leiphone.com/category/industrynews/SPX5rXn2JIfuGELC.html)
- 【2023-7-5】[安卓手机上跑15亿参数大模型，12秒不到就推理完了](https://www.51cto.com/article/759615.html)

- 操作人员在一部没有联网的安卓手机上使用了Stable Diffusion 来生成 AI 图像，整个生成时间不超过 15 秒，整个过程完全在终端进行，但是生成效果却没打一点折扣。 
- [白皮书链接](https://www.qualcomm.cn)
- ![](https://static.leiphone.com/uploads/new/images/20230629/649d22502f28e.jpg?imageView2/2/w/740)
- ![](https://s2.51cto.com/oss/202307/05/4405326526ce7c92c19505e53b006d3fd4d863.gif)
- ![](https://static.leiphone.com/uploads/new/images/20230629/649d225ec79ab.jpg?imageView2/2/w/740)

如果模型大小、提示（prompt）和生成长度小于某个限定值，并且能够提供可接受的精确度，推理即可完全在终端侧进行。如果是更复杂的任务，模型则可以跨云端和终端运行。 

混合 AI 还能支持模型在终端侧和云端同时运行，也就是在终端侧运行轻量版模型时，在云端并行处理完整模型的多个标记（token），并在需要时更正终端侧的处理结果。这能极大限度地解决能耗和成本问题。 

直接从源头减少数据运输过程，隐私泄露的问题便不复存在。 高通指出，混合 AI 架构中有一个“隐私模式”，当用户利用终端侧 AI 向聊天机器人输入健康问题或创业想法等敏感话题时，这个模式会自动开启。


### 苹果 


#### 布局

【2023-8-8】[苹果年薪百万开招AIGC人才，目标：让iPhone本地跑上大模型](https://mp.weixin.qq.com/s/MuZxazt3VXiR0WuAAP8UdQ)
- 苹果想要将大模型压缩到终端，在未来让iPhone/iPad等核心产品直接跑上AIGC技术。苹果想要在核心产品上发力端侧大模型，一大部分原因就是为了**隐私**。就像离线版Siri那样，不经云端直接运行AI软件，不仅可以让程序跑得更快，也能更安全和私密地处理用户数据。
  - 2020年的时候，苹果就斥资近2亿美元收购了一家总部位于西雅图的人工智能初创公司：Xnor，该公司专门在移动设备上运行复杂的机器学习模型，还一度击败了微软、亚马逊和英特尔等大厂的产品。
- 机器智能与神经设计 (Machine Intelligence Neural Design，MIND，属于苹果AIML的一部分) 等团队，要求工程师能够“在苹果下一代推理引擎中定义和帮助实现加速和压缩大型语言模型 (LLM) 的功能”，这指的就是在移动端而非云端。将“最先进的基础模型带入我们口袋里的iPhone，以保护隐私的方式实现下一代基于ML的体验”


### Personal GPT

【2023-8-8】
- [体验地址](https://www.personalgpt.dev/chat/pKEy3n8)
- [Mac OS App](https://apps.apple.com/us/app/personal-gpt/id6448106860?l=zh-Hans-CN)


# 结束