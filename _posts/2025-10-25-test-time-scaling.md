---
layout: post
title:   测试时计算 Test Time Scaling
date:   2025-10-25 18:37:00
categories: 大模型
tags: openai  r1  cot 测试时
excerpt: 大模型专题：测试时计算 Test Time Scaling 规模法则 训练
mathjax: true
permalink: /test_time
---

* content
{:toc}


# 测试时计算


【2025-2-23】[通俗理解Test time Scaling Law、RL Scaling Law和预训练Scaling Law](https://blog.csdn.net/acelit/article/details/145803399)

## 背景知识

### Scaling Law

详见站内：[规模法则](llm_law)

### 模型训练

几种训练方式对比

三者核心区别
- 预训练：决定模型的“知识天花板”（基础能力）。
- RL阶段：决定模型的“价值观和细节”（对齐人类偏好）。
- Test Time：决定模型的“临场发挥”（如何榨干它的潜力）。

- 预训练 Scaling Law（打地基阶段）, 通过堆资源（算力、数据、模型参数）让AI更聪明。核心因素：模型参数、数据量、计算资源
- RL Scaling Law（装修阶段）, 基础模型上加入人类反馈训练，让AI更懂人类需求。表现依赖于模型大小、训练步数、反馈质量

Scaling Law排序
- 性价比排序：Test Time > RL > Pretrain
- 天花板排序：预训练 > RL > Test Time

| 维度|排序（高→低）|核心原因 |
| ---|---| --- |
| 性价比| Test Time > RL > 预训练 | 预训练边际成本高，Test Time和RL在现有模型上优化更高效。|
| 天花板 | 预训练 > RL > TestTime | 基座模型决定全局上限，Test Time仅局部优化。|

三个阶段 Scaling Law 详细比较

|属性|Pretrain|RL|Test time|
| ---- | ---- | ---- | ---- |
|作用|**打地基**|**装修**|**使用**|
|解释|堆资源（算力、数据、模型参数）让AI变得更聪明|基础模型上加入人类反馈训练，让AI更懂人类需求|模型使用时动态调配计算资源提升效果|
|比喻|建房子时，**地基越大、材料越多、施工时间越长**，房子就能盖得越高越稳|装修房子时，**请越厉害的设计师、花越多时间调整细节**，房子会越精致，但到后期提升会变慢|考试时，**花越多时间检查、用越复杂的验算方法**，成绩可能越好，但效率会变低|
|核心因素|1. **模型参数**（房子的“大小”）：神经元越多，模型越“聪明”。<br>2. **数据量**（砖头的“数量”）：喂给模型的文本越多，它学到的知识越广。<br>3. **计算资源**（施工的“时间和人力”）：GPU越多、训练时间越长，模型训练得越充分。|1. **模型大小**（设计师的水平）：模型本身越强，学到的策略越好。<br>2. **训练步数**（装修的时间）：训练越久，模型越能优化细节。<br>3. **反馈质量**（业主的要求）：人类反馈或奖励模型越精准，模型行为越符合预期|1. **计算量**（答题时间）：比如生成答案时尝试多次（如采样多次取最优结果）。<br>2. **技巧调整**（答题策略）：比如调整输出的“随机性”（温度参数）或增加搜索范围（Beam Search）|
|规律|三者需要按比例增加。比如参数翻倍，数据量和计算资源也要翻倍，否则模型表现会“卡住”|初期进步快，**后期边际效益递减**（比如从60分提到80分容易，但从95分提到96分很难）。|增加计算能提升效果，但**成本会飙升，且存在上限**（比如从90分提到95分可能需要10倍计算量）|
|特点|1. **模型越大、数据越多、训练时间越长→效果越好**（类似"书读百遍其义自见"）<br>2. **成本极高**：训练GPT-4要烧几十亿美金<br>3. **遇到瓶颈**：现在数据快用完了（相当于人类把全世界的书都读完了）|1. **用少量高质量数据就能大幅提升逻辑推理能力**<br>2. **成本降低**：可能只需要预训练1%的费用 <br>3. **专攻"高难度考试"**：数学、编程等需要复杂思考的领域|1. **创新点**：不改变模型本身，运行时增加思考时间或调用工具<br>2. **性价比超高**：效果提升成本是预训练的千分之一<br>3. **支持"开卷考试"**：遇到难题自动联网搜索资料|
|示例|写诗机器人：只用100首诗训练一个小模型，它可能只会瞎编；但用10万首诗训练一个超大模型，它就能写出李白风格的诗。|聊天机器人：初期不说脏话，后期要幽默又不冒犯人，需要花更多时间微调。|GPT写小说，生成10个版本挑最好的（消耗更多算力），质量会比直接生成一个版本更高|


详见站内专题：[大模型训练之路](llm_train)


## 测试时扩展 TTS



### o1 引出 TTS

OpenAI o1证明: `测试时扩展`（TTS）可通过推理时分配额外算力，大幅增强LLM的推理能力。

**测试时计算**成为当前提升大模型性能的最新范式。

【2024-9-13】[OpenAI震撼发布o1大模型！强化学习突破LLM推理极限](https://mp.weixin.qq.com/s/sGcx90Q_uI8se-DKosj9dw)

2024年9月13日午夜，OpenAI 正式公开一系列全新 AI 大模型，专门解决难题。

新模型可实现复杂推理，一个通用模型解决比此前的科学、代码和数学模型能做到的更难的问题。

o1 模型一举创造了很多历史记录。
- 奥特曼到科学家们一直在「高调宣传」的草莓大模型。它拥有真正的通用推理能力
- 大模型领域**重现**了当年 AlphaGo 强化学习的成功 —— **给越多算力，就输出越多智能，一直到超越人类水平**。
  - 与 GPT-4o 相比，o1 系列模型对于处理代码的智能体系统来说是一个重大进步。
- 回答问题前先仔细思考，而不是立即脱口而出答案。就像人类大脑的`系统 1` 和`系统 2`
- ChatGPT 已经从仅使用`系统 1`（快速、自动、直观、易出错）进化到了`系统 2` 思维（缓慢、深思熟虑、有意识、可靠）。

结果表明：o1 超越了人类专家，成为第一个通过基准测试的模型。
- 国际数学奥林匹克（IMO）资格考试中，GPT-4o 仅正确解答了 **13%** 的问题，而 o1 模型正确解答了 **83%** 的问题。

### 过度思考

【2025-1-3】[揭秘o1类模型的过度思考](https://mp.weixin.qq.com/s/_LGBi1XImFuV2bDg1kbqFQ)

腾讯AI Lab、上海交通大学

o1类超大型语言模型的过度思考:
- 2+3=？答案仅需5个token，o1类模型凭啥要900个？
- 论文：[Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs](https://arxiv.org/abs/2412.21187)

“o1-like” 大型语言模型 通过**延长思考链**（chain-of-thought，CoT），探索**多种**策略，分解复杂步骤，并进行**双重检查**，增强复杂推理任务的处理能力。

这种方法称为“**测试时计算扩展**”，模型推理阶段分配更多计算资源，以期获得更准确的响应。

“o1-like” 大型语言模型（LLMs）推理问题，“overthinking”（**过度思考**）。处理问题时，尤其是简单问题，分配过多计算资源，对提高答案的准确性几乎没有帮助。
- o1-like 模型消耗的token 比常规模型多出**1953%**
- **资源利用效率**：如何智能且高效地在**测试**期间扩展计算资源，尤其是在面对不同复杂度的问题时。
- **评估和优化**模型效率：提出从结果和过程两个角度出发的新效率指标，以评估o1-like模型在计算资源利用上的合理性，并探索了减轻过度思考问题的策略。
- 保持准确性的同时减少计算开销：通过自我训练范式，提出了减少过度思考的方法，这些方法在不牺牲准确性的前提下，简化了推理过程，减少了生成的解决方案数量。

提出相应的效率指标和优化策略，来提高o1-like模型在AI推理任务中的计算资源利用效率，减少不必要的计算开销。

(1) **扩展测试时计算**（Scaling Test-Time Compute）
- 扩展搜索空间：通过增加搜索空间来提供模型发现和选择正确解决方案的机会
  - 例如：**自我一致性方法**（self-consistency）、**最佳-n解码**（best-of-n decoding）、 **加权多数投票**（weighted majority voting）、 **最小贝叶斯风险解码**（minimum bayes risk decoding）
- 扩展类人思考模式：通过模拟人类思考方式来增强模型的推理能力，
  - 例如：**链式思考**（Chain-of-Thought）、**辩论**（debating）、 **自我纠错**（self-correction）/**自我批评**（self-critique）、 **计划-解决**（plan-and-solve）

(2) **高效思考**（Efficient Thinking）
- **终止推理**：鼓励模型在难以解决问题时通过说“**我不知道**”来终止推理。
- **令牌预算感知推理**（Token-budget-aware reasoning）：提示模型在指定的令牌预算内进行推理。
- **计算预算分配**：根据提示的难度预测计算预算分布，并据此分配计算能力。
- **早期停止策略**：在推理过程中采用早期停止策略以节省计算预算。
- **多代理框架**：使用大型LLMs处理复杂任务，小型LLMs处理简单任务。

尽管上述工作考虑了如何提高模型的推理效率，但主要关注传统模型，而不是具有更长思考链（chain-of-thought）的o1-like模型。

本工作首次提出 o1-like 模型中的过度思考问题，并通过**自我训练**方法来训练模型学习如何高效地思考，而不是简单地限制推理空间或由用户指定Token耗费个数。




## TTS 优化


### 测试时训练 TTT

【2024-11-12】[连OpenAI都推不动Scaling Law了？MIT把「测试时训练」系统研究了一遍，发现还有路](https://www.jiqizhixin.com/articles/2024-11-12-7)

OpenAI 下一代旗舰模型的质量提升幅度不及前两款旗舰模型之间的质量提升，因为高质量文本和其他数据的供应量正在减少，原本的 Scaling Law（用更多的数据训练更大的模型）可能无以为继。此外，OpenAI 研究者 Noam Brown 指出，更先进的模型可能在经济上也不具有可行性，因为花费数千亿甚至数万亿美元训练出的模型会很难盈利。

从预训练来看，Scaling Law 可能会放缓；

但有关推理的 Scaling Law 还未被充分挖掘，OpenAI o1 的发布就证明了这一点。它从后训练阶段入手，借助**强化学习**、原生的**思维链**和更长的**推理时间**，把大模型能力又往前推了一步。
- 这种范式被称为「`测试时计算`」，相关方法包括**思维链提示**、**多数投票采样**（self-consistency）、**代码执行**和**搜索**等。

还有个新概念 —— `测试时训练`（ Test-Time Training ，TTT），二者都试图在测试（推理）阶段通过不同的手段来提升模型的性能，但 `TTT` 会根据测试时输入，通过**显式梯度**步骤更新模型。

这种方法不同于标准微调，因为在数据量极低的环境中运行的 —— 通常是通过单个输入的无监督目标，或应用于一个或两个 in-context 标注示例的有监督目标。


`测试时训练`（Test-Time Training，TTT）范式提供了一种有效的解决方案。
- TTT 利用**模型本身参数**来存储隐藏状态、记忆上文；
- 并在每一步推理中，对模型参数进行**梯度更新**，已实现上文的不断循环流入

这个过程不同于传统的机器学习范式中模型在完成训练后的推理阶段通常保持静态的方式，TTT 在**推理**阶段会针对每一条测试数据一边循环训练一边推理

TTT 范式的预训练阶段，训练过程包含**内部循环**以及**外部循环**两个部分。
- 外部循环遵循传统的下词预测任务，通过**自回归**方式优化模型全局权重参数。
- 内部循环则是基于**自监督**方式来优化隐藏状态。

模型需要在每个时间步动态地更新隐藏状态，使其能够不断适应新的输入数据。这种动态更新的机制类似于一个独立的机器学习模型在每个时间步对输入进行训练和优化

与Transformer 相比，基于TTT 范式的模型具有**线性时间复杂度**，这对于处理长序列数据至关重要。
- 相较于基于SSM 的RWKV 和Mamba 架构，TTT 通过**模型参数**来保存上下文信息，能够更有效地捕捉超长上下文中的语义联系和结构信息。

因此，TTT 在长上下文建模任务中展现出卓越的性能，特别是在需要处理超长上下文的应用场景中。

未来，TTT 范式有望在超长序列处理任务中发挥重要作用。

ttt 替代自注意力层
- 论文标题：[The Surprising Effectiveness of Test-Time Training for Abstract Reasoning](https://ekinakyurek.github.io/papers/ttt.pdf)

将 TTT 有效应用于 few-shot 学习的几个关键要素：
- 在与测试时类似的**合成任务**上进行初始微调；
- 用于构建测试时数据集的增强型 leave-1-out 任务生成策略；
- 训练适用于每个实例的适应器；
- 可逆变换下的自我一致性（self-consistency）方法。

两种不同的 TTT 数据生成方式：
- 一是 in-context learning（ICL）格式；从给定的测试演示中创建 leave-1-out 任务
- 另一种是端到端格式。将每个 i/o 对视为一个单独的任务

实验环节，研究者在抽象与推理语料库（ARC,抽象与推理语料库）中对这些方法进行了评估。ARC 语料库收集了很多极具挑战性的 few-shot 视觉推理问题，被认为是测试 LM 泛化极限的理想基准。目前的大多语言模型在 ARC 上均表现不佳。

TTT 可以显著提高 LM 在 ARC 上的性能 —— 在 1B 模型上将准确率提高到原来的 6 倍，使用 8B 模型时也超过其它已发布的 SOTA 纯神经模型方法。

详见站内: [transformer 专题](transformer#ttt)


### 【2025-2-10】清华: 最优TTS

【2025-2-10】[清华一作1B暴打405B巨无霸，7B逆袭DeepSeek R1！测试Scaling封神](https://www.toutiao.com/article/7470409302235169295)

"以算力换性能"的**测试时拓展**（Test Time Scaling TTS）技术迎来革命性突破
- 仅凭**测试时Scaling**，1B模型竟完胜405B

多机构联手巧妙应用计算最优TTS策略，不仅0.5B模型在数学任务上碾压GPT-4o，7B模型更是力压o1、DeepSeek R1这样的顶尖选手。

测试时计算，也成为了当前提升大模型性能的最新范式。

问题：
- 不同策略模型、过程奖励模型和问题难度级别下，如何最优地扩展测试时计算？
- 扩展计算在多大程度上可以提高大语言模型在复杂任务上的表现，较小的语言模型能否通过这种方法实现对大型模型的超越？

上海AI实验室、清华、哈工大、北邮等研究人员发现，使用计算最优TTS策略，**极小策略模型也可以超越更大的模型**——
- MATH-500和AIME24上，0.5B模型的表现优于`GPT-4o`；3B模型超越了405B模型；7B模型直接胜过o1和`DeepSeek-R1`，还具有更高的推理性能。
上海AI Lab 联合 清华、哈工大等机构，通过计算最优TTS策略
- MATH-500 基准上，**1B**参数的"小模型"竟能在数学推理任务上完胜**405B**参数的"巨无霸"模型
- 在 MATH-500 和 AIME24 基准中
  - **0.5B**模型表现碾压`GPT-4o`
  - **7B**模型直接击败业界顶尖的`o1`和`DeepSeek-R1`。
- 【2025-2-10】论文地址：[Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling](https://arxiv.org/pdf/2502.06703)
- 项目链接：[compute-optimal-tts](https://ryanliu112.github.io/compute-optimal-tts)

TTS是增强LLM推理能力的一种极有前途的方法。

同时，这也体现了研究真正的「弱到强」方法，而非当前的「强到弱」监督，对策略优化的重要性。

重新思考「计算最优」的测试时Scaling
- 计算最优扩展策略应是**奖励感知**的
- 绝对问题难度标准比分位数更有效

计算最优的**测试时Scaling**，为每个问题分配最优计算资源。
- ① 单一PRM作为验证器, 在策略模型的响应上训练PRM并将其用作验证器，以对同一策略模型进行TTS；
- ② 用不同策略模型上训练的PRM来进行TTS。

从强化学习（RL）的角度来看，① 获得在线PRM，② 离线PRM。
- 在线PRM能为策略模型的响应产生更准确的奖励
- 而离线PRM由于分布外（OOD）问题往往会产生不准确的奖励。

对于计算最优TTS的实际应用而言，为每个策略模型训练一个用于防止OOD问题的PRM在计算上是昂贵的。

因此，研究人员在更一般的设置下研究计算最优的TTS策略，即PRM可能是在与用于TTS的策略模型不同的模型上训练的。
- 对于基于搜索的方法，PRM指导每个响应步骤的选择
- 而对于基于采样的方法，PRM在生成后评估响应。

这表明：
- （1）奖励影响所有方法的响应选择；
- （2）对于基于搜索的方法，奖励还会影响搜索过程。

团队使用Llama-3.1-8BInstruct作为策略模型，RLHFlow-PRM-Mistral-8B和RLHFlow-PRM-Deepseek-8B作为PRM，进行了一项初步的案例研究。

如何最优地Scaling测试时计算？
- Q1：如何通过不同的策略模型和PRM来提升TTS？
- Q2：TTS在不同难度问题上的改进情况如何？
- Q3：偏好奖励模型PRM是否对特定响应长度存在偏差或对投票方法敏感？
  - PRM对步骤长度存在偏差
  - PRM对投票方法具有敏感性
- Q4：较小的策略模型，能否在计算最优TTS策略下优于较大的模型？
  - 小模型可以通过计算最优TTS策略，也能一举超越GPT级别的大模型。
- Q5：计算最优TTS与CoT和多数投票相比有何改进？
  - 计算最优TTS显著增强了LLM的推理能力。但随着策略模型参数数量的增加，TTS的改进效果逐渐减小。
- Q6：TTS是否比基于长CoT的方法更有效？
  - TTS比直接在MCTS生成数据上，应用RL或SFT的方法更有效，但不如从强大的推理模型中进行蒸馏的方法有效。
  - TTS在较简单的任务上，比在更复杂的任务上更有效。

详见原文

# 结束

