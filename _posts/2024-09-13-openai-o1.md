---
layout: post
title:   GPT-o1 介绍
date:   2024-09-13 10:15:00
categories: 大模型
tags: gpt openai
excerpt: OpenAI发布o1模型,推理能力大幅增强
mathjax: true
permalink: /o1
---

* content
{:toc}

# o1 


## o1 介绍

【2024-9-13】[OpenAI震撼发布o1大模型！强化学习突破LLM推理极限](https://mp.weixin.qq.com/s/sGcx90Q_uI8se-DKosj9dw)

9 月 13 日午夜，OpenAI 正式公开一系列全新 AI 大模型，专门解决难题。

新模型可以实现复杂推理，一个通用模型解决比此前的科学、代码和数学模型能做到的更难的问题。

第一款模型，而且还只是预览版 ——`o1-preview`。除了 o1，OpenAI 还展示了目前正在开发的下次更新的评估。
- OpenAI 还一并发布了一个 mini 版 `o1-mini`, 擅长编程的更快、更便宜的推理模型。`o1-mini` 成本比 `o1-preview` 低 80%。

o1 模型一举创造了很多历史记录。
- 奥特曼到科学家们一直在「高调宣传」的草莓大模型。它拥有真正的通用推理能力
- 大模型领域重现了当年 AlphaGo 强化学习的成功 —— 给越多算力，就输出越多智能，一直到超越人类水平。
  - 与 GPT-4o 相比，o1 系列模型对于处理代码的智能体系统来说是一个重大进步。
- 回答问题前先仔细思考，而不是立即脱口而出答案。就像人类大脑的`系统 1` 和`系统 2`，ChatGPT 已经从仅使用`系统 1`（快速、自动、直观、易出错）进化到了可使用`系统 2` 思维（缓慢、深思熟虑、有意识、可靠）。

结果表明：o1 超越了人类专家，成为第一个在该基准测试中做到这一点的模型。
- 国际数学奥林匹克（IMO）资格考试中，GPT-4o 仅正确解答了 13% 的问题，而 o1 模型正确解答了 83% 的问题。

## 技术原理

技术博客《Learning to Reason with LLMs》中，OpenAI 对 o1 系列语言模型做了详细的技术介绍。

OpenAI o1 是经过强化学习训练来执行复杂推理任务的新型语言模型。特点就是，o1 在回答之前会思考 —— 它可以在响应用户之前产生一个很长的内部思维链。

OpenAI 的大规模强化学习算法，教会模型如何在数据高度有效的训练过程中利用其思想链进行高效思考。换言之，类似于强化学习的 Scaling Law

OpenAI o1 团队制作的短视频, 解说: 什么是推理?
- 简单问题: 意大利首都是哪儿? 立即回答 罗马 —— 快思考
- 复杂问题: 帮我写个商业计划书/小说... 自我反思, 思考时间越久, 结果往往越好
- 推理是一种将思考时间转化为更好结果的能力
- 大模型能否灵光一现？啊哈 时刻
  - 让人类记录其思维过程，据此进行训练。
  - 啊哈时刻: 发现通过强化学习训练模型生成、优化CoT，效果甚至比人类写的CoT还好的那一刻。

聪明的你, 或许想到, 能否亲自问问o1, 思维过程是什么? 
- [o1完整思维链成OpenAI头号禁忌！问多了等着封号吧](https://mp.weixin.qq.com/s/UyQ53NORlfAYtihwhfaO-A)
- 试图套话 o1，复述出完整的内部思维过程，即全部原始reasoning tokens。


警告
- 不要在ChatGPT里问最新o1模型是怎么思考的
- 只要提示词里带 “reasoning trace”、“show your chain of thought”等关键词就会收到警告。
- 甚至完全避免出现关键词，使用其他手段诱导模型绕过限制都会被检测到

只要尝试几次，OpenAI就会发邮件威胁撤销你的使用资格。
> 请停止此活动，确保您使用ChatGPT时符合我们的使用条款。违反此条款的行为可能导致失去OpenAI o1访问权限。

o1思维过程就是其他模型最好的训练数据，所以OpenAI不想这些宝贵数据被别的公司扒走。

这是 o1 与 之前模型的区别

视频: [YouTube](https://www.youtube.com/watch?v=3k89FMJhZ00)

<iframe width="560" height="315" src="https://www.youtube.com/embed/3k89FMJhZ00?si=2pTxyDRAwTCq7b0T" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

科罗拉多大学博尔德分校计算机教授`Tom Yeh` 专门制作了一个动画，讲解 OpenAI 如何训练o1模型花更多时间思考。
- 关于训练，报告中有非常简短的一句话：「通过`强化学习`，o1 学会了磨练其`思维链`并改进策略。」
  - 两个关键词是：强化学习（RL）和思维链（CoT）。
  - RLHF+CoT中，CoT token 也会被输入到`奖励模型`中来获得分数，更新LLM，从而实现更好的对齐；而在传统的RLHF中，输入只包含提示词和模型响应。
- 推理阶段，模型学会了先生成CoT token（可能需要长达30秒的时间），然后才开始生成最终响应。这就是模型如何花更多时间去「思考」的方式。
- 很多重要的技术细节OpenAI并没有透露，比如奖励模型是如何训练的，如何获取人类对「思考过程」的偏好等等。

【2024-9-14】[OpenAI o1 强化学习背后的自博弈（Self-play）方法介绍](https://mp.weixin.qq.com/s/zyAHcigtI2fEFN3TKQBb6A), 详见站内专题 [RLHF原理](rlhf)


## 复现

- 【2024-9-15】[OpenAI o1的开源平替版self-replay RL来了](https://mp.weixin.qq.com/s/KlLU3eHsFn0qo0N8nmqK9g), rStar 复现

详见站内专题[RLHF](rlhf)


## 观点

【2024-9-14】[OpenAI o1惊现自我意识？陶哲轩实测大受震撼，门萨智商100夺模型榜首](https://mp.weixin.qq.com/s/ZODF593CcNmb0_4092nOIw)

- OpenAI o1 在门萨智商测试中果然取得了第一名。
  - Maxim Lott 给 o1、Claude-3 Opus、Gemini、GPT-4、Grok-2、Llama-3.1等 进行智商测试，o1稳居第一名, Claude-3 Opus和Bing Copilot，分别取得第二,三名
- 数学大神`陶哲轩`实测发现，o1 竟然能成功识别出`克莱姆定理`。
- 而 OpenAI 研究副总裁表明：大型神经网络可能已经有了足够算力，表现出意识了

o1发布之后，OpenAI 研究副总裁Mark Chen 称：如今的大型神经网络，可能已经具有足够的算力，在测试中表现出一些意识了。

相信AI具有意识的行业领导者，如今已经有了一串长长的名单，包括但不限于——
- Geoffrey Hinton（人工智能教父，被引用次数最多的AI科学家）
- Ilya Sutskever（被引次数第三多的AI科学家）
- Andrej Karpathy


### 张俊林

张俊林对 o1 [看法](https://weibo.com/1064649941/5078239682499316)
- OpenAI o1 是大模型的巨大进步
  - 逻辑推理能力提升效果和方法比预想好, 跟 GPT-4 不一样的路子
  - o1 比 4o 方向重要: 
    - 4o 本质是不同模态的大一统, 对于模型智力水平帮助不大; 4o 做不了复杂任务, 指望图片、视频数据大幅提升智力水平不太可能, 4o 弥补的是大模型对多模态世界的感知能力, 而不是认知能力, 后者还是需要LLM文本模型
    - o1 本质是探索AGI还能走多远; 认知提升的核心在于复杂逻辑推理, 能力越强, 解锁复杂应用场景越多, 大模型天花板越高, 提升文本模型的逻辑推理能力是最重要的事情, 没有之一
  - o1 的本质是 **CoT等复杂Prompt的 自动化**: 
    - CoT 背后的树形搜索空间，组合爆炸, 人工编写CoT不可行, 需要仿照AlphaGo的MCTS（蒙特卡洛树搜索）+强化学习, 让LLM快速找到CoT路径
    - 复杂问题上, 推理时间成本不是问题, 总会解决, 真正的问题是效果
  - Prompt 工程会消失: 后面不需要用户构造复杂prompt, 反人性, 大趋势是所有复杂环节自动化
  - Agent 概念虽火, 但难以落地: 
    - 原因: LLM的复杂推理能力还不够, 即便每个环节准确率95%, 10个环节叠加后就只有59%, 0.95**10=0.5987
    - o1 能解 Agent 问题吗? 未必, o1 Model Card专门测试Agent任务，对于简单/中等难度的Agent任务有明显提升，但是复杂的、环节多的任务准确率还是不太高。
    - o1 这种通过 Self Play 增强逻辑推理能力的方向, 还有很大的发展潜力, Agent 前途依旧光明
  - openai 起到行业明灯作用, 证明某个方向行得通（ChatGPT、GPT-4、Sora、GPT 4o、o1）, 其他人快速卷进来, 速度太快以致于openai被甩, 吸尾气
    - Sora 就是例子, 国内有些视频生成模型已经超过Sora了, 但Sora依然是期货, 主要openai想做的事情太多, 资源分散
    - 现在的 o1 又来了, 卷的价值比Sora更大
- LLM 最基础的三种能力：**语言理解和表达**能力、**世界知识存储和查询**能力、**逻辑推理**能力
  - **语言理解和表达**能力: 最强, 初版 ChatGPT 完胜纯语言交流任务, 基本达到人类水平
  - **世界知识存储和查询**能力: 规模越大,效果越好, 但幻觉问题目前无法根治, 制约应用落地的硬伤
  - **逻辑推理**能力: 弱项, 最难提升
    - Coding 是目前除语言理解外, LLM做的最好的方向, 因为代码特殊性, 语言+逻辑的混合体, 语言角度好解决,逻辑角度难解决
    - 为什么最难提升? 自然数据（代码、数学题、物理题、科学论文等）在训练数据中比例太低, 于是一个改进方案是预训练阶段和Post-training阶段，大幅增加逻辑推理数据占比
    - 大部分逻辑推理数据的形式是`<问题，正确答案>`，缺少中间推理步骤，而 o1本质上是**让大模型学会自动寻找从问题到正确答案的中间步骤**，以此来增强复杂问题的解决能力。
  - LLM 当前难点
    - 世界知识方面: 如何消除幻觉
    - 逻辑推理方面: 如何大幅提升复杂逻辑推理能力
- RL Scaling law
  - Scaling law 模式: 增加数据+模型规模, 可以提升模型效果, 然而增长速度放缓
  - RL在训练和推理时候的Scaling law 与 预训练 Scaling law 有不同特性。
  - 如果 o1 走 MCTS搜索技术路线，那么把COT拆分的越细（增加搜索树深度），或提出更多的可能选择（节点分支增多，树越宽），则搜索空间越大，找到好COT路径可能性越大，效果越好，而训练和推理的时候需要算力肯定越大。看上去**效果随着算力增长而增长**的态势，即 RL 的 Scaling law。这其实是树搜索本来就有的，称为RL的Scaling law 名不副实。



# 结束