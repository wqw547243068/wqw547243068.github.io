---
layout: post
title:   大模型推理思考
date:   2024-09-13 10:15:00
categories: 大模型
tags: gpt openai deepseek kimi r1 李飞飞 蒸馏
excerpt: 大模型推理能力专题，包含openai o系列、deepseek r1等长程思考模型
mathjax: true
permalink: /o1
---

* content
{:toc}

# 大模型推理思考 


## OpenAI 推理模型


### 【2024-9-13】o1

【2024-9-13】[OpenAI震撼发布o1大模型！强化学习突破LLM推理极限](https://mp.weixin.qq.com/s/sGcx90Q_uI8se-DKosj9dw)

2024年9月13日午夜，OpenAI 正式公开一系列全新 AI 大模型，专门解决难题。

新模型可实现复杂推理，一个通用模型解决比此前的科学、代码和数学模型能做到的更难的问题。

第一款模型，而且还只是预览版 ——`o1-preview`。除了 o1，OpenAI 还展示了目前正在开发的下次更新的评估。
- OpenAI 还发布 mini 版 `o1-mini`, 擅长编程的更快、更便宜的推理模型。`o1-mini` 成本比 `o1-preview` 低 80%。

o1 模型一举创造了很多历史记录。
- 奥特曼到科学家们一直在「高调宣传」的草莓大模型。它拥有真正的通用推理能力
- 大模型领域**重现**了当年 AlphaGo 强化学习的成功 —— **给越多算力，就输出越多智能，一直到超越人类水平**。
  - 与 GPT-4o 相比，o1 系列模型对于处理代码的智能体系统来说是一个重大进步。
- 回答问题前先仔细思考，而不是立即脱口而出答案。就像人类大脑的`系统 1` 和`系统 2`
- ChatGPT 已经从仅使用`系统 1`（快速、自动、直观、易出错）进化到了`系统 2` 思维（缓慢、深思熟虑、有意识、可靠）。

结果表明：o1 超越了人类专家，成为第一个通过基准测试的模型。
- 国际数学奥林匹克（IMO）资格考试中，GPT-4o 仅正确解答了 **13%** 的问题，而 o1 模型正确解答了 **83%** 的问题。


OpenAI 不在界面中显示**思维链**，而是显示小结。
- 部分原因是担心所谓的“**蒸馏**风险”，因为有人可能会尝试模仿这些推理痕迹，并通过模仿思维链来恢复大量的推理性能

#### o1 意义

OpenAI o1 是大模型技术领域的一个**巨大突破**，除了**复杂逻辑推理能力**获得极大提升外，还有：
- (1) o1 给大模型带来了**自我反思与错误修正**能力
  - GPT-4 逐字输出token, 句子较长时, 难免出现幻觉, 中间 token 有误,但模型**无法纠正前面的错误**, 还是将错就错
  - o1 的思考体现在 生成 **hidden COT** 过程中, 能**发现并纠正**之前的错误, 这对**长链思考**及**复杂任务**非常重要
- (2) 新型 RL Scaling law
- (3) o1之后，`小模型`大行其道真正成为可能
- (4) o1可能会引发“安全对齐”新的范式: 安全能力比GPT 4o强很多
  - 大概用了类似 Anthropic 的“AI宪法”的思路，给定一些安全守则，指明哪些行为能做，哪些不能做
  - 可能引发安全对齐新模式：先把模型的逻辑推理能力加强，然后采取类似“AI宪法”思路
- (5) “强化学习+LLM”的领域泛化能力，可能不局限于理科领域
  - 强化学习适合解决 **Reward比较明确**的复杂问题，典型: 数理化、Coding等有标准答案的学科，所以很多人会质疑o1是否能泛化到更宽的领域
  - OpenAI可能已经找到了一些非数理学科的Reward定义方法，并将这个方法通过RL拓展到更多领域。

o1 RL 大概率用了
- 相对复杂的、类似AlphaGo的MCTS树搜索
- 或 简单树结构拓展，比 如生成多个候选，从中选择最好的（Best-of-N Sampling），这种策略如果连续用，其实也是一种简单的树搜索结构。
- 也有可能两者一起用。

不论怎样，树搜索结构大概率是用了，COT是线性的不假，但这是产出结果，不代表内部思考过程就一定是线性的，靠线性思维推导过程很难解决复杂问题，树形结构几乎是不可避免的。

#### SLM

尽管小模型**语言**能力强、**世界知识**还可以，但**逻辑推理**能力很难提起来，即使通过蒸馏等措施试图把逻辑能力内化到小模型的参数里，效果有但有限

`小模型`和`大模型`差距最大的就是**逻辑推理**能力。
- 纯靠**参数内化**来提升小模型的逻辑推理能力估计提升幅度有限。
- 但 o1 mini 明显是个小模型，其复杂逻辑推理能力非常强，而且看样子可通过配置来提升或者降低它的逻辑推理能力（所谓inference-time Scaling law），如果了解AlphaGo的运作机制的话，会发现这都是比较典型的搜索树的特点，可以通过控制搜索空间大小来提升模型能力。

逻辑推理能力锁定了`小模型`上限。

小模型的能力特点：
- **语言**能力很强不比大模型弱
- **世界知识**不如大模型，但是可以通过给更多数据持续提升
- 受限于模型规模，**逻辑推理**能力能提升但比较困难。
- ![](https://pica.zhimg.com/80/v2-3c3cefb0f2f574642e42b420d891153c_1440w.webp)

小模型的优化重点: **世界知识**和**逻辑推理**能力

而 o1 mini 效果（世界知识弱、逻辑推理强），之后可采用“能力分治”（DCA，Divide-and-Conquer of Ability）模式推进小模型的技术发展
- 把**语言**、**世界知识**及**逻辑推理**三个能力解耦
- 语言能力靠小模型自身
- 逻辑推理靠类似o1的通过RL获得的深度思考能力
- 而世界知识可以靠外挂RAG获得增强

通过“能力分治”，小模型完全可能具备目前最强大模型的能力，这等于真正为小模型扫清了前进路上的障碍，而 SLM 做起来成本又比较低，很多人和机构都可以做这事，这种 DCA模式 将会大行其道，形成一种新的研发小模型的范式。


### 【2024-12-23】o3

【2024-12-23】[最强推理模型o3来了！OpenAI副总裁不慎透露秘密被Altman“闭麦”](https://www.thepaper.cn/newsDetail_forward_29715938)

2024年12月23日，OpenAI 发布推理模型o3，包含两个模型，即`o3`和`o3-mini`
- 前者是高性能推理模型，后者是更小的精简版模型，在保持智能的同时优化性能和成本。

OpenAI又基于o3，训了3个小尺寸的o3模型, `o3-mini` 将于次年1月公布

o3-mini 支持低、中、高三种推理时间模式，用户可根据任务复杂度灵活调整思考时间。

o3模型在多个维度上展现非凡。
- 在软件基准测试SWE-bench Verified中，o3以71.7%的准确率傲视群雄，较其前辈o1模型性能提升超20%。
- 在编程竞技领域，o3于Codeforces竞赛中的评分高达2727分，直逼OpenAI内部顶尖程序员的水平。更令人瞩目的是，在AIME数学竞赛模拟中，o3模型的准确率达到了惊人的96.7%，远超o1的83.3%。

o3模型在ARC-AGI测试中取得了历史性突破，首次跨越人类水平门槛（85%），以87.5%的优异成绩，标志着OpenAI在通往实现人工通用智能（AGI）的征途中又迈出了坚实的一步。

openai
> o3已经接近AGI。
> CEO萨姆·奥尔特曼（Sam Altman）称o3是“一个非常、非常聪明的模型”。

### 【2025-1-31】o3-mini

2025年1月31日，OpenAI 终于发布了 o3-mini

o3-mini 定位：资源受限场景，在极高难度任务上的表现稍逊o3，但仍保留了强大的推理能力，尤其在基础数学问题、日常编程和一般推理任务上表现突出。

o3-mini 定价 1.10 美元/百万输入 token，4.40 美元/每百万输出 token。
- 价格比 OpenAI o1-mini 便宜了 63%，比完全体 o1 便宜 93%，可谓是“骨折价”。
- 当然，定价还是比 DeepSeek 贵了很多（0.14 美元/百万输入 token，0.55 美元/百万输出 token）。

o3-mini 创新地提供了低、中、高三种不同级别的“**推理强度选项**”，用户可根据具体任务灵活调整速度和准确度之间的平衡。

即便是在最低推理级别下，o3-mini 在数学和编程基准测试中的表现也能与 o1-mini 相媲美。而当设置为最高推理级别时，其表现甚至能够超越功能更全面的 o1 模型。

与 o1-mini 相比，o3-mini 将重大错误率降低了 39％，其回答的受欢迎程度提高了 56％。

中等推理级别下，o3-mini 的平均响应时间也从 o1-mini 的 **10.16** 秒缩短到了 **7.7** 秒，提速达 24％。

不足
- o3-mini 不支持视觉功能，因此只能继续使用 o1 进行视觉推理任务。

参考
- [openai-o3-mini](https://openai.com/index/openai-o3-mini/)
- [o3-mini-system-card](https://cdn.openai.com/o3-mini-system-card.pdf)
- 【2025-2-1】[OpenAI o3-mini发布，被DeepSeek逼入“价格战”，免费用户也能尝鲜](https://news.qq.com/rain/a/20250201A03YU800)

#### 【2025-2-7】 o3-mini 思维链开放

2月7日, OpenAI 更新 o3-mini 模型思维链展示方式，提高 AI 推理透明度

OpenAI 宣布公开`o3-mini`模型的**推理思维链**，免费和付费用户可查看其思维过程。

OpenAI为付费用户更新o3-mini-high的思维链，更透明、更详细地展示模型的“推理”步骤以及得出答案的方式。

部分人质疑公开的思维链是否为**原始数据**，因为展示速度较慢且字符数量与原始版本存在差异。OpenAI发言人确认公开的思维链经过后处理，消除不安全内容、简化复杂想法，为非英语用户提供更好的体验。



## 技术原理

“强化学习生成Hidden COT”


### 教程

【2025-1-4】[吴恩达o1推理新课程](https://mp.weixin.qq.com/s/5iS_846hoVYtwe2pg-61TQ)

Reasoning with o1，讲师是 OpenAI 战略解决方案架构主管 Colin Jarvis。
- 课程地址：[Reasoning with o1](https://www.deeplearning.ai/short-courses/reasoning-with-o1)

Reasoning with o1 课程内容主要包括：
- o1 即时工程的基础知识
- 规划和执行多步骤任务
- 创建和编辑代码
- 图像推理
- 可提高模型性能的 Metaprompting


### OpenAI

技术博客《Learning to Reason with LLMs》中，OpenAI 对 o1 系列语言模型做了详细的技术介绍。

OpenAI o1 是经过**强化学习**训练来执行复杂推理任务的新型语言模型。
- 特点: o1 在回答之前会思考 —— 它可以在响应用户之前产生一个很长的内部思维链。
- “强化学习生成Hidden COT”

OpenAI 大规模强化学习算法，教会模型如何在数据高度有效的训练过程中利用其思想链进行高效思考。换言之，类似于强化学习的 `Scaling Law`

OpenAI o1 团队制作的短视频, 解说: 什么是推理?
- **简单**问题: 意大利首都是哪儿? 立即回答 罗马 —— 快思考
- **复杂**问题: 帮我写个商业计划书/小说... 自我反思, 思考时间越久, 结果往往越好
- 推理是一种将思考时间转化为更好结果的能力
- 大模型能否灵光一现？**啊哈** 时刻
  - 让人类记录其思维过程，据此进行训练。
  - 啊哈时刻: 发现通过强化学习训练模型生成、优化CoT，效果甚至比人类写的CoT还好的那一刻。

聪明的你, 或许想到, 能否亲自问问o1, 思维过程是什么? 
- [o1完整思维链成OpenAI头号禁忌！问多了等着封号吧](https://mp.weixin.qq.com/s/UyQ53NORlfAYtihwhfaO-A)
- 试图套话 o1，复述出完整的内部思维过程，即全部原始reasoning tokens。


警告
- 不要在ChatGPT里问最新o1模型是怎么思考的
- 只要提示词里带 “reasoning trace”、“show your chain of thought”等关键词就会收到警告。
- 甚至完全避免出现关键词，使用其他手段诱导模型绕过限制都会被检测到

只要尝试几次，OpenAI就会发邮件威胁撤销你的使用资格。
> 请停止此活动，确保您使用ChatGPT时符合我们的使用条款。违反此条款的行为可能导致失去OpenAI o1访问权限。

o1思维过程就是其他模型最好的训练数据，所以OpenAI不想这些宝贵数据被别的公司扒走。

这是 o1 与 之前模型的区别

### Tom Yeh

视频: [YouTube](https://www.youtube.com/watch?v=3k89FMJhZ00)

<iframe width="560" height="315" src="https://www.youtube.com/embed/3k89FMJhZ00?si=2pTxyDRAwTCq7b0T" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

科罗拉多大学博尔德分校计算机教授`Tom Yeh` 专门制作了一个动画，讲解 OpenAI 如何训练o1模型花更多时间思考。
- 关于训练，报告中有非常简短的一句话：「通过`强化学习`，o1 学会了磨练其`思维链`并改进策略。」
  - 两个关键词是：强化学习（RL）和思维链（CoT）。
  - RLHF+CoT中，CoT token 也会被输入到`奖励模型`中来获得分数，更新LLM，从而实现更好的对齐；而在传统的RLHF中，输入只包含提示词和模型响应。
- 推理阶段，模型学会了先生成CoT token（可能需要长达30秒的时间），然后才开始生成最终响应。这就是模型如何花更多时间去「思考」的方式。
- 很多重要的技术细节OpenAI并没有透露，比如奖励模型是如何训练的，如何获取人类对「思考过程」的偏好等等。

### Self-Play

【2024-9-14】[OpenAI o1 强化学习背后的自博弈（Self-play）方法介绍](https://mp.weixin.qq.com/s/zyAHcigtI2fEFN3TKQBb6A), 详见站内专题 [RLHF原理](rlhf)


### 张俊林

【2024-9-28】[Reverse-o1:OpenAI o1原理逆向工程图解](https://zhuanlan.zhihu.com/p/721952915)

关于Q*、草莓等各种传闻很久了，用**强化学习增强逻辑推理能力**这个大方向八九不离十，但是,**融合LLM和RL来生成Hidden COT**，估计很少人能想到这点，目前看效果确实挺好。

#### o1 训练过程

OpenAI o1 完整训练过程推演
- ![](https://pic3.zhimg.com/80/v2-9fb64105589b35f9a877702acf990746_1440w.webp)

GPT 4 等LLM模型训练一般由“预训练”和“后训练”两个阶段组成。
- “预训练” 通过 Next Token Prediction 来从海量数据吸收语言、世界知识、逻辑推理、代码等基础能力，模型规模越大、训练数据量越多，则模型能力越强，Scaling Law 指这一阶段的模型扩展特性，也是LLM训练最消耗算力资源的地方。
- “后训练” 则分为 SFT、RM 和 PPO 三个过程，统称人工反馈的强化学习（RLHF），这一阶段主要目的有两个: LLM遵循指令做各种任务，内容安全，不让LLM输出不礼貌的内容。而训练好的模型推理（Inference）过程则是对于用户的问题直接逐个生成Token来形成答案。

o1 整个训练和推理过程应与 GPT 4 这类典型LLM有较大区别。
- 首先，“预训练”阶段应该是重新训练的，不太可能是在GPT 4o上通过继续预训练得到。
  - OpenAI官方一再宣称 o1 mini 逻辑推理能力极强，但在世界知识方面很弱。如果是在其它模型上魔改的，世界知识不会比GPT 4o mini更弱，所以侧面说明了是重新训练的；
  - 另外，这也说明了o1这类侧重逻辑推理的模型，在预训练阶段的数据配比方面，应该极大增加了逻辑类训练数据比如STEM数据、代码、论文等的比例，甚至都怀疑o1 mini是否引入了通用数据都不好说，否则不需要老强调知识方面能力弱。
- 在“后训练”阶段，有个环节是用来增强LLM模型的指令遵循能力的，即有RLHF阶段。
  - 因为o1在遵循指令方面能力并不弱，而且生成的Hidden COT片段里明显也包含很多指令性的内容，如果遵循指令能力比较弱，估计对于生成Hidden COT也有负面影响。所以，推断起来这个环节大概在“思考”阶段之前。（但是RLHF阶段未必有RM和PPO）。
  - 但这和GPT 4对应的RLHF阶段应有两个重要的不同：
    - 首先，o1应该在这个阶段没有做内容安全方面的事情，大概率是挪到后面的阶段了。
    - 其次，这个阶段大概率也会极大增强逻辑推理类的指令遵循数据比例，以此进一步加强基座模型的逻辑推理能力。
- 接下来是o1最大的特点，所谓引入了“系统2”的慢思考能力。
  - ClosedAI只说用了RL强化学习，其它任何都没提，技术保密工作一流。由此，只能推断出o1融合了LLM和RL来实现模型“先想后说”的Think能力。
  - OpenAI o1应把“内容安全”相关的能力挪到了“Think”阶段之后，而且做法和GPT 4应该也有很大不同。

详见原文 [Reverse-o1:OpenAI o1原理逆向工程图解](https://zhuanlan.zhihu.com/p/721952915)


## 思考

【2024-9-14】[OpenAI o1惊现自我意识？陶哲轩实测大受震撼，门萨智商100夺模型榜首](https://mp.weixin.qq.com/s/ZODF593CcNmb0_4092nOIw)

- OpenAI o1 在门萨智商测试中果然取得了第一名。
  - Maxim Lott 给 o1、Claude-3 Opus、Gemini、GPT-4、Grok-2、Llama-3.1等 进行智商测试，o1稳居第一名, Claude-3 Opus和Bing Copilot，分别取得第二,三名
- 数学大神`陶哲轩`实测发现，o1 竟然能成功识别出`克莱姆定理`。
- 而 OpenAI 研究副总裁表明：大型神经网络可能已经有了足够算力，表现出意识了

o1发布之后，OpenAI 研究副总裁Mark Chen 称：如今的大型神经网络，可能已经具有足够的算力，在测试中表现出一些意识了。

相信AI具有意识的行业领导者，如今已经有了一串长长的名单，包括但不限于——
- Geoffrey Hinton（人工智能教父，被引用次数最多的AI科学家）
- Ilya Sutskever（被引次数第三多的AI科学家）
- Andrej Karpathy


### 张俊林

张俊林对 o1 [看法](https://weibo.com/1064649941/5078239682499316)
- OpenAI o1 是大模型的巨大进步
  - 逻辑推理能力提升效果和方法比预想好, 跟 GPT-4 不一样的路子
  - o1 比 4o 方向重要: 
    - 4o 本质是不同模态的大一统, 对于模型智力水平帮助不大; 4o 做不了复杂任务, 指望图片、视频数据大幅提升智力水平不太可能, 4o 弥补的是大模型对多模态世界的感知能力, 而不是认知能力, 后者还是需要LLM文本模型
    - o1 本质是探索AGI还能走多远; 认知提升的核心在于复杂逻辑推理, 能力越强, 解锁复杂应用场景越多, 大模型天花板越高, 提升文本模型的逻辑推理能力是最重要的事情, 没有之一
  - o1 的本质是 **CoT等复杂Prompt的 自动化**: 
    - CoT 背后的树形搜索空间，组合爆炸, 人工编写CoT不可行, 需要仿照AlphaGo的MCTS（蒙特卡洛树搜索）+强化学习, 让LLM快速找到CoT路径
    - 复杂问题上, 推理时间成本不是问题, 总会解决, 真正的问题是效果
  - Prompt 工程会消失: 后面不需要用户构造复杂prompt, 反人性, 大趋势是所有复杂环节自动化
  - Agent 概念虽火, 但难以落地: 
    - 原因: LLM的复杂推理能力还不够, 即便每个环节准确率95%, 10个环节叠加后就只有59%, 0.95**10=0.5987
    - o1 能解 Agent 问题吗? 未必, o1 Model Card专门测试Agent任务，对于简单/中等难度的Agent任务有明显提升，但是复杂的、环节多的任务准确率还是不太高。
    - o1 这种通过 Self Play 增强逻辑推理能力的方向, 还有很大的发展潜力, Agent 前途依旧光明
  - openai 起到行业明灯作用, 证明某个方向行得通（ChatGPT、GPT-4、Sora、GPT 4o、o1）, 其他人快速卷进来, 速度太快以致于openai被甩, 吸尾气
    - Sora 就是例子, 国内有些视频生成模型已经超过Sora了, 但Sora依然是期货, 主要openai想做的事情太多, 资源分散
    - 现在的 o1 又来了, 卷的价值比Sora更大
- LLM 最基础的三种能力：**语言理解和表达**能力、**世界知识存储和查询**能力、**逻辑推理**能力
  - **语言理解和表达**能力: 最强, 初版 ChatGPT 完胜纯语言交流任务, 基本达到人类水平
  - **世界知识存储和查询**能力: 规模越大,效果越好, 但幻觉问题目前无法根治, 制约应用落地的硬伤
  - **逻辑推理**能力: 弱项, 最难提升
    - Coding 是目前除语言理解外, LLM做的最好的方向, 因为代码特殊性, 语言+逻辑的混合体, 语言角度好解决,逻辑角度难解决
    - 为什么最难提升? 自然数据（代码、数学题、物理题、科学论文等）在训练数据中比例太低, 于是一个改进方案是预训练阶段和Post-training阶段，大幅增加逻辑推理数据占比
    - 大部分逻辑推理数据的形式是`<问题，正确答案>`，缺少中间推理步骤，而 o1本质上是**让大模型学会自动寻找从问题到正确答案的中间步骤**，以此来增强复杂问题的解决能力。
  - LLM 当前难点
    - 世界知识方面: 如何消除幻觉
    - 逻辑推理方面: 如何大幅提升复杂逻辑推理能力
- RL Scaling law
  - Scaling law 模式: 增加数据+模型规模, 可以提升模型效果, 然而增长速度放缓
  - RL在训练和推理时候的Scaling law 与 预训练 Scaling law 有不同特性。
  - 如果 o1 走 MCTS搜索技术路线，那么把COT拆分的越细（增加搜索树深度），或提出更多的可能选择（节点分支增多，树越宽），则搜索空间越大，找到好COT路径可能性越大，效果越好，而训练和推理的时候需要算力肯定越大。看上去**效果随着算力增长而增长**的态势，即 RL 的 Scaling law。这其实是树搜索本来就有的，称为RL的Scaling law 名不副实。


### 过度思考

【2025-1-3】[揭秘o1类模型的过度思考](https://mp.weixin.qq.com/s/_LGBi1XImFuV2bDg1kbqFQ)

腾讯AI Lab、上海交通大学

o1类超大型语言模型的过度思考:
- 2+3=？答案仅需5个token，o1类模型凭啥要900个？
- 论文：[Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs](https://arxiv.org/abs/2412.21187)

“o1-like” 大型语言模型 通过**延长思考链**（chain-of-thought，CoT），探索**多种**策略，分解复杂步骤，并进行**双重检查**，增强复杂推理任务的处理能力。

这种方法称为“**测试时计算扩展**”，模型推理阶段分配更多的计算资源，以期获得更准确的响应。

“o1-like” 大型语言模型（LLMs）推理问题，“overthinking”（**过度思考**）。处理问题时，尤其是简单问题，分配过多计算资源，对提高答案的准确性几乎没有帮助。
- o1-like 模型消耗的token 比常规模型多出**1953%**
- **资源利用效率**：如何智能且高效地在**测试**期间扩展计算资源，尤其是在面对不同复杂度的问题时。
- **评估和优化**模型效率：提出从结果和过程两个角度出发的新效率指标，以评估o1-like模型在计算资源利用上的合理性，并探索了减轻过度思考问题的策略。
- 保持准确性的同时减少计算开销：通过自我训练范式，提出了减少过度思考的方法，这些方法在不牺牲准确性的前提下，简化了推理过程，减少了生成的解决方案数量。

提出相应的效率指标和优化策略，来提高o1-like模型在AI推理任务中的计算资源利用效率，减少不必要的计算开销。


(1) **扩展测试时计算**（Scaling Test-Time Compute）
- 扩展搜索空间：通过增加搜索空间来提供模型发现和选择正确解决方案的机会
  - 例如：**自我一致性方法**（self-consistency）、**最佳-n解码**（best-of-n decoding）、 **加权多数投票**（weighted majority voting）、 **最小贝叶斯风险解码**（minimum bayes risk decoding）
- 扩展类人思考模式：通过模拟人类思考方式来增强模型的推理能力，
  - 例如：**链式思考**（Chain-of-Thought）、**辩论**（debating）、 **自我纠错**（self-correction）/**自我批评**（self-critique）、 **计划-解决**（plan-and-solve）

(2) **高效思考**（Efficient Thinking）
- **终止推理**：鼓励模型在难以解决问题时通过说“**我不知道**”来终止推理。
- **令牌预算感知推理**（Token-budget-aware reasoning）：提示模型在指定的令牌预算内进行推理。
- **计算预算分配**：根据提示的难度预测计算预算分布，并据此分配计算能力。
- **早期停止策略**：在推理过程中采用早期停止策略以节省计算预算。
- **多代理框架**：使用大型LLMs处理复杂任务，小型LLMs处理简单任务。

尽管上述工作考虑了如何提高模型的推理效率，但主要关注传统模型，而不是具有更长思考链（chain-of-thought）的o1-like模型。

本工作首次提出 o1-like模型中的过度思考问题，并通过**自我训练**方法来训练模型学习如何高效地思考，而不是简单地限制推理空间或由用户指定Token耗费个数。

### DeepSeek R1


【2025-2-6】[Andrej Karpathy 最新视频盛赞 DeepSeek：R1 正在发现人类思考的逻辑并进行复现](https://mp.weixin.qq.com/s/thTwdVgc4lfYRj6WWpKBwA)
- 视频链接：[youtube](https://www.youtube.com/watch?v=7xTGNNLPyMI)
- DeepSeek R1 在性能方面与 OpenAI 模型不相上下，推动了 RL 技术的发展

> 如果只是**模仿**人类玩家、AI 是无法超越人类的，但**纯 RL 算法**却能突破人类限制。


新加坡 Sea AI Lab 等机构研究者再次梳理了类 R1-Zero 的训练过程，并分享了三项重要发现：
1. 在类似 R1-Zero 的训练中，可能并不存在「顿悟时刻」。相反，「顿悟时刻」（如自我反思模式）出现在 epoch 0，即基础模型中。
2. 从基础模型的响应中发现了肤浅的自我反思（SSR），且并不一定会导致正确的最终答案。
3. 仔细研究通过 RL 进行的类 R1-Zero 的训练，响应长度增加的现象并不是因为出现了自我反思，而是 RL 优化设计良好的基于规则的奖励函数的结果。

技术博客：[oat-zero](https://oatllm.notion.site/oat-zero)

### long cot

清华 用 openrlhf 包做的long cot的研究工作。尽可能 Demystifying Long Chain-of-Thought Reasoning in LLMs，通过严格的ablation study得出了11个major takeaway。
- 详情 [twitter](https://x.com/xiangyue96/status/1887332772198371514) 
- 或paper [](https://arxiv.org/pdf/2502.03373)

有意思的发现：
- 1）long cot 比 short cot上限更高（both sft and rl）
- 2）sft使rl过程更加稳定，最后效果也更好
- 3）通过蒸馏已经emergent的long cot大模型比自己通过prompting framework构建long cot的data效果要更好；
- 3）纯rl with final accuracy reward训练使得length scaling非常不稳定，提出了一种cosine reward 把length长度也考虑进入reward，使得训练更加稳定
- 4）context length window size是length scaling的核心，一般8k的context window要比4k要更好，但是16k 在scaling过程中出现了不如8k的效果（原因可能是大的context length可能需要更多的training example）
- 5）scaling up verifiable reward是long cot的核心。我们尝试使用我们之前MAmmoTH2 的noisy的web extracted instruction来scale up verifiable reward，得出了在general reasoning beyond math（e.g.，mmlu-pro）更好的performance
- 6）long cot 的pattern和能力本身就在base model里，我们搜索了cc的子集 openwebmath，发现了很多这样pattern的数据。
- 7）小模型（起码qwen-math-7b）不容易recentivize long cot的behavior（e.g., aha moment）在MATH 场景下。wait, recheck, alternatively这些词在rl训练中没有明显增加（虽然accuracy boost了很多）
- 8）long sft + rl performance 要比 rl from base 或者单纯 sft效果更好。

还有更多takeaway都在paper和twitter thread里。


## 复现


o1 复现的本质: **长CoT数据构造**。

一些开源项目也在尝试复现 OpenAI o1
- `Kimi K0-Math`、`DeepSeek R1 Lite`、阿里`Qwen QwQ`，人大高瓴`STILL-2`等模型的发布
- 让 OpenAI O1 模型复现成为可能，并且在数学，代码领域超越 o1-preview

DeepSeek-R1-Lite-Preview、K0-math、o1-mini、o1-preview 在AIME和MATH两项指标上的成绩。
- DeepSeek-R1-Lite-Preview 在 AIME上的成绩是52.5（pass@1），在MATH上的成绩是91.6（pass@1）；
- K0-math 在AIME上的成绩是53.5，在MATH上的成绩是93.8等。

| o1 模型 | AIME | MATH |
|--|--|--|
| `DeepSeek-R1-Lite-Preview` | 52.5 (pass@1) | 91.6 (pass@1) |
| `K0-math` | 53.5 | 93.8 |
| `o1-mini` | 60 | 90 |
| `o1-preview` | 52.5 | 85.5 |

千问QwQ 32B: 在数学，代码评测集超越o1

|  | `QwQ 32B-preview` | `OpenAI o1-preview` | `OpenAI o1-mini` | `GPT-4o` | `Claude3.5 Sonnet` | `Qwen2.5-72B Instruct` |
|--|--|--|--|--|--|--|
| GPQA <br>Pass@1 | 65.2 | 72.3 | 60.0 | 53.6 | 65.0 | 49.0 |
| AIME <br>Pass@1 | 50.0 | 44.6 | 56.7 | 9.3 | 16.0 | 23.3 |
| MATH-500 <br>Pass@1 | 90.6 | 85.5 | 90.0 | 76.6 | 78.3 | 82.6 |
| LiveCodeBench <br>2024.08-2024.11 | 50.0 | 53.6 | 58.0 | 33.4 | 36.3 | 30.4 |

表格展示了不同模型（QwQ 32B-preview、OpenAI o1-preview、OpenAI o1-mini、GPT-4o、Claude3.5 Sonnet、Qwen2.5-72B Instruct）在四个测试项目（GPQA、AIME、MATH-500、LiveCodeBench）上的成绩（Pass@1）。例如，在GPQA测试项目中，QwQ 32B-preview的成绩是65.2，OpenAI o1-preview的成绩是72.3等。这些数据可以用于比较不同模型在这些特定测试项目上的表现和性能差异。


- 【2024-9-15】[OpenAI o1的开源平替版self-replay RL来了](https://mp.weixin.qq.com/s/KlLU3eHsFn0qo0N8nmqK9g), rStar 复现

In just the past few days, three new AI models from Chinese developers
- `Deepseek R1` (HighFlyer Capital Management)
- `Marco-1` (Alibaba)
- OpenMMLab’s hybrid model 

have entered the fray, challenging OpenAI’s o1 Preview in performance and accessibility.


### 复现方案

如何快速复现 OpenAI o1 的慢思考能力？

#### 数据构造

长CoT数据构造，完全可用中国自己的模型。

国内主流方法
- 基于 `Qwen32B-Instruct` 去蒸馏 O1 类的长 CoT 输出数据
- 或用 `QwQ` /`DeepSeek V3` 构造，人大高瓴的论文给出了详细方法和过程，也有5000条样例数据。
  - 2024-12-26 Deepseek V3 Aider 代码能力排行榜正确率为**48.4%**，仅次于OpenAI o1，超过Claude 3.5 Sonnet

性价比
- `DeepSeek-V3`：输入价格为 0.15美金/每百万tokens，输出价格为 0.3美金/每百万tokens，
- `gpt-4o`：输入价格为 2.5美金/每百万tokens，输出价格 10美金/每百万tokens；
- `claude-3.5-sonnet`：输入价格为3美金/每百万tokens，输出价格15美金/每百万tokens；


构造CoT数据的难点：
- 1、保证prompt到pronse有准确的结果：思考链路是准确的，结果是正确可执行的。
- 2、保证prompt到pronse有错误的结果：思考是错误的方向，结果是不对的。
- 3、正确的引导：如果引导模型，从错误思维方向到正确思维方向转变。
- 4、验证：最最重要的是，以上的数据链路，是批量可以验证的，而不是用人一条条标注数据。
- 5、数据要海量：支撑OpenAI o1训练的数据给不仅仅是数学，代码这些简单场景的的5000条数据。基本上是LLM之前训练的xxT数据全部改造成长CoT数据。


#### 训练阶段

训练阶段
- (0) 第0阶段：**继续预训练** Continued Pretraining
  - 目标：使用CoT（思想链）、代码和数学等大规模数据集, 增强基础模型推理能力。
- (1) 第1阶段：**监督微调** Supervised Fine-Tuning (SFT)
  - 目标：训练模型生成**超长CoT推理链**和**反射指令格式**，为后续强化学习训练奠定基础。
  - 很多开源模型，比如上海交大 `o1-journey-part2` 直接从O1蒸馏数据，后续模型也有从`QwQ`/`DeepSeek`蒸馏数据。预计蒸馏数据占训练数据，至少**40%**以上。
- (2) 第2阶段：**强化学习** Reinforcement Learning
  - 目标：使用强化学习增强大型语言模型的**长推理和反射**能力。
  - 这个阶段开始分歧，包括`Large-Scale RLHF` (PPO)和`蒙特卡洛树搜索`

方案1：PPO
- 数据集和反馈：高质量的数学和代码数据集、奖励模型 (RM)、基于规则的反馈或编译器反馈。
- 优点：高度可扩展，非常适合大规模训练管道。
- 举例： Google DeepMind 开发的通过强化学习进行自我修正 (SCoRe)
- ![](https://pica.zhimg.com/v2-01be669656b1457481b94629bb2d0450_1440w.jpg)

方案2：蒙特卡罗树搜索
- 方法：利用蒙特卡罗树搜索 (MCTS) 生成复杂的推理样本，并结合高质量数据集、RM、基于规则和编译器反馈以及离策略 RL 或 SFT 技术。
- 优点：允许定制CoT格式并可能实现更高的性能上限。
- 挑战：训练流程复杂，使得大规模训练变得更加困难。
- ![](https://pic2.zhimg.com/v2-57c719734235e7258120b59b313cfd9f_1440w.jpg)

paper: AlphaZero-Like Tree-Search can Guide Large Language Model Decoding and Training

所示的训练方法可以用DPO甚至Off-policy RL代替

#### 推理阶段

推断阶段
- 方案1：超长CoT+反射链
- 方案2：基于2蒙特卡罗树搜索的推断

方案一：**超长CoT+反射链**

方法：
- 1、将**长推理链**与**反射机制**相结合。
- 2、实现 **Best-of-N** 或**多数投票**以进行推理扩展。

优点：
- 1、易于实施和扩展。
- 2、快速输出，特别适合**流式**推理。

案例研究：
- DeepSeek R1 Lite 在涉及“1+1”等简单问题的测试中，DeepSeek R1 Lite 展示超长推理过程，并以**流式**格式（20秒）高效输出答案。1+1用了20s时间来计算。

DeepSeek R1 Lite 展示的推理Scaling Law趋势表明，该模型强调增加推理**长度**而不是**宽度**，以提高性能。
- 这种模式之前已在 Google DeepMind 的早期研究中得到证实。
- 推理 Scaling Law 定律：增加推理**长度**比增加**宽度**更有效。
- Deepseek R1 Lite 很可能使用**超长 CoT**，而且甚至**没有**启用**多数投票**或 **Best-of-N**。

![](https://pica.zhimg.com/v2-8de21151496f24d30782fa21befabc8e_1440w.jpg)

2024/11/26 测试了 Kimi k0-Math
- 表现出与 DeepSeek-R1-Lite 类似的行为模式，例如 快速流式输出，没有隐藏推理链。
- 区别：DeepSeek的推理链较长，并在最后提供了推理链摘要。

因此，推测 Kimi K0 Math 也基于 Long CoT 和 反射机制。

开源 Qwen QwQ 的测试
- 在 AIME 等评估数据集上的性能完全通过 Long CoT 实现的，进一步验证了这一观点的重要性。

方案2：基于2蒙特卡罗树搜索的推断

优点：性能没有上限。

挑战：
- 实施复杂度高。
- 昂贵且计算效率低下。
- 短期内难以大规模部署。

rStar (MCTS)推理方法
- ![](https://pica.zhimg.com/v2-17cce655c9cf70221ea8caf65f8f902e_1440w.jpg)



### 【2024-3-18】Quiet-STaR

【2024-3-18】斯坦福 Quiet-STaR
- 论文 [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/pdf/2403.09629)
- 代码 [quiet-star](https://github.com/ezelikman/quiet-star), 以 mistral 为例, 就 5个 py文件
- 解读 [Quiet-STAR: 或许隐藏着OpenAI o1的推理秘密](https://zhuanlan.zhihu.com/p/720792826)

如何提升LLM的推理(reasoning)能力?
- 最直接的做法: fine-tuning，比如在 gsm8k 训练集或其它推理数据集上 fine-tuning
- 如果不想做 fine-tuning，通过 few-shot CoT prompt 做法
- 此外还有 STaR 等

理想中的方案要同时满足 `general` 和 `scalable` 两个条件
- 任意任务 `general`: LLM 推理能力适用于**任意**数据集/任务
- 规模化 `scalable`: 随着数据/LLM size增大，LLM 推理能力也随之增强。

满足 general 和 scalable 两个特性的，只有 language modeling 任务，那么, 能否在language modeling任务中**嵌入推理能力学习子任务**呢？
- GPT-2 模型里说到, 语言模型是无监督多任务学习器, 那是不是可以“增加”一个推理任务？
- 为此，作者设计 `Quiet-STaR` 框架。

Note: 
- `STaR` 通过创建包含推理的QA数据集**显式**增强 LLM 在 QA 类型任务上的推理能力
  - 2022 年,  **Self-Taught Reasoner** `STaR`
- `Quiet-STaR` 则通过语言建模任务**隐式**增强 LLM 通用推理能力，悄悄增强推理能力，故取名 `Quiet-STaR`。
  - 不需要微调

`Quiet-STaR` 原理

让 LLM 每步next token prediction预测时, 进行**思考**(thinking)，也就是**推理**
- 预测下一个token前, 让LLM先生成一条rationale/thought，再做预测。

**训练**总体框架
- 1) Think: 预测每个token前, 先生成**多条thought**
- 2) Talk: 并不是每条thought都参与下一个token预测，**挑选几条**thought参与下个token预测
  - 图中画出来的是只挑选 1 条thought
- 3) Learn: 如何筛选thought呢？这个阶段用了强化学习算法

![](https://pic4.zhimg.com/80/v2-77ba18d105b243b4d4e602be0d09f9ed_1440w.webp)

LLM **预测/推理**过程
- 预测下一个token前，只生成一条 thought
- 然后，进行 2)Talk 阶段进行token预测。


`Quiet-STaR` 计算量太大，训练时, 每个token都需要生成多条thought，一条 thought 就是一条token sequence啊。

如何高效的训练模型，将理论落实到实践?

算法流程图
- ![](https://pica.zhimg.com/80/v2-9cc7b8dc036f891b6821a14bb114e0c4_1440w.webp)
- 详见解读 [Quiet-STAR: 或许隐藏着OpenAI o1的推理秘密](https://zhuanlan.zhihu.com/p/720792826)


实验部分
- 包含推理数据比较多的普通文本 OpenWebMath 和 C4 对 LLM 继续训练
- 然后 在 gsm8k 和 CommonsenseQA 数据集上做 zero-shot 评估
- 模型的 ero-shot推理能力得到了提升，说明 Quiet-STaR 是 general 的。

零样本提升
- GSM8K (5.9%→10.9%)
- CommonsenseQA (36.3%→47.2%) 


### 【2024-9-15】rStar


【2024-9-15】[OpenAI o1的开源平替版self-replay RL来了](https://mp.weixin.qq.com/s/KlLU3eHsFn0qo0N8nmqK9g)

MSQA 和 哈佛 发表 `rStar`，对标OpenAI的超级对齐Q*项目
- 论文《[MUTUAL REASONING MAKES SMALLER LLMS STRONGER PROBLEM-SOLVERS](https://arxiv.org/pdf/2408.06195)》
- 代码链接：[rStar](https://github.com/zhentingqi/rStar)

`rStar` self-play 互推理方法，显著提高了小型语言模型（SLMs）的推理能力，而无需微调或更高级的模型。
- 首先，目标SLM通过丰富的类人推理动作来增强蒙特卡洛树搜索（MCTS），构建更高质量的推理轨迹。
- 接下来，另一个能力与目标SLM相似的SLM充当判别器，验证目标SLM生成的每个轨迹。得分都很高的推理轨迹被认为是相互一致的，因此更有可能是正确的。

`rStar` 解法如下：
- 尽管依赖传统**蒙特卡洛树搜索**（Monte Carlo Tree Search, `MCTS`）让SLMs自我生成推理步骤，但`rStar`提倡在自我探索中使用更丰富的推理动作集。新提出的动作模拟了给定当前推理状态下的人类推理行为，例如分解和搜索特定的推理步骤，提出新的子问题，或重新表述给定问题。这使得SLMs能够在自我探索中生成高质量的候选推理轨迹。
- 为了有效地指导生成的推理轨迹之间的探索，rStar 通过相互一致性的新判别过程增强了MCTS过程。
  - rStar使用第二个能力相似的SLM作为判别器，为MCTS生成的每个候选推理轨迹提供无监督的反馈。为了提高反馈的准确性，rStar向第二个SLM提供采样的部分推理轨迹作为提示，要求其完成剩余的推理步骤。
  - rStar认为相互同意的推理轨迹质量更高。相互一致性反映了在缺乏监督的情况下的常见人类实践，其中同行（即两个SLMs）对推导出的答案的一致性表明了更高的可能性是正确的。因此，相互一致性提供了比其他方法（如自我一致性）更有效的跨任务推理，并避免了训练奖励模型时过度拟合特定任务的风险（类似model ensemble）。

在五个SLMs上的实验表明，rStar 可有效解决多种推理问题，包括GSM8K、GSM-Hard、MATH、SVAMP和StrategyQA。
- `rStar`
  - 将 `LLaMA2-7B` 在 GSM8K数据集上的准确率从**12.51%**提高到**63.91%**
  - 将`Mistral-7B` 准确率从**36.46%**提高到**81.88%**
  - 将`LLaMA3-8BInstruct` 准确率从**74.53%**提高到**91.13%**。




### 【2024-9-17】g1

先有 g1，用提示词策略通过类似o1的推理链, 提高LLM（Llama-3.1 70b）的推理能力。
- g1项目地址：[g1](https://github.com/bklieger-groq/g1)

g1: Using Llama-3.1 70b on Groq to create o1-like reasoning chains

g1 让 Llama3.1 开源模型实现 o1 preview 思考方式
- 动态思维链prompt让开源大模型在复杂问题中展现出惊人的推理能力

g1是一个模仿OpenAI的o1模型推理方式而开发的开源项目，旨在利用 Groq 平台和 Llama-3.1 模型来创建类似于 o1 的推理链，从而提高大型语言模型（LLM）的推理能力。

主要功能：
- 推理链：g1 采用了 o1 模型的推理方法，旨在通过逐步推理来提升模型的决策能力。这种方法可以帮助模型在处理复杂问题时更有效地分析信息。
- 快速响应：项目利用 Groq 的计算能力，使得推理步骤非常迅速，能够及时生成结果。
- JSON 响应示例：项目中包含了生成有效 JSON 响应的示例，这些响应可以用于进一步的应用和开发。


### 【2024-9-18】Peak-Reasoning-7B

【2024-9-18】季逸超 [山寨版 OpenAI o1 实验记录](https://zhuanlan.zhihu.com/p/720575010)
- 用 7B 的 Qwen2 作为 base model，用 8 张 A100 80GB 在 4096 的长度上训练得到 Peak-Reasoning-7B-preview。
- inference-time 似乎没有进行 MCTS 或外置 agentic 的反思，更像是一个在 reasoning path 数据集上训练的 GPT-4o
- o1 的 CoT 可能就是将 reasoning path 作为 scratchpad tokens 放在 output 之前，只是这些 reasoning tokens 长度非常长，质量非常高



### 【2024-10-7】Open-O1


【2024-10-7】 [Open-O1：首个旨在媲美OpenAI o1的项目](https://mp.weixin.qq.com/s/3NSfb4suhnQAsmcq_sUuPA)


与 g1 项目不同，Open-O1 通过策划一组O1风格的思考数据开发的，然后这些数据被用来训练LLaMA和Qwen模型。有两个模型可用：
- OpenO1-V1-LLaMa-8B
- OpenO1-V1-Qwen-7B

Open-O1的愿景：旨在媲美OpenAI O1模型的强大功能，为社区提供先进的开源替代方案。

Open-O1在编码、数学推理、物理、密码、反事实、数据分析、谜题、推理等方面也有很多优秀案例

- 训练方法
  - stage: sft
  - do_train: true
  - finetuning_type: full
  - deepspeed: ds_z3_config.json
- 数据集
  - dataset: 4o_response
  - template: llama3
  - cutoff_len: 4096
  - overwrite_cache: true
  - preprocessing_num_workers: 16
- 训练过程
  - per_device_train_batch_size: 4
  - gradient_accumulation_steps: 2
  - learning_rate: 1.0e-5
  - num_train_epochs: 3.0
  - lr_scheduler_type: cosine
  - warmup_ratio: 0.1
  - bf16: true
  - ddp_timeout: 180000000


### 【2024-11-20】DeepSeek R1

#### DeepSeek-R1-Lite-Preview

【2024-11-20】DeepSeek-R1-Lite-Preview [震撼登场](https://mp.weixin.qq.com/s/7LGNhdszOtYoNRPep76rxw)！推理能力超强，没有黑盒，实时展示推理思考过程，直接叫板OpenAI的o1-preview！
- [DeepSeek-R1-Lite-Preview](http://chat.deepseek.com), 每天50个额度！
- DeepSeek-R1-Lite 目前仍处于迭代开发阶段，仅支持网页使用，暂不支持 API 调用。DeepSeek-R1-Lite 所使用的也是一个较小的基座模型，无法完全释放长思维链的潜力。正式版 DeepSeek-R1 模型将完全开源，公开技术报告，部署API

DeepSeek-R1-Lite 预览版模型在美国数学竞赛（AMC）中难度等级最高的 AIME 以及全球顶级编程竞赛（codeforces）等权威评测中，**大幅超越了 GPT-4o**，甚至 o1-preview 等知名模型
- 预览版在难度较高数学和代码任务上超越o1-preview，大幅领先GPT-4o等。

在六个不同基准测试（AIME 2024、MATH、GPQA Diamond、Codeforces、LiveCodeBench、ZebraLogic）中的表现
- `AIME 2024` ：pass@1，模型第一次尝试就给出正确答案的百分比
  - deepseeker-r1-lite-preview 表现最佳，达到 52.5%。o1-preview 紧随其后，为 44.6%
- `MATH` ：accuracy，模型在数学推理题上的正确率
  - deepseeker-r1-lite-preview 依然领先，正确率为 91.6%。o1-preview 紧随其后（85.5%），与其他模型拉开较大差距
- `GPQA Diamond`：pass@1，模型在高难度问题上的首答正确率
  - o1-preview 领先，达到 73.3%，deepseeker-r1-lite-preview 紧随其后，为 58.5%
- `Codeforces`：rating，模型在编程挑战赛中的分数
  - deepseeker-r1-lite-preview 领先，分数为1450 , o1得分1428
- `LiveCodeBench`：accuracy，编程任务的正确率（2024年8月至11月）
  - o1-preview 小幅领先，正确率为 53.6%。deepseeker-r1-lite-preview 紧随其后，为 51.6%
- `ZebraLogic` ：accuracy，评估逻辑推理任务的正确率
  - o1-preview 占据第一，为 71.4%，deepseeker-r1-lite-preview 紧随其后，为 56.6%


随着思维长度的增加，DeepSeek-R1-Lite-Preview 在 AIME 上的得分稳步提高，这与 OpenAI o1 提出推理缩放规律是一致的，由此,推理缩放具有巨大的潜力

测试问题
- 9.11和9.8哪个大？
- 9.12和9.9哪个大？
- 单词 “strawberry”（草莓）有几个r?
- 单词'blueberrycherryberrycarbonpherry' 有几个r?

回答全都是一次性正确，并且实时的展示出了思考的过程


体验后感觉是：
- **数学**能力：该模型在数学推理问题上看起来很有效。基准测试结果确实反映了模型在数学推理能力上的潜力。这是一个值得密切关注的模型。
- **编码**任务：在解决编程问题时，表现**稍显不足**。例如，在生成用于转置矩阵的bash脚本这样的简单代码问题上，它未能成功解决，而o1模型可以轻松解决。
- **复杂知识理解**：尝试在一个更难的字谜游戏上测试它，但它表现得非常糟糕。公平地说，即使o1模型在这个需要现代知识引用的测试中也同样表现不佳。

|问题类型|DeepSeek-R1|OpenAI o1|结论|
|---|---|---|---|
|数学能力|很有效||超过o1|
|编码任务|稍显不足|轻松解决|不如o1|
|复杂理解|字谜游戏非常糟糕|同样不佳||

总结
- 代码和数学任务上表现出色，这可能得益于DeepSeek团队在这些领域的明确优化。然而，在“推理”步骤上仍有改进空间。
- 模型似乎能够在生成推理步骤时自我纠正，表现出类似原生“自我反思”的能力

#### 【2025-1-20】DeepSeek-R1-Zero

【2025-1-20】 正式发布 `DeepSeek-R1-Zero` 和 改进版 `DeepSeek-R1`
- 代码 [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)
- 解读 [DeepSeek R1来了，追平o1！它现在不但比OpenAI开放，也比它有活力](https://www.msn.cn/zh-cn/news/other/deepseek-r1%E6%9D%A5%E4%BA%86-%E8%BF%BD%E5%B9%B3o1-%E5%AE%83%E7%8E%B0%E5%9C%A8%E4%B8%8D%E4%BD%86%E6%AF%94openai%E5%BC%80%E6%94%BE-%E4%B9%9F%E6%AF%94%E5%AE%83%E6%9C%89%E6%B4%BB%E5%8A%9B/ar-AA1xyCbV?ocid=BingNewsSerp)

发布了三组模型：
- 1）`DeepSeek-R1-Zero` 直接将RL应用于基座模型，没有任何SFT数据
- 2）`DeepSeek-R1` 从经过数千个**长思想链**（CoT）示例微调的检查点开始应用RL
- 3）从 DeepSeek-R1 中**蒸馏**推理能力到小型密集模型。Llama 和 Qwen
  - DeepSeek-R1-Distill-Qwen-32B 超过 OpenAI-o1-mini, 成为 sota

DeepSeek-R1 遵循 MIT License，允许用户通过蒸馏技术借助 R1 训练其他模型。DeepSeek-R1 上线API，对用户开放思维链输出，通过设置 `model='deepseek-reasoner'` 即可调用。DeepSeek 官网与 App 即日起同步更新上线。

DeepSeek在R1基础上，用Qwen和Llama蒸馏了几个不同大小的模型，适配目前市面上对模型尺寸的最主流的几种需求。
- 启发：对小模型来说，**蒸馏优于直接强化学习**：从 DeepSeek-R1 蒸馏得到的小模型在多个推理基准（如 AIME 2024 和 MATH-500）上的表现优于直接对小模型进行强化学习。大模型学到的推理模式在蒸馏中得到了有效传递。

DeepSeek-R1 
- 在 AIME2024上获得了**79.8%**的成绩，略高于 OpenAI-o1-1217。
- 在 MATH-500上，它获得了97.3%的惊人成绩，表现与 OpenAI-o1-1217相当，并明显优于其他模型。
- 在编码相关的任务中，DeepSeek-R1 在代码竞赛任务中表现出专家水平
- 在Codeforces上获得了2029 Elo评级，在竞赛中表现优于96.3%的人类参与者。
- 对于工程相关的任务，DeepSeek-R1的表现略优于OpenAI-o1-1217。

最让人惊叹的是 R1 Zero 训练方法：
- 放弃了过往对预训练大模型来说必不可少甚至最关键的一个训练技巧——SFT。
- 直接将RL应用于基座模型，没有任何SFT数据

问题：由于完全没有人类监督数据的介入，一些时候显得混乱。
- 不停重复、不够流畅、语种混乱

为此，DeepSeek 推出 `DeepSeek-R1`， 用**冷启动**和**多阶段RL**方式，改进训练流程，在 R1 zero基础上训练出更“有人味儿”的R1。这其中的技巧包括：
- **冷启动数据**引入(cold start) —— 针对 `DeepSeek-R1-Zero` 的可读性和语言混杂问题，DeepSeek-R1 通过引入数千条高质量的冷启动数据进行初始微调，显著提升了模型的可读性和多语言处理能力；
- **两阶段强化学习** —— 模型通过两轮强化学习不断优化推理模式，同时对齐人类偏好，提升了多任务的通用性；
- **增强型监督微调**——在强化学习接近收敛时，结合**拒绝采样**（Rejection Sampling）和多领域的数据集，模型进一步强化了写作、问答和角色扮演等非推理能力。

R1 zero 训练过程里，出现了**涌现**时刻，DeepSeek 称为“aha moment”。
- 随着测试阶段计算能力的提升，复杂行为会自发涌现。
- 例如，模型会进行“反思”，即**重新审视并评估**之前的步骤，还会探索解决问题的替代方法。
- 这些行为并非通过明确编程实现，而是模型与强化学习环境交互的自然产物，大大增强了其推理能力，使其能够更高效、更精准地解决复杂任务。


### 【2024-11-22】阿里 Marco-o1


【2024-11-22】[阿里提出Marco-o1：探索开放推理模型在复杂问题解决中的应用与突破](https://mp.weixin.qq.com/s/K86-MN_0T0Q4gJ1bdqL0Kw)

Marco-o1 模型结合 Chain-of-Thought（CoT）微调、蒙特卡罗树搜索（MCTS）、反思机制和创新推理策略，增强大型语言模型（LLM）在复杂现实世界问题解决任务中的推理能力。
- 论文：[Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions](https://arxiv.org/pdf/2411.14405)

问题：能否将o1模型有效地推广到**没有明确标准且奖励难以量化**的更广泛的领域中？

研究难点：
- 在没有明确标准和奖励的情况下，如何使模型能够泛化并解决复杂问题；
- 如何在多语言和翻译领域中实现推理能力的提升。

相关工作
- OpenAI o1 模型在数学、物理和编程等有标准答案的学科中的出色表现，以及Chain-of-Thought（CoT）微调、蒙特卡罗树搜索（MCTS）等技术的应用。

Marco-o1 模型
- **CoT微调**: 使用开源CoT数据集和自开发的合成数据进行全参数微调，以增强模型的推理能力。
- **MCTS集成**: 将LLM与MCTS集成，利用模型输出的置信度来指导搜索并扩展解空间。每个节点代表一个推理状态，可能的动作是LLM的输出，通过softmax函数计算每个token的置信度，并计算平均置信度作为整体奖励信号。
- **推理动作策略**: 实现了不同的推理动作粒度（步骤和迷你步骤），并在MCTS框架内探索这些粒度，以提高搜索效率和准确性。
- **反思机制**: 在每个推理过程结束后，添加反思提示，促使模型自我反思和重新评估其推理步骤，从而提高解决复杂问题的能力。

图见[原文](https://mp.weixin.qq.com/s/K86-MN_0T0Q4gJ1bdqL0Kw)

### 【2024-11-26】Skywork-o1

Skywork-o1开源

国内先开的o1模型竟然是Skywork-o1。

昆仑万维新功能上线Skywork-o1，并且开源了8B版本的o1模型 Skywork-o1-Open-Llama-3.1-8B。
- 官方介绍：[Skywork-o1-Open-Llama-3.1-8B](https://huggingface.co/collections/Skywork/skywork-o1-open-67453df58e12f6c3934738d0) 模型，在各项数学和代码指标上均有大幅提高，将Llama-3.1-8B的性能拉到同生态位SOTA（超越Qwen-2.5-7B instruct）。
- 同时，8B模型解锁了很多较大量级模型，如GPT 4o，无法完成的数学推理任务（如24点计算）。也为推理模型在轻量级设备上部署提供了可能性。
- 同时还开源了2个过程评价模型，Skywork-o1-Open-PRM-Qwen-2.5-1.5B 和 Skywork-o1-Open-PRM-Qwen-2.5-7B。




### 【2025-1-20】kimi k1.5


【2025-1-20】Kimi发布了`k1.5` **多模态思考**模型。
- 继去年 11 月他们发布 k0-math 数学模型，12月发布 k1 视觉思考模型之后，连续第三个月带来 **k 系列**强化学习模型的重磅升级。
- 论文地址：[kimi-k1.5](https://github.com/MoonshotAI/kimi-k1.5)
- [解读](https://www.51cto.com/article/806780.html)

k 系列思考模型路线图

|时间|模型|模态|领域|备注|
|---|---|---|---|---|
|2024-11-16|k0-math|文本|数学||
|2024-12-17|k1|文本、视觉|数学、物理、化学||
|2025-1-20|k1.5|文本、视觉|数理化、代码、通用||
|2025？|kn|更多模态|更多领域||

Kimi k1.5 性能，已经全面追上全球最强模型——OpenAI o1满血版。
- Long CoT模式下，Kimi k1.5的数学、代码、多模态推理能力，达到了长思考SOTA模型OpenAI o1满血版的水平。全球范围内，首次有OpenAI之外的公司达到。
- 而在Short CoT模式下，Kimi k1.5 大幅领先 GPT-4o  和 Claude 3.5 水平。

简单出奇迹，首创long2short思维链
- 通过长**上下文扩展**和**改进的策略优化**方法，结合**多模态**数据训练和**长到短推理路径压缩**技术，提升大模型在复杂推理和多模态任务中的性能和效率。

k1.5设计和训练的四大关键要素：
1. 长上下文扩展
2. 改进的策略优化
3. 简化框架
4. 多模态

k1.5 将 RL 的上下文窗口扩展到128k，提出了基于长推理路径的强化学习公式，并采用在线镜像下降的变体进行稳健的策略优化。

此外，k1.5在文本和视觉数据上联合训练，具有联合推理两种模态的能力。

[技术报告](https://github.com/MoonshotAI/Kimi-k1.5/blob/main/Kimi_k1.5.pdf)

Kimi 和 DeepSeek 论文得出相似结论: 不需要复杂`蒙特卡洛树搜索`(MCTS)、额外昂贵模型副本的**价值函数**和密集的**奖励建模**(PRM)。


|维度|deepseek r1| k1.5|分析|
|---|---|---|---|
|训练思路|不用MCTS/价值函数/奖励建模|不用MCTS/价值函数/奖励建模|相同|
|RL算法|AlphaZero|AlphaGo Master||
|多模态|表现出色|次之||
|开源程度|半开源|全开源||


### 【2025-2-6】Gemini 2.0 Flash-Lite

【2025-2-6】[Gemini 2.0霸榜，价格卷哭DeepSeek V3，性价比新王诞生](https://mp.weixin.qq.com/s/WTRgMrj4vFE7sFecBrjirg)

Deepseek、Qwen 和 o3 的围追堵截下，谷歌一口气连发了三款模型：Gemini 2.0 Pro、Gemini 2.0 Flash ，Gemini 2.0 Flash-Lite

大模型 LMSYS 排行上，`Gemini 2.0-Pro` 冲到了第一名，Gemini-2.0 家族都挺进了前 10。

Pro 版本代表了当前 Google 最先进的 AI 能力，尤其在编码和推理方面表现出类拔萃的性能：
- 超大上下文窗口： 
- 支持高达2M tokens 的上下文处理能力工具集成能力强大： 
- 深度整合 Google 搜索与代码执行功能可用性说明：已在 Google AI Studio、Vertex AI 以及 Gemini Advanced 平台以实验版本形式上线

Gemini 2.0 Flash 定位为“高效主力模型”，设计上侧重于速度与性能的平衡，旨在为需要低延迟响应的应用场景提供理想支持：
- 百万级上下文窗口： 支持 1M tokens 上下文优秀的多模态推理能力： 
- 擅长处理多模态数据，目前支持多模态输入和单模态文本输入未来功能拓展： 
- 图像生成与文本转语音功能即将推出可用性说明： 
- 已在 Vertex AI Studio 和 Google AI Studio 平台正式发布，可通过 Gemini API 接入。

Gemini 2.0 Flash-Lite (Preview)作为“最具成本效益”的模型，Flash-Lite 在速度、成本和性能之间实现了最佳平衡点。
- 高性价比优势： 与 1.5 Flash 相同速度和成本的前提下，多数基准测试中超越 1.5 Flash。
- 百万级上下文窗口： 同样支持 1M tokens 上下文处理能力。根据谷歌放出来的性能评估对比可以看出，Gemini 2.0 Pro Experimental 版本在几乎所有基准测试中都取得了最高分，表现出色：


### 【2024-2-6】S1

斯坦福李飞飞、Percy Liang 等人推出 S1 

李飞飞团队“50美元”复刻DeepSeek R1
- 真相：基于阿里云Qwen模型监督微调而成

李飞飞等斯坦福大学和华盛顿大学的研究人员以**不到50美元**的云计算费用，成功训练出了一个名为`s1`的人工智能推理模型。

该模型在数学和编码能力测试中的表现与OpenAI的O1和DeepSeek的R1等尖端推理模型不相上下。
- s1模型训练并非从零开始，其基座模型为阿里`通义千问`(Qwen)模型。
- s1用50美元训练出新的具有推理能力的模型，实际上只是用从谷歌模型中提炼出来的1000个样本，然后对千问模型进行微调而成。
- s1是通过`蒸馏法`由谷歌推理模型 Gemini 2.0 Flash Thinking Experimental 提炼出来的。


# 结束