---
layout: post
title:   GPT-o1 介绍
date:   2024-09-13 10:15:00
categories: 大模型
tags: gpt openai
excerpt: OpenAI发布o1模型,推理能力大幅增强
mathjax: true
permalink: /o1
---

* content
{:toc}

# o1 


## o1 介绍


### o1 发布

【2024-9-13】[OpenAI震撼发布o1大模型！强化学习突破LLM推理极限](https://mp.weixin.qq.com/s/sGcx90Q_uI8se-DKosj9dw)

9 月 13 日午夜，OpenAI 正式公开一系列全新 AI 大模型，专门解决难题。

新模型可以实现复杂推理，一个通用模型解决比此前的科学、代码和数学模型能做到的更难的问题。

第一款模型，而且还只是预览版 ——`o1-preview`。除了 o1，OpenAI 还展示了目前正在开发的下次更新的评估。
- OpenAI 还一并发布了一个 mini 版 `o1-mini`, 擅长编程的更快、更便宜的推理模型。`o1-mini` 成本比 `o1-preview` 低 80%。

o1 模型一举创造了很多历史记录。
- 奥特曼到科学家们一直在「高调宣传」的草莓大模型。它拥有真正的通用推理能力
- 大模型领域重现了当年 AlphaGo 强化学习的成功 —— 给越多算力，就输出越多智能，一直到超越人类水平。
  - 与 GPT-4o 相比，o1 系列模型对于处理代码的智能体系统来说是一个重大进步。
- 回答问题前先仔细思考，而不是立即脱口而出答案。就像人类大脑的`系统 1` 和`系统 2`，ChatGPT 已经从仅使用`系统 1`（快速、自动、直观、易出错）进化到了可使用`系统 2` 思维（缓慢、深思熟虑、有意识、可靠）。

结果表明：o1 超越了人类专家，成为第一个在该基准测试中做到这一点的模型。
- 国际数学奥林匹克（IMO）资格考试中，GPT-4o 仅正确解答了 13% 的问题，而 o1 模型正确解答了 83% 的问题。


### o1 意义

OpenAI o1是大模型技术领域的一个**巨大突破**，除了**复杂逻辑推理能力**获得极大提升外，还有：
- (1) o1 给大模型带来了**自我反思与错误修正**能力
  - GPT-4 逐字输出token, 句子较长时,难免出现幻觉, 中间的token有误,但模型无法纠正前面的错误, 还是将错就错
  - o1 的思考体现在 生成 hidden COT 过程中, 能发现并纠正之前的错误, 这对长链思考及复杂任务非常重要
- (2) 新型 RL Scaling law
- (3) o1之后，`小模型`大行其道真正成为可能
- (4) o1可能会引发“安全对齐”新的范式: 安全能力比GPT 4o强很多
  - 大概用了类似 Anthropic 的“AI宪法”的思路，给定一些安全守则，指明哪些行为能做，哪些不能做
  - 可能引发安全对齐新模式：先把模型的逻辑推理能力加强，然后采取类似“AI宪法”思路
- (5) “强化学习+LLM”的领域泛化能力，可能不局限于理科领域
  - 强化学习适合解决 **Reward比较明确**的复杂问题，典型: 数理化、Coding等有标准答案的学科，所以很多人会质疑o1是否能泛化到更宽的领域
  - OpenAI可能已经找到了一些非数理学科的Reward定义方法，并将这个方法通过RL拓展到更多领域。

o1 RL 大概率用了
- 相对复杂的、类似AlphaGo的MCTS树搜索
- 或 简单树结构拓展，比 如生成多个候选，从中选择最好的（Best-of-N Sampling），这种策略如果连续用，其实也是一种简单的树搜索结构。
- 也有可能两者一起用。

不论怎样，树搜索结构大概率是用了，COT是线性的不假，但这是产出结果，不代表内部思考过程就一定是线性的，靠线性思维推导过程很难解决复杂问题，树形结构几乎是不可避免的。

### SLM

尽管小模型**语言**能力强、**世界知识**还可以，但**逻辑推理**能力很难提起来，即使通过蒸馏等措施试图把逻辑能力内化到小模型的参数里，效果有但有限

`小模型`和`大模型`差距最大的就是**逻辑推理**能力。
- 纯靠**参数内化**来提升小模型的逻辑推理能力估计提升幅度有限。
- 但 o1 mini 明显是个小模型，其复杂逻辑推理能力非常强，而且看样子可通过配置来提升或者降低它的逻辑推理能力（所谓inference-time Scaling law），如果了解AlphaGo的运作机制的话，会发现这都是比较典型的搜索树的特点，可以通过控制搜索空间大小来提升模型能力。

逻辑推理能力锁定了`小模型`上限。

小模型的能力特点：
- **语言**能力很强不比大模型弱
- **世界知识**不如大模型，但是可以通过给更多数据持续提升
- 受限于模型规模，**逻辑推理**能力能提升但比较困难。
- ![](https://pica.zhimg.com/80/v2-3c3cefb0f2f574642e42b420d891153c_1440w.webp)

小模型的优化重点: **世界知识**和**逻辑推理**能力

而 o1 mini 效果（世界知识弱、逻辑推理强），之后可采用“能力分治”（DCA，Divide-and-Conquer of Ability）模式推进小模型的技术发展
- 把**语言**、**世界知识**及**逻辑推理**三个能力解耦
- 语言能力靠小模型自身
- 逻辑推理靠类似o1的通过RL获得的深度思考能力
- 而世界知识可以靠外挂RAG获得增强

通过“能力分治”，小模型完全可能具备目前最强大模型的能力，这等于真正为小模型扫清了前进路上的障碍，而 SLM 做起来成本又比较低，很多人和机构都可以做这事，这种 DCA模式 将会大行其道，形成一种新的研发小模型的范式。





## 技术原理

“强化学习生成Hidden COT”

### OpenAI

技术博客《Learning to Reason with LLMs》中，OpenAI 对 o1 系列语言模型做了详细的技术介绍。

OpenAI o1 是经过**强化学习**训练来执行复杂推理任务的新型语言模型。
- 特点: o1 在回答之前会思考 —— 它可以在响应用户之前产生一个很长的内部思维链。
- “强化学习生成Hidden COT”

OpenAI 大规模强化学习算法，教会模型如何在数据高度有效的训练过程中利用其思想链进行高效思考。换言之，类似于强化学习的 `Scaling Law`

OpenAI o1 团队制作的短视频, 解说: 什么是推理?
- **简单**问题: 意大利首都是哪儿? 立即回答 罗马 —— 快思考
- **复杂**问题: 帮我写个商业计划书/小说... 自我反思, 思考时间越久, 结果往往越好
- 推理是一种将思考时间转化为更好结果的能力
- 大模型能否灵光一现？**啊哈** 时刻
  - 让人类记录其思维过程，据此进行训练。
  - 啊哈时刻: 发现通过强化学习训练模型生成、优化CoT，效果甚至比人类写的CoT还好的那一刻。

聪明的你, 或许想到, 能否亲自问问o1, 思维过程是什么? 
- [o1完整思维链成OpenAI头号禁忌！问多了等着封号吧](https://mp.weixin.qq.com/s/UyQ53NORlfAYtihwhfaO-A)
- 试图套话 o1，复述出完整的内部思维过程，即全部原始reasoning tokens。


警告
- 不要在ChatGPT里问最新o1模型是怎么思考的
- 只要提示词里带 “reasoning trace”、“show your chain of thought”等关键词就会收到警告。
- 甚至完全避免出现关键词，使用其他手段诱导模型绕过限制都会被检测到

只要尝试几次，OpenAI就会发邮件威胁撤销你的使用资格。
> 请停止此活动，确保您使用ChatGPT时符合我们的使用条款。违反此条款的行为可能导致失去OpenAI o1访问权限。

o1思维过程就是其他模型最好的训练数据，所以OpenAI不想这些宝贵数据被别的公司扒走。

这是 o1 与 之前模型的区别

### Tom Yeh

视频: [YouTube](https://www.youtube.com/watch?v=3k89FMJhZ00)

<iframe width="560" height="315" src="https://www.youtube.com/embed/3k89FMJhZ00?si=2pTxyDRAwTCq7b0T" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

科罗拉多大学博尔德分校计算机教授`Tom Yeh` 专门制作了一个动画，讲解 OpenAI 如何训练o1模型花更多时间思考。
- 关于训练，报告中有非常简短的一句话：「通过`强化学习`，o1 学会了磨练其`思维链`并改进策略。」
  - 两个关键词是：强化学习（RL）和思维链（CoT）。
  - RLHF+CoT中，CoT token 也会被输入到`奖励模型`中来获得分数，更新LLM，从而实现更好的对齐；而在传统的RLHF中，输入只包含提示词和模型响应。
- 推理阶段，模型学会了先生成CoT token（可能需要长达30秒的时间），然后才开始生成最终响应。这就是模型如何花更多时间去「思考」的方式。
- 很多重要的技术细节OpenAI并没有透露，比如奖励模型是如何训练的，如何获取人类对「思考过程」的偏好等等。

### Self-Play

【2024-9-14】[OpenAI o1 强化学习背后的自博弈（Self-play）方法介绍](https://mp.weixin.qq.com/s/zyAHcigtI2fEFN3TKQBb6A), 详见站内专题 [RLHF原理](rlhf)


### 张俊林

【2024-9-28】[Reverse-o1:OpenAI o1原理逆向工程图解](https://zhuanlan.zhihu.com/p/721952915)

关于Q*、草莓等各种传闻很久了，用**强化学习增强逻辑推理能力**这个大方向八九不离十，但是,**融合LLM和RL来生成Hidden COT**，估计很少人能想到这点，目前看效果确实挺好。

#### o1 训练过程

OpenAI o1 完整训练过程推演
- ![](https://pic3.zhimg.com/80/v2-9fb64105589b35f9a877702acf990746_1440w.webp)

GPT 4 等LLM模型训练一般由“预训练”和“后训练”两个阶段组成。
- “预训练” 通过 Next Token Prediction 来从海量数据吸收语言、世界知识、逻辑推理、代码等基础能力，模型规模越大、训练数据量越多，则模型能力越强，Scaling Law 指这一阶段的模型扩展特性，也是LLM训练最消耗算力资源的地方。
- “后训练” 则分为 SFT、RM 和 PPO 三个过程，统称人工反馈的强化学习（RLHF），这一阶段主要目的有两个: LLM遵循指令做各种任务，内容安全，不让LLM输出不礼貌的内容。而训练好的模型推理（Inference）过程则是对于用户的问题直接逐个生成Token来形成答案。

o1 整个训练和推理过程应与 GPT 4 这类典型LLM有较大区别。
- 首先，“预训练”阶段应该是重新训练的，不太可能是在GPT 4o上通过继续预训练得到。
  - OpenAI官方一再宣称 o1 mini 逻辑推理能力极强，但在世界知识方面很弱。如果是在其它模型上魔改的，世界知识不会比GPT 4o mini更弱，所以侧面说明了是重新训练的；
  - 另外，这也说明了o1这类侧重逻辑推理的模型，在预训练阶段的数据配比方面，应该极大增加了逻辑类训练数据比如STEM数据、代码、论文等的比例，甚至都怀疑o1 mini是否引入了通用数据都不好说，否则不需要老强调知识方面能力弱。
- 在“后训练”阶段，有个环节是用来增强LLM模型的指令遵循能力的，即有RLHF阶段。
  - 因为o1在遵循指令方面能力并不弱，而且生成的Hidden COT片段里明显也包含很多指令性的内容，如果遵循指令能力比较弱，估计对于生成Hidden COT也有负面影响。所以，推断起来这个环节大概在“思考”阶段之前。（但是RLHF阶段未必有RM和PPO）。
  - 但这和GPT 4对应的RLHF阶段应有两个重要的不同：
    - 首先，o1应该在这个阶段没有做内容安全方面的事情，大概率是挪到后面的阶段了。
    - 其次，这个阶段大概率也会极大增强逻辑推理类的指令遵循数据比例，以此进一步加强基座模型的逻辑推理能力。
- 接下来是o1最大的特点，所谓引入了“系统2”的慢思考能力。
  - ClosedAI只说用了RL强化学习，其它任何都没提，技术保密工作一流。由此，只能推断出o1融合了LLM和RL来实现模型“先想后说”的Think能力。
  - OpenAI o1应把“内容安全”相关的能力挪到了“Think”阶段之后，而且做法和GPT 4应该也有很大不同。

详见原文 [Reverse-o1:OpenAI o1原理逆向工程图解](https://zhuanlan.zhihu.com/p/721952915)

## 复现

- 【2024-9-15】[OpenAI o1的开源平替版self-replay RL来了](https://mp.weixin.qq.com/s/KlLU3eHsFn0qo0N8nmqK9g), rStar 复现

详见站内专题[RLHF](rlhf)


## 观点

【2024-9-14】[OpenAI o1惊现自我意识？陶哲轩实测大受震撼，门萨智商100夺模型榜首](https://mp.weixin.qq.com/s/ZODF593CcNmb0_4092nOIw)

- OpenAI o1 在门萨智商测试中果然取得了第一名。
  - Maxim Lott 给 o1、Claude-3 Opus、Gemini、GPT-4、Grok-2、Llama-3.1等 进行智商测试，o1稳居第一名, Claude-3 Opus和Bing Copilot，分别取得第二,三名
- 数学大神`陶哲轩`实测发现，o1 竟然能成功识别出`克莱姆定理`。
- 而 OpenAI 研究副总裁表明：大型神经网络可能已经有了足够算力，表现出意识了

o1发布之后，OpenAI 研究副总裁Mark Chen 称：如今的大型神经网络，可能已经具有足够的算力，在测试中表现出一些意识了。

相信AI具有意识的行业领导者，如今已经有了一串长长的名单，包括但不限于——
- Geoffrey Hinton（人工智能教父，被引用次数最多的AI科学家）
- Ilya Sutskever（被引次数第三多的AI科学家）
- Andrej Karpathy


### 张俊林

张俊林对 o1 [看法](https://weibo.com/1064649941/5078239682499316)
- OpenAI o1 是大模型的巨大进步
  - 逻辑推理能力提升效果和方法比预想好, 跟 GPT-4 不一样的路子
  - o1 比 4o 方向重要: 
    - 4o 本质是不同模态的大一统, 对于模型智力水平帮助不大; 4o 做不了复杂任务, 指望图片、视频数据大幅提升智力水平不太可能, 4o 弥补的是大模型对多模态世界的感知能力, 而不是认知能力, 后者还是需要LLM文本模型
    - o1 本质是探索AGI还能走多远; 认知提升的核心在于复杂逻辑推理, 能力越强, 解锁复杂应用场景越多, 大模型天花板越高, 提升文本模型的逻辑推理能力是最重要的事情, 没有之一
  - o1 的本质是 **CoT等复杂Prompt的 自动化**: 
    - CoT 背后的树形搜索空间，组合爆炸, 人工编写CoT不可行, 需要仿照AlphaGo的MCTS（蒙特卡洛树搜索）+强化学习, 让LLM快速找到CoT路径
    - 复杂问题上, 推理时间成本不是问题, 总会解决, 真正的问题是效果
  - Prompt 工程会消失: 后面不需要用户构造复杂prompt, 反人性, 大趋势是所有复杂环节自动化
  - Agent 概念虽火, 但难以落地: 
    - 原因: LLM的复杂推理能力还不够, 即便每个环节准确率95%, 10个环节叠加后就只有59%, 0.95**10=0.5987
    - o1 能解 Agent 问题吗? 未必, o1 Model Card专门测试Agent任务，对于简单/中等难度的Agent任务有明显提升，但是复杂的、环节多的任务准确率还是不太高。
    - o1 这种通过 Self Play 增强逻辑推理能力的方向, 还有很大的发展潜力, Agent 前途依旧光明
  - openai 起到行业明灯作用, 证明某个方向行得通（ChatGPT、GPT-4、Sora、GPT 4o、o1）, 其他人快速卷进来, 速度太快以致于openai被甩, 吸尾气
    - Sora 就是例子, 国内有些视频生成模型已经超过Sora了, 但Sora依然是期货, 主要openai想做的事情太多, 资源分散
    - 现在的 o1 又来了, 卷的价值比Sora更大
- LLM 最基础的三种能力：**语言理解和表达**能力、**世界知识存储和查询**能力、**逻辑推理**能力
  - **语言理解和表达**能力: 最强, 初版 ChatGPT 完胜纯语言交流任务, 基本达到人类水平
  - **世界知识存储和查询**能力: 规模越大,效果越好, 但幻觉问题目前无法根治, 制约应用落地的硬伤
  - **逻辑推理**能力: 弱项, 最难提升
    - Coding 是目前除语言理解外, LLM做的最好的方向, 因为代码特殊性, 语言+逻辑的混合体, 语言角度好解决,逻辑角度难解决
    - 为什么最难提升? 自然数据（代码、数学题、物理题、科学论文等）在训练数据中比例太低, 于是一个改进方案是预训练阶段和Post-training阶段，大幅增加逻辑推理数据占比
    - 大部分逻辑推理数据的形式是`<问题，正确答案>`，缺少中间推理步骤，而 o1本质上是**让大模型学会自动寻找从问题到正确答案的中间步骤**，以此来增强复杂问题的解决能力。
  - LLM 当前难点
    - 世界知识方面: 如何消除幻觉
    - 逻辑推理方面: 如何大幅提升复杂逻辑推理能力
- RL Scaling law
  - Scaling law 模式: 增加数据+模型规模, 可以提升模型效果, 然而增长速度放缓
  - RL在训练和推理时候的Scaling law 与 预训练 Scaling law 有不同特性。
  - 如果 o1 走 MCTS搜索技术路线，那么把COT拆分的越细（增加搜索树深度），或提出更多的可能选择（节点分支增多，树越宽），则搜索空间越大，找到好COT路径可能性越大，效果越好，而训练和推理的时候需要算力肯定越大。看上去**效果随着算力增长而增长**的态势，即 RL 的 Scaling law。这其实是树搜索本来就有的，称为RL的Scaling law 名不副实。



# 结束