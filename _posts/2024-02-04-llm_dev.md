---
layout: post
title:  大模型时代开发模式 Development Mode in the Era of LLM
date:   2024-02-04 10:00:00
categories: 大模型
tags: llm 对话系统
excerpt: 大模型时代的开发模式，如何用LLM模式重构已有应用？
mathjax: true
permalink: /llm_dev
---

* content
{:toc}


# 大模型的开发模式

## 开发模式

### 模式总结

人机协同三种模式
-   `AI Embedded` **嵌入**：某个环节里去调用大模型
  - 使用提示词来设定目标，与AI进行语言交流，然后AI协助完成目标
-   `AI Copilot` **辅助**：每个环节都可以跟大模型进行交互
  - 人类和AI各自发挥作用，AI介入到工作流程中，从提供建议到协助完成流程的各个阶段。
-   `AI Agent` **代理**：任务交给大模型，大模型即可自行计划、分解和自动执行
  - 人类设定目标并提供资源（计算能力），然后监督结果。
  - Agent承担了大部分工作。
  - AutoGPT代表了一种完全自动化的实现方式，试图抵达AGI的理想状态，即提出需求后机器人能够自动完成任务
  - AI技术的自动化范式 —— AutoGPT
  - 基于Agents的自动化团队——GPTeam，许多流程都可以被自动化执行。市场调研、问卷调查、品牌计划等等，都可以由AI来完成。
  - 自动化品牌营销公司——AutoCorp
- `Society` 模式
- ![](https://pic1.zhimg.com/v2-0999ee748a33bc812f5f9ca73af4a41c_b.jpg)
- ![](https://pic2.zhimg.com/80/v2-768dd5453f9ebe858d60b90bd1b3143d_1440w.webp)

【2023-11-21】更完整的[图解](https://www.51cto.com/article/766924.html) 
- ![](https://s4.51cto.com/oss/202309/15/f8059a9831ca7b2230f159744b9b0cfee2099c.png)

第三种模式将成为未来人机交互的主要模式。


### 方案一：AI Copilot 辅助
  
以LLM为核心，end2end架构实现新版对话系统，用prompt engineering复现原有主要模块

既然大模型（尤其是GPT系列）这么厉害，下游任务只需调prompt，那就用prompt去完成对话功能，替换原有功能模块就好了。
-   角色模拟：system prompt中设置即可，大模型的强项
-   闲聊任务：满足，一次调用即可
-   简易问答：如果是通用领域知识，一次调用即可，如果是垂直领域，需要额外融入知识（prompt中植入/微调），如果涉及实时查询、工具，还需要结合Plugin、Function Call，一般1-3次调用
-   多轮任务：prompt中设置对话逻辑（类似上一代有限状态机FSM），简单任务满足，功能接口借助Plugin、Function Call实现，同时增加记忆单元（如借助LangChain），但复杂任务不好办，如：任务状态处理逻辑复杂、场景嵌套、API较多等。即便是FSM场景也受限（如图），而订餐这类场景只是多轮对话中的一种简单形式，至于更复杂的循环、中断、嵌套、多阶就不用提了，如信用卡业务场景下，各种流程跳转，简易对话无能为力，只好用比FSM（图）更高级的方法，Frame（槽填充+树）、Goal（树+栈+字典），大模型用data-driven结合强化学习更合适。

![](https://pic2.zhimg.com/v2-f5dd6cfaabce299a8341b9e584a99311_b.jpg)

-   推荐型：边聊边推，设计推荐的prompt，一次调用，但依赖具体形态，聊天场景推荐对时延要求低，启用流式输出可以缓冲，而输入是输入提示这类场景，大模型的速度就堪忧了。

全部采用大模型后，以常规的对话系统为例，一次对话过程可能涉及1~10次大模型调用，这用户体验可想而知。

这种思路是把原来对话系统所有的功能都用生成式方法（自回归语言模型，GPT为代表）解决，实际上，生成式只是小部分，大部分任务是理解式任务（掩码语言模型，BERT为代表），如意图识别、槽位抽取、状态跟踪等输出空间有限，这时用生成式方法，天然就慢半拍。

实际落地时，新的问题出来了：
-   极度依赖prompt：提示语稍微变更下，加个空格，结果可能就相差十万八千里，每个场景都需要仔细调试prompt，换个模型又要重新开始。
-   速度慢：实在是慢，正常情况1-3s回复，如果句子长，要持续等待，直至流式输出结束。这对高并发、低时延要求的对话产品简直是“噩耗”。
-   不可控：即便在prompt里明确要求不要超过30字，结果LLM当成耳边风，还是会超出字数。任务型对话里的业务处理逻辑往往要求准确无误了。
-   幻觉：一本正经的胡说八道
-   黑盒：大模型到底是怎么执行对话策略的？不知道，充满了玄学意味，涌现到底是个啥？这对高度可控的场景十分致命。

对于速度问题，短期内只能依靠别的方法提速了，如：
-   各种模型加速推理技术
-   部分功能回退：如不适合生成式方法的NLU/DM
-   推进end2end方法：将多轮会话训入大模型，延续之前的end2end思路

### 方案二：AI Embedded 嵌入

在LLM基础上，加领域语料，增量预训练、微调，融入领域知识，根据业务场景，增加特殊逻辑

这部分涉及两部分工作
-   基座大模型训练：各行各业都在训自己的大模型，金融、医疗、教育、数字人，甚至宗教
-   业务场景落地：适配业务场景，升级或重构现有对话产品的局部

各行各业都在训自己的大模型，金融、医疗、教育、数字人，甚至宗教

人物个性模拟上
-   国外有 Character.ai，用户根据个人偏好定制 AI 角色并和它聊天；
-   国内有阿里的脱口秀版GPT——鸟鸟分鸟，并且已经在天猫精灵上为个人终端行业的客户做了演示

![](https://pic3.zhimg.com/v2-0b37c3a5fe96467fc16fc6cbf3c671aa_b.jpg)

应用场景很多，略

### 方案三：AI Agent 代理

抛弃过往模块化思路，站在任务角度，通过Agent去执行对话任务，如：

最近不少人不再卷大模型了，开始卷 AI Agents
-   LLM诞生之初，大家对于其能力的边界还没有清晰的认知，以为有了LLM就可以直通AGI了，路线: LLM -> AGI
-   过了段时间，发现LLM的既有问题（幻觉问题、容量限制…），并不能直接到达AGI，于是路线变成了: LLM -> Agent -> AGI
-   借助一个/多个Agent，构建一个新形态，继续实现通往AGI的道路。但这条路是否能走通，以及还面临着哪些问题，有待进一步验证。

由于大模型的出现，AI Agents 衍生出了一种新的架构形式: 《LLM Powered Autonomous Agents》
-   将最重要的「任务规划」部分或完全交由LLM，而做出这一设计的依据在于默认：LLM具有任务分解和反思的能力。

最直观的公式

> `Agent` = `LLM` + Planning + Feedback + Tool use

![](https://pic3.zhimg.com/v2-ad871f9f1bc3fcd67eebe51cb1bd1d56_b.jpg)

这种思路更加灵活，贴近AGI，想象空间巨大，成为继模型训练后又一个角斗场。

详见往期文章：[大模型智能体](https://mp.weixin.qq.com/s%3F__biz%3DMjM5ODY2OTQyNg%3D%3D%26mid%3D2649769408%26idx%3D1%26sn%3D9639a1efaca760b539d39af9d95192cc%26chksm%3Dbec3d8dd89b451cb1b10c39bc879bd6c3fbf7aff2ea711bbcb09961c2722dda8e81f31f8d3c0%26token%3D858594018%26lang%3Dzh_CN%23rd)


## LLM 开发范式

### 开发范式总结

![](https://pic3.zhimg.com/v2-c5748909f4dcbced967e8333f61345ce_b.jpg)

总结: `预训练`(pre-training) → `微调`(fine-tune) → `提示工程`(prompt engineering)
- （1）`pre-training`(`预训练`)： **通识教育**，教小孩认字、学算数、做推理，这个步骤产出基础大模型。
- （2）`fine-tune`(`微调`)：**专业课**，比如学法律的会接触一些法律的条款、法律名词；学计算机的会知道什么叫计算机语言。
- （3）`prompt engineering`(`提示工程`)：**职业训练**，AI应用的准确率要达到商用级别（超过人的准确率），就需要 prompt engineer，PE 重要性

有些场景中（2）可以省略。  

### 开发范式演进

LLM时代Prompt Engineer开发范式
- 第一层：**简单Prompt**: 即编写一个提示词（Prompt）去调用大模型，最简单。
- 第二层：**Plugin插件**: 用大模型插件（Plugin）去调各种API，以及**Function Call**。
- 第三层：Prompt Engineering **Workflow** + OpenAI API
  - 基于提示词工程的`工作流`（workflow）编排。AI应用就是基于工作流实现。
- 第四层：向量数据库**集成** `RAG`
  - 向量数据库包含数据特征值，落地方案如 VectorDB，包括做知识库、做客服机器人。
- 第五层：**AI Agents**, 让大模型自己做递归。
  - Agent 原理: AI自己对任务进行拆解，再进一步递归、继续拆解，直到有一步，AI可以将这个任务执行，最后再将任务合并起来，往前递归回来，合并为一个工程。
- 第六层：**领域模型 Domain Model**
  - 专业模型为什么重要？大参数**基础模型**的训练和推理成本非常高，而**专业模型**速度快、成本低、准确率高，因为有行业的高质量数据，所以准确率高；进而可以形成数据飞轮，形成自己的竞争优势。

## LLM 开发平台

给开发者提供定制Bot的能力

### LLM 能力局限

局限性
- LLM 具备通用领域能力，但特定领域能力不足或缺乏
- LLM 请求信息有限制
  - GPT-3.5: 4k → 16k
  - GPT-4: 8k → 32k → 128k
- LLM 无法调用外部服务、执行本地命令

### Bot组成

Bot常见组成要素
- Bot`基本信息`：name、description、icon
- **人设/指令**：又称 `system prompt`，以下简称 sp
- **工具**：`plugin`/`tool`，使用一方、三方工具，增强模型能力
  - 配合 LLM 的 Function Call 使用
- **工作流**：`workflow`，工具升级版，执行更加复杂的逻辑
- **记忆单元**：`Memory`
  - 短期记忆：Short Term Memory, `Profiles`
    - 以**变量**形式存在，直接嵌入 sp 中; 提供基础存储，持久化变量
      - 变量: <span style='color:blue'>taste='川菜'</span>
      - 使用: <span style='color:green'>附近有什么好吃的</span>, llm 从memory中召回饮食偏好，匹配附近川菜
    - 以**表格**形式存在，LLM 通过SQL存取数据
      - 读书笔记类Bot
      - 使用: <span style='color:blue'>刚读了毛选第五卷，记录下</span>
      - llm: <span style='color:green'>已经帮你保存到数据库了，读的是毛选，第五卷</span>
      - 分析: `INSERT INTO book_notes(name, section, note) VALUES('毛选','第五卷', '-')`
  - 长期记忆：Long Term Memory
    - `Datasets`: 以外挂知识库方式存在，将领域知识向量化存储，通过 RAG 方式召回，补充到 sp 中 
- **定时任务**：`Task`, 开发者设置定时任务，如每天8点推送最新消息
- **智能体**： `Agent`，执行更复杂的任务

其它
- 欢迎语：onboarding/opening dialogue
- 问题推荐：suggest


### Bot 进化史

过程
- (0) `Prompt`: 通过 sp 设置 bot 的画像信息，技能，按指定角色进行会话
- (1) `Tool-解析`: LLM + Parser
  - 让 LLM 输出 特定格式字符串(如json)，代码解析后再调用 Tools
- (2) `Tool-Function Call`: LLM 识别需要什么函数，并自动自动函数，总结结果并返回
- (3) `Workflow` 工作流: 
  - 起因: 
    - ① FC模式不稳定，单次成功率低
    - ② 逻辑简单, 负责逻辑需要嵌入code，成本高
  - 改进: 低代码模式，将llm,api,code编排起来
  - 节点: 
    - start 开始节点，定义输入数据
    - end 结束节点，处理结束数据
  - 组件: LLM, API, Code, Knowledge, IF
    - `LLM`: 大模型调用设置，包含请求方式(单次/批量)、参数(模型/温度)、输入、输出、prompt
    - `Code`: 代码模块, js代码后处理
    - `Knowledge`: 根据用输入召回n个片段，列表返回
    - `IF`: 条件分支
- (4) `Multi-Agent`: 
  - 起因: 
    - ① workflow 是plugin加强版，缺少用户交互
    - ② Bot执行依赖prompt理解,内容多，造成时间成本大
    - ③ 复杂交互逻辑用sp实现，成本高，难以维护
  - 解法:
    - `Agent` = `Prompt` + `Skill`(plugin/workflow) + `Jump Condition`
    - `Multi-Agent` 代理人模式: 每次由单个Agent回复，几种模式
      - Flow **对话流**: 顺序处理对话流程, 适合有明显状态迁移的Agent
      - Reverse **回溯**: 当前Agent无法作答时，回到上一个Agent/StartAgent
      - Host **路由**: 每次对话从StartAgent重新路由到合适的Agent
- (5)  


### 



# 结束