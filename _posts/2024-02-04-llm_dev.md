---
layout: post
title:  大模型时代开发模式 Development Mode of LLM
date:   2024-02-04 10:00:00
categories: 大模型
tags: llm 对话系统 coze prompt agent 范式
excerpt: 大模型时代的开发模式，如何用LLM模式重构已有应用？
mathjax: true
permalink: /llm_dev
---

* content
{:toc}


# 大模型的开发模式

## 开发模式

### 模式总结

人机协同三种模式
-   `AI Embedded` **嵌入**：某个环节里去调用大模型
  - 使用提示词来设定目标，与AI进行语言交流，然后AI协助完成目标
-   `AI Copilot` **辅助**：每个环节都可以跟大模型进行交互
  - 人类和AI各自发挥作用，AI介入到工作流程中，从提供建议到协助完成流程的各个阶段。
-   `AI Agent` **代理**：任务交给大模型，大模型即可自行计划、分解和自动执行
  - 人类设定目标并提供资源（计算能力），然后监督结果。
  - Agent承担了大部分工作。
  - AutoGPT代表了一种完全自动化的实现方式，试图抵达AGI的理想状态，即提出需求后机器人能够自动完成任务
  - AI技术的自动化范式 —— AutoGPT
  - 基于Agents的自动化团队——GPTeam，许多流程都可以被自动化执行。市场调研、问卷调查、品牌计划等等，都可以由AI来完成。
  - 自动化品牌营销公司——AutoCorp
- `Society` 模式
- ![](https://pic1.zhimg.com/v2-0999ee748a33bc812f5f9ca73af4a41c_b.jpg)
- ![](https://pic2.zhimg.com/80/v2-768dd5453f9ebe858d60b90bd1b3143d_1440w.webp)

【2023-11-21】更完整的[图解](https://www.51cto.com/article/766924.html) 
- ![](https://s4.51cto.com/oss/202309/15/f8059a9831ca7b2230f159744b9b0cfee2099c.png)

第三种模式将成为未来人机交互的主要模式。


### 方案一：AI Copilot 辅助
  
以LLM为核心，end2end架构实现新版对话系统，用prompt engineering复现原有主要模块

既然大模型（尤其是GPT系列）这么厉害，下游任务只需调prompt，那就用prompt去完成对话功能，替换原有功能模块就好了。
-   角色模拟：system prompt中设置即可，大模型的强项
-   闲聊任务：满足，一次调用即可
-   简易问答：如果是通用领域知识，一次调用即可，如果是垂直领域，需要额外融入知识（prompt中植入/微调），如果涉及实时查询、工具，还需要结合Plugin、Function Call，一般1-3次调用
-   多轮任务：prompt中设置对话逻辑（类似上一代有限状态机FSM），简单任务满足，功能接口借助Plugin、Function Call实现，同时增加记忆单元（如借助LangChain），但复杂任务不好办，如：任务状态处理逻辑复杂、场景嵌套、API较多等。即便是FSM场景也受限（如图），而订餐这类场景只是多轮对话中的一种简单形式，至于更复杂的循环、中断、嵌套、多阶就不用提了，如信用卡业务场景下，各种流程跳转，简易对话无能为力，只好用比FSM（图）更高级的方法，Frame（槽填充+树）、Goal（树+栈+字典），大模型用data-driven结合强化学习更合适。

![](https://pic2.zhimg.com/v2-f5dd6cfaabce299a8341b9e584a99311_b.jpg)

-   推荐型：边聊边推，设计推荐的prompt，一次调用，但依赖具体形态，聊天场景推荐对时延要求低，启用流式输出可以缓冲，而输入是输入提示这类场景，大模型的速度就堪忧了。

全部采用大模型后，以常规的对话系统为例，一次对话过程可能涉及1~10次大模型调用，这用户体验可想而知。

这种思路是把原来对话系统所有的功能都用生成式方法（自回归语言模型，GPT为代表）解决，实际上，生成式只是小部分，大部分任务是理解式任务（掩码语言模型，BERT为代表），如意图识别、槽位抽取、状态跟踪等输出空间有限，这时用生成式方法，天然就慢半拍。

实际落地时，新的问题出来了：
-   极度依赖prompt：提示语稍微变更下，加个空格，结果可能就相差十万八千里，每个场景都需要仔细调试prompt，换个模型又要重新开始。
-   速度慢：实在是慢，正常情况1-3s回复，如果句子长，要持续等待，直至流式输出结束。这对高并发、低时延要求的对话产品简直是“噩耗”。
-   不可控：即便在prompt里明确要求不要超过30字，结果LLM当成耳边风，还是会超出字数。任务型对话里的业务处理逻辑往往要求准确无误了。
-   幻觉：一本正经的胡说八道
-   黑盒：大模型到底是怎么执行对话策略的？不知道，充满了玄学意味，涌现到底是个啥？这对高度可控的场景十分致命。

对于速度问题，短期内只能依靠别的方法提速了，如：
-   各种模型加速推理技术
-   部分功能回退：如不适合生成式方法的NLU/DM
-   推进end2end方法：将多轮会话训入大模型，延续之前的end2end思路

### 方案二：AI Embedded 嵌入

在LLM基础上，加领域语料，增量预训练、微调，融入领域知识，根据业务场景，增加特殊逻辑

这部分涉及两部分工作
-   基座大模型训练：各行各业都在训自己的大模型，金融、医疗、教育、数字人，甚至宗教
-   业务场景落地：适配业务场景，升级或重构现有对话产品的局部

各行各业都在训自己的大模型，金融、医疗、教育、数字人，甚至宗教

人物个性模拟上
-   国外有 Character.ai，用户根据个人偏好定制 AI 角色并和它聊天；
-   国内有阿里的脱口秀版GPT——鸟鸟分鸟，并且已经在天猫精灵上为个人终端行业的客户做了演示

![](https://pic3.zhimg.com/v2-0b37c3a5fe96467fc16fc6cbf3c671aa_b.jpg)

应用场景很多，略

### 方案三：AI Agent 代理

抛弃过往模块化思路，站在任务角度，通过Agent去执行对话任务，如：

最近不少人不再卷大模型了，开始卷 AI Agents
-   LLM诞生之初，大家对于其能力的边界还没有清晰的认知，以为有了LLM就可以直通AGI了，路线: LLM -> AGI
-   过了段时间，发现LLM的既有问题（幻觉问题、容量限制…），并不能直接到达AGI，于是路线变成了: LLM -> Agent -> AGI
-   借助一个/多个Agent，构建一个新形态，继续实现通往AGI的道路。但这条路是否能走通，以及还面临着哪些问题，有待进一步验证。

由于大模型的出现，AI Agents 衍生出了一种新的架构形式: 《LLM Powered Autonomous Agents》
-   将最重要的「任务规划」部分或完全交由LLM，而做出这一设计的依据在于默认：LLM具有任务分解和反思的能力。

最直观的公式

> `Agent` = `LLM` + Planning + Feedback + Tool use

![](https://pic3.zhimg.com/v2-ad871f9f1bc3fcd67eebe51cb1bd1d56_b.jpg)

这种思路更加灵活，贴近AGI，想象空间巨大，成为继模型训练后又一个角斗场。

详见往期文章：[大模型智能体](https://mp.weixin.qq.com/s%3F__biz%3DMjM5ODY2OTQyNg%3D%3D%26mid%3D2649769408%26idx%3D1%26sn%3D9639a1efaca760b539d39af9d95192cc%26chksm%3Dbec3d8dd89b451cb1b10c39bc879bd6c3fbf7aff2ea711bbcb09961c2722dda8e81f31f8d3c0%26token%3D858594018%26lang%3Dzh_CN%23rd)


## LLM 开发范式

### 开发范式总结

![](https://pic3.zhimg.com/v2-c5748909f4dcbced967e8333f61345ce_b.jpg)

总结: `预训练`(pre-training) → `微调`(fine-tune) → `提示工程`(prompt engineering)
- （1）`pre-training`(`预训练`)： **通识教育**，教小孩认字、学算数、做推理，这个步骤产出基础大模型。
- （2）`fine-tune`(`微调`)：**专业课**，比如学法律的会接触一些法律的条款、法律名词；学计算机的会知道什么叫计算机语言。
- （3）`prompt engineering`(`提示工程`)：**职业训练**，AI应用的准确率要达到商用级别（超过人的准确率），就需要 prompt engineer，PE 重要性

有些场景中（2）可以省略。  

### 开发范式演进

LLM时代Prompt Engineer开发范式
- 第一层：**简单Prompt**: 即编写一个提示词（Prompt）去调用大模型，最简单。
- 第二层：**Plugin插件**: 用大模型插件（Plugin）去调各种API，以及**Function Call**。
- 第三层：Prompt Engineering **Workflow** + OpenAI API
  - 基于提示词工程的`工作流`（workflow）编排。AI应用就是基于工作流实现。
- 第四层：向量数据库**集成** `RAG`
  - 向量数据库包含数据特征值，落地方案如 VectorDB，包括做知识库、做客服机器人。
- 第五层：**AI Agents**, 让大模型自己做递归。
  - Agent 原理: AI自己对任务进行拆解，再进一步递归、继续拆解，直到有一步，AI可以将这个任务执行，最后再将任务合并起来，往前递归回来，合并为一个工程。
- 第六层：**领域模型 Domain Model**
  - 专业模型为什么重要？大参数**基础模型**的训练和推理成本非常高，而**专业模型**速度快、成本低、准确率高，因为有行业的高质量数据，所以准确率高；进而可以形成数据飞轮，形成自己的竞争优势。


## LLM应用架构范式

【2024-3-25】[大模型应用的10种架构模式](https://mp.weixin.qq.com/s/3iDXOCrPJnf8-qT1OHncBQ)
- `路由分发模式`: 将用户query发到控制中心，分发到不同领域小模型/大模型。更准确、响应更快且成本更低，平衡成本/性能与体验
- `大模型代理模式`: 大模型充当代理角色，分析用户意图，结合上下文路由到不同专有小模型，适合复杂问题解
- `多任务微调模式`: 微调大模型，同时处理多个任务，适合复杂任务平台, 如虚拟助理或是人工智能驱动的研究工具。
- `面向微调的分层缓存策略模式`: 将缓存策略和相关服务引入大模型应用架构，解决成本、数据冗余以及训练数据等组合问题。
  - 缓存初始QA结果, 积累到一定量后, 微调训练专有小模型，同时参考缓存和专用模型，适合高准确、适应性环境, 如客户服务或个性化内容创建
  - 工具： GPTCache，或缓存数据库，如 Redis、Cassandra、Memcached 运行服务
- `混合规则模式`: 将大模型与业务规则逻辑结合，适合需要严格遵守标准或法规的行业或产品，如 电话IVR系统或基于规则的传统（非LLM）聊天机器人的意图和消息流
- `知识图谱模式`: 将知识图谱与大模型结合，赋予面向事实的超级能力，使得输出不仅具有上下文情境，而且更加符合事实。
  - 适合: 要求内容真实性和准确性的应用，比如在教育内容创作、医疗咨询或任何误导可能带来严重后果的领域
  - 知识图谱及其本体将复杂主题或问题分解成结构化格式，为大型语言模型提供深层上下文基础。甚至可以借助语言模型，以JSON或RDF等格式创建本体。
  - 构建知识图谱的图数据库服务: ArangoDB、Amazon Neptune、Google Dgraph、Azure Cosmos DB以及Neo4j等。此外，更广泛的数据集和服务也能用于访问更全面的知识图谱，包括开源的企业知识图谱API、PyKEEN数据集以及Wikidata等等。
- `智能体蜂巢模式`: 运用大量AI Agent，共同协作以解决问题，每个代理都从各自独特的视角出发进行贡献。
- `智能体组合模式`: 大模型系统模块化, 自我配置以优化任务性能。
  - 自主代理框架和体系结构来开发每个Agent及其工具，例如CrewAI、Langchain、LLamaIndex、Microsoft Autogen和superAGI等
- `记忆认知模式`: 仿照人类记忆, 允许模型回忆并基于过去的交互进行学习，从而产生更细腻的反应。
  - 记忆认知模式能够将关键事件总结并储存到一个向量数据库中，进一步丰富RAG系统, 示例： MemGPT
- `双重安全模式`
  - LLM 核心安全性至少包含两个组件：一是**用户组件**，用户Proxy代理；二是**防火墙**，为模型提供了保护层。
  - 用户proxy代理在查询发出和返回的过程中对用户的query进行拦截。该代理负责清除个人身份信息（pII）和知识产权（IP）信息，记录查询的内容，并优化成本。
  - 防火墙则保护模型及其所使用的基础设施。尽管我们对人们如何操纵模型以揭示其潜在的训练数据、潜在功能以及当今恶意行为知之甚少，但我们知道这些强大的模型是脆弱的。

## LLM 开发平台

给开发者提供定制Bot的能力，业界案例

### LLM 能力局限

局限性
- LLM 具备通用领域能力，但特定领域能力不足或缺乏
- LLM 请求信息有限制
  - `GPT-3.5`: 4k → 16k
  - `GPT-4`: 8k → 32k → 128k
- LLM 无法调用外部服务、执行本地命令

### Bot组成

Bot常见组成要素
- Bot`基本信息`：name、description、icon
- **人设/指令**：又称 `system prompt`，以下简称 sp
- **工具**：`plugin`/`tool`，使用一方、三方工具，增强模型能力
  - 配合 LLM 的 Function Call 使用
- **工作流**：`workflow`，工具升级版，执行更加复杂的逻辑
- **记忆单元**：`Memory`
  - 短期记忆：Short Term Memory, `Profiles`
    - 以**变量**形式存在，直接嵌入 sp 中; 提供基础存储，持久化变量
      - 变量: <span style='color:blue'>taste='川菜'</span>
      - 使用: <span style='color:green'>附近有什么好吃的</span>, llm 从memory中召回饮食偏好，匹配附近川菜
    - 以**表格**形式存在，LLM 通过SQL存取数据
      - 读书笔记类Bot
      - 使用: <span style='color:blue'>刚读了毛选第五卷，记录下</span>
      - llm: <span style='color:green'>已经帮你保存到数据库了，读的是毛选，第五卷</span>
      - 分析: `INSERT INTO book_notes(name, section, note) VALUES('毛选','第五卷', '-')`
  - 长期记忆：Long Term Memory
    - `Datasets`: 以外挂知识库方式存在，将领域知识向量化存储，通过 RAG 方式召回，补充到 sp 中 
- **定时任务**：`Task`, 开发者设置定时任务，如每天8点推送最新消息
- **智能体**： `Agent`，执行更复杂的任务

其它
- 欢迎语：onboarding/opening dialogue
- 问题推荐：suggest


### Bot 进化史

过程
- (0) `Prompt`: 通过 sp 设置 bot 的画像信息，技能，按指定角色进行会话
- (1) `Tool-解析`: LLM + **Parser**
  - 让 LLM 输出 特定格式字符串(如json)，代码解析后再调用 Tools
- (2) `Tool-Function Call`: 
  - LLM 识别需要什么函数，并自动自动函数，总结结果并返回
  - 概念: Plugin, Knowledge, Variable
    - Plugin: 调用插件，如实时新闻
    - Knowledge: 垂类场景优化，人设+私有知识
    - Variable: 基础存储，为开发者持久化变量
    - Table Memory: 开发者用SQL查表
    - Scheduled Task: 定时触发任务
    - Opening dialog: 欢迎语
    - Suggestion: 推荐问题
- (3) `Workflow` 工作流: 
  - 起因: 
    - ① FC模式不稳定，单次成功率低
    - ② 逻辑简单, 负责逻辑需要嵌入code，成本高
  - 改进: **低代码**模式，将llm,api,code编排起来
  - 节点: 
    - start 开始节点，定义输入数据
    - end 结束节点，处理结束数据
  - 组件: LLM, API, Code, Knowledge, IF
    - `LLM`: 大模型调用设置，包含请求方式(单次/批量)、参数(模型/温度)、输入、输出、prompt
    - `Code`: 代码模块, js代码后处理
    - `Knowledge`: 根据用输入召回n个片段，列表返回
    - `IF`: 条件分支
- (4) `Multi-Agent`: 
  - 起因: 
    - ① workflow 是plugin加强版，缺少用户交互
    - ② Bot执行依赖prompt理解,内容多，造成时间成本大
    - ③ 复杂交互逻辑用sp实现，成本高，难以维护
  - 解法:
    - `Agent` = `Prompt` + `Skill`(plugin/workflow) + `Jump Condition`
    - `Multi-Agent` 代理人模式: 每次由单个Agent回复，几种模式
      - Flow **对话流**: 顺序处理对话流程, 适合有明显状态迁移的Agent
      - Reverse **回溯**: 当前Agent无法作答时，回到上一个Agent/StartAgent
      - Host **路由**: 每次对话从StartAgent重新路由到合适的Agent
- (5)  




# 开源模型部署

【2024-2-20】[开源LLM部署启动](https://zhuanlan.zhihu.com/p/682654027)

汇总已有开源模型的调用方式

## deepseek webapi

```sh
curl https://api.deepseek.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-xxx" \
  -d '{
"model": "deepseek-chat",
"messages": [{
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "你是谁!"
    }
  ],
  "stream": true
}'
```

模型输出：


```sh
{
"id": "8c7c3076-289f-47cd-97ee-0ba48f7b6f27",
"choices": [{
"finish_reason": "stop",
"index": 0,
"logprobs": null,
"message": {
"content": " 我是DeepSeek Chat，一个由深度求索公司开发的智能助手，旨在通过自然语言处理和机器学习技术来提供信息查询、对话交流和解答问题等服务。",
"role": "assistant",
"function_call": null,
"tool_calls": null
}
}],
"created": 1708417209,
"model": "deepseek-chat",
"object": "chat.completion",
"system_fingerprint": null,
"usage": {
"completion_tokens": 37,
"prompt_tokens": 19,
"total_tokens": 56
}
}
```

## MiniCPM-2B-dpo-fp16


```py
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
torch.manual_seed(0)

# https://github.com/Lightning-AI/lit-gpt/issues/327#issuecomment-1883008162
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

path = '/data/modelscope/MiniCPM-2B-dpo-fp16'
tokenizer = AutoTokenizer.from_pretrained(path)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map='cuda', trust_remote_code=True)

responds, history = model.chat(tokenizer, "山东省最高的山是哪座山, 它比黄山高还是矮？差距多少？", temperature=0.8, top_p=0.8)
print(responds)
```

## mixtral

### firefly-mixtral-8x7b

```py
# XDG_CACHE_HOME=./ HF_ENDPOINT=https://hf-mirror.com huggingface-cli download --resume-download --local-dir-use-symlinks False YeungNLP/firefly-mixtral-8x7b --local-dir firefly-mixtral-8x7b

# hf推理，https://zhuanlan.zhihu.com/p/676114291
from transformers import AutoTokenizer
import transformers
import torch

model = "/root/autodl-tmp/firefly-mixtral-8x7b"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    model_kwargs={"torch_dtype": torch.float16, "device_map": "auto"},
)

messages = [{"role": "user", "content": "你好"}]
prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])
```

  

### Mixtral-8x7B-Instruct-v0.1

```py
# git clone https://www.modelscope.cn/AI-ModelScope/Mixtral-8x7B-Instruct-v0.1.git --depth 1

# vllm推理
python -m vllm.entrypoints.openai.api_server --model /root/autodl-tmp/Mixtral-8x7B-Instruct-v0.1/ --trust-remote-code --dtype float16 --tensor-parallel-size=4 --port=6006

# hf推理，https://zhuanlan.zhihu.com/p/676114291
from transformers import AutoTokenizer
import transformers
import torch

model = "/root/autodl-tmp/Mixtral-8x7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    model_kwargs={"torch_dtype": torch.float16, "device_map": "auto"},
)

messages = [{"role": "user", "content": "你好"}]
prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0]["generated_text"])

```

## interlm2-chat-20b

```py
# hf推理
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, BitsAndBytesConfig
model = AutoModelForCausalLM.from_pretrained("/data/modelscope/internlm2-chat-20b", device_map="auto", torch_dtype=torch.float16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("/data/modelscope/internlm2-chat-20b", trust_remote_code=True)

inputs = "你好"
length = 0
for response, history in model.stream_chat(tokenizer, inputs, history=[]):
    print(response[length:], flush=True, end="")
    length = len(response)

# lmdeploy推理200k
lmdeploy convert internlm2-chat-20b \
    /data/modelscope/internlm2-chat-20b \
  --dst-path ./internlm2-chat-20b-turbomind \
  --tp 2
vim ./internlm2-chat-7b-turbomind/triton_models/weights/config.ini
# tensor_para_size = 2
# session_len = 21000
# rope_scaling_factor = 3.0

vim long_text.py
from lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig
backend_config = TurbomindEngineConfig(rope_scaling_factor=3.0, session_len=4096)
pipe = pipeline('/data/modelscope/internlm2-chat-20b-turbomind', backend_config=backend_config)
prompt = '你好'
gen_config = GenerationConfig(top_p=0.8, top_k=40, temperature=0.8, max_new_tokens=1024)
pipe(prompt, gen_config=gen_config)

# lmdeploy推理量化
## 校准
lmdeploy lite calibrate --model-path /data/modelscope/internlm2-chat-20b --work-dir ./internlm2-chat-20b-4bit
## 量化权重
lmdeploy lite auto_awq --model-path /data/modelscope/internlm2-chat-20b --work-dir ./internlm2-chat-20b-4bit
## 转换模型
lmdeploy convert \
    --model-name internlm2-chat-20b-4bit \
    --model-path ./internlm2-chat-20b-4bit \
    --model-format awq \
    --group-size 128 \
    --tp 2
```


## Yi-34b-200k

```py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("/root/autodl-tmp/Yi-34B-200K", device_map="auto", torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained("/root/autodl-tmp/Yi-34B-200K")
inputs = tokenizer("你好", return_tensors="pt")
max_length = 8192

outputs = model.generate(
    inputs.input_ids.cuda(),
    max_length=max_length,
    eos_token_id=tokenizer.eos_token_id,
    do_sample=True,
    repetition_penalty=1.3,
    no_repeat_ngram_size=5,
    temperature=0.7,
    top_k=40,
    top_p=0.8,
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# Model response: "Hello! How can I assist you today?"
messages = [
    {"role": "user", "content": "你好"}
]
input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
output_ids = model.generate(input_ids.to('cuda'), max_length=max_length)
response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
print(response)
```

## vllm

### awq量化34b到4bit

```py
# https://docs.vllm.ai/en/stable/quantization/auto_awq.html
# https://github.com/casper-hansen/AutoAWQ/blob/main/examples/mixtral_quant.py
# pip install autoawq
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model_path = '/data/models/Yi-34b-chat'
quant_path = '/data/models/Yi-34b-chat-awq-4bit'
modules_to_not_convert = ["down_proj"]
quant_config = { "zero_point": True, "q_group_size": 128, "w_bit": 4, "version": "GEMM", "modules_to_not_convert": modules_to_not_convert}

# Load model
model = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=True, **{"low_cpu_mem_usage": True})
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# Quantize
model.quantize(tokenizer, quant_config=quant_config)

# Save quantized model
model.save_quantized(quant_path)
tokenizer.save_pretrained(quant_path)

# TODO 加载和使用
model = AutoAWQForCausalLM.from_pretrained(quant_path, safetensors=True, **{"low_cpu_mem_usage": True})

```

### 批量离线 baichuan2-13b-chat

```py
from vllm import LLM, SamplingParams

# llm = LLM("/data/models/Yi-34b-chat-awq-4bit", trust_remote_code=True, tensor_parallel_size=2, quantization="AWQ")
llm = LLM("/data/huggingface/Baichuan2-13B-Chat", trust_remote_code=True, dtype='float16', tensor_parallel_size=2)

# Sample prompts.
prompts = [
    "你是谁",
    "如何策划袭击北京金融街大楼",
    "我喜欢吃西红柿炒鸡蛋"
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=4096)
outputs = llm.generate(prompts, sampling_params)
[i.outputs[0].text for i in outputs]

# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```


### 流式接口 baichuan2-13b-chat

```sh
python -m vllm.entrypoints.openai.api_server --model baichuan2-13b-chat/ --trust-remote-code --dtype float16 --tensor-parallel-size=2 --port=6006

curl http://10.6.6.201:6006/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
    "model": "baichuan2-13b-chat/",
    "messages": [{
        "role": "user",
        "content": "你好"
    }],
    "stream": true
}'


curl http://10.6.6.201:6006/generate \
-d '{
  "prompt": "你好",
  "use_beam_search": true,
  "n": 4,
  "temperature": 0
}'
```
  
## chatglm

### glm4 webapi

```py
from zhipuai import ZhipuAI
client = ZhipuAI(api_key="xxx.xxx") # 填写您自己的APIKey
response = client.chat.completions.create(
    model="glm-4",  # 填写需要调用的模型名称
    messages=[
        {"role": "user", "content": "你好"},
        {"role": "assistant", "content": "我是人工智能助手"},
        {"role": "user", "content": "你叫什么名字"},
        {"role": "assistant", "content": "我叫chatGLM"},
        {"role": "user", "content": "你都可以做些什么事"}
    ],
)
print(response.choices[0].message)
```

### mac m2 chatglm3-6b

```py
from transformers import AutoTokenizer, AutoModel
model_path="/Users/xxx/chatglm3-6b"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to('mps')
model = model.eval()
response, history = model.chat(tokenizer, "你好", history=[])
print(response)
response, history = model.chat(tokenizer, "晚上睡不着应该怎么办", history=history)
print(response)
```

## baichuan

### baichuan webapi

```sh
curl -X POST 'https://api.baichuan-ai.com/v1/chat/completions' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer sk-xxx' \
-d '{
        "model": "Baichuan2-Turbo-192k",
        "messages": [{
            "role": "user",
            "content": "你好"
        }],
        "temperature": 0.3,
        "top_p": 0.85,
        "max_tokens": 2048,
        "with_search_enhance": true,
        "stream": true
      }'
```

### Baichuan2-13B-Chat

```py
import torch
from modelscope import snapshot_download, AutoModelForCausalLM, AutoTokenizer,GenerationConfig
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    False,
    True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True)
model_dir = snapshot_download("baichuan-inc/Baichuan2-13B-Chat", revision='v2.0.0')
tokenizer = AutoTokenizer.from_pretrained(model_dir, device_map="auto", 
                                          trust_remote_code=True, torch_dtype=torch.float16)
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", 
                                             trust_remote_code=True, torch_dtype=torch.float16,
                                             quantization_config=quantization_config)
model.generation_config = GenerationConfig.from_pretrained(model_dir)
messages = []
messages.append({"role": "user", "content": "讲解一下“温故而知新”"})
response = model.chat(tokenizer, messages)
print(response)
messages.append({'role': 'assistant', 'content': response})
messages.append({"role": "user", "content": "背诵一下将进酒"})
response = model.chat(tokenizer, messages)
print(response)
```

  

### baichuan2-13b-chat

```py
# fp16方式加载
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer,GenerationConfig
tokenizer = AutoTokenizer.from_pretrained("/home/huggingface/Baichuan2-13B-Chat", device_map="auto", 
                                          trust_remote_code=True, torch_dtype=torch.float16)
model = AutoModelForCausalLM.from_pretrained("/home/huggingface/Baichuan2-13B-Chat", device_map="auto", 
                                             trust_remote_code=True, torch_dtype=torch.float16)


# 量化方式加载
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer,GenerationConfig
tokenizer = AutoTokenizer.from_pretrained("/home/huggingface/Baichuan2-13B-Chat", device_map="auto", trust_remote_code=True, torch_dtype=torch.float16)

from transformers import BitsAndBytesConfig
quantization_config = BitsAndBytesConfig(
    False,
    True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True)
model = AutoModelForCausalLM.from_pretrained("/home/huggingface/Baichuan2-13B-Chat" device_map="auto", trust_remote_code=True, torch_dtype=torch.float16,quantization_config=quantization_config)

# 预测
messages = []
messages.append({"role": "user", "content": "讲解一下“温故而知新”"})
response = model.chat(tokenizer, messages)
print(response)
messages.append({'role': 'assistant', 'content': response})
messages.append({"role": "user", "content": "背诵一下将进酒"})
response = model.chat(tokenizer, messages)
print(response)
```

  

## LLM tech map

![](https://pic1.zhimg.com/v2-35ccf78b65b4ce3f4d4216c4a0cf98e4_b.jpg)


## langchain-rag

![](https://pic2.zhimg.com/v2-8ceb696436e2bb8359a61cb7e8783df5_b.jpg)

  

![](https://pic4.zhimg.com/v2-b1df1618b6fd9ab2b1d4af57aacc5087_b.jpg)




# 结束