---
layout: post
title:  类脑计算
date:   2025-09-01 12:00:00
categories: 大模型 人工智能
tags: LLM 大模型 AGI 大脑 类脑 鹦鹉 意识
excerpt: 大模型发展方向之一：类脑计算
mathjax: true
permalink: /brain_compute
---

* content
{:toc}

# 类脑计算

## 背景

当大模型还在为处理更长文本扩充万亿参数时，人类大脑却能在稳定的生理消耗下，高效存储数十年的记忆与知识

当前 LLM可能已是传统机器学习范式的巅峰，其固有的**黑箱问题**是通往AGI的根本障碍。

必须重写机器学习的底层范式。


## 类脑实现


### 2024.7.11 Yan


【2024-7-11】 RockAI 推出 Yan 模型，放弃transformer架构, 探索类脑思路

改进点
- (1) transformer 换成 MCSD
  - 论文 [MCSD: An Ef?cient Language Model with Diverse Fusion](https://arxiv.org/pdf/2406.12230)
- (2) 局部模态激活
  - transformer架构: 问 1+1=?, 会激活所有参数, 算力消耗太大, 人脑不是这样
  - 类脑机制: 人脑按听说看等功能分区, 根据任务激活对应区域，其它区域处于抑制状态, 这样功耗很低, 才20w, 相当于电灯泡 

整体水平接近主流的transformer，部分性能超越
- 3b 模型, 大小5G，优化后，内存占用仅1G
- 端侧设备上运行，性能超过 transformer 30% 以上

问题
- 如何判断激活哪个区域? **仿真神经元选择算法**, 一个单独的小型神经网络, 随着训练的进行,从随机选择迭代到针对性选择
- 训练上有什么技巧? 

`Yan 1.3`: 群体智能单元大模型
- 训练效率提升7倍、推理吞吐量提升5倍、记忆能力提升3倍
- 秒级影响、非transformer结构、端到端多模态、满足大部分端侧设备
  - 国内能在手机cpu上运行LLM的公司不超过3家

现在大模型训练反常识：训练一个模型，花费的计算资源太多，有的甚至要启动核电站训练。

视频介绍
- [站起来了！国内这家AI公司用新技术挑战ChatGPT权威](https://www.bilibili.com/video/BV19LCUYuEKP/?spm_id_from=333.999.0.0&vd_source=ec1c777505e146eb20d947449d6bba6e) RockAI联创邹佳思


<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=113328533868595&bvid=BV19LCUYuEKP&cid=26349866723&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>

OpenAI GPT 在attention路上深耕，并非唯一出路。

改进
- 量化？
  - 文本模态上量化，能保留80-90%的效果，而图像、视频大幅度下滑
  - 量化后，权重固定，无法再学习

国内大模型机会
- 基础创新: 弯道超车的机会，卡脖子问题
  - deepseek 推出 MLA/O1复现
  - RockAI(岩山科技) 目标：把attention拿掉; 国内能在手机上运行的LLM不超过3家, Yan 模型解决端侧推理资源开销大的问题
  - 国内蹦出来一批LLM，原因是 Llama 开源了。。。META 计划闭源
  - 人才要求: 数学+算法都强，且愿意坐冷板凳
- 应用创新
  - 国内做应用很强
  - 人才要求：交叉学科背景，如 懂医学+AI

`斑马鱼`
- 只有几百万神经元，但避障能力非常强，这对智能驾驶很有启发
- 还不清楚大脑神经有没有量子效应。
如果斑马鱼神经网络有量子效应，那么鱼脑计算效率肯定是高效的，这在需要投入多少算力可能有的参考。

机器人
- 宇树科技、智源，机器人行业还需要5年沉淀

### 2024.8.25 内生复杂性类脑网络

【2024-8-25】[放弃Scaling Law！中科院、清北提出内生复杂性类脑网络：让AI像人脑一样“小而强”](https://mp.weixin.qq.com/s/BiR9DQcCdXVbN7L1SoEbXg)

如果 AI 模型像人脑一样，**规模小，耗能少**，但具备同样复杂功能，那现阶段 AI 模型训练的耗能大、难理解的瓶颈是不是就能解决了？

中国科学院自动化研究所`李国齐`、`徐波`研究员团队联合清华大学、北京大学等团队便取得突破
- 借鉴大脑神经元**复杂动力学**特性，提出“基于**内生复杂性**”的**类脑神经元模型**构建方法，而非基于 Scaling Law 去构建更大、更深和更宽的神经网络。
- 这种方法不仅改善了传统模型通过向外拓展规模带来的计算资源消耗问题，还保持了性能，内存使用量减少了 4 倍，处理速度提高了 1 倍。


研究论文
- “[Network model with internal complexity bridges artificial intelligence and neuroscience](https://www.nature.com/articles/s43588-024-00674-9)”， Nature Computational Science
- 共同通讯作者为中国科学院自动化所李国齐研究员、徐波研究员，北京大学田永鸿教授。共同一作是清华大学钱学森班的本科生何林轩（自动化所实习生），数理基科班本科生徐蕴辉（自动化所实习生），清华大学精仪系博士生何炜华和林逸晗。

李国齐解释说
- 构建更大、更复杂的神经网络的流行方法，称为“基于**外生复杂性**”，消耗了大量的能源和计算能力，同时缺乏可解释性。
- 相比之下，拥有 1000 亿个神经元和 1000 万亿个突触连接的人脑仅需 20 瓦的功率即可高效运行。

加州大学圣克鲁斯分校 Jason Eshraghian 团队在评论文章中表示，这一发现暗示了 AI 发展的潜在转变。尽管大语言模型（LLM）的成功展示了通过大量参数计数和复杂架构的外部复杂性的力量，但这项新的研究表明，增强内部复杂性可能提供了改善 AI 性能和效率的替代路径。

AI 中内部与外部复杂性之争仍然开放，两种方法在未来发展中都可能发挥作用。通过重新审视和深化神经科学与 AI 之间的联系，我们可能会发现构建更高效、更强大，甚至更“类脑”的 AI 系统的新方法。


效果怎么样？

首先展示了`脉冲神经网络神经元` `LIF`（Leaky Integrate and Fire）模型和 `HH`（Hodgkin-Huxley）模型在动力学特性上存在等效性，进一步从理论上证明了 HH 神经元可以和四个具有特定连接结构的时变参数 LIF 神经元（tv-LIF）动力学特性等效。

基于这种等效性，团队通过设计微架构提升计算单元的内生复杂性，使 HH 网络模型能够模拟更大规模 LIF 网络模型的动力学特性，在更小的网络架构上实现与之相似的计算功能。进一步，团队将由四个 tv-LIF 神经元构建的“HH 模型”（tv-LIF2HH）简化为 s-LIF2HH 模型，通过仿真实验验证了这种简化模型在捕捉复杂动力学行为方面的有效性。

结果表明，HH 和 s-LIF2HH 网络具有相似的噪声鲁棒性，而鲁棒性源自 HH 神经元的动态复杂性和 s-LIF2HH 的复杂拓扑，而不仅仅是神经元数量。这表明，模型内部复杂性与外部复杂性之间具有等效性，并且它们在深度学习任务中比具有简单动力学增加规模的模型有更加明显的优势。

局限性 
- HH 和 s-LIF2HH 模型在深度学习实验中具有不同的脉冲模式，这表明模拟中近似的动态特性可能不是它们可比性的良好解释。这种现象可能源于它们基本单元（HH 神经元和 s-LIF2HH 子网络）固有的相似复杂性。
- 此外，由于神经元**非线性**和**脉冲机制**的局限性，本研究仅在小型网络中进行了，未来将研究更大规模的网络和单个网络中多种神经元模型的影响。


### 【2024-12-17】天琴

【2024-12-17】[全球首台100亿神经元类脑异构融合智算在横琴诞生](https://pc.nfnews.com/38828/10354522.html)

从人脑中借鉴运作原理，启发创造类脑智能技术，再反哺到人脑机制和神经医学的研究中去，这样的良性循环，让参与本次研讨会并进行现场考察的与会代表印象深刻。

2024年12月17日，“2024年类脑智算创新产品发布会暨神经医学大模型研讨会”在横琴举办。广东省智能科学与技术研究院（下称“广东省智能院”）发布第二代`天琴芯`类脑处理芯片`LYRA-β Max`、第二代`天琴`类脑晶圆计算芯片`LYRA-β eXtreme`、类脑计算卡、高密度类脑算力服务器等创新产品。

类脑智能计算芯片方面，类脑芯片联合实验室本次发布了第二代天琴芯类脑处理芯片LYRA-β Max，进一步拓展脉冲神经元计算规模达460万，计算性能较实验室上一代成果提升约2.7倍。实验室还在单张标准尺寸PCIe卡上实现多颗LYRA-β Max类脑芯片的互联集成和分布式计算，研发出可支持脉冲神经元计算规模最大达2600万以上的类脑计算卡。

不仅如此，实验室采用全新一代晶圆级集成技术，基于自主研发的存算融合、事件触发、线性可扩展的类脑计算架构，推出了第二代天琴类脑晶圆计算芯片LYRA-β eXtreme，单芯片脉冲神经元计算规模达4亿以上，持续刷新类脑算力纪录。

集成与配套技术方面，由智能计算系统联合实验室迭代推出的类脑血管相变散热系统，高效模拟人脑血管散热模式，相较市场上的风冷技术可减少87%的散热能耗，相较液冷技术可减少45%的散热能耗。实验室融合了自研的超高算力密度整机集成、类脑血管相变液冷、无风扇高功率氮化镓电源等技术，推出了高密度类脑算力服务器，可支持单机4亿以上脉冲神经元计算规模。

### 【2025-3-14】上海交大 BriLLM

BriLLM及其 SiFu 学习框架，并非对现有模型的改良，而是基于大脑宏观工作原理, 对学习机制的一次彻底重构。

上海交通大学团队为AGI发展开辟一条根植于生物智能、完全透明且高效的新路。

【2025-3-14】上海交大首发「类人脑」大模型 BriLLM，彻底脱离 Transformer架构，打造全新类脑大语言模型 BriLLM。
- 论文: [BRILLM: BRAIN-INSPIRED LARGE LANGUAGE MODEL](https://arxiv.org/abs/2503.11299v7)
- 代码: [BriLLM0.5](https://github.com/brillm05/BriLLM0.5)
- 模型: [BriLLM0.5](https://huggingface.co/BriLLM/BriLLM0.5)
- 解读: [上海交大发布全球首个“类人脑”大模型，引领机器学习新范式](https://zhuanlan.zhihu.com/p/1940787321860526971)

采用全新的`信号全连接流动`（SiFu）学习范式，从根本上重塑了机器学习的基础。
- ![](https://pic1.zhimg.com/v2-d1b2723c50ca1d6b095e4183a0d7836c_1440w.jpg)

为解决现有 Transformer 模型面临的**黑箱不透明**、**二次方复杂度**、**上下文长度依赖**等核心局限

亮点
- 范式创新：SiFu学习是对主流深度学习范式的一次勇敢颠覆，其追求可解释性和生物 plausibility 的立意非常具有前瞻性。
- 可解释性强：“节点即语义”的设计从根本上解决了黑箱问题。如果模型出错，理论上可以追溯到是哪条信号通路（即哪个词元组合的连接）出了问题，如同对大脑进行功能成像。
- 理论优雅：模型的设计原则与宏观神经科学发现高度契合，理论框架自洽且具有很强的解释力。

### 原理


BriLLM 融合了两大关键的神经认知原理：大脑处理信息依赖的量大原则
- （1）**静态语义映射**：将词元（token）精确映射到类似大脑皮层功能区的特化节点；
  - 不同语义概念（如“狗”、“爱”、“肉”）被映射到大脑皮层的特定、固定的区域
- （2）**动态信号传播**：模拟电生理信息流在节点间的流动来完成计算。
  - 认知过程由电生理信号在这些功能区之间传播、整合而产生

SiFu机制是对这两大原则的计算模拟。

这一架构实现了三大突破：
- 完全的模型可解释性
- 与上下文长度无关的模型扩展性
- 首次对类脑处理进行全局尺度的模拟。

要点
- 提出SiFu学习新范式：
  - 最根本的创新：提出`信号全连接流动`（Signal Fully-connected flowing, `SiFu`）学习范式。抛弃了传统深度学习的“黑箱”计算模式，转而模拟大脑的宏观信息处理机制。
  - 这标志着从传统机器学习向类脑学习的范式级演进。
- 构建完全可解释的BriLLM模型：
  - 基于SiFu范式，BriLLM 每个计算节点（Node）都唯一且静态地对应一个具体的词元（Token）。
  - 模型的每一部分都有明确的语义，计算过程（即信号在节点间的流动）变得完全透明，实现了“认知过程可追溯性”，从根本上解决了AI的可解释性难题。
- 实现上下文无关的模型扩展：
  - BriLLM的模型大小仅与词汇量和节点维度有关，而与处理的上下文长度完全无关。
  - 这与Transformer架构中计算/内存复杂度随上下文长度二次方增长（O(L^2)）形成鲜明对比。BriLLM在处理超长序列上具有天然的、类似大脑的效率优势。
- 首次实现宏观尺度的大脑模拟：
  - 不同于以往仅借鉴神经元等微观特征的研究，BriLLM是首个在系统层面对大脑进行宏观功能（静态语义分区 + 动态信号整合）模拟的计算模型，为探索更接近生物智能的AGI提供了具体的工程蓝图。

示例
- 输入"dog"、"love"，模型通过计算信号流向各个候选词（"meat"、"apple"等）的能量，选择能量值最高的"meat"（0.89）作为预测输出。
- ![](https://pic2.zhimg.com/v2-34e05c3646d2ab8bb206377475bc9b05_1440w.jpg)

BriLLM架构：如何实现信号流动？

BriLLM是SiFu机制的具体实现



### 效果

初步 10-20亿参数模型已展现出与GPT-1相当的生成能力和稳定的学习动态。

可扩展性分析证实，将模型扩展至1000-2000亿参数规模并处理4万词元上下文是完全可行的。

结果分析：
- 学习稳定性：训练损失曲线平稳下降，证明模型能够有效学习语言模式
- 生成能力：从续写案例（下表）可以看出，模型具备了基础的上下文理解和生成能力，能够生成语法通顺、语义相关的文本。
  - 例如，输入“阿根廷探戈是起源于”，模型续写“阿根廷探戈是起源于阿根廷或乌拉圭”。作者称，这一能力达到了早期GPT-1的核心生成水准。
- 可扩展性：即便将词汇量扩大到4万（与现代LLM相当），通过稀疏训练也可将BriLLM的参数控制在1000-2000亿，与主流LLM相当，同时保持其长文本处理的独特优势

BriLLM 为构建基于生物学原理的AGI建立了全新范式。

B站视频：[突破Transformer！交大首发「类人脑」大模型BriLLM](https://www.bilibili.com/video/BV1nNYQz2Et5/?vd_source=ec1c777505e146eb20d947449d6bba6e)

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=115064489117007&bvid=BV1nNYQz2Et5&cid=31837194637&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>





### 【2025-8-2】浙大 悟空

2025年8月2日，浙江大学**脑机智能**全国重点实验室发布新一代**神经拟态**类脑计算机—`Darwin Monkey`（“悟空”）。
- [浙大官方](https://mp.weixin.qq.com/mp/wappoc_appmsgcaptcha?poc_token=HBHjkmijWbiYpUsOLeqQlN2U6CUiS8W-2RpumQ41&target_url=https%3A%2F%2Fmp.weixin.qq.com%2Fs%3F__biz%3DMjM5NDgxNTQwNQ%3D%3D%26mid%3D2650972801%26idx%3D1%26sn%3Db1728c1fb15e92725e0c0f20bffbd919%26scene%3D21#wechat_redirect)
- ![](https://pic1.zhimg.com/v2-466d36df390f5aeaef837272580cec46_1440w.jpg)

该成果是浙江大学类脑计算团队继2020年9月份研制成功我国首台亿级神经元类脑计算机Darwin Mouse（“米奇”）之后，取得又一重要突破。

“悟空”
- 由15台刀片式神经拟态类脑服务器组成，每台刀片式类脑服务器内部集成了64颗达尔文3代类脑计算芯片。
- `脉冲神经元`规模超过20亿，神经突触超过千亿，其神经元数量已接近猕猴大脑规模，在典型运行状态下功耗约为2000瓦。

![](https://pic2.zhimg.com/v2-25a3809fb8257e2061d956080b33d333_1440w.jpg)

“悟空”
- 不仅能运行DeepSeek，完成逻辑推理、内容生成和数学求解等智能任务
- 还能模拟秀丽线虫、斑马鱼、小鼠以及猕猴等不同神经元规模的动物大脑，为脑科学研究提供了新的手段。
- ![](https://picx.zhimg.com/v2-2e271814de4ee4d2f26d6cc3e14d1c45_1440w.jpg)

这是国际首台神经元规模超过20亿的基于专用神经拟态芯片的类脑计算机。

人脑是一部极其高效的“计算机”，能举一反三、融会贯通，可处理视觉、听觉、语言、学习、推理、决策、规划等各类任务。

类脑计算仿照生物神经网络工作原理，设计计算机系统，构建像大脑一样低功耗、高并行、高效率、智能化的计算系统。

还团队还研制了新一代达尔文类脑操作系统，采用分层资源管理架构，通过负载感知调度算法与动态时间片划分机制等技术，实现了神经拟态任务的高效并发调度与系统资源的动态优化。


### 【2025-9-5】瞬悉

【2025-9-10】[用国产GPU训练的国产大模型来了！能耗暴降97.7%](https://news.qq.com/rain/a/20250910A089GA00)

2025年9月5日，中国科学院自动化研究所发布`类脑脉冲`大模型“`瞬悉1.0`”（SpikingBrain-1.0）的技术报告。

SpikingBrain-7B 开源模型仅用主流大模型 **2%** 的预训练数据，就实现了 Qwen2.5-7B **90%** 的性能，并与Llama-3.1-8B等众多开源Transformer模型相媲美的性能。

我国首次提出大规模**类脑线性**基础模型架构，也是我国首次在国产GPU算力集群上构建类脑脉冲大模型的训练和推理框架。


#### 起因

为什么需要新型非Transformer架构的大模型？

SpikingBrain 联合团队
- Transformer 架构面临固有缺点：训练计算开销随序列长度呈**平方级**增长，推理时的显存占用也随序列长度线性增加，带来海量资源消耗。这限制了模型处理超长序列（100万个token以上的序列）的能力。
- Transformer 架构本质上依赖“外生复杂性”，即通过堆叠更多神经元和更大规模计算来提升智能水平。
  - 与此对比，人脑以极低的能耗（约为20W）实现了高度复杂的智能，其神经元具有丰富的内部动力学与多样性。
  - 大模型或许存在另一条“内生复杂性”的发展路径，通过充分利用生物神经网络在神经元和神经环路上的结构和功能特性，打造下一代模型架构。

#### 脉冲神经网络

低功耗`脉冲神经网络`（SNN）方案，被认为是通往更通用AI系统的新一代低功耗类脑神经网络方案之一。

其工作方式与大脑类似，只在需要的时候发送信号，因此功耗较低。
- 复杂的脉冲神经元可以用几个小神经元组合来实现同样的效果，这让构建高效的类脑网络成为可能。

基于上述理论研究，SpikingBrain 团队在模型架构中集成了混合高效注意力、MoE模块和脉冲编码三大核心组件。
- 1、**混合高效注意力**
  - 注意力机制是大语言模型的核心计算单元。SpikingBrain整合了不同注意力机制的优势，7B版本模型采用层间混合的线性注意力与SWA，兼顾全局信息检索和局部依赖。
  - 而更大规模的SpikingBrain-76B则使用层内并行混合，将线性、SWA与全量softmax注意力结合，同一层中并行运行多种注意力机制，可高效处理全局信息、局部依赖和长程依赖。
- 2、**混合专家模块**
  - SpikingBrain从Qwen2.5-7B-Base（稠密模型）扩展而来。为了在现有稠密模型的基础上高效扩展，得到稀疏的混合专家模型，SpikingBrain团队使用了上采样（Upcycling）技术。
  - 这一方法的核心是通过参数复制和输出缩放，使扩展后的模型在初始状态下与原模型保持一致，从而避免性能损失。
- 3、**脉冲神经元**
  - 脉冲神经元是脉冲神经网络的基本单元。工程应用中常见的LIF（Leaky Integrate-and-Fire）模型，能在一定程度上模拟生物神经元的核心特性。但LIF存在神经元过度沉默或过度激活问题，从而影响模型精度与能效的平衡。


SpikingBrain 团队提出了**自适应阈值**脉冲神经元（Adaptive-threshold Spiking Neurons），可保持神经元适度激活，避免过度兴奋或静息。

#### 训练

训练过程中，SpikingBrain团队将 Qwen2.5-7B-Base 转换为类脑脉冲大模型，主要包含3个环节。
- 持续预训练和长序列扩展中，模型使用了约150B tokens的数据，将序列长度从8K逐步扩展至128K。其训练数据量仅占从头训练所需的2%，实现了高效模型转换。
- 监督微调环节中，通过使用不同领域的数据集以及由DeepSeek-R1蒸馏得到的高质量推理数据集，模型在通用知识、对话和推理等方面的能力逐步提升。
- 模型还需要经过脉冲化编码。受生物神经系统启发，SpikingBrain团队提出将大模型的连续激活值转换为整数脉冲序列的策略。

推理阶段，整数脉冲计数会被展开成稀疏脉冲序列，以适配事件驱动计算。

SpikingBrain提供三种编码方式：
- 二值脉冲简单低能耗；
- 三值脉冲支持类似生物神经系统的兴奋-抑制调控，减少时间步和脉冲总数；
- 二进制脉冲可在高计数场景下显著降低计算量和能耗。

SpikingBrain 仍然选择了在国产沐曦GPU集群上进行训练，沐曦软件平台通过MoE优化、计算通信并行、显存优化、算子融合和自动调优等手段实现适配。
这一适配过程包括Triton适配、CUDA向MACA（沐曦兼容CUDA的软件栈）框架迁移两部分。这两条路径针对模型内部不同算子进行优化，结合形成适用于沐曦GPU的硬件适配方案。


#### 效果

经过三阶段SFT对齐训练后，SpikingBrain-76B在通用知识、长序列建模及指令跟随能力上，与同量级开源对话模型相当，同时保持预训练获得的通用能力，未出现过拟合现象，显示了架构在对齐训练中的稳定性和可扩展性。

SpikingBrain-7B 在多个基准测试上恢复了基座模型 Qwen2.5-7B 约90%的性能，整体水平与Mistral-7B、Llama-3-8B等先进Transformer模型相当，表明高效线性注意力在降低推理复杂度的同时仍能保持较强的建模能力。

SpikingBrain 训练和推理全过程均在**国产算力**上完成，使用沐曦股份曦云C550 GPU组成的集群。
- 训练过程中，集群连续运行2周未中断，这也证明了构建国产自主可控的新型非Transformer大模型架构生态的可行性。

除了极高的数据效率之外，SpikingBrain还在**推理效率**上实现数量级提升。
- 100万个token上下文场景下，SpikingBrain-7B 生成首个token的耗时，比Qwen2.5-7B降低了96.2%。
- 这一特性也使得SpikingBrain尤其适合超长序列处理任务，如在法律和医学文档分析、复杂多智能体模拟、高能粒子物理实验、DNA序列分析、分子动力学轨迹等。
- 能耗方面，该模型平均乘加运算能耗相比传统FP16和INT8运算，分别降低了97.7%和85.2%。

SpikingBrain-76B混合线性MoE模型几乎完全恢复了基座模型性能。

在长序列推理场景中，SpikingBrain-7B模型在100万个token长度下TTFT（生成第一个Token所需时间）相比Transformer架构加速达到26.5倍，400万Token长度下加速超过100倍。

训练性能方面，7B模型在128K序列长度下的训练吞吐量为Qwen2.5-7B的5.36倍，这与推理性能提升基本一致。

同时在手机CPU端64K、128K、256K长度下，SpikingBrain较Llama3.2的同规模模型推理速度分别提升4.04倍、7.52倍、15.39倍。

#### 模型

SpikingBrain-1.0 共有7B参数量和76B参数量两个版本。

2025年9月3日
- 7B版本的模型已在GitHub、魔搭等平台开源。
- 76B版本的模型暂未开源，但提供了体验链接。

- 开源地址：[SpikingBrain-7B](https://github.com/BICLab/SpikingBrain-7B)
- 技术报告：[SpikingBrain_Report_Chi.pdf](https://github.com/BICLab/SpikingBrain-7B/blob/main/SpikingBrain_Report_Chi.pdf)
- 体验链接：[demo](https://controller-fold-injuries-thick.trycloudflare.com/)



# 结束
