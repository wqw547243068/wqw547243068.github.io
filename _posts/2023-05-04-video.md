---
layout: post
title:  "视频理解 - Video Understanding"
date:   2023-05-04 08:01:00
categories: 计算机视觉
tags: 视频理解 ffmpeg 视频 moviepy lux you-get 扩散模型 sora
excerpt: 视频理解，文生视频技术
mathjax: true
permalink: /video
---

* content
{:toc}

# 视频

【2023-5-5】 [FesianXu](github.com/FesianXu): [万字长文漫谈视频理解](https://zhuanlan.zhihu.com/p/158702087)

作为多媒体中重要的信息载体，视频的地位可以说是数一数二的，然而目前对于AI算法在视频上的应用还不够成熟，理解视频内容仍然是一个重要的问题亟待解决攻克。

## 什么是视频

### 视频为什么重要？

以视频为代表的动态多媒体，结合了音频，视频，是当前的，更是未来的**互联网流量之王**。 

国家互联网信息办公室的中国互联网络发展状况统计报告
>- 截至 2018 年 12 月，网络视频、网络音乐和网络游戏的用户规模分别为 6.12 亿、5.76 亿和 4.84 亿，使用率分别为 73.9%、69.5%和 58.4%。短视频用户规模达 6.48 亿，网民使用比例为 **78.2%**。
>- 截至 2018 年 12 月，网络视频用户规模达 6.12 亿，较 2017 年底增加 3309 万，占网民 整体的 73.9%；手机网络视频用户规模达 5.90 亿，较 2017 年底增加 4101 万，占手机 网民的 72.2%。

2018年各类应用使用时长占比
- ![](https://pic3.zhimg.com/80/v2-6a77c9189cfad7ccaa9dba9ed5cf38ee_1440w.webp)
- 包括短视频在内的视频用户时长占据了约20%的用户时长，占据了绝大多数的流量，同时网络视频用户的规模也在逐年增加。

固定带宽/4G平均下载速率变化曲线
- ![](https://pic3.zhimg.com/80/v2-a0c8b32b0c72774ade5a1500eb47673a_1440w.webp)

## 视频理解

理解视频(understanding the video) 是一件非常抽象的事情，在神经科学尚没有完全清晰，如果按照人类感知去理解，终将陷入泥淖。

在理解视频这个任务中，到底在做什么？
- 首先，对比于文本，图片和音频，视频特点：动态的按照时间排序的图片序列
- 然而，图片**帧**间有着密切的联系，存在上下文联系；
- 视频有音频信息。

因此进行视频理解，先要再时间序列上建模，同时还需要空间上的关系组织。

目前理解视频主要集中在**以人为中心**的角度进行，由于视频是动态的，因此描述视频中的物体随着时间变化，在进行什么动作，很重要。
- **动作识别**在视频理解中占据了一个很重要的地位。

视频分析的主要难点：
- 需要大量的算力，视频的大小远大于图片数据，需要更大的算力进行计算。
- 低质量，很多真实视频拍摄时有着较大的运动模糊，遮挡，分辨率低下，或者光照不良等问题，容易对模型造成较大的干扰。
- 需要大量的数据标签，特别是在深度学习中，对视频的时序信息建模需要海量的训练数据才能进行。时间轴不仅仅是添加了一个维度那么简单，其对比图片数据带来了时序分析，因果分析等问题。

### 子任务

理解视频具体子任务：
- 视频**动作分类**：对视频中的动作进行分类
- 视频**动作定位**：识别原始视频中某个动作的开始帧和结束帧
- 视频**场景识别**：对视频中的场景进行分类
- **原子动作**提取
- 视频**文字说明**（Video Caption）：给给定视频配上文字说明，常用于视频简介自动生成和跨媒体检索
- **集群**动作理解：对某个集体活动进行动作分类，常见的包括排球，篮球场景等，可用于集体动作中关键动作，高亮动作的捕获。
- 视频**编辑**。
- **视频问答**系统（Video QA）：给定一个问题，系统根据给定的视频片段自动回答
- 视频**跟踪**：跟踪视频中的某个物体运动轨迹
- 视频**事件理解**：不同于动作，动作是一个更为短时间的活动，而事件可能会涉及到更长的时间依赖
- ...


## 视频数据


### 视频数据模态

视频动作理解是非常广阔的研究领域，输入的视频形式也不一定是常见的**RGB视频**，还可能是depth**深度图序列**，Skeleton**关节点信息**，IR**红外光谱**等。
- ![](https://pic3.zhimg.com/80/v2-34fd2b816579389d8dc46e7c8d705ab2_1440w.webp)

**RGB视频**是最容易获取的模态，然而随着很多**深度摄像头**的流行，**深度图序列**和**骨骼点序列**的获得也变得容易起来。
- 深度图和骨骼点序列对比RGB视频来说，其对光照的敏感性较低，数据冗余较低，有着许多优点。

### 视频动作分类数据集

公开的视频动作分类数据集有很多，比较流行的`in-wild数据集`主要是在YouTube上采集到的，包括以下的几个。
- HMDB-51，该数据集在YouTube和Google视频上采集，共有6849个视频片段，共有51个动作类别。
- UCF101，有着101个动作类别，13320个视频片段，大尺度的摄像头姿态变化，光照变化，视角变化和背景变化。
  - ![](https://pic4.zhimg.com/80/v2-ee25f80c4c52c9ef7559a94f22dde737_1440w.webp)
- sport-1M，也是在YouTube上采集的，有着1,133,157 个视频，487个运动标签。
  - ![](https://pic4.zhimg.com/80/v2-b10e40e4aaf656e3d366a11235ba2e33_1440w.webp)
- YouTube-8M, 有着6.1M个视频，3862个机器自动生成的视频标签，平均一个视频有着三个标签。
- YouTube-8M Segments，是YouTube-8M的扩展，其任务可以用在视频动作定位，分段（Segment，寻找某个动作的发生点和终止点），其中有237K个人工确认过的分段标签，共有1000个动作类别，平均每个视频有5个分段。该数据集鼓励研究者利用大量的带噪音的视频级别的标签的训练集数据去训练模型，以进行动作时间段定位。
  - ![](https://pic1.zhimg.com/80/v2-d134999c9d2c1fc4f9ae9923fc8bb7a0_1440w.webp)
- Kinectics 700，这个系列的数据集同样是个巨无霸，有着接近650,000个样本，覆盖着700个动作类别。每个动作类别至少有着600个视频片段样本。

以上数据集模态都是RGB视频，还有些数据集是多模态的：
- NTU RGB+D 60： 包含有60个动作，多个视角，共有约50k个样本片段，视频模态有RGB视频，深度图序列，骨骼点信息，红外图序列等。
- NTU RGB+D 120：是NTU RGB+D 60的扩展，共有120个动作，包含有多个人-人交互，人-物交互动作，共有约110k个样本，同样是多模态的数据集。


## 视频理解方法


### 早期CV方法


深度学习之前，CV算法工程师是**特征工程师**，手动设计特征，而这是一个非常困难的事情。

手动设计特征并且应用在视频分类的主要套路有：
- 特征设计：挑选合适的特征描述视频
  - 局部特征（Local features）：比如HOG（梯度直方图 ）+ HOF（光流直方图）
  - 基于轨迹的（Trajectory-based）：Motion Boundary Histograms（MBH）[4]，improved Dense Trajectories （iDT） ——有着良好的表现，不过计算复杂度过高。
- 集成挑选好的局部特征： 光是局部特征或者基于轨迹的特征不足以描述视频的全局信息，通常需要用某种方法集成这些特征。
  - 视觉词袋（Bag of Visual Words，BoVW），BoVW提供了一种通用的通过局部特征来构造全局特征的框架，其受到了文本处理中的词袋（Bag of Word，BoW）的启发，主要在于构造词袋（也就是字典，码表）等。
  - ![](https://pic2.zhimg.com/80/v2-c5105cbf72aca4c7e49cb67212d2ba99_1440w.webp)
  - Fisher Vector，FV同样是通过集成局部特征构造全局特征表征
  - ![](https://pic4.zhimg.com/80/v2-4951d630efffccb82d5c9923e98f3ba7_1440w.webp)
  - 要表征视频的时序信息，我们主要需要表征的是动作的运动（motion）信息，这个信息通过帧间在时间轴上的变化体现出来，通常我们可以用光流（optical flow）进行描述，如TVL1和DeepFlow。
  - ![](https://pic3.zhimg.com/80/v2-82fe89075d26b6c676fe50f5ae3a4eee_1440w.webp)


### 深度学习CV方法

深度学习时代，视频动作理解主要工作量在于如何设计合适的**深度网络**，而不是手动设计特征。设计这样的深度网络的过程中，需要考虑两个方面内容：
- 模型方面：什么模型可以最好的从现有的数据中捕获时序和空间信息。
- 计算量方面：如何在不牺牲过多的精度的情况下，减少模型的计算量。

组织时序信息是构建视频理解模型的一个关键点，Fig 3.2展示了若干可能的对多帧信息的组织方法。
- Single Frame，只是考虑了当前帧的特征，只在最后阶段融合所有的帧的信息。
- Late Fusion，晚融合使用了两个共享参数的特征提取网络（通常是CNN）进行相隔15帧的两个视频帧的特征提取，同样也是在最后阶段才结合这两帧的预测结果。
- Early Fusion，早融合在第一层就对连续的10帧进行特征融合。
- Slow Fusion，慢融合的时序感知野更大，同时在多个阶段都包含了帧间的信息融合，伴有层次（hierarchy）般的信息。这是对早融合和晚融合的一种平衡。

最终的预测阶段，从整个视频中采样若各个片段，对这采样的片段进行动作类别预测，其平均或者投票将作为最终的视频预测结果。
- ![](https://pic3.zhimg.com/80/v2-901a384d969876d7038f02dc395c092a_1440w.webp)


## 视频生成

文生视频模型通常在非常短的视频片段上进行训练，需要使用计算量大且速度慢的**滑动窗口**方法来生成长视频。

因此，训得的模型难以部署和扩展，并且在保证上下文一致性和视频长度方面很受限。

文生视频的任务面临着多方面的独特挑战。主要有：
- 计算挑战： 确保帧间空间和时间一致性会产生长期依赖性，从而带来高计算成本，使得大多数研究人员无法负担训练此类模型的费用。
- 缺乏高质量的数据集： 用于文生视频的多模态数据集很少，而且通常数据集的标注很少，这使得学习复杂的运动语义很困难。
- 视频字幕的模糊性： “如何描述视频从而让模型的学习更容易”这一问题至今悬而未决。为了完整描述视频，仅一个简短的文本提示肯定是不够的。一系列的提示或一个随时间推移的故事才能用于生成视频。

- 【2024-1-19】[一文纵览文生图/文生视频技术发展路径与应用场景](https://mp.weixin.qq.com/s/pOLIf6JVQ_b8v3T6LcA7Fg)

### 文生视频发展

主流文生视频技术发展路径
- 1、早期发展（2016 年以前）
- 2、奠基任务：GAN/VAE/flow-based （2016-2019 年）
  - 早期研究主要使用基于 GAN 和 VAE 的方法在给定文本描述的情况下自回归地生成视频帧 （如 Text2Filter 及 TGANs-C）。
  - 虽然这些工作为文生视频这一新计算机视觉任务奠定了基础，但它们的应用范围有限，仅限于低分辨率、短距以及视频中目标的运动比较单一、孤立的情况。
  - GAN: 模型参数量小，较轻便，所以更加擅长对单个或多个对象类进行建模。
  - 但由于其训练过程的不稳定性，针对复杂数据集则极具挑战性，稳定性较差、生成图像缺乏多样性。
  - GAN 代表作：VGAN、TGAN、VideoGPT、MoCoGAN、DVD-GAN、DIGAN
- 3、自回归模型及扩散模型生成阶段 （2019-2023）
  - 与 GANs 相比，自回归模型具有明确的密度建模和稳定的训练优势，自回归模型可以通过帧与帧之间的联系，生成更为连贯且自然视频。
  - 但是自回 归模型受制于计算资源、训练所需的数据、时间，模型本身参数数量通常比扩散模型大，对于计算资源要求及数据集的要求往往高于其他模型。
  - 但因为 transformer 比 diffusion 更适合 scale up，且视频的时间序列结构很适合转化为预测下一帧的任务形态。
  - 自回归模型发展三个阶段：
    - 早期 **逐像素** 视觉合成：早期自回归模型，生成质量差、成本高，仅用于低分辨率图像视频，示例有 PixelCNN,PixelRNN,Image Transformer,Video Transformer,iGPT
    - VQ-VAE 出现，预训练广泛应用: 离散视觉标记化方法使得高效大规模训练用于图像视频合成，示例有 GODIVA,VideoGPT
    - 视频当做图像的时间序列，降低成本，但可能效果不佳；示例有 NUWA, CogVideo, Phenaki
- 4、未来发展趋势（2024-?）
- 5、视频生成模型 mapping


### 扩散模型

扩散模型已成为 AI 视频生成领域的主流技术路径，由于扩散模型在图像生成方面的成功，其启发了基于扩散模型的视频生成的模型。

经典扩散模型

|模型名称|发布时间|发布组织|介绍|
|---|---|---|---|
|Video Diffusion Model|2022.4|Google|支持图像和视频数据的联合训练,减少小批量梯度的方差并加快优化,生主成长和更高分辨率的视频。|
|Make-A-Video|2022.9|Meta|利用联合文本-图像先验,绕过对配对文本一视频数据的需求,潜在地扩展到更多的视频数据。|
|Imagen Video|2022.1|Google|采用级联扩散视频模型,验证了在高清视频生成中的简洁性和有效性,文本生成图像设置中的冻结编码器文本调节和无分类器指导转移到视频生成仍然有效。|
|Tune-A-Video|2022.12|新加坡国立,腾讯|使用预训练T2l模型生成T2V的框架,引入了用于T2V生成的一次性视频页调谐,消除了大规模视频数据集训练的负担,提出了有效的注意力调整和结构反转,可以显著提高时间一致性。|
|Gen-1|2023.2|Runway|将潜在扩散模型扩展到视频生成,通过将时间层引入到预训练的图像模型中并对图像和视频进行联合训练,无需额外训练和预处理。|
|Dreamix|2023.2|Google|提出了第一个基于文本的真实视频外观和运动编辑的方法,通过一种新颖的混合微调模型,可以显著提高运动编辑的质量。通过在简单的图像预处理操作之上应用视频编辑器方法,为文本引导的图像动画提供新的框架品。|
|NUWA-XL|2023.3|MRSA|"扩散超过扩散"的架构,"从粗到细"生成长视频,支持并行推理,这大大加快了长视频的生成速度。|
|Text2Video-Zero|2023.3|UT Austin, U of Oregon|提出零样本的文本生成视频的方法,仅使用预先训练的文本到图像扩散模型,而无需任何进一步的微调或优化,通过在潜在代码中编码运动动力学,并使用新的跨顿注意力重新编程每个侦的自我注意力,强制执行时间一致的生成。|
|VideoLDM|2023.4|NVIDIA|利用预先训练的图像DM并将其转换为视频生成器通过插入学习以时间一致的方式对齐图像的时间层,提出了一种有效的方法用于训练基于LDM的高高分辨率、长期一致的视频生成模型。|
|PYoCo|2023.5|NVIDIA|提出一种视频扩散噪声,用于微调文本到视频的文本到图像扩散模型,通过用噪声先验微调预训练的eDi-l模型来构建大规模的文本到视频扩散模型,并实现最先进的结果。|


## Text2Video

视频生成
- Deepfake
- VideoGPT
- GliaCloud
- ImageVideo

### 文生视频总结

`Text-to-Video`: `Phenaki` 、 `Soundify`
- `Phenaki` 由谷歌打造，基于新的编解码器架构C-ViViT将视频压缩为离散嵌入，能够在时空两个维度上压缩视频，在时间上保持自回归的同时，还能自回归生成任意长度的视频
- `Soundify` 是 Runway 开发的一个系统，目的是将声音效果与视频进行匹配，即制作音效。具体包括分类、同步和混合三个模块，首先模型通过对声音进行分类，将效果与视频匹配，随后将效果与每一帧进行比较，插入对应的音效。

AIGC [视频生成工具汇总](https://aigc.cn/#term-635)
- [artflow](https://artflow.ai), 支持换人、背景、音色。 换脸，卡通，真人，图像，跟着内容自动变换表情
- [synthesia](https://www.synthesia.io/free-ai-video-demo#SalesPitchNew)
- [invideo](https://invideo.io/make/add-text-to-video-online/): Text to video maker, Convert Blog and Article to Videos

### D-ID

[D-ID](https://www.d-id.com/): Digital People Text-to-Video
- Create and interact with talking avatars at the touch of a button, to increase engagement and reduce costs.
- [Studio](https://studio.d-id.com/)

制作过程介绍：
- [由AI制作视频----ChatGPT+MidJourney+d-id](https://www.toutiao.com/video/7203362904022712832/?log_from=de42079a586b7_1681187887298)

#### 复现故人

【2023-4-11】[上海小伙用AI技术“复活”已故奶奶：讲着方言 像生前一样“唠叨”](https://hb.ifeng.com/c/8OscmJLl1H5)
- 上海一位24岁的00后视觉设计师，他用AI工具生成了奶奶的虚拟数字人，并和她用视频对话。
- ![](https://x0.ifengimg.com/ucms/2023_15/6887B0A44072FDA2D5979B0D2D1AC87F3885762F_size9679_w568_h320.gif)

### 【META】Make-A-Video

【2022-10】Meta公布了一个能够生成高质量短视频的工具——[Make-A-Video](https://makeavideo.studio/)，利用这款工具生成的视频非常具有想象力。
- [Introducing Make-A-Video: An AI system that generates videos from text](https://ai.facebook.com/blog/generative-ai-text-to-video/)
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/4bc8759f50f747ff99ac4cd92eb2816b~noop.image)

### 【谷歌】Imagen Video与Phenaki

【2022-10-8】[图像生成卷腻了，谷歌全面转向文字→视频生成](https://www.toutiao.com/article/7151774108186083843)，挑战分辨率和长度; 文本转图像上卷了大半年之后，Meta、谷歌等科技巨头又将目光投向了一个新的战场：文本转视频。

谷歌公司 CEO Sundar Pichai 亲自安利了这一领域的最新成果：两款文本转视频工具——`Imagen Video` 与 `Phenaki`。
- `Imagen Video`主打视频品质
- `Phenaki`主要挑战视频**长度**，可以说各有千秋。

- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/d85eca195ae541da80c8b7b93d247fa7~noop.image)

生成式建模在最近的文本到图像 AI 系统中取得了重大进展，比如 `DALL-E 2`、`Imagen`、`Parti`、`CogView` 和 `Latent Diffusion`。
- 扩散模型在密度估计、文本到语音、图像到图像、文本到图像和 3D 合成等多种生成式建模任务中取得了巨大成功。

谷歌要做的是从文本生成视频。
- 以往的视频生成工作集中于: 具有**自回归模型**的受限数据集、具有自回归先验的**潜变量模型**以及近来的**非自回归潜变量**方法。扩散模型也已经展示出了出色的中等分辨率视频生成能力。

谷歌推出了 [Imagen Video](https://imagen.research.google/video/)，[论文地址](https://imagen.research.google/video/paper.pdf),它是一个基于级联视频扩散模型的文本条件视频生成系统。
- 给出文本提示，Imagen Video 就可以通过一个由 frozen T5 文本编码器、基础视频生成模型、级联时空视频超分辨率模型组成的系统来生成高清视频。


### 【阿里】大模型

【2023-3-22】阿里达摩院已在AI模型社区“魔搭”ModelScope上线了“文本生成视频大模型”。
- 整体模型参数约17亿，目前只支持英文输入。扩散模型采用Unet3D结构，通过从纯高斯噪声视频中，迭代去噪的过程，实现视频生成的功能。
- 模型还不支持中文输入，而且生成的视频长度多在2-4秒，等待时间从20多秒到1分多钟不等，画面的真实度、清晰度以及长度等方面还有待提升。
- 扩散模型采用 Unet3D 结构，通过从纯高斯噪声视频中，迭代去噪的过程，实现视频生成的功能
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/d8270a324501430a92d7b341c88a6f93~tplv-obj:256:256.image?_iz=97245&from=post&x-expires=1687392000&x-signature=CWJsHaBungH1EZQ8TnuAFKHo2Dw%3D)
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/4585b6cb44064c84bc92b49330b87af4~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681786689&x-signature=MI%2FjQcOy6i1UfPS%2FJEHP3ue2Lqc%3D)
- [体验地址](https://modelscope.cn/studios/damo/text-to-video-synthesis/summary)


### 视频风格化

【2022-9-7】通过将预训练的语言图像模型（pretrained language-image models）调整为视频识别，以此将对比语言图像预训练方法（contrastive language-image pretraining）扩展到视频领域；
- 为了捕捉视频中帧沿时间维度的远程依赖性，提出了一个跨帧的注意力机制，明确了跨帧的信息交换。此外该模块非常轻量化，可以无缝插入预训练的语言图像模型。
- [项目地址](https://github.com/microsoft/videox)
- [论文地址](https://arxiv.org/abs/2208.02816)


### 【Runway】Gen-2

【2023-4-11】视频领域的Midjourney，[AI视频生成新秀Gen-2内测作品流出](https://www.toutiao.com/article/7220311597607109172), 博主 Nick St. Pierre
- 论文：[Structure and Content-Guided Video Synthesis with Diffusion Models](https://arxiv.org/abs/2302.03011)
- [Gen-2](https://research.runwayml.com/gen2)还处于婴儿期，后面一定会更好。

【2023-11-4】[Gen-2颠覆AI生成视频！一句话秒出4K高清大片，网友：彻底改变游戏规则](https://mp.weixin.qq.com/s/GnTncBzzSuydrXgRhbBaCg)

Gen-2，迎来了“iPhone时刻”般的史诗级更新 —— 依旧是简单一句话输入，不过这一次，视频效果一口气拉到了4K超逼真的高度

跟 PIKA这位AI生成视频顶流相比，Gen-2 目前无论是在画质的清晰度，视频的流畅度等方面，都是更胜一筹。

视频生成的AI工具
- 2023年2月，诞生 Gen-1
- 2023年3月20日，发布（论文3月11号）Gen-2，带来了八大功能：
  - 文生视频、文本+参考图像生视频、静态图片转视频、视频风格迁移、故事板（Storyboard）、Mask（比如把一只正在走路的小白狗变成斑点狗）、渲染和个性化（比如把甩头小哥秒变海龟人）。

提示：
> Gen-1已经可以开始玩了(125次机会用完之后就只能按月付费了），Gen-2还没有正式对公开放。

背后的公司Runway成立于2018年，为《瞬息全宇宙》特效提供过技术支持，也参与了Stable Diffusion的开发（妥妥的潜力股）。

一句话拍大片, 只凭一句提示词就能完成，不需要借鉴其它图片和视频。
- 提示词: “一个身材匀称or对称（symmetrical）的男人在酒吧接受采访”
- 生成结果：只见一个身着深色衬衣的男人正望着对方侃侃而谈，眼神和表情透露着一股认真和坦率，对面的人则时不时点头以示附和。
- 视频
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/ea2d0cf1dfb04ea89ddc8f08c102777a~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681786689&x-signature=PbyMYIc86AaaHRXgBMd72K3NQ4I%3D)
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/dced5a1a3de047ebbe5859932cfe25d3~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681786689&x-signature=SeHvpIdxeoheyHWTNVZt5G%2FKBfI%3D)


### 【微软】 NUWA-XL

微软亚研院最新发布了一个可以根据文字生成超长视频的AI：NUWA-XL。

只用16句简单描述，它就能get一段长达11分钟的动画：
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/ae8148f3988748cb9311049f69fa80ec~noop.image?_iz=58558&from=article.pc_detail&x-expires=1681786689&x-signature=EItKdYur12hOgkGcYdSNyTtBL%2Fs%3D)

### Invideo

Invideo 将任何内容或想法转换为视频


### EasyPhoto 肖像动画

【2023-11-8】[EasyPhoto : 让你的AIGC肖像动起来](https://zhuanlan.zhihu.com/p/665725712?utm_psn=1705720579523686400)

[EasyPhoto](https://github.com/aigc-apps/sd-webui-EasyPhoto) 一个基于SDWebUI生态的**AIGC插件**，专门用于生成真/像/美的AI写真

EasyPhoto 支持用户上传少量自己的图片，快速训练个性化的人像**Lora模型**，并进行多种肖像生成。

最近基于AnimateDiff提出的运动先验模型，拓展了EasyPhoto的肖像生成能力，使之能够生成动态的肖像。
- ![](https://pic2.zhimg.com/80/v2-d01d77478dc225baa78dbd4fcc7c03f1_1440w.webp)


- 文到视频: 
  - ![](https://pic3.zhimg.com/v2-828e089d51a1421f380ea78256dbe3c2_b.webp)
- 图到视频: 用户上传首图或首尾图，以完成指定人像的生成, 支持基于真人的风格转换、切换
  - ![](https://pic3.zhimg.com/v2-801aaa76ad68acb5d6a44cad08f26bea_b.webp)
- 视频到视频: 模板换脸功能也可以非常自然地应用于视频
  - ![](https://pic2.zhimg.com/v2-5a36f2426f13ca8709f8187eb6f9eec5_b.webp)

### Stable Video Diffusion


【2023-11-22】[Stable Video Diffusion来了，代码权重已上线](https://www.toutiao.com/article/7304115172606706217)

AI 画图的著名公司 Stability AI，终于入局 AI 生成视频了。产品已经横跨图像、语言、音频、三维和代码等多种模态

本周二，基于 Stable Diffusion 的视频生成模型 Stable Video Diffusion 来了，AI 社区马上开始了热议。
- 论文地址：[stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasetss](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)
- 项目地址：[generative-models](https://github.com/Stability-AI/generative-models)

现在，可以基于原有的**静止图像**来生成一段几秒钟的**视频**。

基于 Stability AI 原有的 Stable Diffusion 文生图模型，Stable Video Diffusion 成为了开源或已商业行列中为数不多的视频生成模型之一。
- ![](https://p26-sign.toutiaoimg.com/tos-cn-i-6w9my0ksvp/9ceb891041ce46da929fd11bdc0b9fc7~noop.image?_iz=58558&from=article.pc_detail&lk3s=953192f4&x-expires=1701257165&x-signature=2O6dyCcY4wn9h0sq4iW86rmUcWA%3D)

Stable Video Diffusion 以两种图像到视频模型的形式发布，能够以每秒 3 到 30 帧之间的可定制帧速率生成 14 和 25 帧的视频。

在外部评估中，Stability AI 证实这些模型超越了用户偏好研究中领先的闭源模型（runway、pika Labs）


### pika

Pika Labs （[pika.art](https://pika.art/)） 是一款创新的视频创建工具，可以将文本和图像转换为引人入胜的视频
- An idea-to-video platform that brings your creativity to motion
- [elon musk video](https://cdn.pika.art/entry.mp4)

Pika Labs 推出了创新的文本和图像转视频平台，只需打字即可激发你的创造力。这个平台让你能够将旅程中的图像转换为 Discord 上引人入胜的视频！测试阶段是免费
- 【2023-8-13】[Pika Labs 新魔法：一键将图像转为视频](https://zhuanlan.zhihu.com/p/649709202)
- 【2023-11-29】[Pika Labs发布1.0版本AI视频生成器，满足多种风格视频需求](https://www.4anet.com/p/11v7bd7d2bc837e4)，Pika 1.0采用了全新的AI模式，能够以3D动画、动漫、卡通和电影等多种风格生成和编辑视频。

体验方式
- 点击[链接](https://discord.com/invite/pika) 加入 Pika
- 进入 Pika Labs Discord 服务器，请前往“#generate”频道
- 使用“/create”命令添加图像以及提示说明
- ![](https://pic4.zhimg.com/80/v2-e52a1cf2bcef17921b706bfed3b27077_1440w.webp)

与 Gen-2 不同的是，文本将提供对视频创建的更大控制，PikaLabs 可以更好地结合文本以与图像完美配合
- ![](https://pic3.zhimg.com/v2-ea0b2d162d0be38ded9a7f3a3c40b112_b.jpg)


### animate anyone

【2023-11-28】阿里发布animate anyone简直逆天，一张照片生成任意动作视频
- 论文：[Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](https://arxiv.org/pdf/2311.17117.pdf)
- github: [AnimateAnyone](https://github.com/HumanAIGC/AnimateAnyone)
- 演示[视频](https://humanaigc.github.io/animate-anyone/)

实现方法
- ![](https://humanaigc.github.io/animate-anyone/static/images/f2_img.png)

<iframe width="560" height="315" src="https://www.youtube.com/embed/8PCn5hLKNu4?si=Lv_0-iwvvkjBycrB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

### MagicAnimate

字节跳动新开源基于SD 1.5的 MagicAnimate，只需要一张照片和一组动作，就能生成近似真人的舞蹈视频。

开源地址：
- [magicanimate](https://showlab.github.io/magicanimate/)
- github [magic-animate](https://github.com/magic-research/magic-animate)

以后在社交平台上看到的小姐姐舞蹈短视频很可能就是AI生成的。
- ![](https://showlab.github.io/magicanimate/assets/figure/Framework.png)
- [视频地址](https://showlab.github.io/magicanimate/assets/app/multi/multi_dancing.mp4)

```sh
# sd-vae-ft-mse
git lfs clone https://huggingface.co/stabilityai/sd-vae-ft-mse
# stable-diffusion-v1-5
git lfs clone https://huggingface.co/runwayml/stable-diffusion-v1-5
# MagicAnimate 模型
git lfs clone https://huggingface.co/zcxu-eric/MagicAnimate
```

### WALT

【2023-12-12】斯坦福 [李飞飞谷歌破局之作！用Transformer生成逼真视频，下一个Pika来了？](https://mp.weixin.qq.com/s/T4wGCB2aX-3eilUakKFJtw)
- 论文：[Photorealistic Video Generation with Diffusion Models](https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf)
- [Photorealistic Video Generation with Diffusion Models](https://walt-video-diffusion.github.io)

支持
- Text-to-Video Examples [video](https://walt-video-diffusion.github.io/assets/t2v_webm_wm/desert_swan.webm)
- Image-to-Video Examples [video](https://walt-video-diffusion.github.io/assets/i2v_webm_wm/spaceship.webm)
- Consistent 3D Camera Motion

英伟达高级科学家Jim Fan转发评论道：
- 2022年是影像之年
- 2023是声波之年
- 而2024，是视频之年！

- 首先，研究人员使用因果编码器在共享潜在空间中压缩图像和视频。
- 其次，为了提高记忆和训练效率，研究人员使用基于窗口注意力的Transformer架构来进行潜在空间中的联合空间和时间生成建模。
  - 研究人员的模型可以根据自然语言提示生成逼真的、时间一致的运动：

两个关键决策，组成三模型级联

W.A.L.T的方法有两个关键决策。
- 首先，研究者使用**因果编码器**在统一的潜在空间内联合压缩图像和视频，从而实现跨模态的训练和生成。
- 其次，为了提高记忆和训练效率，研究者使用了为空间和时空联合生成建模量身定制的窗口注意力架构。
  - 通过这两个关键决策，团队在已建立的视频（UCF-101 和 Kinetics-600）和图像（ImageNet）生成基准测试上实现了SOTA，而无需使用无分类器指导。
- 最后，团队还训练了三个模型的级联，用于文本到视频的生成任务，包括一个基本的潜在视频扩散模型和两个视频超分辨率扩散模型，以每秒8帧的速度，生成512 x 896分辨率的视频。

W.A.L.T的关键是将图像和视频编码到一个共享的潜在空间中。
- ![](https://walt-video-diffusion.github.io/assets/images/system_fig.png)

Transformer主干通过具有两层窗口限制注意力的块来处理这些潜在空间——空间层捕捉图像和视频中的空间关系，而时空层模拟视频中的时间动态，并通过身份注意力掩码传递图像。

### Sora

当大家还沉迷如何用文生文，文生图时，OpenAI掏出来了一个视频生成模型Sora。
- Sora 根据**文本指令**或**静态图像**生成长达1分钟视频的扩散模型
- 视频中还包含精细复杂的场景、生动的角色表情以及复杂的镜头运动 —— 目前市面上视频模型做不到

#### Sora 介绍

OpenAI全新发布文生视频模型[Sora](https://openai.com/sora)
- 经纬创投: [一石激起千层浪，揭秘Sora的技术报告](https://mp.weixin.qq.com/s/sTcXw9oWo7rV5_97I8VgXg)

从皮肤纹理到瞳孔睫毛，Sora的还原度达到了「没有AI味」

特点: “60s超长长度”、“单视频多角度镜头”和“世界模型”
- **60s超长视频**: 
  - 其它AI视频还挣扎在4s连贯性的边缘，OpenAI直接60s
- **单视频多角度**镜头: 
  - 当前AI工作流都是**单镜头**单生成，一个视频里面有多角度的镜头，主体还能保证完美的一致性，这无法想象
  - OpenAI直接一句Prompt，在一分钟的镜头里，实现了多角度的镜头切换...而且...物体一致...
- **世界模型**: 
  - 世界模型最难的是收集、清洗数据。
  - Runway 世界模型，毫无动静。
  - 但是OpenAI的Sora，直接来了一波大的，已经能懂物理规律了。

OpenAI [Sora](https://openai.com/sora)

目前Sora只面向邀请的制作者和安全专家开放测试，还没有公测时间表。

而堪称「世界模型」的技术报告仍然没有公开具体的训练细节。
- 技术报告: Video generation models as world simulators（视频生成模型作为世界模拟器）

Sora训练数据源未知，“ClosedAI”原则，并没有透露相关信息。

#### 观点

英伟达高级研究科学家Jim Fan认为：
> Sora是一款数据驱动的物理模拟引擎，通过一些去噪和梯度计算来学习复杂的渲染、「直觉」物理、长远规划推理和语义基础。它直接输入文本/图像并输出视频像素，通过大量视频、梯度下降，在神经参数中隐式地学习物理引擎，它不会在循环中显式调用虚拟引擎5，但虚拟引擎5生成的（文本、视频）对有可能会作为合成数据添加到训练集中。

LeCun表示：
>「仅根据文字提示生成逼真的视频，并不代表模型理解了物理世界。生成视频的过程与基于世界模型的因果预测完全不同」。

#### Sora 原理

Sora是一种扩散模型，通过从一开始看似静态噪声的视频出发，经过多步的噪声去除过程，逐渐生成视频。Sora不仅能够一次性生成完整的视频，还能延长已生成的视频。通过让模型能够预见多帧内容，团队成功克服了确保视频中的主体即便暂时消失也能保持一致性的难题。

Sora模型逐渐拥有了一项新能力，叫做**三维一致性**。
- Sora能够生成动态视角的视频。同时随着视角的移动和旋转，人物及场景元素在三维空间中仍然保持一致的运动状态。
- AI理解三维物理世界跟人类方式不一样，它采用了一种拓扑结构上的理解。
- 视频的视角发生变化，那么相应的纹理映射也要改变。Sora 真实感非常强，纹理映射在拓扑结构上就得非常准确。
- 三维一致性能力使Sora能够模拟来自现实世界中人物、动物和环境的某些方面。


OpenAI为了训练出Sora，将各类视觉数据转化为**统一表示**。
- `块`（patches）, 类似于大语言模型中的token，块将**图像**或**视频帧**分割成的一系列小块区域。这些块是模型处理和理解原始数据的基本单元。
- 对于视频生成模型而言，`块`不仅包含了局部的**空间**信息，还包含了**时间**维度上的连续变化信息。模型可以通过学习patches之间的关系来捕捉运动、颜色变化等复杂视觉特征，并基于此重建出新的视频序列。

这样的处理方式有助于模型理解和生成视频中的连贯动作和场景变化，从而实现高质量的视频内容生成。

OpenAI 在块的基础上，将其压缩到**低维度潜在空间**，再将其分解为“`时空块`”（spacetime patches）。
- 潜在空间 是一个能够在复杂性降低和细节保留之间达到近乎最优的平衡点，极大地提升了视觉保真度
- `时空块`从视频帧序列中提取出, 具有固定大小和形状的**空间-时间区域**。相较于`块`，`时空块`强调了**连续性**，模型通过`时空块`来观察视频内容随时间和空间的变化规律。

为了制造这些`时空块`，OpenAI训练了一个网络，降低视觉数据的维度，叫做`视频压缩网络`。这个网络接受原始视频作为输入，并输出一个在时间和空间上都进行了压缩的潜在表示。Sora在这个压缩后的潜在空间中进行训练和生成视频。同时，OpenAI也训练了一个相应的解码器模型，用于将生成的潜在向量映射回像素空间。

“块”非常接近token，这些块的作用也应该和token差不太多。
- 对于给定的压缩输入视频，OpenAI 直接提取一系列块作为Transformer token使用
- 然后这些时空块会被进一步编码并传递给Transformer网络进行全局自注意力学习。
- 最后利用Transformer的强大能力来处理并生成具有不同属性的视频内容。

这一方案同样适用于图像，因为图像可以看作是仅有一帧的视频。

基于块的表示方法使得Sora能够对不同分辨率、时长和宽高比的视频和图像进行训练。在推理阶段，可以通过在一个适当大小的网格中排列随机初始化的块来控制生成视频的尺寸。

此外，Sora 支持其他类型的输入，比如：图像或视频，以达到图片生成视频、视频生成视频的效果。这一特性使得Sora能够执行广泛的图像和视频编辑任务——例如制作完美循环播放的视频、为静态图像添加动画效果、向前或向后延展视频时间轴等。



#### 不足

Sora对涌现物理的理解脆弱，并不完美，仍会产生严重、不符合常识的幻觉，还不能很好掌握物体间的相互作用。

Sora作为一个模拟器存在着不少局限性。
- 模拟基本物理交互（如玻璃破碎）时的准确性不足
  - 无法准确模拟许多基本交互的物理过程，以及其他类型的交互，比如吃食物。物体状态的变化并不总是能够得到正确的模拟，这说明很多现实世界的物理规则是没有办法通过现有的训练来推断的。
- 长视频中出现的逻辑不连贯或物体会无缘无故地出现。
  - 比如，随着时间推移，有的人物、动物或物品会消失、变形或者生出分身；
  - 或者出现一些违背物理常识的闹鬼画面，像穿过篮筐的篮球、悬浮移动的椅子。如果将这些镜头放到影视剧里或者作为精心制作的长视频的素材，需要做很多修补工作。

#### Case

Prompt: 

```py
A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.
# 一位时尚女性走在充满温暖霓虹灯和动画城市标牌的东京街道上。她穿着黑色皮夹克、红色长裙和黑色靴子，拎着黑色钱包。她戴着太阳镜，涂着红色口红。她走路自信又随意。街道潮湿且反光，在彩色灯光的照射下形成镜面效果。许多行人走来走去。

# Prompt: 多镜头
A beautiful silhouette animation shows a wolf howling at the moon, feeling lonely, until it finds its pack.
# 一个美丽的剪影动画展示了一只狼对着月亮嚎叫，感到孤独，直到它找到狼群。
# Prompt: 世界模型
A cat waking up its sleeping owner demanding breakfast. The owner tries to ignore the cat, but the cat tries new tactics and finally the owner pulls out a secret stash of treats from under the pillow to hold the cat off a little longer.
# 提示：一只猫叫醒熟睡的主人，要求吃早餐。主人试图忽视这只猫，但猫尝试了新的策略，最后主人从枕头下拿出秘密藏匿的零食，让猫再呆一会儿。

# Prompt: 
A Chinese Lunar New Year celebration video with Chinese Dragon.
# 提示：与中国龙一起庆祝中国农历新年的视频。

```



# 视频处理工具


## 视频下载


### 视频网站

- twitter 视频下载： 【2023-11-19】[twitterxz](twitterxz.com), 输入链接即可
- YouTube 视频下载：(更多方法参考知乎帖子：[如何下载youtube视频](https://www.zhihu.com/question/51714507?sort=created))，①修改网址：youtube→yout②修改网址：youtube→youtubeme，或者加now③修改网址：youtube→ssyoutube④独立网站，如noTube，[Audio](https://audio.rip/), [y2mate](https://www.y2mate.com/youtube/3-ZST_bGW3Q)

- 【2020-3-29】[在线下载YouTube视频](http://www.youtube-video-downloader.xyz/)
- （1）每个视频域名稍作修改即可（youtube.com->kissyoutube.com）,[SaveMedia](https://savemedia.com/)提供，在线下载+系列视频自动推荐
- (2) `you-get` python代码下载, `pip3 install you-get`
  - you-get 'https://www.youtube.com/watch?v=jNQXAC9IVRw', 支持的视频网站范围广. 
  - 【2018-9-20】视频下载工具[annie](https://github.com/iawia002/annie), 更名 [Lux](https://github.com/iawia002/lux)，覆盖几乎所有视频网站; Lux is a fast and simple video downloader built with Go
    - lux http://.... 
    - 支持 抖音、b站、youtube等
- 【2019-04-24】安装annie，可批量下载，windows下：[scoop](https://scoop.sh/) install annie，步骤：Set-ExecutionPolicy RemoteSigned -scope CurrentUser; iex (new-object net.webclient).downloadstring('https://get.scoop.sh')
- (3) [clipconverter](http://www.clipconverter.cc/)

视频号

【2023-12-12】如何下载视频号上的视频？
- 加机器人（`视频妙取-H`）好友, 也可以搜公众号、关注。
- 然后把视频号播放页转发给机器人
- 对话框出现H5链接，点击就能观看、下载了

### 流视频下载

视频下载工具：[流媒体下载的10种方法](http://www.jianshu.com/p/e7d2c3a624f6)
- [硕鼠](http://www.flvcd.com/)(可以下载流视频，可按专辑下载)，[硕鼠Mac版下载地址](http://www.pc6.com/mac/118056.html)（官网地址有问题）
- [维棠](http://www.vidown.cn/)

[m3u8 视频在线提取工具](https://github.com/Momo707577045/m3u8-downloader)
- [体验地址](https://blog.luckly-mjw.cn/tool-show/m3u8-downloader/index.html)

m3u8视频格式简介

m3u8视频格式原理：
- 将完整的视频拆分成多个 .ts 视频碎片，.m3u8 文件详细记录每个视频片段的地址。
- 视频播放时，会先读取 .m3u8 文件，再逐个下载播放 .ts 视频片段。
- 常用于直播业务，也常用该方法规避视频窃取的风险。加大视频窃取难度。

鉴于 m3u8 以上特点，无法简单通过视频链接下载，需使用特定下载软件。
- 但软件下载过程繁琐，试错成本高。
- 使用软件的下载情况不稳定，常出现浏览器正常播放，但软件下载速度慢，甚至无法正常下载的情况。
- 软件被编译打包，无法了解内部运行机制，不清楚里面到底发生了什么。


工具特点
- 无需安装，打开网页即可用。
- 强制下载现有片段，无需等待完整视频下载完成。
- 操作直观，精确到视频碎片的操作。

### 实测

lux 实测
- 支持 抖音、b站、youtube等

```sh
brew install lux
# b站视频
lux https://www.bilibili.com/video/BV1vu411x74Z/?spm_id_from=333.337.search-card.all.click&vd_source=ec1c777505e146eb20d947449d6bba6e
# b站快捷下载，提供ep或av值即可
lux -i ep198381 av21877586
# 抖音视频
lux https://www.douyin.com/video/7263993359235140923
# youtube视频
lux https://www.youtube.com/watch?v=hQSewmXxUho
lux https://www.douyin.com/shipin/7309342173366044735
# -------- 参数 -------
# 调试模式
lux -i -d "http://www.bilibili.com/video/av20088587"
# 输出参数
lux -o ../ -O "hello" "https://example.com"
# 使用cookie
lux -c "name=value; name2=value2" "https://www.bilibili.com/video/av20203945"
lux -c cookies.txt "https://www.bilibili.com/video/av20203945"
# refer
lux -r "https://www.bilibili.com/video/av20383055/" "http://cn-scnc1-dx.acgvideo.com/"
# 使用代理
HTTP_PROXY="http://127.0.0.1:1087/" lux -i "https://www.youtube.com/watch?v=Gnbch2osEeo"
HTTP_PROXY="socks5://127.0.0.1:1080/" lux -i "https://www.youtube.com/watch?v=Gnbch2osEeo"
# 多线程
--multi-thread or -m
--thread or -n
# --------------

# 多视频下载
lux -i "https://www.bilibili.com/video/av21877586" "https://www.bilibili.com/video/av21990740"
# 从文件中读取url链接，批量下载
#   -start
#       	File line to start at (default 1)
#   -end
#       	File line to end at
#   -items
#       	File lines to download. Separated by commas like: 1,5,6,8-10
lux -F ~/Desktop/u.txt

```

音频提取

```sh
video_file = "实拍工程苹果醋制作全过程，全程自动化，居然用无人机采摘苹果.mp4"
ffmpeg -i $video_file -vn audio.mp3
```

## 字幕提取


### VSE

[Video-subtitle-extractor](https://github.com/YaoFANGUK/video-subtitle-extractor) (VSE) 是一款将视频中的硬字幕提取为外挂字幕文件(srt格式)的软件

功能：
- 提取视频中的关键帧
- 检测视频帧中文本的所在位置
- 识别视频帧中文本的内容
- 过滤非字幕区域的文本
- 去除水印、台标文本、原视频硬字幕，可配合：video-subtitle-remover (VSR)
- 去除重复字幕行，生成srt字幕文件
- 支持视频字幕批量提取
- 多语言：支持简体中文（中英双语）、繁体中文、英文、日语、韩语、越南语、阿拉伯语、法语、德语、俄语、西班牙语、葡萄牙语、意大利语等87种语言的字幕提取
- 多模式：
  - 快速：（推荐）使用轻量模型，快速提取字幕，可能丢少量字幕、存在少量错别字
  - 自动：（推荐）自动判断模型，CPU下使用轻量模型；GPU下使用精准模型，提取字幕速度较慢，可能丢少量字幕、几乎不存在错别字
  - 精准：（不推荐）使用精准模型，GPU下逐帧检测，不丢字幕，几乎不存在错别字，但速度非常慢

项目特色：
- 采用本地进行OCR识别，无需设置调用任何API，不需要接入百度、阿里等在线OCR服务即可本地完成文本识别
- 支持GPU加速，GPU加速后可以获得更高的准确率与更快的提取速度

![](https://github.com/YaoFANGUK/video-subtitle-extractor/raw/main/design/demo.png)



## ffmpeg

ffmpeg [下载](http://ffmpeg.org/download.html)

### ffmpeg介绍

【2023-1-10】
- [ffmpeg的基本用法](https://segmentfault.com/a/1190000040982815)
- [FFmpeg 视频处理入门教程](https://www.ruanyifeng.com/blog/2020/01/ffmpeg.html)
- [ffmpeg的图形化操作](https://ffmpeg.guide/graph/demo)

ffmpeg主要组成部分
- 1、libavformat：用于各种音视频封装格式的生成和解析，包括获取解码所需信息以生成解码上下文结构和读取音视频帧等功能，包含demuxers和muxer库；
- 2、libavcodec：用于各种类型声音/图像编解码；
- 3、libavutil：包含一些公共的工具函数；
- 4、libswscale：用于视频场景比例缩放、色彩映射转换；
- 5、libpostproc：用于后期效果处理；
- 6、ffmpeg：是一个命令行工具，用来对视频文件转换格式，也支持对电视卡实时编码；
- 7、ffsever：是一个HTTP多媒体实时广播流服务器，支持时光平移；
- 8、ffplay：是一个简单的播放器，使用ffmpeg 库解析和解码，通过SDL显示；

在这组成部分中，需要熟悉基础概念有
- `容器`(Container): 容器就是一种文件格式，比如flv，mkv等。包含下面5种流以及文件头信息。
  - 视频文件本身其实是一个`容器`（container），里面包括了`视频`和`音频`，也可能有`字幕`等其他内容。
  - 视频格式：MP4，MKV，WebM，AVI。可以用 ffmpeg -formats 查看
- `流`(Stream): 是一种视频数据信息的传输方式，5种流：音频，视频，字幕，附件，数据。
- `帧`(Frame): 帧代表一幅静止的图像，分为I帧，P帧，B帧。
- `编解码器`(Codec): 是对视频进行压缩或者解压缩，`CODEC` =COde （`编码`） +DECode（`解码`）
  - 视频和音频都需要经过编码，才能保存成文件。不同的编码格式（CODEC），有不同的压缩率，会导致文件大小和清晰度的差异。
  - 常用的视频编码格式：ffmpeg -codecs
    - H.262、H.264、H.265 —— 有版权，但可以免费使用
    - VP8、VP9、AV1 —— 无版权
    - MP3、AAC —— 音频编码格式
  - `编码器`（encoders）是实现某种编码格式的库文件。只有安装了某种格式的编码器，才能实现该格式视频/音频的编码和解码。
  - FFmpeg 内置的视频编码器。
    - libx264：最流行的开源 H.264 编码器
    - NVENC：基于 NVIDIA GPU 的 H.264 编码器
    - libx265：开源的 HEVC 编码器
    - libvpx：谷歌的 VP8 和 VP9 编码器
    - libaom：AV1 编码器
  - 音频编码器: ffmpeg -encoders
    - libfdk-aac
    - aac
- `复用`/`解复用`(mux/demux): 
  - 把不同的流按照某种容器的规则放入容器，这种行为叫做`复用`（mux）
  - 把不同的流从某种容器中解析出来，这种行为叫做`解复用`(demux)



### 测试

[测试人工智能自动语音识别系统](https://cloud.tencent.com/developer/article/1644302)

样本是这四句话：
> - Due to delays, we need to reconsider our schedule this week.
> - As we've discussed, we need to put our most experienced staff on this.
> - Can you suggest an alternative to the restructuring?
> - We'll implement quality assurance processes before the final review.

故意读得磕磕巴巴，每个音频大约在13秒。但是录制出来的是m4a格式，得转换下，这里用ffmpeg



### ffmpeg安装

1. ffmpeg[下载](http://ffmpeg.org/download.html)
2. 解压到指定目录，将bin文件目录添加到path路径（电脑-属性-高级系统设置-环境变量-path-新建）
3. 命令行（windows+r 输入cmd）输入：ffmpeg -version 出结果表示成功。

### ffmpeg使用

ffmpeg命令格式

```sh
ffmpeg {1} {2} -i {3} {4} {5}
```

五个部分的参数依次如下。
1. 全局参数
1. 输入文件参数
1. 输入文件
1. 输出文件参数
1. 输出文件

常用命令
- 可用的bit流 ：ffmpeg –bsfs
- 可用的编解码器：ffmpeg –codecs
- 可用的解码器：ffmpeg –decoders
- 可用的编码器：ffmpeg –encoders
- 可用的过滤器：ffmpeg –filters
- 可用的视频格式：ffmpeg –formats
- 可用的声道布局：ffmpeg –layouts
- 可用的license：ffmpeg –L
- 可用的像素格式：ffmpeg –pix_fmts
- 可用的协议：ffmpeg -protocals

1. 视频格式转换：ffmpeg -i num.mp4 -codec copy num2.avi
  - 将num.mp4复制并转换为num2.avi
  - 注：-i 后表示要进行操作的文件
2. gif制作：ffmpeg -i num.mp4 -vframes 20 -y -f gif num3.gif
  - 将num.mp4的前20帧制作为gif并命名为num3
3. 视频截取：ffmpeg -i num.mp4 -ss 0 -t 3 -codec copy cut1.mp4
  - -ss后数字表示截取时刻，-t后数字表示截取时长
  - 截取视频某一时刻为图片：ffmpeg -i num.mp4 -y -f image2 -ss 2 -t 0.001 -s 400x300 pic.jpg
  - 将2s时刻截取为400x300大小的名为pic.jpg的图片（-ss后的数字为截取时刻）
4. 每秒截取一张图片：ffmpeg -i num.mp4 -r 1 image%d.jpg
  - 将视频num.mp4进行每秒截取一张图片，并命名为imagei.jpg（i=1，2，3...）
  - 注：-r后的数字表示每隔多久截取一张。

#### 全局参数

主要全局参数：
- -i 设定输入流 
- -f 设定输出格式 
- -ss 开始时间 

输出视频文件参数：
- -b 设定视频流量(码率)，默认为200Kbit/s 
- -r 设定帧速率，默认为25 
- -s 设定画面的宽与高 
- -aspect 设定画面的比例 
- -vn 不处理视频 
- -vcodec 设定视频编解码器，未设定时则使用与输入流相同的编解码器 
- -qscale 0 保留原始的视频质量

输出音频文件参数：
- -ar 设定采样率 
- -ac 设定声音的Channel数 
- -acodec 设定声音编解码器，未设定时则使用与输入流相同的编解码器 
- -an 不处理音频

```yml
-c：指定编码器
-c copy：直接复制，不经过重新编码（这样比较快）
-c:v：指定视频编码器
-c:a：指定音频编码器
-i：指定输入文件
-an：去除音频流
-vn： 去除视频流
-preset：指定输出的视频质量，会影响文件的生成速度，有以下几个可用的值 ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow。
-y：不经过确认，输出时直接覆盖同名文件。
```


#### 获取媒体文件信息

```sh
ffmpeg -i file_name
ffmpeg -i video_file.mp4
ffmpeg -i audio_file.mp3
ffmpeg -i video_file.mp4 -hide_banner # hide_banner 来隐藏掉ffmpeg本身的信息
ffmpeg -i audio_file.mp3 -hide_banner
```

#### 转换文件格式

转换媒体文件
- ffmpeg 最有用的功能：在不同媒体格式之间进行自由转换。
- 指明输入和输出文件名就行，ffmpeg 会从后缀名猜测格式，这个方法同时适用于视频和音频文件

```sh
ffmpeg -i video_input.mp4 video_output.avi 
ffmpeg -i video_input.webm video_output.flv 
ffmpeg -i audio_input.mp3 audio_output.ogg 
ffmpeg -i audio_input.wav audio_output.flac
# 同时指定多个输出后缀：这样会同时输出多个文件
ffmpeg -i audio_input.wav audio_output_1.mp3 audio_output_2.ogg 
ffmpeg -formats # 看支持的格式
# 用 -hide_banner 来省略一些程序信息。
# 用 -qscale 0 来保留原始的视频质量：
ffmpeg -i video_input.wav -qscale 0 video_output.mp4
```

#### 视频中抽取音频

为了从视频文件中抽取音频，直接加一个 **-vn** 参数就可以了
- 一些常见的比特率有: 96k, 128k, 192k, 256k, 320k (mp3也可以使用最高的比特率)。
- 其他常用参数: -ar (**采样率**: 22050, 441000, 48000), -ac (**声道数**), -f (**音频格式**, 通常会自动识别的). -ab 也可以使用 -b:a 来替代.

```sh
ffmpeg -i video.mp4 -vn audio.mp3
# 复用原有文件的比特率，使用 -ab (音频比特率)来指定编码比特率比较好
ffmpeg -i video.mp4 -vn -ab 128k audio.mp3
ffmpeg -i video.mov -vn -ar 44100 -ac 2 -b:a 128k -f mp3 audio.mp3
```

#### 视频中抽取视频（让视频静音）

用 -an 来获得**纯视频** (之前是 -vn)

```sh
ffmpeg -i video_input.mp4 -an -video_output.mp4
ffmpeg -i input.mp4 -vcodec copy -an output.mp4
# 这个 -an 标记会让所有的音频参数无效，因为最后没有音频会产生。
```

#### 视频中提取图片

这个功能很实用
- 一些幻灯片里面提取所有的图片

```sh
ffmpeg -i video.mp4 -r 1 -f image2 image-%3d.png
```

解释：
- -r 代表了帧率（一秒内导出多少张图像，默认25）
- -f 代表了输出格式 (image2 实际上上 image2 序列的意思）

最后一个参数 (输出文件) 有一个有趣的命名：
- 使用 %3d 来指示输出的图片有三位数字 (000, 001, 等等.)。
- 也可以用 %2d (两位数字) 或者 %4d (4位数字) 

Note: 同样也有将图片转变为视频/幻灯片的方式。

#### 更改视频分辨率或长宽比

用 -s 参数来缩放视频:

```sh
ffmpeg -i video_input.mov -s 1024x576 video_output.mp4
# 用 -c:a 来保证音频编码是正确的:
ffmpeg -i video_input.h264 -s 640x480 -c:a video_output.mov
# 用-aspect 来更改长宽比:
ffmpeg -i video_input.mp4 -aspect 4:3 video_output.mp4
```

#### 为音频增加封面图片

音频变成视频，全程使用一张图片（比如专辑封面）。当想往某个网站上传音频，但那个网站又仅接受视频（比如YouTube, Facebook等）的情况下会非常有用。

```sh
ffmpeg -loop 1 -i image.jpg -i audio.wav -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4
# 只要改一下编码设置 (-c:v 是 视频编码， -c:a 是音频编码) 和文件的名称就能用了。
```

Note: 如果使用一个较新的ffmpeg版本（4.x），就可以不指定 -strict experimental

#### 为视频增加字幕

给视频增加字幕，比如一部外文电源，使用下面的命令：

```sh
ffmpeg -i video.mp4 -i subtitles.srt -c:v copy -c:a copy -preset veryfast -c:s mov_text -map 0 -map 1 output.mp4
# 可以指定自己的编码器和任何其他的音频视频参数
```

#### 压缩媒体文件

压缩文件可以极大减少文件的体积，节约存储空间，这对于文件传输尤为重要。通过ffmepg，有好几个方法来压缩文件体积。
- 文件压缩的太厉害会让文件质量显著降低。

首先，对于音频文件，可以通过降低比特率(使用 -b:a 或 -ab):

```sh
ffmpeg -i audio_input.mp3 -ab 128k audio_output.mp3
ffmpeg -i audio_input.mp3 -b:a 192k audio_output.mp3
```

再次重申，一些常用的比特率有: 
- 96k, 112k, 128k, 160k, 192k, 256k, 320k.
- 值越大，文件所需要的体积就越大。

对于视频文件，选项就多了，一个简单的方法是通过降低视频比特率 (通过 -b:v):

```sh
ffmpeg -i video_input.mp4 -b:v 1000k -bufsize 1000k video_output.mp4
# 视频的比特率和音频是不同的（一般要大得多）。
```

也可以使用 -crf 参数 (恒定质量因子). 较小的crf 意味着较大的码率。同时使用 libx264 编码器也有助于减小文件体积。这里有个例子，压缩的不错，质量也不会显著变化：

```sh
ffmpeg -i video_input.mp4 -c:v libx264 -crf 28 video_output.mp4
# crf 设置为20 到 30 是最常见的，不过您也可以尝试一些其他的值。
# 降低帧率在有些情况下也能有效（不过这往往让视频看起来很卡）:
ffmpeg -i video_input.mp4 -r 24 video_output.mp4
# -r 指示了帧率 (这里是 24)。
# 还可以通过压缩音频来降低视频文件的体积，比如设置为立体声或者降低比特率：
ffmpeg -i video_input.mp4 -c:v libx264 -ac 2 -c:a aac -strict -2 -b:a 128k -crf 28 video_output.mp4
# -strict -2 和 -ac 2 是来处理立体声部分的。
```

#### 裁剪媒体文件（基础）

想要从开头开始剪辑一部分，使用T -t 参数来指定一个时间:

```sh
ffmpeg -i input_video.mp4 -t 5 output_video.mp4 
ffmpeg -i input_audio.wav -t 00:00:05 output_audio.wav
```

这个参数对音频和视频都适用，上面两个命令做了类似的事情：保存一段5s的输出文件（文件开头开始算）。上面使用了两种不同的表示时间的方式，一个单纯的数字（描述）或者 HH:MM:SS (小时, 分钟, 秒). 第二种方式实际上指示了结束时间。

也可以通过 -ss 给出一个开始时间，-to 给出结束时间：

```sh
ffmpeg -i input_audio.mp3 -ss 00:01:14 output_audio.mp3
ffmpeg -i input_audio.wav -ss 00:00:30 -t 10 output_audio.wav 
ffmpeg -i input_video.h264 -ss 00:01:30 -to 00:01:40 output_video.h264 
ffmpeg -i input_audio.ogg -ss 5 output_audio.ogg
```

可以看到 开始时间 (-ss HH:MM:SS), 持续秒数 (-t duration), 结束时间 (-to HH:MM:SS), 和开始秒数 (-s duration)的用法.

可以在媒体文件的任何部分使用这些命令。

#### 输出YUV420原始数据

对于一下做底层编解码的人来说，有时候常要提取视频的YUV原始数据。 
- ffmpeg -i input.mp4 output.yuv

只想要抽取某一帧YUV呢？ 简单，你先用上面的方法，先抽出jpeg图片，然后把jpeg转为YUV。 比如： 你先抽取10帧图片。 

```sh
ffmpeg -i input.mp4 -ss 00:00:20 -t 10 -r 1 -q:v 2 -f image2 pic-%03d.jpeg
#结果：
# -rw-rw-r-- 1 hackett hackett    296254  7月 20 16:08 pic-001.jpeg
# -rw-rw-r-- 1 hackett hackett    300975  7月 20 16:08 pic-002.jpeg
# -rw-rw-r-- 1 hackett hackett    310130  7月 20 16:08 pic-003.jpeg
# -rw-rw-r-- 1 hackett hackett    268694  7月 20 16:08 pic-004.jpeg
# -rw-rw-r-- 1 hackett hackett    301056  7月 20 16:08 pic-005.jpeg
# 然后，随便挑一张，转为YUV: 
ffmpeg -i pic-001.jpeg -s 1440x1440 -pix_fmt yuv420p xxx3.yuv
# 如果-s参数不写，则输出大小与输入一样。当然了，YUV还有yuv422p啥的，你在-pix_fmt 换成yuv422p就行啦！
```

#### 视频添加logo

```sh
ffmpeg -i input.mp4 -i logo.png -filter_complex overlay output.mp4
```

#### 提取视频ES数据

```sh
ffmpeg –i input.mp4 –vcodec copy –an –f m4v output.h264
```

#### 视频编码格式转换

比如一个视频的编码是MPEG4，想用H264编码，咋办？

```sh
ffmpeg -i input.mp4 -vcodec h264 output.mp4
# 相反也一样
ffmpeg -i input.mp4 -vcodec mpeg4 output.mp4
```

#### 添加字幕

语法 –vf subtitles=file

```sh
ffmpeg -i jidu.mp4 -vf subtitles=rgb.srt output.mp4
```

### ffmpeg 高级用法

高级用法
1. 分割媒体文件
2. 拼接媒体文件
3. 将图片转变为视频
4. 录制屏幕
5. 录制摄像头
6. 录制声音
7. 截图

```sh
# 分割: -t 00:00:30 为界分成两个文件（音频、视频都行）, 可以指定多个分割点
ffmpeg -i video.mp4 -t 00:00:30 video_1.mp4 -ss 00:00:30 video_2.mp4
# 拼接：concat, 把join.txt里的文件合并为 output.mp4
ffmpeg -f concat -i join.txt output.mp4
# 图片→视频：image2pipe 同一种格式（png或jpg）的图片文件
cat my_photos/* | ffmpeg -f image2pipe -i - -c:v copy video.mkv
cat my_photos/* | ffmpeg -framerate 1 -f image2pipe -i - -c:v copy video.mkv # 指定帧率，framerate
cat my_photos/* | ffmpeg -framerate 0.40 -f image2pipe -i - -i audio.wav -c copy video.mkv # 加入声音 audio.wav
# 录屏：x11grab，屏幕分辨率 (-s)，按 q 或者 CTRL+C 以结束录制屏幕
ffmpeg -f x11grab -s 1920x1080 -i :0.0 output.mp4
-s $(xdpyinfo | grep dimensions | awk '{print $2;}') # 获取真实分辨率
ffmpeg -f x11grab -s $(xdpyinfo | grep dimensions | awk '{print $2;}') -i :0.0 output.mp4 # 完整写法
# 录摄像头：q 或者 CTRL+C 来结束录制。
ffmpeg -i /dev/video0 output.mkv 
# 录声音：Linux上同时是使用 ALSA 和 pulseaudio 来处理声音的。 ffmpeg 可以录制两者
# 在 pulseaudio, 必须强制指定(-f) alsa 然后指定 default 作为输入t (-i default):
ffmpeg -f alsa -i default output.mp3
ffmpeg -i /dev/video0 -f alsa -i default -c:v libx264 -c:a flac -r 30 output.mkv # 指定编码器及帧率
ffmpeg -f x11grab -s $(xdpyinfo | grep dimensions | awk '{print $2;}') -i :0.0 -i audio.wav -c:a copy output.mp4 # 提供音频文件
# 截图
ffmpeg -i input.flv -f image2 -vf fps=fps=1 out%d.png # 每隔一秒截一张图
ffmpeg -i input.flv -f image2 -vf fps=fps=1/20 out%d.png # 每隔20秒截一张图
```

过滤器 是 ffmpeg 中最为强大的功能。在ffmepg中有数不甚数的过滤器存在，可以满足各种编辑需要

ffmpeg 过滤器：
1. 视频缩放
2. 视频裁剪
3. 视频旋转
4. 音频声道重映射
5. 更改播放速度

```sh
# 过滤器基本结构
# 指定视频过滤器 (-vf, -filter:v的简写) 和 音频过滤器 (-af, -filter:a的简写)，单引号连接
ffmpeg -i input.mp4 -vf "filter=setting_1=value_1:setting_2=value_2,etc" output.mp4
ffmpeg -i input.wav -af "filter=setting_1=value_1:setting_2=value_2,etc" output.wav
# 视频缩放
ffmpeg -i input.mp4 -vf "scale=w=800:h=600" output.mp4 # 绝对大小
ffmpeg -i input.mkv -vf "scale=w=1/2*in_w:h=1/2*in_h" output.mkv # 数学运算，相对大小
# 视频裁剪：除了w和h，还需要指定裁剪原点（视频中心）
ffmpeg -i input.mp4 -vf "crop=w=1280:h=720:x=0:y=0" output.mp4 
ffmpeg -i input.mkv -vf "crop=w=400:h=400" output.mkv
ffmpeg -i input.mkv -vf "crop=w=3/4*in_w:h=3/4*in_h" output.mkv # 相对大小
# 视频旋转
ffmpeg -i input.avi -vf "rotate=90*PI/180" # 按照指定弧度顺时针旋转 90°
ffmpeg -i input.mp4 -vf "rotate=PI" # 上下颠倒
# 声道重映射
ffmpeg -i input.mp3 -af "channelmap=1-0|1-1" output.mp3 # 将右声道（1）同时映射到左（0）右（1）两个声道（左边的数字是输入，右边的数字是输出）。
# 更改音量
ffmpeg -i input.wav -af "volume=1.5" output.wav  # 音量 1.5倍
ffmpeg -i input.ogg -af "volume=0.75" output.ogg # 0.75倍
# 视频播放速度：setpts（视频播放）、atempo（音频播放）
ffmpeg -i input.mkv -vf "setpts=0.5*PTS" output.mkv # 视频加速
ffmpeg -i input.mp4 -vf "setpts=2*PTS" output,mp4 # 视频减速一半
ffmpeg -i input.wav -af "atempo=0.75" output.wav # 音频减速
ffmpeg -i input.mp3 -af "atempo=2.0,atempo=2.0" ouutput.mp3 # 音频加速
```


## moviepy

MoviePy 是开源软件，原作者为[Zulko](https://github.com/Zulko)，并根据MIT licence发行。
- 在Windows、Mac和Linux环境中以Python2或Python3运行。

python 视频处理模块 moviepy
- 基本操作：剪切、拼接、插入标题、视频合成（即非线性编辑）、视频处理和创建高级特效。
- 常见视频格式进行读写，包括GIF。

文档
- [moviepy中文文档](https://moviepy-cn.readthedocs.io/zh/latest/index.html)
- 【2022-2-17】[【首发】python moviepy 的用法，看这篇就能入门](https://bbs.huaweicloud.com/blogs/331588)


### 工作原理

MoviePy
- 用ffmpeg软件读取和导出视频和音频文件。
- 用（可选）ImageMagick来生成文字和制作GIF文件。
- 不同媒体的处理依靠Python的快速的数学库Numpy。
- 高级效果和增强功能使用一些Python的图片处理库（PIL，Scikit-image，scipy等）。

![](http://zulko.github.io/moviepy/_images/explanations.jpeg)

MoviePy 核心对象是**剪辑**，可用`AudioClips`或`VideoClips`来处理。
- **剪辑**可被修改（剪切、降低速度、变暗等）或与其他剪辑混合组成新剪辑。
- **剪辑**可被预览（使用PyGame或IPython Notebook），也可生成文件（如MP4文件、GIF文件、MP3文件等）。

以VideoClips为例，它由一个视频文件、一张图片、一段文字或者一段卡通动画而来。可包含音频轨道（即AudioClip）和一个遮罩（一种特殊的VideoClip），用于表明当两个剪辑混合时，哪一部分的画面被隐藏）。


### 安装

MoviePy 依赖 
- Numpy 、 imageio 、 Decorator 和 tqdm
- MoviePy 依赖 FFMPEG 软件对视频进行读写, 初次使用时，FFMPEG将会自动由ImageIO下载和安装

可选依赖
- ImageMagick: 添加文字时用到, 安装后会被MoviePy自动检测到，除了Windows环境
- PyGame: 视频和声音预览中会使用到
- 高级的图片处理: PIL(Pillow), Scipy, ScikitImage, OpenCV

```sh
pip install moviepy
```

测试

```py
from moviepy.editor import *
```

mac 上执行报错

```sh
ImportError: cannot import name 'is_ascii' from 'charset_normalizer.utils' (/Users/bytedance/miniconda3/envs/py310/lib/python3.10/site-packages/charset_normalizer/utils.py)
```

解决
- 安装chardet

```sh
pip install chardet
```


### 视频信息

视频的分辨率和时间

```py
from moviepy.editor import *

video = VideoFileClip('1644974996.mp4')
print(dir(video))

size = os.path.getsize('1644974996.mp4') # 文件大小
print(size)
print(video.size) # 获取分辨率
print(video.duration) # 获取视频总时长
# 视频封面
clip.save_frame("frame.jpg")  # 保存第1帧
clip.save_frame("frame.png", t=2)  # 保存2s时刻的那1帧
```

### 播放速度

读取视频，调用 `speedx()` 方法，其中设置要加速到的倍数

```py
from moviepy.editor import *

clip = VideoFileClip('./1644974996.mp4')

video_1 = clip.speedx(2) # 加速两倍
video_1.write_videofile('sss.mp4')

```


### 视频裁剪

VideoFileClip 类的构造函数如下所示：

```py
__init__(self, filename, has_mask=False,
	audio=True, audio_buffersize=200000,
	target_resolution=None, resize_algorithm='bicubic',
	audio_fps=44100, audio_nbytes=2, verbose=False,
	fps_source='tbr')
```

其中, 只有 filename 为**必填**项，其余都为选填内容。
- filename：视频**文件名**，一般常见格式都支持；
- has_mask：是否包含**遮罩**；
- audio：是否加载**音频**；
- audio_buffersize：音频**缓冲区**大小；
- target_resolution：加载后需要变换到的**分辨率**；
- resize_algorithm：调整分辨率的算法，默认是 bicubic，可以设置为 bilinear，fast_bilinear；
- audio_fps：声音的**采样频率**；
- audio_nbytes：采样的**位数**；
- verbose：是否输出处理信息。
- subclip(t1,t2) 方法的含义为截取t1到t2时间段内的片段。
- write_videofile() 方法用于视频输出。

`subclip(t_start,t_end)` 方法时间参数
- 秒:  `(t_start=10)` ，以秒表示
- 分:  `(t_start=(1,20))` ，以1分20秒
- 时: `(t_start=(0,1,20))` 或者 `(t_start=(00:01:20))` , 以**小时: 分钟: 秒**形式表示
-  t_end 的默认值是视频长度


```py
from moviepy.editor import *
import time

clip = VideoFileClip('./1644974996.mp4').subclip(10, 20)
# 保存视频片段
new_file = str(int(time.time())) + '_subclip.mp4'
clip.write_videofile(new_file)
# 片段转gif
clip.write_gif('demo.gif',fps=5) # 生成之后的文件小
clip.write_gif('demo.gif',fps=15) # 生成之后的文件大
```


### 音频提取

`VideoFileClip` 对象的 **audio** 属性，获取视频的音频部分，然后 调用 `set_audio()` 方法对文件进行音频设置
- 注意: 合成的音频和视频等于长的。

```py
from moviepy.editor import *

# 读取2个视频文件 
videoclip_a = VideoFileClip("1644974996.mp4")
videoclip_b = VideoFileClip("1644974998.mp4")

# 去掉音频
video = video.without_audio() # 去除声音
video.write_videofile('cc.mp4')

# 提取A视频的音频
audio_a = videoclip_a.audio # 提取音频
audio_a.write_audiofile('a.mp3')  # 写入音频文件

# 给B设置音频，注意视频最终合成的大小会依据长的为准
videoclip_c = videoclip_b.set_audio(audio_a)

# 输出新的视频文件
videoclip_c.write_videofile("videoclip_c.mp4")
```

### 视频合成

视频合成，也称为**非线性编辑**，把许多视频剪辑放在一起，变成一个新剪辑。

视频剪辑都会带有音轨和遮罩

两种合成方法
- **连接**： 变成一个更长的剪辑，一个接一个播放
- **堆**：并排组成画面更大的剪辑

#### 连接 concatenate_videoclips

用 concatenate_videoclips 函数进行**连接**操作。


```py
from moviepy.editor import VideoFileClip, concatenate_videoclips

clip1 = VideoFileClip("myvideo.mp4")
clip2 = VideoFileClip("myvideo2.mp4").subclip(50,60)
clip3 = VideoFileClip("myvideo3.mp4")
# final_clip 是一个剪辑，使clip 1、2和3一个接一个播放。
final_clip = concatenate_videoclips([clip1,clip2,clip3])
final_clip.write_videofile("my_concatenation.mp4")
```

注意
- 剪辑不一定必须要**相同**尺寸。
- 如果各剪辑尺寸不同，那么将被居中播放，而画面大小足够包含最大的剪辑，而且可以选择一种颜色来填充边界部分。
  - 通过 transition=my_clip 选项来在剪辑之间加一个过场

#### 堆 clip_array

clip_array 函数对剪辑进行堆叠操作

```py
from moviepy.editor import VideoFileClip, clips_array, vfx
clip1 = VideoFileClip("myvideo.mp4").margin(10) # add 10px contour
clip2 = clip1.fx( vfx.mirror_x)
clip3 = clip1.fx( vfx.mirror_y)
clip4 = clip1.resize(0.60) # downsize 60%
final_clip = clips_array([[clip1, clip2],
                          [clip3, clip4]])
final_clip.resize(width=480).write_videofile("my_stack.mp4")
```

效果
- ![](http://zulko.github.io/moviepy/_images/stacked.jpeg)


#### 合成 CompositeVideoClip

`CompositeVideoClip` 类提供了非常灵活的方法来合成剪辑，但比 `concatenate_videoclips` 和 `clips_array` 更复杂一些。

```py
# 当前video播放clip1，clip2在clip1的上层，而clip3在clip1和clip2的上层
video = CompositeVideoClip([clip1,clip2,clip3])
video = CompositeVideoClip([clip1,clip2,clip3], size=(720,460)) # 修改尺寸
clip1 = clip1.set_start(5) # start after 5 seconds # 起始时间
video = CompositeVideoClip([clip1, # starts at t=0
                            clip2.set_start(5), # start at t=5s
                            clip3.set_start(9)]) # start at t=9s
# 淡入效果
video = CompositeVideoClip([clip1, # starts at t=0
                            clip2.set_start(5).crossfadein(1),
                            clip3.set_start(9).crossfadein(1.5)])
# 剪辑定位
video = CompositeVideoClip([clip1,
                            clip2.set_pos((45,150)),
                            clip3.set_pos((90,100))])
clip2.set_pos((45,150)) # x=45, y=150 , in pixels
clip2.set_pos("center") # automatically centered
# clip2 is horizontally centered, and at the top of the picture
clip2.set_pos(("center","top"))
# clip2 is vertically centered, at the left of the picture
clip2.set_pos(("left","center"))
# clip2 is at 40% of the width, 70% of the height of the screen:
clip2.set_pos((0.4,0.7), relative=True)
# clip2's position is horizontally centered, and moving down !
clip2.set_pos(lambda t: ('center', 50+t) )
```

指定坐标的时候请记住，y坐标的0位置在图片的最上方
- ![](http://zulko.github.io/moviepy/_images/videoWH.jpeg)

视频剪辑混合在一起时，MoviePy将会把它们各自的音轨自动合成为最终剪辑的音轨

```py
from moviepy.editor import *
# ... make some audio clips aclip1, aclip2, aclip3
concat = concatenate_audioclips([aclip1, aclip2, aclip3])
compo = CompositeAudioClip([aclip1.volumex(1.2),
                            aclip2.set_start(5), # start at t=5s
                            aclip3.set_start(9)])
```

实例

```py
# Import everything needed to edit video clips
from moviepy.editor import *
# 读取视频
video_file = "../download/apple.mp4"
# Load myHolidays.mp4 and select the subclip 00:00:50 - 00:00:60
clip = VideoFileClip(video_file).subclip(50,60)
# 降低音量 Reduce the audio volume (volume x 0.8)
clip = clip.volumex(0.8)

# 生成字幕：中文字幕无法识别 Generate a text clip. You can customize the font, color, etc.
txt_clip = TextClip("Apple 苹果制作过程",fontsize=70,color='white')
# Say that you want it to appear 10s at the center of the screen
txt_clip = txt_clip.set_pos('center').set_duration(10)

# 视频合成 Overlay the text clip on the first video clip
video = CompositeVideoClip([clip, txt_clip])

# 保存合成视频 Write the result to a file (many options available !)
#video.write_videofile("my_works.mp4") # mp4 格式，清晰，但没有声音
video.write_videofile("my_works.webm") # webm 格式，有声音，但画面模糊
```



# 视频应用


## 抖音视频

抖音视频是一项画面+声音+文字的艺术

### 视频转化分析

流程
- 推荐：抖音视频推荐机制（阶梯）
- 观看：
  - 直接忽略：
  - 中途跳过：
  - 完整播放：
- 反馈：
  - 点赞：
  - 评论：
  - 收藏：
  - 转发：
  - 关注：
- 转化：导流
  - 关注
  - 小程序

![](https://nimg.ws.126.net/?url=http%3A%2F%2Fdingyue.ws.126.net%2F2022%2F0926%2F6bd63314j00rit5vr016xd000v900xtp.jpg&thumbnail=660x2147483647&quality=80&type=jpg)

### 视频拍摄

拍摄小提示
- 01：**画面+声音+文字**
  - 抖音视频是一项画面+声音+文字的艺术，除了传统声画，大家一定要注意标题和画面文案的协调搭配。
- 02：**情绪表达**
  - 反转只是情绪表达的一种形式，你的情绪表达足够到位，足够引起共鸣，不一定要费尽心思做反转的。
- 03：结尾**引导关注**
  - 结尾时尽量做一个引导关注，对拉粉很有帮助。

发布时还可以选择定位在人群密集的地方，因为系统也会优先推荐给附近的人看，展示的概率会大一些。
新闻类的短视频适合用脚本大纲，故事性强的短视频适合用分镜头脚本，不需要剧情的短视频适合用文学脚本。
很多场景视频还要设计拍摄地址，脚本的范式

### 抖音工具

视频处理
- 脚本文案: 短视频脚本一般分为3种，**分镜头脚本**、**拍摄提纲**、以及**文学脚本**。
  - [文案范例](http://static.kancloud.cn/mhsm/dyzsfx/2381665), 
- 提取文案：提取视频链接里的文案信息，可用工具
  - 微信小程序：”轻抖”，使用[方法](http://static.kancloud.cn/mhsm/dyzsfx/2381648)
  - [媒小三](https://www.meixiaosan.com/shortvideotext.html)，PC站点
  - [享享猫去水印](https://wangzhe.smzdw.cn/page/appdownload)，支持批量提取、修改、提取音频、去水印等；
    - 【2023-5-12】收费，邮箱, video三七二一, [会员](https://h.zzrjcp.com/user/)才行
- 下载视频


## 视频洗稿


怎样正确的洗稿呢？
- 举例：这个男人叫阿伟为了钱可以抛弃一切，下手也是相当的狠。
- 洗稿：这个男人非常凶残为了钱不择手段。

大概中心意思没有变，但突出重点【男人、钱、凶残】，更加简洁明了，改编成自己的语句。

GPT Prompt
> 帮我修改─篇电影解说文案,以叙述故事的口吻写出，让文案更适合电影解说，原文核心意思不变，看起来像俩篇文案，文案开头修改的一定要吸引观众的眼球。准备好了我给你发送原文案。

【2023-12-10】文案洗稿流程 [参考](https://www.douyin.com/shipin/7309342173366044735)
1. 确定选题
1. 提取文案
  1. 找对标热门视频
  1. 链接转文字
  1. 提取文案
1. 文案修改
  1. 提取框架
  1. 发散性创作
  1. 删减优化
1. 润色
  1. 口语化
1. 确定标题


# 结束