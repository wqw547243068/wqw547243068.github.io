---
layout: post
title:   大模型落地技术：模型编辑
date:   2024-11-23 16:52:00
categories: 大模型
tags: llm 编辑 模型 
excerpt: 大模型工业落地的技术经验：如何快速修复基座模型的知识点？
mathjax: true
permalink: /llm_edit
---

* content
{:toc}


# 模型编辑

更多LLM技术落地方案见站内专题：[大模型应用技术方案](llm_solution)

模型学习知识的过程, 与人类学习知识的过程相对应
- 预训练: 中小学，接触到海量世界知识，形成自己的知识体系
- 微调: 大学时，深入学习不同学科知识
- 编辑: 读研，针对特定知识进行研讨，纠正错误认知。

## 问题

预训练大语言模型, 生成内容可能存在**偏见、毒性、知识错误**等问题。
- 偏见: 内容中包含刻板印象和社会偏见等不公正的观点
- 毒性: 内容中包含有害成分
- 知识错误: 信息与事实不符。
  - “斑马的皮肤是什么颜色的？” ChatGPT 错误地回答“肉色”，而实际上斑马的皮肤是黑色的，这就是一个知识错误


## 思考

解决方法
- （1）模型**重训练**: 将大语言模型“回炉重造”—— 用清洗过的数据重新进行预训练，但**成本过高**，舍本逐末。
- （2）模型**微调**: 对大语言模型“继续教育”—— 利用**高效微调**技术向大语言模型注入新知识，但因为新知识相关样本有限，容易诱发`过拟合`和`灾难性遗忘`，得不偿失。
- （3）模型**编辑**: 仅对模型中特定知识点进行修正的模型编辑技术应运而生


成本分析
- 多种参数高效微调方法，直接调整十亿级参数量模型，会产生较高的**训练成本**；
- 修改模型错误所需的**新知识样本有限**，因此直接用相应知识点相关的样本微调模型,容易导致`过拟合`和`灾难性遗忘`。

因此，**重新预训练**和**微调**都不适用于实际大语言模型的偏见矫正、毒性祛除、以及知识错误纠正。

模型编辑应运而生, **精准**、**高效**修正大语言模型中的特定知识点，能够满足大语言模型对特定知识点进行更新需求。


## 模型编辑介绍

### 什么是模型编辑

模型编辑领域尚缺乏统一标准，不同研究对相关概念的定义各不相同。

不同工作中提到: `基于知识的模型编辑`（KME, Knowledge Model Editing） 和`知识编辑`（KE, Knowledge Editing）等概念, 统一为`模型编辑`（ME, ModelEditing）。

有些研究用“编辑”（edit） 或“事实”（fact） 表示具体编辑对象，这些概念统一为“`知识点`”。

### 模型编辑目标

模型编辑目标：修正大语言模型使其输出相关知识点的期望结果，同时不影响其他**无关**输出。
- 相关知识点: 斑马皮肤是什么颜色？肉色→黑色
- 无关知识点: 其它

模型编辑
- 首要目标纠正模型的错误回答
- 其次精准控制编辑范围
- 还要保证模型编辑效率

### 模型编辑难点

模型编辑过程远比理论定义复杂
- 知识内在关联性：指定知识点可能与其它知识点关联，牵一发而动全身

需要从多方面控制模型编辑过程


难点
- 如何精确控制模型编辑范围？

### 模型编辑性质

模型编辑的性质归纳为五个方面，分别为：
- `准确性`（Accuracy）、`泛化性`（Generality）、`可迁移性`（Portability）、`局部性`（Locality）和`高效性`（Efficiency）。


模型编辑性质: 逐层扩大
- `准确性`: 衡量对某个知识点 k 的直接修改是否有效 
  - 斑马皮肤什么颜色
  - 准确性是模型编辑的最基本要求
- `泛化性`: 模型能否适应目标问题 xk 其他形式，即判断模型在面对与 xk 具有语义相似性的问题时，能否给出统一的目标答案 yk。
  - 剃毛后的斑马什么颜色
  - 请告诉我斑马的肤色
- `可迁移性`: 将特定知识点 k 迁移到其它相关问题上的能力,最重要的是构建可迁移性数据
  - 黑色皮肤的马是什么马
  - 斑马肤色与毛发颜色一样吗
- `局部性`: 模型不影响其他不相关问题的输出, 确保局部性能够降低模型编辑的副作用，是模型编辑相较于朴素微调的重要改进。
  - 赤兔马皮肤什么颜色
  - 斑马喜欢吃什么
  - 鸵鸟会飞吗
- `高效性`: 模型编辑的时间成本和资源消耗, 直接影响到模型编辑的可行性和实用性。


## 模型编辑方法

现有编辑方法分为：`外部拓展法`和`内部修改法`
- `外部拓展法`: 通过设计特定训练程序，使模型在保持原有知识的同时学习新信息。—— 不改变模型参数
  - 外部拓展法包括`知识缓存法`和`附加参数法`
- `内部修改法`: 通过调整模型内部特定层或神经元，来实现对模型输出的精确控制
  - 内部修改法包括`元学习法`和`定位编辑法`

分类图见浙大书籍: [大模型基础-第五章知识编辑](https://github.com/ZJU-LLMs/Foundations-of-LLMs/blob/main/readme.md) 


### 总结

见浙大书籍: [大模型基础-第五章知识编辑](https://github.com/ZJU-LLMs/Foundations-of-LLMs/blob/main/readme.md) 表5.2: 模型编辑方法比较

外部拓展法中
- 基于知识缓存的 SERAC 在无需额外训练的情况下提供了高效的编辑能力，保证了高准确性、泛化性和局部性，适合快速响应和批量编辑，但可迁移性较差，编辑缓存和推理模块仍有待优化。
- 基于附加参数的 CaliNET 和T-Patcher 提供了对模型的直接编辑能力，但CaliNET 对不同模型和数据的适应性较差，而T-Patcher 虽然保持了高准确性、泛化性和可迁移性，但在批量编辑时，对内存的需求较高。

在内部修改法中
- 基于元学习法的KE 和MEND 将元知识看作超网络，通过使模型“学习如何编辑”，提高了泛化性和训练效率，且支持批量编辑。
  - 然而，基于元学习的方法训练过程设计较为复杂，在应用于大型模型时可能受限。
- 基于定位编辑的KN、ROME 和MEMIT 则专注于精确定位和编辑模型内部的特定知识。
  - ROME 和MEMIT 都能保证高准确性、泛化性、可迁移性和局部性，然而这两种方法主要针对decoder-only 模型设计，因此未比较其在encoder-decoder 架构模型上的表现。
  - 此外，MEMIT 在ROME 的基础上针对批量编辑进行了优化，在满足高效性的同时依然能够保证其它性质的稳定。


### 外部拓展法

外部拓展法的核心思想：
- 将新知识存储在附加的外部参数或外部知识库中，将其和原始模型一起作为编辑后模型。

这种方法尤其适合具有良好可扩展性的预训练大语言模型，因为它提供了足够的空间来容纳大量参数，能够存储更多新知识。

该方法**不会改变原始模型参数**，可降低对模型内部预训练知识的干扰。

根据外部组件是否直接整合进模型本身的推理过程，外部拓展法又可划分为`知识缓存法`和`附加参数法`。
- **知识缓存**类似于勇者的技能书，需要时可查阅获取特定知识；
- 而**附加参数**则如同可升级装备，直接增强勇者的整体能力。

分析
- `知识缓存法`通过引入编辑缓存机制，有效地辅助模型在庞大的知识体系中迅速定位并检索最新信息；
- `附加参数法`通过引入额外参数，实现了对模型特定输出的精细调整。
  
这两种方法的核心优势在于对原始模型的最小化干预，保证了模型编辑的局部性。


#### 知识缓存法

知识缓存法中包括三个主要组件，分别为`门控单元`、`编辑缓存`和`推理模块`。
- `编辑缓存`充当知识存储库，保存需要修改的知识，这些知识由用户通过不同的形式指定。
  - 知识点的存储形式: 事实知识、自然语言补丁和正则表达式
  - `事实知识`以问题-答案对 (xk, yk) 存储编辑实例
    - 这种存储形式适用于答案明确的事实性问题，SERA 是一种代表性方法。
  - `自然语言补丁`LanguagePatch 按照“如果⋯⋯那么⋯⋯”的句式描述编辑知识，类似于Prompt。
    - 这种存储形式适用于修正模型对自然语言中非字面含义语句的理解，且便于人们创建、编辑或移除补丁，使得模型能够通过人类反馈不断修正输出。
  - `正则表达式`是一种基于文本匹配和替换的技术，它使用特定的模式来识别和修改文本中的特定部分，适用于精确的文本语义替换。
    - 这是一种早期的原始方法，然而由于编写复杂、泛化性低，因此在模型编辑中并不常用。
- `门控单元`判断输入问题与编辑缓存中的知识的相关程度，可通过分类或噪声对比估计等任务进行训练。
- `推理模块`获取原始输入问题和编辑缓存中的知识作为输入，通过监督训练的方式学习预测用户期望的结果。

#### 附加参数法

与`知识缓存法`相比，`附加参数法`可将外部参数整合进模型结构，从而有效利用和扩展模型的功能。

这类方法与`参数高效微调`中的参数附加方法类似，都是将外部参数插入到模型中的特定位置，冻结原始模型，只训练新引入的参数以修正模型输出

不同方法将外部参数插入到模型的不同位置。
- CALINET 和 T-Patcher 通过修改模型最后一层Transformer 的全连接前馈模块来实现
- GRACE 则将外部参数以适配器的形式插入模型的特定Transformer 层中，插入位置随模型变化而变化，如BERT 的倒数第2 层和GPT2-XL 的第36 层


#### 问题

外部拓展法的实际有效性在很大程度上取决于对知识的存储与检索能力，这种依赖会导致存储资源需求的增加。

因此，在具体应用时，要保证模型局部性和应对存储限制之间寻求平衡。


### 内部修改法

与需要额外存储空间的外部拓展法不同，`内部修改法`能让模型在不增加物理存储负担的情况下直接优化自身。

内部修改法通过更新原始模型的内部参数来为模型注入新知识，能够优化模型的自我学习和适应能力，提高其在特定任务上的表现，而不是仅仅停留在表面的知识积累。

`内部修改法`又可以分为`元学习法`和`定位编辑法`。
- `元学习法`: 通过“学习如何学习”来获取元知识，再基于元知识实现模型编辑；
- `定位编辑法`: 则专注于对模型局部参数的修改，首先识别与目标知识最相关的模型参数，然后仅更新这些特定参数，通过“先定位后编辑”的策略节省更新模型所需成本。


#### 元学习法

`元学习`指模型“**学习如何学习**”（Learning to Learn）的过程。

基于元学习的模型编辑方法，让模型“**学习如何编辑**”（Learning to Edit），核心思想是使模型从一系列编辑任务中提取通用的知识，并将其应用于未见过的编辑任务，这部分知识被称为元知识ω。

元知识是模型在进行编辑前可以利用的知识，包括优化器参数、超网络 等多种形式。

元知识的训练过程被称为`元训练`，其目标是获得一个较好的元知识ω，使得后续的每次编辑只需少量样本即可快速收敛。
- 元训练过程可以看作一个双层优化问题

实现方法
- ENN 将元知识看作优化器参数，通过更新优化器参数，使后续编辑中模型参数的训练更为高效，从而使模型学习如何快速编辑
- KE 将元知识作为超网络，提出了一种通过训练超网络来学习模型参数更新值的方法
- MEND 通过低秩分解的方法来优化超网络辅助模型参数更新的过程，进一步提升普适性

基于元学习的编辑方法通过“学习如何编辑”来提高模型在面对新编辑任务时的适应性和泛化能力，能够从一系列编辑任务中提取通用的知识，进而在遇到未见过的编辑任务时，仅使用少量样本训练即可快速收敛，从而节省计算资源和时间。

然而，元学习编辑方法也存在不足之处。
- 训练过程较为复杂，应用于大型模型时常常面临训练成本高的问题。尽管KE 和MEND 通过使用超网络和梯度低秩分解等技术优化了参数更新过程，减少了计算资源的需求，但仍需进一步提升其对更复杂任务和更大规模模型的适应性与效率。
- -全局视角对模型参数进行更新，即使添加了与局部性相关的损失函数，也有可能会对模型原本的知识产生影响，导致模型的不稳定。


#### 定位编辑法

与`元学习法`相比，`定位编辑法`修改的是原始模型的局部参数。
- 先定位到需要修改的参数位置
- 然后对该处的参数进行修改。

实现定位前需要了解大语言模型中知识的存储机制。

当前，对知识存储机制的探索主要依靠定性实验来完成。
- Transformer中全连接前馈模块可看作存储知识的键值存储体

方案
- KN 提出了知识神经元的概念
- ROME 设计了一种因果跟踪实验，进一步探索中间层全连接前馈模块与知识的关系，优化了知识存储机制的结论
- MEMIT 在ROME 的基础上扩展到对不同知识的大规模编辑，可以同时执行数千次编辑。

定位编辑法修改大语言模型的局部参数，在保持模型整体结构和性能的同时，能够对特定知识点进行精准的更新和编辑。

与其他方法相比，定位编辑法可同时保持较高的准确性、泛化性和局部性，且适用于各类模型。


## 模型编辑应用

大语言模型面临着更新成本高、隐私保护难、安全风险大等问题，模型编辑技术为解决这些问题提供了新的思路。
- 通过对预训练模型进行细粒度编辑，可以**灵活地修改和优化模型**，而无需从头开始训练，大大降低了模型更新的成本。
- 同时，模型编辑技术可以针对性地修改特定事实，有效保护隐私信息，降低数据泄露风险。
- 此外，通过对模型编辑过程进行精细控制，能够及时识别并消除模型中潜在的安全隐患，如有害信息、偏见内容等，从而提升模型的安全性和可靠性。

应用场景
- (1) 精准模型更新: 模型编辑技术可以快速、精准地修正模型的特定行为。通过识别并修改相关的模型参数，可以在短时间内修复模型的回答
  - 2023 年12 月，网友发现用中文询问“你是谁”这种问题时，Gemini Pro 会回答“我是百度文心大模型”。然而，仅仅一天之后，Gemini Pro 便不再回答类似的内容
- (2) 保护被遗忘权
  - 被遗忘权（RTBF，Right to be forgotten) 指在某些情况下，将某人的私人信息从互联网搜索和其他目录中删除的权利
  - LLM 会记忆和使用个人信息，所以同样受到被遗忘权的法律约束
  - Nasr 等人发现，只要让大语言模型一直重复一个词，它就有可能在一定次数后失控，甚至毫无防备说出某人的个人隐私信息。
  - 解法: DPEN 结合了模型编辑和机器遗忘（Machine Unlearning）技术，采用定位编辑的思路，通过引入隐私神经元检测器先识别和定位与私有信息相关的模型参数，然后利用隐私神经元编辑器将这些参数的激活值设为零，有效地遗忘了隐私信息
- (3) 提升模型安全
  - 模型可能会产生有害、偏见或不当的输出，影响用户体验和决策公平。模型编辑同样也可以用来提升模型的安全性，构建更可靠的智能模型。



# 结束
