---
layout: post
title:  大语言模型代码详解 LLM Code and Arch
date:   2024-05-01 12:00:00
categories: 大模型
tags: gpt ChatGPT LLM norm
excerpt: 大模型架构、代码理解、改进
mathjax: true
permalink: /llm_code
---

* content
{:toc}

# 大语言模型代码详解

## transformer

transformer block 主要由三种结构组成
- `MHA`(Multi-Head Attention)，多头注意力模块，下图绿色部分。
- `Add&Norm`，归一化模块，下图蓝色部分。
- `FFN`，前馈网络模块，下图粉色部分
- ![](https://pic3.zhimg.com/80/v2-e6d2a145d9fdfbb05ad48ccb7c04d2a6_1440w.webp)

gemm-like 算子

## 分词

几种
- Byte-Pair Encoding (BPE)是当前SOTA的LM模型常用的分词方法
- BERT使用的是Wordpiece，属于BPE的改进版，都是sub-word分词器

1 token ≈ 0.75 word


### 位置编码

位置编码 Position Encoding

- 原始transformer(attention is all you need)里面用的是**三角式**位置编码
- BERT使用**可学习**位置编码，预设的位置个数是512，因此最大序列长度为512
  - dim=768，就用384组三角函数来表征


### RoPE



### ALiBi

ALiBi(Attention with Linear Biases)的相对位置编码机制。

ALiBi的出发点是希望能提升位置编码的外推能力，原因是在实际使用中的输入token长度可能会远远大于训练阶段使用的最大token限制。


## 激活函数

### GeLU

GeLU (Gaussian Error Linear Unit) 中文名为**高斯误差线性单元**，受 RELU 和 dropout 启发
- RELU是激活小的时候乘以0
- dropout是随机乘以0
- GeLU就是概率性的乘以0 (但是跟dropout不同，用确定性的表达式给出)
- ![](https://pic2.zhimg.com/80/v2-7d21613a6d6bc7e26d815b76930abdf1_1440w.webp)

### SwiGLU

SwiGLU 和 GeGLU非常相似，只是把GeLU激活换成了Swish激活


## Norm

preNorm/postNorm/DeepNorm



NLP任务中经常使用Layer Normalization(LN)而非Batch Normalization(BN)

随机优化理论中学习率往往设置为**常数**或者**逐渐衰减** (decay)，从而保证算法收敛，这种学习率的设置方法也与机器学习里很多任务上的实际经验类似。

然而，不管是设置学习率为常数还是使学习率逐渐衰减都不能让Transformer很好地收敛。

优化Transformer结构时，还需要在训练的初始阶段设置一个非常小（接近0）的学习率，经过一定的迭代轮数后逐渐增长到初始的学习率，这个过程称作`warm-up`阶段（学习率预热）。

Warm-up 是原始Transformer结构优化时, 一个**必备**学习率调整策略。
- Transformer结构对于warm-up的超参数（持续轮数、增长方式、初始学习率等）非常敏感，若调整不慎，往往会使得模型无法正常收敛。
- ![](https://pic1.zhimg.com/80/v2-c9e89a08c38ee251c85284ce5f7bdd20_1440w.webp)

Transformer结构的优化非常困难，其具体表现在：
- warm-up阶段超参数敏感；
- 优化过程收敛速度慢。

### 总结


- BERT当时使用的是Post-Norm的结构, 同时期的GPT-1也是用该结构
- 后来的GPT2使用Pre-Norm。

Post-LN vs. Pre-LN vs. Sandwich-LN
- ![](https://pic4.zhimg.com/80/v2-846c38e3363778f7894afb61294e79cf_1440w.webp)

分析
- Pre-Norm 比 Post-Norm 参数更好调，但是最终模型精度要比Post-Norm略差。

分析公式: x + F(x), 哪里插入 normalization ?
- Norm 计算公式: 假设x和y是相互独立的均值为0，方差为1的随机变量, Norm(x+y) = (x+y)/根号2
- `PreNorm`: x1 = x + Norm(F(x))
- `PostNorm`: x1 = Norm(x + F(x))

|Norm类型|基本公式|递归展开式|
|---|---|---|
|PreNorm|x1 = x + Norm(F(x))|[原文](https://zhuanlan.zhihu.com/p/640784855)|
|PostNorm|x1 = Norm(x + F(x))|原文](https://zhuanlan.zhihu.com/p/640784855)|
|SanwichNorm|||
|DeepNorm|||

展开公式
- 输出的方差会很大，因此需要在输出加个额外的LayerNorm (GPT2的设计)
- Pre-Norm把网络的实际深度变浅了，并且增加了宽度
- Pre-Norm 网络层数有水分，这个可能是导致模型最终精度不如Post-Norm的原因。

两种Norm的特点总结如下
- Post-Norm 会削弱残差的作用，深度保持，但是收敛和调参困难
- Pre-Norm 会将网络变成**浅且宽**的结构，收敛容易，但是精度会有一定损失

### PostNorm

论文提出了两种Layer Normalization方式并进行了对比。
- [《On Layer Normalization in the Transformer Architecture》](https://openreview.net/pdf?id=B1x8anVFPr)

把Transformer架构中传统的Add&Norm方式叫做`Post-LN`，并针对Post-LN，模型提出了`Pre-LN`，即把layer normalization加在残差连接之前
- ![](https://pic2.zhimg.com/80/v2-acff46aa054d0a3c35abcfff8b006879_1440w.webp)

造成Post-LN Transformer梯度分布出现问题的**核心**原因
- 各子层之后 Layer Normalization 层会使得**各层的输入尺度与层数L无关**，因此当Layer Normalization对梯度进行归一化时，也与层数L无关。
- Pre-LN 最后一层Layer Normalization层的输入尺寸的量级只有Post-LN的 根号(1/L) 倍，并且每个LN层都会对梯度以 
根号L 的比例归一化。所以对于Pre-LN结构来说，其每层梯度范数都近似不变。

相比于Post-LN结构梯度分布的不稳定，Pre-LN在各层之间梯度范数几乎保持不变，这种结构明显更利于优化器进行优化。而在进行一定轮数的 warm-up后，Post-LN 梯度范数也基本保持不变，并且其量级非常小(上图中绿色)，这也验证了Post-LN在warm-up阶段的训练不稳定性问题。

实验表明
- 当使用Pre-LN结构时，warm-up阶段已不再是必需，并且Pre-LN结构可以大幅提升Transformer的收敛速度。
- 对于机器翻译任务（IWSLT/WMT)，不需要warm-up的Pre-LN结构可以比Post-LN收敛快1倍左右
- 而在BERT上，Pre-LN在下游任务上达到和Post-LN相同的性能也只需要后者迭代轮数的1/3左右，并且最终的效果也更好。

### SanwichNorm

Sandwich-Norm，基于Pre-Norm再加一个

### DeepNorm

Nguyen和Salazar(2019)发现相对于`Post-LN`，`Pre-LN`能够提升Transformer的稳定性。
- 然而，`Pre-LN`在底层的梯度往往大于顶层，导致其性能不及`Post-LN`。

为了缓解这一问题，研究人员通过更好的初始化方式或模型架构来改进深度Transformer。
- 基于Post-Norm做了改进，出现了`Deep-Norm`，能训练1000层的Transformer

```py
def deepnorm(x):
	return LayerNorm(x * alpha + f(x))

def deepnorm_init(w):
	if w is ['ffn', 'v_proj', 'out_proj']:
        nn.init.xavier_normal_(w, gain=β)
    elif w is ['q_proj', 'k_proj']:
        nn.init.xavier_normal_(w, gain=1)
```

这些方法可以使多达数百层的Transformer模型实现稳定化，然而以往的方法没有能够成功地扩展至1000层。
- [DeepNet: Scaling Transformers to 1,000 Layers](https://arxiv.org/pdf/2203.00555.pdf)

`DeepNorm` 结合了`Post-LN`的良好性能以及`Pre-LN`的训练稳定性。
- 与 `Post-LN` 相比， DeepNorm 在执行层归一化之前 Up-Scale 了残差连接。

相较于Post-LN模型，DeepNet的模型更新几乎保持恒定。

参考:
- [再谈Layer-Norm：Pre-LN、Post-LN、DeepNorm](https://zhuanlan.zhihu.com/p/480783670)


## 注意力


### 掩码

【2024-8-14】[注意力机制中三种掩码技术详解和Pytorch实现](https://zhuanlan.zhihu.com/p/709073236?utm_psn=1807095944686211073)

掩码类型
- `填充掩码` Padding Mask
- `序列掩码` Sequence Mask
- `前瞻掩码` Look-ahead Mask

|掩码类型|目的|应用|功能|
|---|---|---|---|
|`填充掩码` Padding Mask|屏蔽输入数据中的无关数据|长度不一致的输入数据|识别有效数据|
|`序列掩码` Sequence Mask|模型关注范围,包含填充数据|精确控制信息流|指示哪些数据有效/填充|
|`前瞻掩码` Look-ahead Mask|防止看到未来信息|自回归模型|保证时序正确性，防止信息泄露|

掩码之间的关系
- `填充掩码`（Padding Mask）和`序列掩码`（Sequence Mask）都是在处理**序列数据**时使用的技术，帮助模型正确处理变长输入序列
- 但应用场景和功能有些区别。这两种掩码深度学习模型中被一起使用，尤其是在需要处理不同长度序列的场景下。
  - `填充掩码`指示哪些数据是填充的，用在输入数据预处理和模型的输入层。确保模型在处理或学习过程中不会将填充部分的数据当作有效数据来处理，从而影响模型的性能。在诸如Transformer模型的自注意力机制中，填充掩码用于阻止模型将注意力放在填充的序列上。
  - `序列掩码`用于更广泛的上下文中，它不仅可以指示填充位置，还可以用于其他类型的掩蔽，如在序列到序列的任务中掩蔽未来的信息（如解码器的自回归预测）。序列掩码确保模型在处理过程中**只关注当前及之前的信息**，而不是未来的信息，这对于保持信息的时序依赖性非常重要。
  - `填充掩码`多用于模型的输入阶段或在注意力机制中排除无效数据的影响
  - `序列掩码`则可能在模型的多个阶段使用，特别是在需要控制信息流的场景中。
- `前瞻掩码`用于**控制时间序列的信息流**，确保在生成序列的每个步骤中模型只能利用到当前和之前的信息。这是生成任务中保持模型正确性和效率的关键技术。



#### 填充掩码 Padding Mask

深度学习中，处理序列数据时，"填充掩码"（Padding Mask）是一个重要概念。

当序列数据长度不一致时，通常需要对短的序列进行**填充**（padding），以确保所有序列的长度相同，这样才能进行批处理。

这些填充的部分实际上是没有任何意义，不应该对模型的学习产生影响。

```py
import torch

def create_padding_mask(seq, pad_token=0):
    mask = (seq == pad_token).unsqueeze(1).unsqueeze(2)
 return mask  # (batch_size, 1, 1, seq_len)

# Example usage
seq = torch.tensor([[7, 6, 0, 0], [1, 2, 3, 0]])
padding_mask = create_padding_mask(seq)
print(padding_mask)
```

#### 序列掩码 Sequence Mask

序列掩码用于**隐藏**输入序列的某些部分。避免在计算注意力分数时考虑到填充位置的影响

比如在双向模型中，想要根据特定标准忽略序列的某些部分。

RNNs 本身可处理不同长度的序列，但在**批处理**和某些架构中，仍然需要**固定长度**的输入。序列掩码在这里可以帮助RNN忽略掉序列中的填充部分，特别是在计算最终序列输出或状态时。

在训练模型时，序列掩码也可以用来确保在计算损失函数时，不会将填充部分的预测误差纳入总损失中，从而提高模型训练的准确性和效率。

序列掩码表示为一个与序列数据维度相同的**二进制**矩阵或向量，其中1表示实际数据，0表示填充数据

```py
def create_sequence_mask(seq):
    seq_len = seq.size(1)
    mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1)
    return mask  # (seq_len, seq_len)

# Example usage
seq_len = 4
sequence_mask = create_sequence_mask(torch.zeros(seq_len, seq_len))
print(sequence_mask)
```


#### 前瞻掩码 Look-ahead Mask

前瞻掩码，也称为**因果掩码**或**未来掩码**，用于自回归模型中，以防止模型在生成序列时窥视未来的符号, 确保了给定位置的预测仅依赖于该位置之前的符号。

前瞻掩码通过在自注意力机制中屏蔽（即设置为一个非常小的负值，如负无穷大）**未来时间步**的信息来工作。这确保了在计算每个元素的输出时，模型只能使用到当前和之前的信息，而不能使用后面的信息。

这种机制对于保持**自回归属性**（即一次生成一个输出，且依赖于前面的输出）是必要的。

前瞻掩码通常表示为一个**上三角矩阵**，其中对角线及对角线以下的元素为0（表示这些位置的信息是可见的），对角线以上的元素为1（表示这些位置的信息是不可见的）。

计算注意力时，这些为1的位置会被设置为一个非常小的负数（通常是负无穷），这样经过softmax函数后，这些位置的权重接近于0，从而不会对输出产生影响。

```py
def create_look_ahead_mask(size):
    mask = torch.triu(torch.ones(size, size), diagonal=1)
    return mask  # (seq_len, seq_len)

# Example usage
look_ahead_mask = create_look_ahead_mask(4)
print(look_ahead_mask)
```


### 注意力掩码

注意力机制中，掩码被用来修改注意力得分

```py
import torch.nn.functional as F

def scaled_dot_product_attention(q, k, v, mask=None):
    matmul_qk = torch.matmul(q, k.transpose(-2, -1))
    dk = q.size()[-1]
    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))

    if mask is not None:
        scaled_attention_logits += (mask * -1e9)

    attention_weights = F.softmax(scaled_attention_logits, dim=-1)
    output = torch.matmul(attention_weights, v)
    return output, attention_weights

# Example usage
d_model = 512
batch_size = 2
seq_len = 4

q = torch.rand((batch_size, seq_len, d_model))
k = torch.rand((batch_size, seq_len, d_model))
v = torch.rand((batch_size, seq_len, d_model))
mask = create_look_ahead_mask(seq_len)

attention_output, attention_weights = scaled_dot_product_attention(q, k, v, mask)
print(attention_output)
```

## Decoder 

LLM 进化树
- 粉色分支，Encoder-only 框架(也叫Auto-Encoder)，典型代表如BERT等
- 绿色分支，Encoder-decoder 框架，典型代表如T5和GLM等
- 蓝色分支，Decoder-only 框架(也叫Auto-Regressive)，典型代表如GPT系列/LLaMa/PaLM等
- ![](https://pic2.zhimg.com/80/v2-bd53b7e73678a77e909271f47295d21d_1440w.webp)


直观对比
- ![](https://pic2.zhimg.com/80/v2-f096c6925eddd283ae74720f8dc02e95_1440w.webp)
- 横轴代表了输入token，纵轴代表相对应每个位置的输出token
- 左图为encoder-only，输出token都能看到所有输入token。例如 y1 这一行可以看到输入 x1-x5
- 中图为decoder-only，输出token只能看到历史的输入token。例如 y3 这行只能看到 x1-x3, x4 和 x5 看不到
- 右图为encoder-decoder，前k个输出token可以看到所有k个输入token，从k+1的输出token开始只能看到历史的输入token。例如 y1 能看到输入(y3也可以)，而开始只能看到输入x1-x4
- encoder-decoder简化使用causal with prefix示意

三种结构不同的LLM，往往擅长处理不同的任务

|	 | NLU任务	| conditioned-NLG任务	| unconditioned-NLG任务	| 典型代表 |
|	--- | ---	| ---	| ---	| --- |
| Encoder-only架构 |	效果最好 |	|		| BERT |
| Encoder-decoder架构 |		| 效果最好	| 	| T5和GLM |
| Decoder-only架构	|	|	| 效果最好	| GPT系列/LLaMa/PaLM |
| 典型代表 | 文本情感分析，词性标注，信息检索 |	机器翻译，自动摘要 |	QA，ChatBot	 |

### 微软 YOCO

【2024-5-13】[YOCO：打破传统Decoder-only架构，内存消耗仅为Transformer的六分之一](https://mp.weixin.qq.com/s/X4HSyEreN4L4xTizC-_mow)

模型架构还只有三大类：Decoder-Only、Encoder-Only、Encoder-Decoder。

微软亚洲研究院推出了一种创新性的 Decoder-Decoder 架构 `YOCO`（You Only Cache Once）。通过**自解码器**和**交叉解码器**的独特架构，YOCO 仅需缓存一次键值对，从而显著降低 GPU 内存的使用。
- 论文 [You Only Cache Once: Decoder-Decoder Architectures for Language Models](https://arxiv.org/abs/2405.05254)

模型评估中，YOCO 展现出与同规模 Transformer 模型相媲美的性能，并在语言建模评估、模型大小扩展以及长上下文处理方面具有显著优势。特别是在降低 GPU 内存占用和缩短预填充延迟方面，

YOCO 整体架构设计如下，分为`自解码器`（Self-Decoder）和`交叉解码器`（Cross-Decoder）两部分。

YOCO 实现了“**模型越大，内存越省**”，为自然语言处理领域带来了全新的研究和应用范式。
- YOCO 仅缓存一次键值对，可大幅降低 GPU 内存需求，且保留全局注意力能力。

打破 GPT 系列开创的 `Decoder-Only` 架构——提出 `Decoder-Decoder` 新型架构，名为 `YOCO` (You Only Cache Once)。
- 在处理 512K 上下文长度时，标准 Transformer 内存使用是 YOCO 的6.4倍，预填充延迟是 YOCO 的30.3倍，而 YOCO 的吞吐量提升到标准 Transformer 的9.6倍。


# 结束

