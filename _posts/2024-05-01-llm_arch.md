---
layout: post
title:  大语言模型代码详解 LLM Code and Arch
date:   2024-05-01 12:00:00
categories: 大模型
tags: gpt ChatGPT LLM norm
excerpt: 大模型架构、代码理解、改进
mathjax: true
permalink: /llm_code
---

* content
{:toc}

# 大语言模型代码详解

## transformer

transformer block 主要由三种结构组成
- `MHA`(Multi-Head Attention)，多头注意力模块，下图绿色部分。
- `Add&Norm`，归一化模块，下图蓝色部分。
- `FFN`，前馈网络模块，下图粉色部分
- ![](https://pic3.zhimg.com/80/v2-e6d2a145d9fdfbb05ad48ccb7c04d2a6_1440w.webp)

gemm-like 算子

## 分词

几种
- Byte-Pair Encoding (BPE)是当前SOTA的LM模型常用的分词方法
- BERT使用的是Wordpiece，属于BPE的改进版，都是sub-word分词器

1 token ≈ 0.75 word


### 位置编码

位置编码 Position Encoding

- 原始transformer(attention is all you need)里面用的是**三角式**位置编码
- BERT使用**可学习**位置编码，预设的位置个数是512，因此最大序列长度为512
  - dim=768，就用384组三角函数来表征


### RoPE



### ALiBi

ALiBi(Attention with Linear Biases)的相对位置编码机制。

ALiBi的出发点是希望能提升位置编码的外推能力，原因是在实际使用中的输入token长度可能会远远大于训练阶段使用的最大token限制。


## 激活函数

### GeLU

GeLU (Gaussian Error Linear Unit) 中文名为**高斯误差线性单元**，受 RELU 和 dropout 启发
- RELU是激活小的时候乘以0
- dropout是随机乘以0
- GeLU就是概率性的乘以0 (但是跟dropout不同，用确定性的表达式给出)
- ![](https://pic2.zhimg.com/80/v2-7d21613a6d6bc7e26d815b76930abdf1_1440w.webp)

### SwiGLU

SwiGLU 和 GeGLU非常相似，只是把GeLU激活换成了Swish激活


## Norm

preNorm/postNorm/DeepNorm



NLP任务中经常使用Layer Normalization(LN)而非Batch Normalization(BN)

随机优化理论中学习率往往设置为**常数**或者**逐渐衰减** (decay)，从而保证算法收敛，这种学习率的设置方法也与机器学习里很多任务上的实际经验类似。

然而，不管是设置学习率为常数还是使学习率逐渐衰减都不能让Transformer很好地收敛。

优化Transformer结构时，还需要在训练的初始阶段设置一个非常小（接近0）的学习率，经过一定的迭代轮数后逐渐增长到初始的学习率，这个过程称作`warm-up`阶段（学习率预热）。

Warm-up 是原始Transformer结构优化时, 一个**必备**学习率调整策略。
- Transformer结构对于warm-up的超参数（持续轮数、增长方式、初始学习率等）非常敏感，若调整不慎，往往会使得模型无法正常收敛。
- ![](https://pic1.zhimg.com/80/v2-c9e89a08c38ee251c85284ce5f7bdd20_1440w.webp)

Transformer结构的优化非常困难，其具体表现在：
- warm-up阶段超参数敏感；
- 优化过程收敛速度慢。

### 总结


- BERT当时使用的是Post-Norm的结构, 同时期的GPT-1也是用该结构
- 后来的GPT2使用Pre-Norm。

Post-LN vs. Pre-LN vs. Sandwich-LN
- ![](https://pic4.zhimg.com/80/v2-846c38e3363778f7894afb61294e79cf_1440w.webp)

分析
- Pre-Norm 比 Post-Norm 参数更好调，但是最终模型精度要比Post-Norm略差。

分析公式: x + F(x), 哪里插入 normalization ?
- Norm 计算公式: 假设x和y是相互独立的均值为0，方差为1的随机变量, Norm(x+y) = (x+y)/根号2
- `PreNorm`: x1 = x + Norm(F(x))
- `PostNorm`: x1 = Norm(x + F(x))

|Norm类型|基本公式|递归展开式|
|---|---|---|
|PreNorm|x1 = x + Norm(F(x))|[原文](https://zhuanlan.zhihu.com/p/640784855)|
|PostNorm|x1 = Norm(x + F(x))|原文](https://zhuanlan.zhihu.com/p/640784855)|
|SanwichNorm|||
|DeepNorm|||

展开公式
- 输出的方差会很大，因此需要在输出加个额外的LayerNorm (GPT2的设计)
- Pre-Norm把网络的实际深度变浅了，并且增加了宽度
- Pre-Norm 网络层数有水分，这个可能是导致模型最终精度不如Post-Norm的原因。

两种Norm的特点总结如下
- Post-Norm 会削弱残差的作用，深度保持，但是收敛和调参困难
- Pre-Norm 会将网络变成**浅且宽**的结构，收敛容易，但是精度会有一定损失

### PostNorm

论文提出了两种Layer Normalization方式并进行了对比。
- [《On Layer Normalization in the Transformer Architecture》](https://openreview.net/pdf?id=B1x8anVFPr)

把Transformer架构中传统的Add&Norm方式叫做`Post-LN`，并针对Post-LN，模型提出了`Pre-LN`，即把layer normalization加在残差连接之前
- ![](https://pic2.zhimg.com/80/v2-acff46aa054d0a3c35abcfff8b006879_1440w.webp)

造成Post-LN Transformer梯度分布出现问题的**核心**原因
- 各子层之后 Layer Normalization 层会使得**各层的输入尺度与层数L无关**，因此当Layer Normalization对梯度进行归一化时，也与层数L无关。
- Pre-LN 最后一层Layer Normalization层的输入尺寸的量级只有Post-LN的 根号(1/L) 倍，并且每个LN层都会对梯度以 
根号L 的比例归一化。所以对于Pre-LN结构来说，其每层梯度范数都近似不变。

相比于Post-LN结构梯度分布的不稳定，Pre-LN在各层之间梯度范数几乎保持不变，这种结构明显更利于优化器进行优化。而在进行一定轮数的 warm-up后，Post-LN 梯度范数也基本保持不变，并且其量级非常小(上图中绿色)，这也验证了Post-LN在warm-up阶段的训练不稳定性问题。

实验表明
- 当使用Pre-LN结构时，warm-up阶段已不再是必需，并且Pre-LN结构可以大幅提升Transformer的收敛速度。
- 对于机器翻译任务（IWSLT/WMT)，不需要warm-up的Pre-LN结构可以比Post-LN收敛快1倍左右
- 而在BERT上，Pre-LN在下游任务上达到和Post-LN相同的性能也只需要后者迭代轮数的1/3左右，并且最终的效果也更好。

### SanwichNorm

Sandwich-Norm，基于Pre-Norm再加一个

### DeepNorm

Nguyen和Salazar(2019)发现相对于`Post-LN`，`Pre-LN`能够提升Transformer的稳定性。
- 然而，`Pre-LN`在底层的梯度往往大于顶层，导致其性能不及`Post-LN`。

为了缓解这一问题，研究人员通过更好的初始化方式或模型架构来改进深度Transformer。
- 基于Post-Norm做了改进，出现了`Deep-Norm`，能训练1000层的Transformer

```py
def deepnorm(x):
	return LayerNorm(x * alpha + f(x))

def deepnorm_init(w):
	if w is ['ffn', 'v_proj', 'out_proj']:
        nn.init.xavier_normal_(w, gain=β)
    elif w is ['q_proj', 'k_proj']:
        nn.init.xavier_normal_(w, gain=1)
```

这些方法可以使多达数百层的Transformer模型实现稳定化，然而以往的方法没有能够成功地扩展至1000层。
- [DeepNet: Scaling Transformers to 1,000 Layers](https://arxiv.org/pdf/2203.00555.pdf)

`DeepNorm` 结合了`Post-LN`的良好性能以及`Pre-LN`的训练稳定性。
- 与 `Post-LN` 相比， DeepNorm 在执行层归一化之前 Up-Scale 了残差连接。

相较于Post-LN模型，DeepNet的模型更新几乎保持恒定。

参考:
- [再谈Layer-Norm：Pre-LN、Post-LN、DeepNorm](https://zhuanlan.zhihu.com/p/480783670)


## Decoder 

LLM 进化树
- 粉色分支，Encoder-only 框架(也叫Auto-Encoder)，典型代表如BERT等
- 绿色分支，Encoder-decoder 框架，典型代表如T5和GLM等
- 蓝色分支，Decoder-only 框架(也叫Auto-Regressive)，典型代表如GPT系列/LLaMa/PaLM等
- ![](https://pic2.zhimg.com/80/v2-bd53b7e73678a77e909271f47295d21d_1440w.webp)


直观对比
- ![](https://pic2.zhimg.com/80/v2-f096c6925eddd283ae74720f8dc02e95_1440w.webp)
- 横轴代表了输入token，纵轴代表相对应每个位置的输出token
- 左图为encoder-only，输出token都能看到所有输入token。例如 y1 这一行可以看到输入 x1-x5
- 中图为decoder-only，输出token只能看到历史的输入token。例如 y3 这行只能看到 x1-x3, x4 和 x5 看不到
- 右图为encoder-decoder，前k个输出token可以看到所有k个输入token，从k+1的输出token开始只能看到历史的输入token。例如 y1 能看到输入(y3也可以)，而开始只能看到输入x1-x4
- encoder-decoder简化使用causal with prefix示意

三种结构不同的LLM，往往擅长处理不同的任务

|	 | NLU任务	| conditioned-NLG任务	| unconditioned-NLG任务	| 典型代表 |
|	--- | ---	| ---	| ---	| --- |
| Encoder-only架构 |	效果最好 |	|		| BERT |
| Encoder-decoder架构 |		| 效果最好	| 	| T5和GLM |
| Decoder-only架构	|	|	| 效果最好	| GPT系列/LLaMa/PaLM |
| 典型代表 | 文本情感分析，词性标注，信息检索 |	机器翻译，自动摘要 |	QA，ChatBot	 |

### 微软 YOCO

【2024-5-13】[YOCO：打破传统Decoder-only架构，内存消耗仅为Transformer的六分之一](https://mp.weixin.qq.com/s/X4HSyEreN4L4xTizC-_mow)

模型架构还只有三大类：Decoder-Only、Encoder-Only、Encoder-Decoder。

微软亚洲研究院推出了一种创新性的 Decoder-Decoder 架构 `YOCO`（You Only Cache Once）。通过**自解码器**和**交叉解码器**的独特架构，YOCO 仅需缓存一次键值对，从而显著降低 GPU 内存的使用。
- 论文 [You Only Cache Once: Decoder-Decoder Architectures for Language Models](https://arxiv.org/abs/2405.05254)

模型评估中，YOCO 展现出与同规模 Transformer 模型相媲美的性能，并在语言建模评估、模型大小扩展以及长上下文处理方面具有显著优势。特别是在降低 GPU 内存占用和缩短预填充延迟方面，

YOCO 整体架构设计如下，分为`自解码器`（Self-Decoder）和`交叉解码器`（Cross-Decoder）两部分。

YOCO 实现了“**模型越大，内存越省**”，为自然语言处理领域带来了全新的研究和应用范式。
- YOCO 仅缓存一次键值对，可大幅降低 GPU 内存需求，且保留全局注意力能力。

打破 GPT 系列开创的 `Decoder-Only` 架构——提出 `Decoder-Decoder` 新型架构，名为 `YOCO` (You Only Cache Once)。
- 在处理 512K 上下文长度时，标准 Transformer 内存使用是 YOCO 的6.4倍，预填充延迟是 YOCO 的30.3倍，而 YOCO 的吞吐量提升到标准 Transformer 的9.6倍。


# 结束

