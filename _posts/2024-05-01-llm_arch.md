---
layout: post
title:  大语言模型模块详解 LLM Arch & Code
date:   2024-05-01 12:00:00
categories: 大模型
tags: gpt ChatGPT LLM norm
excerpt: 大模型架构、代码理解、改进
mathjax: true
permalink: /llm_arch
---

* content
{:toc}

# 大语言模型代码详解


## LLM 原理

【2024-9-6】大模型白盒子构建指南
- GitHub：[tiny-universe](github.com/datawhalechina/tiny-universe)

从原理出发、以“白盒”为导向、围绕大模型全链路的“手搓”大模型指南，帮助有深度学习基础的读者从底层原理出发，“纯手搓”搭建一个清晰、可用的大模型系统，包括大模型本身、RAG 框架、Agent 系统及大模型评估体系。最近新增了从零开始pretrain Llama3部分。

亮点
- 全流程 从零手搓
- 包含LLM全流程，从Model，到RAG，Agent，Eval，打造LLM 全栈教程
- 区别于大型的算法包,项目代码对初级开发者更 简洁清晰 ，更"白盒子"
- 后续会持续迭代更新项目，如动手训练一个 Tiny-llama3 ，动手制作 垂直领域数据集 等等。

内容：[github content](https://github.com/datawhalechina/tiny-universe/tree/main/content)
- 深入剖析大模型原理——Qwen Blog
- 如何评估你的大模型——Tiny Eval
- 纯手工搭建 RAG 框架——Tiny RAG
- 手搓一个最小的 Agent 系统——Tiny Agent
- 深入理解大模型基础——Tiny Transformer
- 从零开始pretrain Llama3—— Tiny Llama

## transformer

### 主要结构

transformer block 主要由三种结构组成
- `MHA`(Multi-Head Attention)，多头注意力模块，下图绿色部分。
- `Add&Norm`，归一化模块，下图蓝色部分。
- `FFN`，前馈网络模块，下图粉色部分
- ![](https://pic3.zhimg.com/80/v2-e6d2a145d9fdfbb05ad48ccb7c04d2a6_1440w.webp)

gemm-like 算子

### 问题


- 1，transformer 结构？
- 2，Layer Normalization 怎么计算，以及为什么要在transformer 用LN不用BN
- 3，多头注意力机制是怎么计算的，以及为什么需要多头注意力机制


### 思考


#### self-attention 计算方法

Self-Attention 计算公式

$ self-attention = \frac{softmax(Q*K^T)}{\sqrt{d_k}}*K$

#### 为什么要除以根号dk？

目的: 点积操作后保持**数值稳定性**。
- 第一，这种缩放确保了在较大维度 dk 下，softmax 输入值不会太大，避免梯度消失问题，训练更加稳定。
- 第二，选择 sqrt(dk) 使 Q*K 结果`均值`和`方差`保持不变，类似于`归一化`。

#### self-attention 为什么用多头?

self-attention 用多头
- self-attention 模型对当前位置信息编码时，将注意力过度集中于自身位置，通过多头注意力机制来解决这一问题。
- 多头注意力机制还能够给予注意力层的输出包含有不同子空间中的编码表示信息，从而增强模型表达能力。


#### Layer Normalization 怎么计算

Layer Normalization 怎么计算



为什么要在transformer 用LN不用BN


## BERT


### 思考


#### BERT预训练任务？

BERT预训练任务
- 第一，Masked Language Model（MLM），即对mask掉的15%的token进预测
- 第二，Next Sentence Predict，判断两个句子是否是相连的两个句子。


#### BERT Finetune

怎么用BERT完成finetune
- 参考迁移学习的范式，为新任务或者数据集添加一个**分类头**或者使用**Adapter**


#### GPT vs BERT

结构和工作原理上的差异
- BERT 是 Encoder-only 结构，属于自监督训练方式，通过MLM和NSP两大预训练任务，主要用于下游任务抽特征。
- GPT 是 Decoder-only，自回归训练，预测下一个词分布，依赖大语料库，GPT-3可以表现出Few-shot/zero-shot learning。



#### 有哪些mask类型

BERT token分3种情况 mask，作用是什么？
- BERT MLM任务中，总token 15% 会被**随机Mask**掉。

确定要Mask掉的单词之后
- 80% 会直接替换为 `[Mask]`
- 10% 替换为其它**任意单词**
- 10% 保留**原始Token**。

原因
- 如果句子某个Token 100% 被mask掉，那么 fine-tuning 时, 模型就会有一些没有见过的单词。
- 加入随机Token: Transformer要保持对每个输入token的**分布式表征**，迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的**纠错**能力。

#### warm-up 策略

学习率 warm-up 策略是怎样的？

warmup 在训练最初使用较小的学习率来启动，并很快切换到大学习率而后进行常见的 decay。

- 刚开始模型对数据的“分布”理解为零，或者是说“均匀分布”（当然这取决于你的初始化）；
- 第一轮训练，每个数据点对模型来说都是新的，模型会很快地进行数据分布修正，如果这时候学习率就很大，极有可能导致开始的时候就对该数据“过拟合”，后面要通过多轮训练才能拉回来，浪费时间。
- 训练一段时间（2-3轮）后，模型已经对每个数据点看过几遍了，当前的batch而言有了一些正确的先验，较大学习率就不那么容易会使模型学偏，所以可以适当调大学习率。

这个过程是 warmup。

为什么之后还要decay呢？
- 当模型训到一定阶段后（比如十个epoch），模型分布就已经比较固定，能学到的新东西就比较少了。
- 如果还沿用较大的学习率，就会破坏这种稳定性，已经接近loss的local optimal了，为了靠近这个point，要慢慢来。


## 分词

几种
- Byte-Pair Encoding (BPE)是当前SOTA的LM模型常用的分词方法
- BERT使用的是Wordpiece，属于BPE的改进版，都是sub-word分词器

1 token ≈ 0.75 word


### 位置编码

位置编码 Position Encoding

- 原始transformer(attention is all you need)里面用的是**三角式**位置编码
- BERT使用**可学习**位置编码，预设的位置个数是512，因此最大序列长度为512
  - dim=768，就用384组三角函数来表征


### RoPE


旋转位置编码作用在什么地方？
- 三角函数位置编码是与Embedding直接相加，RoPE位置编码采用类似哈达马积相乘的形式。
- 其次，旋转位置编码只在query, key 注入位置信息，然后计算attention score，三角函数的位置编码，query, key和value都有位置信息。

### ALiBi

ALiBi(Attention with Linear Biases)的相对位置编码机制。

ALiBi的出发点是希望能提升位置编码的外推能力，原因是在实际使用中的输入token长度可能会远远大于训练阶段使用的最大token限制。


## 激活函数

### GeLU

GeLU (Gaussian Error Linear Unit) 中文名为**高斯误差线性单元**，受 RELU 和 dropout 启发
- RELU是激活小的时候乘以0
- dropout是随机乘以0
- GeLU就是概率性的乘以0 (但是跟dropout不同，用确定性的表达式给出)
- ![](https://pic2.zhimg.com/80/v2-7d21613a6d6bc7e26d815b76930abdf1_1440w.webp)

### SwiGLU

SwiGLU 和 GeGLU非常相似，只是把GeLU激活换成了Swish激活

## 损失函数


### 损失函数介绍

详见站内专题: [损失函数](loss)

### 问题

- Cross Entropy Loss 怎么写？


## Norm


preNorm/postNorm/DeepNorm

NLP任务中经常使用 Layer Normalization(LN) 而非 Batch Normalization(BN)

随机优化理论中学习率往往设置为**常数**或者**逐渐衰减** (decay)，从而保证算法收敛，这种学习率的设置方法也与机器学习里很多任务上的实际经验类似。

然而，不管是设置学习率为常数还是使学习率逐渐衰减都不能让Transformer很好地收敛。

优化Transformer结构时，还需要在训练的初始阶段设置一个非常小（接近0）的学习率，经过一定的迭代轮数后逐渐增长到初始的学习率，这个过程称作`warm-up`阶段（学习率预热）。

Warm-up 是原始Transformer结构优化时, 一个**必备**学习率调整策略。
- Transformer结构对于warm-up的超参数（持续轮数、增长方式、初始学习率等）非常敏感，若调整不慎，往往会使得模型无法正常收敛。
- ![](https://pic1.zhimg.com/80/v2-c9e89a08c38ee251c85284ce5f7bdd20_1440w.webp)

Transformer结构的优化非常困难，其具体表现在：
- warm-up阶段超参数敏感；
- 优化过程收敛速度慢。

### 总结


- BERT当时使用的是Post-Norm的结构, 同时期的GPT-1也是用该结构
- 后来的GPT2使用Pre-Norm。

Post-LN vs. Pre-LN vs. Sandwich-LN
- ![](https://pic4.zhimg.com/80/v2-846c38e3363778f7894afb61294e79cf_1440w.webp)

分析
- Pre-Norm 比 Post-Norm 参数更好调，但是最终模型精度要比Post-Norm略差。

分析公式: x + F(x), 哪里插入 normalization ?
- Norm 计算公式: 假设x和y是相互独立的均值为0，方差为1的随机变量, Norm(x+y) = (x+y)/根号2
- `PreNorm`: x1 = x + Norm(F(x))
- `PostNorm`: x1 = Norm(x + F(x))

|Norm类型|基本公式|递归展开式|
|---|---|---|
|PreNorm|x1 = x + Norm(F(x))|[原文](https://zhuanlan.zhihu.com/p/640784855)|
|PostNorm|x1 = Norm(x + F(x))|原文](https://zhuanlan.zhihu.com/p/640784855)|
|SanwichNorm|||
|DeepNorm|||

展开公式
- 输出的方差会很大，因此需要在输出加个额外的LayerNorm (GPT2的设计)
- Pre-Norm把网络的实际深度变浅了，并且增加了宽度
- Pre-Norm 网络层数有水分，这个可能是导致模型最终精度不如Post-Norm的原因。

两种Norm的特点总结如下
- Post-Norm 会削弱残差的作用，深度保持，但是收敛和调参困难
- Pre-Norm 会将网络变成**浅且宽**的结构，收敛容易，但是精度会有一定损失




### LayerNorm

Layer Normalization

按照 layer normalization 位置不同，可分为 **post layer norm** 和 **pre layer norm**
- `PostNorm` **post layer norm**。原始transformer中，layer normalization 放在残差连接之后，称为`post LN`。
  - 使用Post LN的深层transformer模型容易出现**训练不稳定**问题。
  - post LN随着transformer层数的加深，梯度范数逐渐增大，导致了训练的不稳定性。
- `PreNorm` **pre layer norm**。将 layer norm 放在残差连接的过程中，self-attention 或FFN块之前，称为“`Pre LN`”。
  - Pre layer norm 在每个transformer层的梯度范数近似相等，有利于提升训练稳定性，但缺点是pre LN可能会轻微影响transformer模型的性能，为了提升训练稳定性，GPT3、PaLM、BLOOM、OPT等大语言模型都采用了pre layer norm。
- `LayerNorm`：LayerNorm对每一层的所有激活函数进行标准化，使用它们的均值和方差来重新定位和调整激活函数。其公式如下：
- `RMSNorm`：RMSNorm通过仅使用激活函数的均方根来重新调整激活，从而提高了训练速度。
- `DeepNorm`：为了进一步稳定深度Transformer的训练，Microsoft推出了DeepNorm。这是一个创新的方法，它不仅用作标准化，还作为残差连接。有了DeepNorm的帮助，可以轻松训练高达1000层的Transformer模型，同时保持稳定性和高性能。
  - 其中，GLM-130B 和 ChatGLM 采用`DeepNorm`。其公式如下：其中SublayerSublayer是FFN或Self-Attention模块。


### RMSNorm

RMSNorm 比LayerNorm 好在哪?
- 第一，虽然时间复杂度一致，但 RMSNorm 比 LayerNorm 少了**减去均值以及加上bias**的计算，这在目前大模型预训练的计算量下就能够体现出训练速度上的优势了。
- 第二，RMSNorm 模型效果好于 LayerNorm (LN取得成功的原因可能是`缩放不变性`，而不是`平移不变性`)。


### PostNorm

论文提出了两种Layer Normalization方式并进行了对比。
- [《On Layer Normalization in the Transformer Architecture》](https://openreview.net/pdf?id=B1x8anVFPr)

把Transformer架构中传统的Add&Norm方式叫做`Post-LN`，并针对Post-LN，模型提出了`Pre-LN`，即把layer normalization加在残差连接之前
- ![](https://pic2.zhimg.com/80/v2-acff46aa054d0a3c35abcfff8b006879_1440w.webp)

造成Post-LN Transformer梯度分布出现问题的**核心**原因
- 各子层之后 Layer Normalization 层会使得**各层的输入尺度与层数L无关**，因此当Layer Normalization对梯度进行归一化时，也与层数L无关。
- Pre-LN 最后一层Layer Normalization层的输入尺寸的量级只有Post-LN的 根号(1/L) 倍，并且每个LN层都会对梯度以 
根号L 的比例归一化。所以对于Pre-LN结构来说，其每层梯度范数都近似不变。

相比于Post-LN结构梯度分布的不稳定，Pre-LN在各层之间梯度范数几乎保持不变，这种结构明显更利于优化器进行优化。而在进行一定轮数的 warm-up后，Post-LN 梯度范数也基本保持不变，并且其量级非常小(上图中绿色)，这也验证了Post-LN在warm-up阶段的训练不稳定性问题。

实验表明
- 当使用Pre-LN结构时，warm-up阶段已不再是必需，并且Pre-LN结构可以大幅提升Transformer的收敛速度。
- 对于机器翻译任务（IWSLT/WMT)，不需要warm-up的Pre-LN结构可以比Post-LN收敛快1倍左右
- 而在BERT上，Pre-LN在下游任务上达到和Post-LN相同的性能也只需要后者迭代轮数的1/3左右，并且最终的效果也更好。

### SanwichNorm

Sandwich-Norm，基于Pre-Norm再加一个

### DeepNorm

Nguyen和Salazar(2019)发现相对于`Post-LN`，`Pre-LN`能够提升Transformer的稳定性。
- 然而，`Pre-LN`在底层的梯度往往大于顶层，导致其性能不及`Post-LN`。

为了缓解这一问题，研究人员通过更好的初始化方式或模型架构来改进深度Transformer。
- 基于Post-Norm做了改进，出现了`Deep-Norm`，能训练1000层的Transformer

```py
def deepnorm(x):
	return LayerNorm(x * alpha + f(x))

def deepnorm_init(w):
	if w is ['ffn', 'v_proj', 'out_proj']:
        nn.init.xavier_normal_(w, gain=β)
    elif w is ['q_proj', 'k_proj']:
        nn.init.xavier_normal_(w, gain=1)
```

这些方法可以使多达数百层的Transformer模型实现稳定化，然而以往的方法没有能够成功地扩展至1000层。
- [DeepNet: Scaling Transformers to 1,000 Layers](https://arxiv.org/pdf/2203.00555.pdf)

`DeepNorm` 结合了`Post-LN`的良好性能以及`Pre-LN`的训练稳定性。
- 与 `Post-LN` 相比， DeepNorm 在执行层归一化之前 Up-Scale 了残差连接。

相较于Post-LN模型，DeepNet的模型更新几乎保持恒定。

参考:
- [再谈Layer-Norm：Pre-LN、Post-LN、DeepNorm](https://zhuanlan.zhihu.com/p/480783670)


## 注意力


### 掩码

【2024-8-14】[注意力机制中三种掩码技术详解和Pytorch实现](https://zhuanlan.zhihu.com/p/709073236?utm_psn=1807095944686211073)

掩码类型
- `填充掩码` Padding Mask
- `序列掩码` Sequence Mask
- `前瞻掩码` Look-ahead Mask

|掩码类型|目的|应用|功能|
|---|---|---|---|
|`填充掩码` Padding Mask|屏蔽输入数据中的无关数据|长度不一致的输入数据|识别有效数据|
|`序列掩码` Sequence Mask|模型关注范围,包含填充数据|精确控制信息流|指示哪些数据有效/填充|
|`前瞻掩码` Look-ahead Mask|防止看到未来信息|自回归模型|保证时序正确性，防止信息泄露|

掩码之间的关系
- `填充掩码`（Padding Mask）和`序列掩码`（Sequence Mask）都是在处理**序列数据**时使用的技术，帮助模型正确处理变长输入序列
- 但应用场景和功能有些区别。这两种掩码深度学习模型中被一起使用，尤其是在需要处理不同长度序列的场景下。
  - `填充掩码`指示哪些数据是填充的，用在输入数据预处理和模型的输入层。确保模型在处理或学习过程中不会将填充部分的数据当作有效数据来处理，从而影响模型的性能。在诸如Transformer模型的自注意力机制中，填充掩码用于阻止模型将注意力放在填充的序列上。
  - `序列掩码`用于更广泛的上下文中，它不仅可以指示填充位置，还可以用于其他类型的掩蔽，如在序列到序列的任务中掩蔽未来的信息（如解码器的自回归预测）。序列掩码确保模型在处理过程中**只关注当前及之前的信息**，而不是未来的信息，这对于保持信息的时序依赖性非常重要。
  - `填充掩码`多用于模型的输入阶段或在注意力机制中排除无效数据的影响
  - `序列掩码`则可能在模型的多个阶段使用，特别是在需要控制信息流的场景中。
- `前瞻掩码`用于**控制时间序列的信息流**，确保在生成序列的每个步骤中模型只能利用到当前和之前的信息。这是生成任务中保持模型正确性和效率的关键技术。



#### 填充掩码 Padding Mask

深度学习中，处理序列数据时，"填充掩码"（Padding Mask）是一个重要概念。

当序列数据长度不一致时，通常需要对短的序列进行**填充**（padding），以确保所有序列的长度相同，这样才能进行批处理。

这些填充的部分实际上是没有任何意义，不应该对模型的学习产生影响。

```py
import torch

def create_padding_mask(seq, pad_token=0):
    mask = (seq == pad_token).unsqueeze(1).unsqueeze(2)
 return mask  # (batch_size, 1, 1, seq_len)

# Example usage
seq = torch.tensor([[7, 6, 0, 0], [1, 2, 3, 0]])
padding_mask = create_padding_mask(seq)
print(padding_mask)
```

#### 序列掩码 Sequence Mask

序列掩码用于**隐藏**输入序列的某些部分。避免在计算注意力分数时考虑到填充位置的影响

比如在双向模型中，想要根据特定标准忽略序列的某些部分。

RNNs 本身可处理不同长度的序列，但在**批处理**和某些架构中，仍然需要**固定长度**的输入。序列掩码在这里可以帮助RNN忽略掉序列中的填充部分，特别是在计算最终序列输出或状态时。

在训练模型时，序列掩码也可以用来确保在计算损失函数时，不会将填充部分的预测误差纳入总损失中，从而提高模型训练的准确性和效率。

序列掩码表示为一个与序列数据维度相同的**二进制**矩阵或向量，其中1表示实际数据，0表示填充数据

```py
def create_sequence_mask(seq):
    seq_len = seq.size(1)
    mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1)
    return mask  # (seq_len, seq_len)

# Example usage
seq_len = 4
sequence_mask = create_sequence_mask(torch.zeros(seq_len, seq_len))
print(sequence_mask)
```


#### 前瞻掩码 Look-ahead Mask

前瞻掩码，也称为**因果掩码**或**未来掩码**，用于自回归模型中，以防止模型在生成序列时窥视未来的符号, 确保了给定位置的预测仅依赖于该位置之前的符号。

前瞻掩码通过在自注意力机制中屏蔽（即设置为一个非常小的负值，如负无穷大）**未来时间步**的信息来工作。这确保了在计算每个元素的输出时，模型只能使用到当前和之前的信息，而不能使用后面的信息。

这种机制对于保持**自回归属性**（即一次生成一个输出，且依赖于前面的输出）是必要的。

前瞻掩码通常表示为一个**上三角矩阵**，其中对角线及对角线以下的元素为0（表示这些位置的信息是可见的），对角线以上的元素为1（表示这些位置的信息是不可见的）。

计算注意力时，这些为1的位置会被设置为一个非常小的负数（通常是负无穷），这样经过softmax函数后，这些位置的权重接近于0，从而不会对输出产生影响。

```py
def create_look_ahead_mask(size):
    mask = torch.triu(torch.ones(size, size), diagonal=1)
    return mask  # (seq_len, seq_len)

# Example usage
look_ahead_mask = create_look_ahead_mask(4)
print(look_ahead_mask)
```


### 注意力掩码

注意力机制中，掩码被用来修改注意力得分

```py
import torch.nn.functional as F

def scaled_dot_product_attention(q, k, v, mask=None):
    matmul_qk = torch.matmul(q, k.transpose(-2, -1))
    dk = q.size()[-1]
    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))

    if mask is not None:
        scaled_attention_logits += (mask * -1e9)

    attention_weights = F.softmax(scaled_attention_logits, dim=-1)
    output = torch.matmul(attention_weights, v)
    return output, attention_weights

# Example usage
d_model = 512
batch_size = 2
seq_len = 4

q = torch.rand((batch_size, seq_len, d_model))
k = torch.rand((batch_size, seq_len, d_model))
v = torch.rand((batch_size, seq_len, d_model))
mask = create_look_ahead_mask(seq_len)

attention_output, attention_weights = scaled_dot_product_attention(q, k, v, mask)
print(attention_output)
```

## Decoder 

LLM（Language Model）是一种处理自然语言的模型架构，对输入的序列进行预测和生成。

### Encoder vs Decoder 


LLM 可以使用 `encoder-decoder`或者`decoder-only`架构。

LLM 都是 seq2seq 任务，用户输入一个文本prompt，LLM对应输出一个文本completion。
- 示例： `A Survey of Large Language Models `
- 用户输入 prompt为 `A Survey of`，期望语言模型输出 `Large Language Models`

| 模型架构	| 代表LLM	| 注意力机制	| 是否属于Decoder-Only |
| ---	| ---	| ---	| --- |
| `Causal Decoder`  |	GPT3/ChatGPT/ChatGLM2-6B |	纯单向 |	YES |
| `Encoder-Decoder` |	Flan-T5	| 输入双向，输出单向 |	NO |
| `Prefix Decoder`  |	GLM-130B/ChatGLM-6B	| 输入双向，输出单向 |	YES |

训练目标上，ChatGLM-6B 训练任务是**自回归文本填空**。
- 相比于采用causal decoder-only结构的大语言模型，采用prefix decoder-only结构的ChatGLM-6B存在一个劣势：训练效率低。
- causal decoder结构会在所有的token上计算损失，而prefix decoder只会在输出上计算损失，而不计算输入上的损失。
- 相同数量的训练tokens的情况下，prefix decoder要比causal decoder的效果差，因为训练过程中实际用到的tokens数量要更少。

分析
1. `Causal Decoder`
  - `Causal Decoder` 架构典型代表是`GPT`系列模型，单向注意力掩码，以确保每个输入token**只能**注意到过去的token和它本身，输入和输出的token通过Decoder以相同的方式进行处理。
  - 灰色代表对应的两个token互相之间看不到，否则就代表可以看到。例如，”Survery”可以看到前面的“A”，但是看不到后面的“of”。
  - Causal Decoder的sequence mask矩阵是一种典型的下三角矩阵。
2. `Encoder-Decoder`
  - Transformer最初被提出来时, 采用Encoder-Decoder架构，模型包含两部分Encoder和Decoder，两部分参数独立。
  - 其中Encoder将输入序列处理为一种**中间表示**，而Decoder则基于中间表示**自回归**地生成目标序列。
3. `Prefix Decoder`
  - `Prefix Decoder` 架构也被称为`non-causal Decoder`架构，注意力机制和Encoder-Decoder很接近，也是输入部分采用双向注意力，而输出部分采用单向注意力。
- ![](https://cdn.labellerr.com/language%20models-4/Screenshot%202023-05-21%20233029.webp)
- 原文[Exploring Architectures and Configurations for Large Language Models (LLMs)](https://www.labellerr.com/blog/exploring-architectures-and-configurations-for-large-language-models-llms/)

以上三种架构都可以通过MoE技术扩展，针对输入只激活一部分权重，典型代表: `Switch Transformer` and `GLaM`
- All three architecture types can be extended using the mixture-of-experts (MoE) scaling technique, which sparsely activates a subset of neural network weights for each input.
- This approach has been used in models like Switch Transformer and GLaM, and increasing the number of experts or the total parameter size has shown significant performance improvements.


#### 为什么LLM都是`decoder-only`架构？

为什么现在的LLM 都是 Decoder only 架构？
- 第一，**双向注意力**可能存在低秩问题。双向注意力带来的低秩问题会导致效果下降。
- 第二，在同等参数量、同等推理成本下，Decoder-only架构更优。
- 第三， Decoder-only zero-shot 能力更强。

decoder-only架构具有一定优势。
- 首先，`decoder-only`模型更简单，因为只需要生成输出，不需要考虑输入。这样可以减少计算负担，提高模型的训练和推理速度。
- 另外，`decoder-only`模型更好地解决语言建模问题，如自然语言生成、文本分类等问题。
- 其次，`decoder-only`模型更好地利用预训练任务数据。在预训练任务中
  - decoder-only模型只需要通过`掩码语言建模`（Masked Language Modelling）任务来学习上下文和语言规律。
  - 而encoder-decoder模型需要尝试预测中间**编码表示**。这对于decoder-only模型来说是一种更容易的任务，因此可以更好地利用数据，使得模型表现更好。
- 最后，`decoder-only`模型可以更好地处理**长序列**。
  - encoder-decoder模型需要在解码的时候进行**对齐**操作，因此当输入序列长度变化时，需要重新对齐，导致计算复杂度的提升。
  - 而decoder-only模型不需要进行**对齐**操作，可更好地处理长序列。

综上所述，decoder-only架构具有简单、高效、更好地利用预训练数据以及更好地处理长序列等优势
- 作者：[态灵AI](https://www.zhihu.com/question/588325646/answer/2942671075)

[成诚](https://www.zhihu.com/question/588325646/answer/3422090041)补充
- Encoder-Decoder 架构没落
  - 代表模型T5最大模型只有 11B,效果不佳
  - T5 很难使用 Pipeline Parallelism, 即便 Megatron 支持 T5 训练（[代码](https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_t5.py)），效果非常差，增加一倍 GPU，开启 PP 反而会让总吞吐下降
  - 流水并行是千卡以上分布式训练中最重要的特性
- 即便 prefix decoder架构 也纷纷转向 decoder-only
  - GLM-130B 不是 Decoder-Only 架构，但是 GLM-3 及以后（含 GLM-4）的 GLM 系列模型都改为 `Decoder-Only`架构。 就连 GLM 团队都抛弃了原有架构，Follow `LLaMa` 了。
  - HuggingFace 上可以尝试 GLM-130B 的 Playground，即使仅从 Foundation-Model 的角度评价，效果也很糟糕。
- Decoder-Only 架构最核心优势是便于 Scale Up，基于 Scaling Laws 的实际训练成本最低
  - LLM 时代，如果新算法结构可能有 **5%** 效果提升，但是引入了额外 **50%** 的 **训练成本**（计算时间 or 通信量） ，那这个新算法是负优化。
  - 因为这 50% 的训练成本，基于 Scaling Laws 在原模型上多训练 50% 的 tokens ，或者训练大一半的模型， 带来的最终提升都远大于新算法的 5%。
- 至此 2023 年下半年之后的所有 LLM （可以被用户使用的 Chat 模型）均为 Decoder-Only 架构

Encoder-Only、Encoder-Decoder、Decoder-Only 三者架构： 
- 相同参数量的训练效率上：`Decoder-Only` > `Encoder-Only` > `Encoder-Decoder`
- 现行分布式并行策略下，可以扩展参数量上限 和 分布式集群规模的上限：`Decoder-Only` >> `Encoder-Only` >> `Encoder-Decoder`

Encoder-Only 并不适合做 NLP 生成式任务（Chat）
- 目前在 CV 领域应用的比较多（ViT），输入/输出通常是固定像素大小的图片，token 之间没有非常强的先后依赖关系，因此先排除 BERT

总结
- T5 Scale up 到 100B、500B 的难度很大，训练成本的增加远远高于 GPT。 
- 因此也许 100B 的 T5 训练 10T tokens 的模型能力比 100B 的 GPT 更强，但为此要支付的算力/时间成本远大于 100B GPT 训练 10T tokens，以至于：
- 没有公司愿意支付这样的代价我还不如支付相同的代价，让 GPT 多训练更多倍的 Tokens；
- 或者训练一个参数量大很多的 GPT



### 文本生成

#### beam search 原理


流程如下：
- 初始化：设定一个宽度参数 beam width，表示每步保留的最优候选解的数量。
- 递归过程：从起始状态开始，模型会预测第一个词语的所有可能选项，并根据它们的概率保留前beam width个概率最高的选项作为候选路径。
- 扩展路径：对每个保留下来的候选路径，模型接着预测第二个词语，将当前词语添加到之前路径上，并计算新的完整路径的概率。再次保留概率最高的beam width条路径。
- 迭代求解：重复步骤3的过程，直到达到终止条件（如遇到结束符号或者达到预设的最大长度）。
- 最后，Beam Search返回的是整个搜索过程中找到的最高概率路径作为最终输出序列。

与直接采样（Sampling）的区别：

Beam Search：
- 一种**确定性**策略，总是选择**概率最高**的若干选项继续进行下一轮生成。
- 保证生成结果具有较高的概率质量，但可能会牺牲一定的**多样性**，因为不是所有低概率的序列都被考虑。
- 可能导致过拟合于训练数据中出现频率较高的模式，产生“僵化”或“机械”的输出。

直接采样（Random Sampling 或 Top-k Sampling 等）：
- 一种**随机**策略，每次生成新词时，可以按照词汇表中每个词的概率分布进行随机抽样。
- 采样方法能够生成更加**多样化**的输出，更有可能探索到新颖和未见的序列组合，有助于解决Beam Search可能导致的过于保守的问题。
- 直接采样的不确定性较大，生成的结果不一定是全局最优解，而且对于较差的概率分布可能出现生成结果质量较低的情况。

总结
- Beam Search旨在寻找**最大概率路径**，确保生成结果的合理性和准确性
- 而直接采样则通过引入随机性来增强输出的**多样性**和**创造性**，两者在实际应用中可以根据需求权衡精度和多样性来进行选择。



### Encoder vs Decoder

LLM 进化树
- 粉色分支，`Encoder-only` 框架(也叫`Auto-Encoder`)，典型 BERT等
- 绿色分支，`Encoder-decoder` 框架，典型代表如`T5`和`GLM`等
- 蓝色分支，`Decoder-only` 框架(也叫`Auto-Regressive`)，典型代表如`GPT`系列/`LLaMa`/`PaLM`等
- ![](https://pic2.zhimg.com/80/v2-bd53b7e73678a77e909271f47295d21d_1440w.webp)


直观对比
- ![](https://pic2.zhimg.com/80/v2-f096c6925eddd283ae74720f8dc02e95_1440w.webp)
- 横轴代表了输入token，纵轴代表相对应每个位置的输出token
- 左图 encoder-only，输出token都能看到所有输入token。例如 y1 这一行可以看到输入 x1-x5
- 中图 decoder-only，输出token只能看到历史的输入token。例如 y3 这行只能看到 x1-x3, x4 和 x5 看不到
- 右图为encoder-decoder，前k个输出token可以看到所有k个输入token，从k+1的输出token开始只能看到历史的输入token。例如 y1 能看到输入(y3也可以)，而开始只能看到输入x1-x4
- encoder-decoder 简化使用causal with prefix示意

三种结构不同的LLM，往往擅长处理不同的任务

|	 | NLU任务	| conditioned-NLG任务	| unconditioned-NLG任务	| 典型代表 |
|	--- | ---	| ---	| ---	| --- |
| `Encoder-only`架构 |	效果最好 |	|		| BERT |
| `Encoder-decoder`架构 |	| 效果最好	| 	| T5和GLM |
| `Decoder-only`架构	|	|	| 效果最好	| GPT系列/LLaMa/PaLM |
| 典型代表 | 文本情感分析，词性标注，信息检索 |	机器翻译，自动摘要 |	QA，ChatBot	 |


### 思考


待定


### 微软 YOCO

【2024-5-13】[YOCO：打破传统Decoder-only架构，内存消耗仅为Transformer的六分之一](https://mp.weixin.qq.com/s/X4HSyEreN4L4xTizC-_mow)

模型架构还只有三大类：Decoder-Only、Encoder-Only、Encoder-Decoder。

微软亚洲研究院推出了一种创新性的 Decoder-Decoder 架构 `YOCO`（You Only Cache Once）。通过**自解码器**和**交叉解码器**的独特架构，YOCO 仅需缓存一次键值对，从而显著降低 GPU 内存的使用。
- 论文 [You Only Cache Once: Decoder-Decoder Architectures for Language Models](https://arxiv.org/abs/2405.05254)

模型评估中，YOCO 展现出与同规模 Transformer 模型相媲美的性能，并在语言建模评估、模型大小扩展以及长上下文处理方面具有显著优势。特别是在降低 GPU 内存占用和缩短预填充延迟方面，

YOCO 整体架构设计如下，分为`自解码器`（Self-Decoder）和`交叉解码器`（Cross-Decoder）两部分。

YOCO 实现了“**模型越大，内存越省**”，为自然语言处理领域带来了全新的研究和应用范式。
- YOCO 仅缓存一次键值对，可大幅降低 GPU 内存需求，且保留全局注意力能力。

打破 GPT 系列开创的 `Decoder-Only` 架构——提出 `Decoder-Decoder` 新型架构，名为 `YOCO` (You Only Cache Once)。
- 在处理 512K 上下文长度时，标准 Transformer 内存使用是 YOCO 的6.4倍，预填充延迟是 YOCO 的30.3倍，而 YOCO 的吞吐量提升到标准 Transformer 的9.6倍。


## LLM 面试


【2024-8-23】[LLMs 相关知识及面试题](https://dongnian.icu/llm_interview_note/#//#/?id=llms-%e7%9b%b8%e5%85%b3%e7%9f%a5%e8%af%86%e5%8f%8a%e9%9d%a2%e8%af%95%e9%a2%98)



1，怎么根据llama的config文件判断出模型的参数量

2，batch size怎么设置

答：应该设置在 16-128，太大或太小都不好, 太大了收敛的不好，太小了训练不稳定

3，说一下batch size和lr之间的关系

答：一般来说 bs设置大了，lr也可以设置的更大一点


# 结束

