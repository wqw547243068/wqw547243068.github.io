---
layout: post
title:   å¤§æ¨¡å‹è½åœ°æŠ€æœ¯ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒ LLM PEFT
date:   2024-09-21 16:52:00
categories: å¤§æ¨¡å‹
tags: peft lora prompt moe
excerpt: å¤§æ¨¡å‹å·¥ä¸šè½åœ°çš„æŠ€æœ¯ç»éªŒæ€»ç»“ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒ PEFT
mathjax: true
permalink: /peft
---

* content
{:toc}

# PEFT


**å‚æ•°é«˜æ•ˆç²¾ç»†è°ƒæ•´**ï¼ˆPEFTï¼‰ï¼šä¿®æ”¹é€‰å®šå‚æ•°ï¼Œä»¥å®ç°æ›´é«˜æ•ˆçš„é€‚åº”ã€‚è¿›ä¸€æ­¥è°ƒæ•´é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªæ›´æ–°å…¶æ€»å‚æ•°çš„ä¸€å°éƒ¨åˆ†
- PEFT æ–¹æ³•å¯è®­ç»ƒçš„éƒ¨åˆ†ä¸åŒã€‚ä¸€äº›æŠ€æœ¯ä¼˜å…ˆè®­ç»ƒåŸå§‹æ¨¡å‹å‚æ•°çš„**é€‰å®šéƒ¨åˆ†**ã€‚å…¶ä»–æ–¹æ³•é›†æˆå¹¶è®­ç»ƒè¾ƒå°çš„**é™„åŠ ç»„ä»¶**ï¼Œå¦‚é€‚é…å™¨å±‚ï¼Œè€Œä¸ä¿®æ”¹åŸå§‹ç»“æ„
- ![](https://pic2.zhimg.com/80/v2-1d62f9b57373a592407db8aedd90b681_1440w.webp)

LoRAæ˜¯æœ€å¸¸ç”¨çš„ PEFT æ–¹æ³•ï¼Œä½¿ç”¨**é‡å‚æ•°åŒ–**ï¼Œè¿™ç§æŠ€æœ¯é€šè¿‡æ‰§è¡Œä½ç§©è¿‘ä¼¼æ¥ç¼©å°å¯è®­ç»ƒå‚æ•°çš„é›†åˆã€‚
- LoRA ä¼˜ç‚¹ï¼š
  - ä»»åŠ¡åˆ‡æ¢æ•ˆç‡ - åˆ›å»ºæ¨¡å‹çš„ä¸åŒç‰ˆæœ¬ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡å˜å¾—æ›´å®¹æ˜“ã€‚ä½ å¯ä»¥ç®€å•åœ°å­˜å‚¨é¢„è®­ç»ƒæƒé‡çš„å•ä¸ªå‰¯æœ¬ï¼Œå¹¶æ„å»ºè®¸å¤šå° LoRA æ¨¡å—ã€‚å½“ä½ ä»ä»»åŠ¡åˆ‡æ¢åˆ°ä»»åŠ¡æ—¶ï¼Œä½ åªæ›¿æ¢çŸ©é˜µ A å’Œ Bï¼Œå¹¶ä¿ç•™ LLMã€‚è¿™æ˜¾è‘—å‡å°‘äº†å­˜å‚¨éœ€æ±‚ã€‚
  - éœ€è¦æ›´å°‘çš„ GPU - LoRA å°† GPU å†…å­˜éœ€æ±‚å‡å°‘äº†æœ€å¤š 3 å€ï¼Œå› ä¸ºæˆ‘ä»¬ä¸è®¡ç®—/é‡æ–°è®­ç»ƒå¤§å¤šæ•°å‚æ•°ã€‚
  - é«˜ç²¾åº¦ - åœ¨å„ç§è¯„ä¼°åŸºå‡†ä¸Šï¼ŒLoRA çš„æ€§èƒ½è¢«è¯æ˜å‡ ä¹ç­‰åŒäºå…¨é¢å¾®è°ƒ - è€Œä¸”åªéœ€è¦ä¸€éƒ¨åˆ†æˆæœ¬
- PEFT ç›¸æ¯”å…¨é¢å¾®è°ƒçš„ä¼˜åŠ¿
  - æ›´é«˜æ•ˆå’Œæ›´å¿«çš„è®­ç»ƒ
  - ä¿ç•™é¢„è®­ç»ƒçš„çŸ¥è¯†

ã€2024-2-27ã€‘PEFT: 
- LORAã€QLoRA
- Adapter Tuning
- Prefix Tuningã€Prompt Tuningã€P-Tuning åŠ P-Tuning v2 ç­‰

7ä¸ªä¸»æµå¾®è°ƒæ–¹æ³•åœ¨Transformerç½‘ç»œæ¶æ„çš„ä½œç”¨ä½ç½®å’Œ[ç®€è¦è¯´æ˜](https://zhuanlan.zhihu.com/p/681254858?utm_psn=1745759328311623680)
- ![](https://pic4.zhimg.com/80/v2-615cae7a66974b32cc4e8b7ebbd4e5a7_1440w.webp)

è¯¦è§: [ä¸€æ–‡å½»åº•ææ‡‚Fine-tuning - å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-Efficient Fine-Tuningï¼‰](https://mp.weixin.qq.com/s/OFufH9hSFdLsntQkNwL4LA), å«å›¾è§£

æ›´å¤šLLMæŠ€æœ¯è½åœ°æ–¹æ¡ˆè§ç«™å†…ä¸“é¢˜ï¼š[å¤§æ¨¡å‹åº”ç”¨æŠ€æœ¯æ–¹æ¡ˆ](llm_solution)

## PEFT å‚æ•°é«˜æ•ˆå¾®è°ƒ

### è§£å†³ä»€ä¹ˆé—®é¢˜

èµ·å› ï¼šè®­ç»ƒæ¨¡å¼
- å…¨å‚æ•°å¾®è°ƒï¼šå¯¹ç‰¹å®šä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œ Full FineTuningï¼ˆå…¨å‚æ•°å¾®è°ƒï¼‰ï¼Œ**å¤ªè¿‡ä½æ•ˆ**ï¼›
- éƒ¨åˆ†å‚æ•°å¾®è°ƒï¼šå›ºå®šé¢„è®­ç»ƒæ¨¡å‹çš„æŸäº›å±‚ï¼Œåªå¾®è°ƒæ¥è¿‘ä¸‹æ¸¸ä»»åŠ¡çš„é‚£å‡ å±‚å‚æ•°ï¼Œåˆéš¾ä»¥è¾¾åˆ°**è¾ƒå¥½æ•ˆæœ**ã€‚

### è§£å†³æ€è·¯

PEFTæŠ€æœ¯é€šè¿‡**æœ€å°åŒ–**å¾®è°ƒå‚æ•°çš„æ•°é‡å’Œè®¡ç®—å¤æ‚åº¦ï¼Œæ¥æé«˜é¢„è®­ç»ƒæ¨¡å‹åœ¨æ–°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œä»è€Œç¼“è§£å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„è®­ç»ƒæˆæœ¬ã€‚
- å³ä½¿è®¡ç®—èµ„æºå—é™ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†æ¥è¿…é€Ÿé€‚åº”æ–°ä»»åŠ¡ï¼Œå®ç°é«˜æ•ˆçš„è¿ç§»å­¦ä¹ ã€‚

å› æ­¤ï¼ŒPEFTæŠ€æœ¯æé«˜æ¨¡å‹æ•ˆæœçš„åŒæ—¶ï¼Œå¤§å¤§ç¼©çŸ­æ¨¡å‹è®­ç»ƒæ—¶é—´å’Œè®¡ç®—æˆæœ¬ï¼Œè®©æ›´å¤šäººèƒ½å¤Ÿå‚ä¸åˆ°æ·±åº¦å­¦ä¹ ç ”ç©¶ä¸­æ¥ã€‚


## PEFT æ–¹æ³•

PEFT ä¸»è¦æ–¹æ³•ï¼š
- Prefix Tuningï¼ˆåœ¨**æ¨¡å‹è¾“å…¥å±‚**æ·»åŠ **å¯è®­ç»ƒ** `å‰ç¼€`åµŒå…¥ï¼‰
- LoRAï¼ˆé€šè¿‡`ä½ç§©çŸ©é˜µ`è¿‘ä¼¼æ¨¡å‹å‚æ•°æ›´æ–°ï¼‰
- Adapter Tuningï¼ˆåœ¨æ¨¡å‹å±‚é—´æ’å…¥**å°å‹ç¥ç»ç½‘ç»œ**adaptersï¼‰ã€‚


æ–¹æ³•
- `Prefix Tuning`ï¼šä¸full fine-tuningæ›´æ–°æ‰€æœ‰å‚æ•°çš„æ–¹å¼ä¸åŒï¼Œè¯¥æ–¹æ³•æ˜¯åœ¨è¾“å…¥tokenä¹‹å‰æ„é€ ä¸€æ®µä»»åŠ¡ç›¸å…³çš„virtual tokensä½œä¸ºPrefixï¼Œç„¶åè®­ç»ƒçš„æ—¶å€™åªæ›´æ–°Prefixéƒ¨åˆ†çš„å‚æ•°ï¼Œè€ŒTransformerä¸­çš„å…¶ä»–éƒ¨åˆ†å‚æ•°å›ºå®šã€‚è¯¥æ–¹æ³•å…¶å®å’Œæ„é€ Promptç±»ä¼¼ï¼Œåªæ˜¯Promptæ˜¯äººä¸ºæ„é€ çš„â€œæ˜¾å¼â€çš„æç¤º,å¹¶ä¸”æ— æ³•æ›´æ–°å‚æ•°ï¼Œè€ŒPrefixåˆ™æ˜¯å¯ä»¥å­¦ä¹ çš„â€œéšå¼â€çš„æç¤ºã€‚åŒæ—¶ï¼Œä¸ºäº†é˜²æ­¢ç›´æ¥æ›´æ–°Prefixçš„å‚æ•°å¯¼è‡´è®­ç»ƒä¸ç¨³å®šçš„æƒ…å†µï¼Œä»–ä»¬åœ¨Prefixå±‚å‰é¢åŠ äº†MLPç»“æ„(ç›¸å½“äºå°†Prefixåˆ†è§£ä¸ºæ›´å°ç»´åº¦çš„Inputä¸MLPçš„ç»„åˆåè¾“å‡ºçš„ç»“æœ)ï¼Œè®­ç»ƒå®Œæˆåï¼Œåªä¿ç•™Prefixçš„å‚æ•°ã€‚
- `Prompt Tuning`ï¼šè¯¥æ–¹æ³•å¯ä»¥çœ‹ä½œæ˜¯Prefix Tuningçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œåªåœ¨è¾“å…¥å±‚åŠ å…¥prompt tokensï¼Œå¹¶ä¸éœ€è¦åŠ å…¥MLPè¿›è¡Œè°ƒæ•´æ¥è§£å†³éš¾è®­ç»ƒçš„é—®é¢˜ã€‚éšç€é¢„è®­ç»ƒæ¨¡å‹å‚æ•°é‡çš„å¢åŠ ï¼ŒPrompt Tuningçš„æ–¹æ³•ä¼šé€¼è¿‘fine-tuningçš„ç»“æœã€‚
- `P-Tuning`ï¼šè¯¥æ–¹æ³•çš„æå‡ºä¸»è¦æ˜¯ä¸ºäº†è§£å†³è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šå¤§æ¨¡å‹çš„Promptæ„é€ æ–¹å¼ä¸¥é‡å½±å“ä¸‹æ¸¸ä»»åŠ¡çš„æ•ˆæœã€‚P-Tuningå°†Promptè½¬æ¢ä¸ºå¯ä»¥å­¦ä¹ çš„Embeddingå±‚ï¼Œå¹¶ç”¨MLP+LSTMçš„æ–¹å¼æ¥å¯¹prompt embeddingè¿›è¡Œä¸€å±‚å¤„ç†ã€‚
- `P-Tuning v2`ï¼šè®©Prompt Tuningèƒ½å¤Ÿåœ¨ä¸åŒå‚æ•°è§„æ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹ã€é’ˆå¯¹ä¸åŒä¸‹æ¸¸ä»»åŠ¡çš„ç»“æœä¸Šéƒ½è¾¾åˆ°åŒ¹æ•ŒFine-tuningçš„ç»“æœã€‚ç›¸æ¯”Prompt Tuningå’ŒP-tuningçš„æ–¹æ³•ï¼ŒP-Tuning v2æ–¹æ³•åœ¨å¤šå±‚åŠ å…¥äº†Prompts tokensä½œä¸ºè¾“å…¥ï¼Œå¸¦æ¥ä¸¤ä¸ªæ–¹é¢çš„å¥½å¤„ï¼š
  - å¸¦æ¥æ›´å¤šå¯å­¦ä¹ çš„å‚æ•°ï¼ˆä»P-tuningå’ŒPrompt Tuningçš„0.1%å¢åŠ åˆ°0.1%-3%ï¼‰ï¼ŒåŒæ—¶ä¹Ÿè¶³å¤Ÿå‚æ•°é«˜æ•ˆã€‚
  - åŠ å…¥åˆ°æ›´æ·±å±‚ç»“æ„ä¸­çš„Promptèƒ½ç»™æ¨¡å‹é¢„æµ‹å¸¦æ¥æ›´ç›´æ¥çš„å½±å“ã€‚
- `Adapter Tuning`ï¼šè¯¥æ–¹æ³•è®¾è®¡äº†Adapterç»“æ„ï¼ˆé¦–å…ˆæ˜¯ä¸€ä¸ªdown-projectå±‚å°†é«˜ç»´åº¦ç‰¹å¾æ˜ å°„åˆ°ä½ç»´ç‰¹å¾ï¼Œç„¶åè¿‡ä¸€ä¸ªéçº¿å½¢å±‚ä¹‹åï¼Œå†ç”¨ä¸€ä¸ªup-projectç»“æ„å°†ä½ç»´ç‰¹å¾æ˜ å°„å›åŸæ¥çš„é«˜ç»´ç‰¹å¾ï¼›åŒæ—¶ä¹Ÿè®¾è®¡äº†skip-connectionç»“æ„ï¼Œç¡®ä¿äº†åœ¨æœ€å·®çš„æƒ…å†µä¸‹èƒ½å¤Ÿé€€åŒ–ä¸ºidentityï¼‰ï¼Œå¹¶å°†å…¶åµŒå…¥Transformerçš„ç»“æ„é‡Œé¢ï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œå›ºå®šä½åŸæ¥é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ä¸å˜ï¼Œåªå¯¹æ–°å¢çš„Adapterç»“æ„è¿›è¡Œå¾®è°ƒã€‚åŒæ—¶ä¸ºäº†ä¿è¯è®­ç»ƒçš„é«˜æ•ˆæ€§ï¼ˆä¹Ÿå°±æ˜¯å°½å¯èƒ½å°‘çš„å¼•å…¥æ›´å¤šå‚æ•°ï¼‰ã€‚
- `LoRA`ï¼šåœ¨æ¶‰åŠåˆ°çŸ©é˜µç›¸ä¹˜çš„æ¨¡å—ï¼Œå¼•å…¥Aã€Bè¿™æ ·ä¸¤ä¸ªä½ç§©çŸ©é˜µæ¨¡å—å»æ¨¡æ‹Ÿfull fine-tuningçš„è¿‡ç¨‹ï¼Œç›¸å½“äºåªå¯¹è¯­è¨€æ¨¡å‹ä¸­èµ·å…³é”®ä½œç”¨çš„ä½ç§©æœ¬è´¨ç»´åº¦è¿›è¡Œæ›´æ–°ã€‚

### PEFT å®ç°

PEFTå®ç°å·¥å…·ï¼š
- PEFTï¼šHuggingfaceæ¨å‡ºçš„PEFTåº“ã€‚
- unify-parameter-efficient-tuningï¼šä¸€ä¸ªå‚æ•°é«˜æ•ˆè¿ç§»å­¦ä¹ çš„ç»Ÿä¸€æ¡†æ¶ã€‚

Parameter-Efficient Fine-Tuning (`PEFT`) æ˜¯HuggingFace å¼€æºçš„ä¸€ä¸ªé«˜æ•ˆå¾®è°ƒå¤§æ¨¡å‹åº“ï¼Œæ”¯æŒåœ¨ LLM ä¸Šåˆ›å»ºå’Œå¾®è°ƒé€‚é…å™¨å±‚ã€‚
- peft ä¸  ğŸ¤— Accelerate æ— ç¼é›†æˆï¼Œç”¨äºåˆ©ç”¨äº† DeepSpeed å’Œ Big Model Inference çš„å¤§è§„æ¨¡æ¨¡å‹ã€‚

ã€2023-7-11ã€‘[Promptç³»åˆ—é«˜æ•ˆè°ƒå‚åŸç†è§£æ](https://mp.weixin.qq.com/s/webUB5j8nNQsthTFQNiqpA), [æ™ºæºç¤¾åŒº](https://hub.baai.ac.cn/view/28876)

PEFTå†…ç½®7ç§ä¸»æµé«˜æ•ˆè°ƒå‚æ–¹æ³•
- `LoRA`: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS
- `Prefix Tuning`: Prefix-Tuning: Optimizing Continuous Prompts for Generation,
- `P-Tuning v2`: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
- `P-Tuning`: GPT Understands, Too
- `Prompt Tuning`: The Power of Scale for Parameter-Efficient Prompt Tuning
- `AdaLoRA`: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning
- `QLoRA`: QLoRA: Efficient Finetuning of Quantized LLMs

æ—¶é—´çº¿
- ![](https://simg.baai.ac.cn/hub-detail/f81b72c3cedac0b670dd7c68144b718c1692420361295.webp)

|æ—¶é—´|æœºæ„|æ–¹æ³•|å¤‡æ³¨|
|---|---|---|---|
|2021.1|stanford|Prefix Tuning|Prompt Series|
|2021.3|Tsinghua|P-Tuning|Prompt Series|
|2021.9|Google|Prompt Tuning|Prompt Series|
|2021.11|Microsoft|LoRA|LoRA Series|
|2022.3|Tsinghua|P-Tuning v2|Prompt Series|
|2023.3|Microsoft|AdaLoRA|LoRA Series|
|2023.5|Washington|QLoRA|LoRA Series|

ç›®å‰åŒ…å«LoRAï¼ŒPrefix Tuningï¼ŒPrompt Tuningï¼ŒP-Tuning å››ç§ç®—æ³•
*   LoRA
*   [Prefix Tuning](https://arxiv.org/pdf/2110.07602.pdf)
  - Prefix Tuning ç®—æ³•æ˜¯æ ¹æ® ä¸‹æ¸¸ä»»åŠ¡ "å‰ç¼€æŒ‡ä»¤æ–‡æœ¬" çš„æ‰€æœ‰å±‚çš„embedingè¡¨ç¤ºï¼Œå­¦ä¹ åˆ°çš„å‰ç¼€æŒ‡ä»¤æ–‡æœ¬å‘é‡å¯ä»¥æŒ–æ˜å¤§æ¨¡å‹çš„æ½œåŠ›å»å¼•å¯¼æ¨¡å‹å®Œæˆç‰¹å®šä»»åŠ¡ã€‚
  - ![](https://pic3.zhimg.com/80/v2-9a6b5792cf60079429d067fc629e65ae_1440w.webp)
*   [P-Tuning](https://arxiv.org/pdf/2103.10385.pdf)
  - P-Tuning ç®—æ³•å’Œ Prefix Tuning çš„æƒ³æ³•å¾ˆç›¸ä¼¼ï¼Œæƒ³é€šè¿‡å¾®è°ƒ"æŒ‡ä»¤æ–‡æœ¬",è®©æŒ‡ä»¤æ–‡æœ¬å»æŒ–æ˜å¤§æ¨¡å‹çš„æ½œåŠ›å»å®Œæˆç‰¹å®šçš„ä»»åŠ¡ã€‚ä½†æ˜¯ P-Tuning åªå­¦ä¹  "æŒ‡ä»¤æ–‡æœ¬" è¾“å…¥å±‚embedingçš„çš„è¡¨ç¤ºã€‚ ä¸ºäº†å¢å¼º "æŒ‡ä»¤æ–‡æœ¬"çš„è¿ç»­æ€§ï¼Œé‡‡ç”¨äº†ä¸€ä¸ª MLP(LSTM) çš„ç»“æœå»encoding "æŒ‡ä»¤æ–‡æœ¬"ã€‚ä»å¾®è°ƒå‚æ•°é‡æ¥çœ‹åªæœ‰ 0.65% æ¯” Prefix Tuning å’Œ LoRA è¿™äº›åœ¨æ‰€æœ‰å±‚éƒ½å¢åŠ å‚æ•°çš„æ–¹æ³•è¦å°‘ã€‚
  - ![](https://pic3.zhimg.com/80/v2-7540fb5d913adcae8be308fce31befea_1440w.webp)
*   [Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)
  - Prompt Tuning ç®—æ³•å’Œ P-Tuning å¾ˆåƒï¼Œä¸”æ›´ç®€å•ï¼Œå°±æ˜¯æ˜¯æ ¹æ® ä¸‹æ¸¸ä»»åŠ¡ "æŒ‡ä»¤æ–‡æœ¬" è¾“å…¥å±‚embedingçš„çš„è¡¨ç¤ºã€‚ Prompt Tuning æ²¡æœ‰å¢åŠ ä»»ä½•çš„å±‚ï¼Œç›´æ¥ä½¿ç”¨å¾®è°ƒæŒ‡ä»¤æ–‡æœ¬(prompt) çš„embedingå‘é‡ã€‚
  - ![](https://pic3.zhimg.com/80/v2-b281f773be36787dddd0f06e782384b2_1440w.webp)

[è¯¦è§](https://zhuanlan.zhihu.com/p/618695885)

[Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft) (PEFT)

å•ä¸ª 24GB GPU ä¸Šä½¿ç”¨ä¸Šè¿°å·¥å…·ä½¿ç”¨ RL å¾®è°ƒ 20B å‚æ•°é‡çš„ LLM, è¯¦è§é‡åŒ–[quantization](https://hf.co/docs/transformers/main/en/main_classes/quantization)
- ä¸å…¨ç²¾åº¦æ¨¡å‹ç›¸æ¯”ï¼Œä»¥ **8ä½**ç²¾åº¦åŠ è½½æ¨¡å‹æœ€å¤šå¯èŠ‚çœ **4å€**çš„å†…å­˜
- è°ƒç”¨ from_pretrained æ–¹æ³•æ—¶ç®€å•åœ°æ·»åŠ æ ‡å¿— load_in_8bit=True

è¯¦è§ï¼š[åœ¨ä¸€å¼  24 GB çš„æ¶ˆè´¹çº§æ˜¾å¡ä¸Šç”¨ RLHF å¾®è°ƒ 20B LLMs](https://mp.weixin.qq.com/s/7nmegO1UYObO0-eUDTKnMg)

### PEFT ä¸è¶³

ç›¸æ¯”å…¨å‚æ•°å¾®è°ƒï¼Œé«˜æ•ˆå¾®è°ƒæŠ€æœ¯ç›®å‰å­˜åœ¨çš„ä¸¤ä¸ªé—®é¢˜ï¼š
- æ¨ç†é€Ÿåº¦ä¼šå˜æ…¢
- æ¨¡å‹ç²¾åº¦ä¼šå˜å·®


## åº”ç”¨ç¤ºä¾‹

å…¸å‹åº”ç”¨ï¼š
- `ChatGLM-Tuning` ï¼šä¸€ç§å¹³ä»·çš„chatgptå®ç°æ–¹æ¡ˆï¼ŒåŸºäºæ¸…åçš„ ChatGLM-6B + LoRA è¿›è¡Œfinetuneã€‚
- `Alpaca-Lora`ï¼šä½¿ç”¨ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰å¤ç°æ–¯å¦ç¦ç¾Šé©¼çš„ç»“æœã€‚Stanford Alpaca æ˜¯åœ¨ LLaMA æ•´ä¸ªæ¨¡å‹ä¸Šå¾®è°ƒï¼Œè€Œ Alpaca-Lora åˆ™æ˜¯åˆ©ç”¨ Lora æŠ€æœ¯ï¼Œåœ¨å†»ç»“åŸæ¨¡å‹ LLaMA å‚æ•°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¾€æ¨¡å‹ä¸­åŠ å…¥é¢å¤–çš„ç½‘ç»œå±‚ï¼Œå¹¶åªè®­ç»ƒè¿™äº›æ–°å¢çš„ç½‘ç»œå±‚å‚æ•°ã€‚ç”±äºè¿™äº›æ–°å¢å‚æ•°æ•°é‡è¾ƒå°‘ï¼Œè¿™æ ·ä¸ä»…å¾®è°ƒçš„æˆæœ¬æ˜¾è‘—ä¸‹é™ï¼Œè¿˜èƒ½è·å¾—å’Œå…¨æ¨¡å‹å¾®è°ƒç±»ä¼¼çš„æ•ˆæœã€‚
- `BLOOM-LORA`ï¼šç”±äºLLaMAçš„é™åˆ¶ï¼Œæˆ‘ä»¬å°è¯•ä½¿ç”¨Alpaca-Loraé‡æ–°å®ç°BLOOM-LoRAã€‚


## å¾®è°ƒåŸç†

FineTune å¾®è°ƒ

é¢„è®­ç»ƒæ¨¡å‹åœ¨å°è§„æ¨¡ç‰¹å®šæ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒï¼Œè°ƒæ•´æ¨¡å‹æƒé‡ï¼Œé€‚åº”ç‰¹å®šä»»åŠ¡æˆ–æé«˜å…¶æ€§èƒ½ã€‚
- ![](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*JSJBBnslBE9S5i77Rz9r_g.png)

ã€2023-6-25ã€‘[å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆä¸ƒï¼‰-æœ€ä½³å®è·µã€æ€»ç»“](https://mp.weixin.qq.com/s/M-7ZudD0dvscsApryiPIYw)

å‚æ•°é«˜æ•ˆå¾®è°ƒç»¼è¿°è®ºæ–‡ï¼š
- ã€2023-3-28ã€‘[Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf)
- ä¸­æ–‡[è§£è¯»](https://zhuanlan.zhihu.com/p/627537421)

å‡ ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•è¿›è¡Œäº†ç®€å•çš„æ¦‚è¿°ï¼Œä¸»è¦æœ‰å¦‚ä¸‹å‡ ç±»ï¼š
- `Additive` å¢åŠ **é¢å¤–å‚æ•°**ï¼Œæ³¨æ„åŠ›ä¸FFNååŠ ä¸€ä¸ª**å…¨è¿æ¥å±‚**
  - å¦‚ï¼šPrefix Tuningã€Prompt Tuningã€Adapter TuningåŠå…¶å˜ä½“ã€‚
  - soft prompts è½¯æç¤º
    - Prompt Tuning: è¾“å…¥å‰é¢åŠ å…¥ä¸€äº›æ–°çš„å¯å­¦ä¹ çš„åµŒå…¥å‘é‡
    - Prefix-Tuning: åœ¨æ‰€æœ‰å±‚å‰é¢åŠ å…¥å¯å­¦ä¹ çš„å‚æ•°
    - Intrinsic Prompt Tuning (IPT): ç”¨è‡ªç¼–ç å™¨æ¥å‹ç¼©soft Compact
  - adapters-like é€‚é…å™¨
    - Adapter: åœ¨æ³¨æ„åŠ›ä¸FFNååŠ ä¸€ä¸ªå…¨è¿æ¥å±‚
    - AdaMix: é‡‡ç”¨MOEç­–ç•¥å¼•å…¥å¤šä¸ªAdapters
  - å…¶å®ƒ
    - Ladder-Side Tuning (LST): åœ¨æ¯ä¸ªTransformer blockæ—è¾¹å¼•å…¥ä¸€ä¸ªå°å‹çš„Transformeræ¥è®¡ç®—æ›´æ–°çš„å‚æ•°ï¼Œç±»ä¼¼äºLora
    - (IA)3: ç¼©æ”¾ key valueä»¥åŠFFNçš„ æ¿€æ´»å‡½æ•°
- `Selecttive` é€‰å–ä¸€**éƒ¨åˆ†å‚æ•°**æ›´æ–°ï¼Œå¦‚ï¼šBitFitã€‚
  - BitFitï¼šä»…æ›´æ–°bias
  - DiffPruning: maskæ‰ä¸€äº›è®­ç»ƒçš„å‚æ•°
  - Freeze and Reconfigure (FAR): æŒ‰ç…§è¡Œæ¥åˆ’åˆ†ä¸º è®­ç»ƒçš„è¡Œä¸ å†»ç»“çš„è¡Œ
  - FishMask: ä½¿ç”¨Fisherä¿¡æ¯çŸ©é˜µæ¥é€‰å– top-på‚æ•°è¿›è¡Œæ›´æ–°
- `Reparametrization` å¼•å…¥**é‡å‚æ•°åŒ–**ï¼Œå¦‚ï¼šLoRAã€AdaLoRAã€QLoRAã€‚
  - Intrinsic SAID: æ›´æ–°ä¸€ä¸ªä½ç»´ç©ºé—´çš„å‘é‡
  - LoRA: æ›´æ–°æ—è·¯ï¼Œä¸”æ—è·¯è®¾è®¡ä¸ºä¸€ä¸ªä¸‹é‡‡æ ·ä¸ä¸€ä¸ªä¸Šé‡‡æ ·
  - KronA: ä½¿ç”¨å…‹ç½—å†…å…‹ç§¯æ¥å‡å°Loraçš„è®¡ç®—å¼€æ”¯
- **æ··åˆ**é«˜æ•ˆå¾®è°ƒï¼Œå¦‚ï¼šMAM Adapterã€UniPELTã€‚
  - SparseAdapter: ä½¿ç”¨ä¸€ä¸ªç»´åº¦è¾ƒå¤§çš„Adapterï¼Œå¹¶å¯¹è¿™ä¸ªAdapterç¨€ç–åŒ–ï¼Œé¿å…å‚æ•°è¿‡å¤š
  - MAM Adapter: å¹¶è¡Œçš„Adapter, FFN layer and soft prompt.
  - UniPELT: å°†LoRa Prefix-tuning å’Œ Adapter ä½¿ç”¨gatæœºåˆ¶åˆå¹¶
  - Compacter: ä½¿ç”¨å…‹ç½—å†…å…‹ç§¯ï¼Œå¹¶ä¸”æ¯å±‚å…±äº«å‚æ•°çš„ Adapter
  - S4: æ¢ç´¢äº†è¿™äº›æ–¹æ³•ç»“åˆèµ·æ¥çš„æ•ˆæœ

é«˜æ•ˆå¾®è°ƒç²—ç•¥åˆ†ä¸ºä¸‰ç±»ï¼š
- åŠ é¢å¤–å‚æ•° `A` + é€‰å–ä¸€éƒ¨åˆ†å‚æ•°æ›´æ–° `S` + å¼•å…¥é‡å‚æ•°åŒ– `R`
- ![](https://pic2.zhimg.com/80/v2-ed42c72dfe5b849dfeb5df142f270675_1440w.webp)

å„ç§æ–¹æ³•å¯¹æ¯”
- ![](https://pic1.zhimg.com/80/v2-87347f7802c02861ec2ed937d5a0422c_1440w.webp)

## BitFit

å¯¹å¾®è°ƒæœºåˆ¶çš„ä¸€ç§ç§¯ææ¢ç´¢ï¼Œé€šè¿‡**ä»…è°ƒæ•´bias**å°±æœ‰ä¸é”™çš„æ•ˆæœï¼Œä½†æ²¡æœ‰å…·ä½“é˜è¿°åŸç†ï¼Œé€šè¿‡çŒœæµ‹åŠ å®éªŒå¾—åˆ°çš„ç»“æœã€‚

è§‚ç‚¹ï¼š
> å¾®è°ƒè¿‡ç¨‹ä¸æ˜¯è®©æ¨¡å‹**é€‚åº”**å¦å¤–çš„æ•°æ®åˆ†å¸ƒï¼Œè€Œæ˜¯è®©æ¨¡å‹æ›´å¥½çš„**åº”ç”¨å‡º**æœ¬èº«çš„è¡¨å¾èƒ½åŠ›ã€‚

ç‰¹ç‚¹ï¼š
- è®­ç»ƒå‚æ•°é‡æå°ï¼ˆçº¦0.1%ï¼‰ã€‚
- å¤§éƒ¨åˆ†ä»»åŠ¡ä¸Šæ•ˆæœä¼š**å·®äº**LoRAã€Adapterç­‰æ–¹æ³•ã€‚

## Prefix Tuning

Prefix Tuning é€šè¿‡åœ¨æ¨¡å‹è¾“å…¥å±‚ä¹‹å‰æ·»åŠ å¯è®­ç»ƒçš„`å‰ç¼€åµŒå…¥`ï¼ˆprefix embeddingsï¼‰æ¥å½±å“æ¨¡å‹çš„è¾“å‡ºã€‚

è¿™äº›å‰ç¼€åµŒå…¥ä¸åŸå§‹è¾“å…¥æ‹¼æ¥åä¸€èµ·è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œè€Œæ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†ä¿æŒä¸å˜ã€‚

Prefix Tuningï¼ˆå‰ç¼€å¾®è°ƒï¼‰

ä»€ä¹ˆæ˜¯Prefix Tuningï¼Ÿ
- Prefix Tuning åœ¨åŸå§‹æ–‡æœ¬è¿›è¡Œè¯åµŒå…¥ä¹‹åï¼Œåœ¨**å‰é¢**æ‹¼æ¥ä¸Šä¸€ä¸ª**å‰ç¼€çŸ©é˜µ**ï¼Œæˆ–å°†å‰ç¼€çŸ©é˜µæ‹¼åœ¨æ¨¡å‹æ¯ä¸€å±‚çš„è¾“å…¥å‰ã€‚
- è¿™ä¸ªå‰ç¼€ä¸è¾“å…¥åºåˆ—ä¸€èµ·ä½œä¸ºæ³¨æ„åŠ›æœºåˆ¶çš„è¾“å…¥ï¼Œä»è€Œå½±å“æ¨¡å‹å¯¹è¾“å…¥åºåˆ—çš„ç†è§£å’Œè¡¨ç¤ºã€‚
- ç”±äºå‰ç¼€å¯å­¦ä¹ ï¼Œå¯åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ ¹æ®ç‰¹å®šä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”æ–°çš„é¢†åŸŸæˆ–ä»»åŠ¡ã€‚

æ³¨æ„
- finetune æ›´æ–°æ•´ä¸ªæ¨¡å‹æƒé‡
- prefix tuning åˆ™åªæ›´æ–°å‰ç¼€ç¼–ç , æ¨¡å‹ç¼–ç ä¸åŠ¨, fixed

è¯¦è§: [ä¸€æ–‡å½»åº•ææ‡‚Fine-tuning - å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-Efficient Fine-Tuningï¼‰](https://mp.weixin.qq.com/s/OFufH9hSFdLsntQkNwL4LA), å«å›¾è§£

### ã€2021.3.2ã€‘Prompt Tuning -- ç¦»æ•£token

å—è¯­è¨€æ¨¡å‹ in-context learningèƒ½åŠ›å¯å‘ï¼Œåªè¦æœ‰åˆé€‚çš„ä¸Šä¸‹æ–‡ï¼Œè¯­è¨€æ¨¡å‹å°±å¯ä»¥å¾ˆå¥½çš„è§£å†³è‡ªç„¶è¯­è¨€ä»»åŠ¡ã€‚

é’ˆå¯¹ä¸åŒä»»åŠ¡ï¼Œä»…åœ¨è¾“å…¥å±‚å¼•å…¥virtual tokenå½¢å¼çš„**è½¯æç¤º**ï¼ˆsoft promptï¼‰ã€‚

ç‰¹ç‚¹ï¼š
- ç›¸å¯¹äºPrefix Tuningï¼Œå‚ä¸è®­ç»ƒçš„å‚æ•°é‡å’Œæ”¹å˜çš„**å‚æ•°é‡æ›´å°**ï¼Œæ›´èŠ‚çœæ˜¾å­˜ã€‚
- å¯¹ä¸€äº›ç®€å•çš„NLU ä»»åŠ¡è¿˜ä¸é”™ï¼Œä½†å¯¹**ç¡¬åºåˆ—**æ ‡è®°ä»»åŠ¡ï¼ˆå³`åºåˆ—æ ‡æ³¨`ï¼‰è¡¨ç°æ¬ ä½³ã€‚

**Prompt Tuning with soft prompts**

è¾“å…¥å±‚å¢åŠ å¯è®­ç»ƒçš„Soft Promptå‚æ•°ï¼Œå‚æ•°é•¿åº¦ä¸€èˆ¬åœ¨20-100ä¸ªï¼Œæ¯ä¸ªå‚æ•°çš„embeddingç»´åº¦å’Œè¯è¡¨tokençš„embeddingç»´åº¦ç›¸åŒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- ![](https://pic1.zhimg.com/80/v2-85458b36242c77954879891f231bf2ec_1440w.webp)

ç›¸è¾ƒäº**å…¨å‚æ•°å¾®è°ƒ**ï¼ŒPrompt Tuningä¹Ÿæ˜¯é€šè¿‡**å†»ç»“LLMçš„åŸå§‹å‚æ•°**ï¼Œæ·»åŠ å°‘é‡é¢å¤–è®­ç»ƒå‚æ•°ä»¥è¾¾åˆ°åŠ é€Ÿè®­ç»ƒçš„ç›®çš„ï¼›
- ![](https://pic1.zhimg.com/80/v2-027a29613e0f2d780b44cbc0261c3218_1440w.webp)

Prompt Tuning å®é™…æ•ˆæœä»ä¸‹é¢å›¾ä¸­çœ‹å‡ºï¼š
- 1ï¼Œå½“æ¨¡å‹å‚æ•°ä¸å¤§çš„æ—¶å€™ï¼ŒPrompt Tuningæ¯”å…¨å‚æ•°å¾®è°ƒçš„æ•ˆæœå·®ä¸€ç‚¹ï¼Œä½†æ˜¯é«˜äºå•çº¯çš„Promptå·¥ç¨‹ï¼›
- 2ï¼Œå½“æ¨¡å‹å‚æ•°åœ¨100äº¿æ—¶ï¼ŒPrompt Tuningçš„æ•ˆæœå’Œå…¨å‚æ•°å¾®è°ƒçš„æ•ˆæœä¸€æ ·å¥½ï¼›
- ![](https://pic2.zhimg.com/80/v2-549a94de38e793377170c271a46bf9bd_1440w.webp)

Prompt Tuning å¯è§£é‡Šæ€§è¯´æ˜ï¼š
1.  å¯¹äºå·²å®Œæˆè®­ç»ƒçš„prompt embeddingæ¥è¯´ï¼Œæ˜¯æ— æ³•ä¸è¯è¡¨ä¸­ä»»ä½•tokenè¡¨ç¤ºå¯¹åº”çš„ï¼ˆTrained soft-prompt embedding does not correspond to a known tokenï¼‰ï¼›
2.  ä½†æ˜¯è§‚å¯Ÿå…¶é‚»åŸŸèŒƒå›´å†…çš„tokenè¡¨ç¤ºå¯ä»¥çœ‹å‡ºå…¶å…·æœ‰ç›¸åŒçš„è¯­ä¹‰ï¼Œèƒ½å¤Ÿè¡¨ç¤ºç›¸åŒçš„æ„æ€ï¼ˆbut nearest neighbors form a semantic group with similar meaningsï¼‰ï¼›
- ![](https://pic3.zhimg.com/80/v2-2baec89ee6fc953eb3cc690264b67caa_1440w.webp)
- ![](https://pic1.zhimg.com/80/v2-5da93b477f40d3253d743edbdbe26478_1440w.webp)

å›ºå®šé¢„è®­ç»ƒå‚æ•°ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡é¢å¤–æ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªembeddingï¼Œä¹‹åæ‹¼æ¥queryæ­£å¸¸è¾“å…¥LLMï¼Œå¹¶åªè®­ç»ƒè¿™äº›embeddingã€‚
- å·¦å›¾ä¸ºå•ä»»åŠ¡å…¨å‚æ•°å¾®è°ƒï¼Œå³å›¾ä¸ºprompt tuningã€‚
- ![](https://pic2.zhimg.com/80/v2-2f6f96f9cf6614111e9a2e7c0eb9fbcd_1440w.webp)



```py
from peft import PromptTuningConfig, get_peft_model
peft_config = PromptTuningConfig(task_type="SEQ_CLS", num_virtual_tokens=10)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)
model = get_peft_model(model, peft_config)
```


### ã€2021.8.1ã€‘Prefix Tuning -- è¿ç»­token

prompt tuning æ˜¯ Prefix Tuning ç®€åŒ–ç‰ˆæœ¬

prefix tuning ä¾ç„¶æ˜¯å›ºå®šé¢„è®­ç»ƒå‚æ•°ï¼Œä½†é™¤äº†æ¯ä¸ªä»»åŠ¡é¢å¤–æ·»åŠ ä¸€ä¸ªæˆ–å¤šä¸ªembeddingä¹‹å¤–ï¼Œåˆ©ç”¨å¤šå±‚æ„ŸçŸ¥ç¼–ç prefixï¼Œæ³¨æ„å¤šå±‚æ„ŸçŸ¥æœºå°±æ˜¯prefixçš„ç¼–ç å™¨ï¼Œä¸å†åƒprompt tuningç»§ç»­è¾“å…¥LLMã€‚

prompt tuning é’ˆå¯¹ç‰¹å®šä»»åŠ¡æ‰¾åˆ°**ç¦»æ•£tokenå‰ç¼€**ï¼ŒèŠ±è´¹å¾ˆé•¿æ—¶é—´

prefix-tuning ä½¿ç”¨è¿ç»­çš„**virtual token** embeddingæ¥æ›¿æ¢ç¦»æ•£token

åœ¨æ¯ä¸€ä¸ªTransformerå±‚éƒ½å¸¦ä¸Šä¸€äº›virtual tokenä½œä¸ºå‰ç¼€ï¼Œä»¥é€‚åº”ä¸åŒçš„ä»»åŠ¡ã€‚

transformerä¸­çš„æ¯ä¸€å±‚ï¼Œå¥å­è¡¨å¾å‰é¢æ’å…¥å¯è®­ç»ƒçš„virtual token embeddingã€‚å¯¹äºè‡ªå›å½’æ¨¡å‹(GPTç³»åˆ—)ï¼Œåœ¨å¥å­å‰æ·»åŠ **è¿ç»­å‰ç¼€**ï¼Œå³ `z=[prefix;x;y]` ã€‚å¯¹äºEncoder-Decoderæ¨¡å‹(T5)ï¼Œåˆ™åœ¨Ecoderå’ŒDecoderå‰éƒ½æ·»åŠ è¿ç»­å‰ç¼€ `z=[prefix;x|prefix'|y]`

æ·»åŠ å‰ç¼€çš„è¿‡ç¨‹å¦‚å›¾æ‰€ç¤ºã€‚
- ![](https://pic2.zhimg.com/v2-2fd28b31d2a3261fa17c50cdaee19a05_r.jpg)

è™½ç„¶ï¼Œprefix-tuningå¹¶æ²¡æœ‰æ·»åŠ å¤ªå¤šçš„é¢å¤–å‚æ•°ã€‚ä½†æ˜¯ï¼Œprefix-tuning**éš¾ä»¥ä¼˜åŒ–**ï¼Œä¸”ä¼šå‡å°‘ä¸‹æ¸¸ä»»åŠ¡çš„åºåˆ—é•¿åº¦ã€‚

ç‰¹ç‚¹ï¼š
- å‰ç¼€Tokenä¼šå ç”¨åºåˆ—é•¿åº¦ï¼Œæœ‰ä¸€å®šé¢å¤–è®¡ç®—å¼€é”€ã€‚
- Prefix Tuningçš„çº¿æ€§æ’å€¼æ¯”è¾ƒå¤æ‚ã€‚

```py
embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)
transform = torch.nn.Sequential(
    torch.nn.Linear(token_dim, encoder_hidden_size),
    torch.nn.Tanh(),
    torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),
)
```

prefix tuning ä»£ç 

```py
peft_config = PrefixTuningConfig(task_type="CAUSAL_LM", num_virtual_tokens=20)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)
model = get_peft_model(model, peft_config)
```


### ã€2021.11.2ã€‘P-Tuning

æ‰‹åŠ¨å°è¯•æœ€ä¼˜çš„æç¤ºæ— å¼‚äºå¤§æµ·æé’ˆï¼Œäºæ˜¯æœ‰äº†**è‡ªåŠ¨ç¦»æ•£æç¤ºæœç´¢**æ–¹æ³•

ä½†æç¤ºæ˜¯ç¦»æ•£çš„ï¼Œç¥ç»ç½‘ç»œæ˜¯è¿ç»­çš„ï¼Œæ‰€ä»¥å¯»æ‰¾æœ€ä¼˜æç¤ºå¯èƒ½æ˜¯æ¬¡ä¼˜çš„ã€‚

p-tuning ä¾ç„¶å›ºå®šLLMå‚æ•°ï¼Œåˆ©ç”¨å¤šå±‚æ„ŸçŸ¥æœºå’ŒLSTMå¯¹promptè¿›è¡Œç¼–ç ï¼Œç¼–ç ä¹‹åä¸å…¶ä»–å‘é‡è¿›è¡Œæ‹¼æ¥ä¹‹åæ­£å¸¸è¾“å…¥LLMã€‚
- æ³¨æ„ï¼Œè®­ç»ƒååªä¿ç•™promptç¼–ç ä¹‹åçš„å‘é‡å³å¯ï¼Œæ— éœ€ä¿ç•™ç¼–ç å™¨
- GPTåœ¨P-tuning åŠ æŒä¸‹å¯è¾¾åˆ°ç”šè‡³è¶…è¿‡BERTåœ¨NLUé¢†åŸŸçš„æ€§èƒ½ã€‚

prompt ç¼–ç å™¨ç»“æ„

```py
self.lstm_head = torch.nn.LSTM(
                    input_size=self.input_size,
                    hidden_size=self.hidden_size,
                    num_layers=num_layers,
                    dropout=lstm_dropout,
                    bidirectional=True,
                    batch_first=True,
  )

self.mlp_head = torch.nn.Sequential(
    torch.nn.Linear(self.hidden_size * 2, self.hidden_size * 2),
    torch.nn.ReLU(),
    torch.nn.Linear(self.hidden_size * 2, self.output_size),
)
self.mlp_head(self.lstm_head(input_embeds)[0])
```


å°†Promptè½¬æ¢ä¸ºå¯å­¦ä¹ çš„**Embeddingå±‚**ï¼Œå¹¶ç”¨ MLP+LSTM æ–¹å¼å¯¹ Prompt Embedding è¿›è¡Œä¸€å±‚å¤„ç†ã€‚
- ç›¸æ¯” Prefix Tuningï¼Œä»…åœ¨è¾“å…¥å±‚åŠ å…¥**å¯å¾®**çš„virtual tokenï¼›
- å¦å¤–ï¼Œvirtual token ä½ç½®ä¹Ÿä¸ä¸€å®šæ˜¯å‰ç¼€ï¼Œæ’å…¥çš„ä½ç½®æ˜¯å¯é€‰çš„ã€‚

ç‰¹ç‚¹ï¼š
- å¼•å…¥ä¸€ä¸ªprompt encoderï¼ˆç”±ä¸€ä¸ªåŒå‘çš„LSTM+ä¸¤å±‚MLPç»„æˆï¼‰æ¥å»ºæ¨¡virtual tokençš„ç›¸äº’ä¾èµ–ä¼šæ”¶æ•›æ›´å¿«ï¼Œæ•ˆæœæ›´å¥½ã€‚

ä»£ç æ ·ä¾‹ï¼š

```py
peft_config = PromptEncoderConfig(task_type="CAUSAL_LM", num_virtual_tokens=20, encoder_hidden_size=128)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)
model = get_peft_model(model, peft_config)
```

### ã€2022.3.20ã€‘P-Tuning v2

p-tuning é—®é¢˜ï¼šå°å‚æ•°é‡æ¨¡å‹ä¸Šè¡¨ç°å·®

V2ç‰ˆæœ¬ç±»ä¼¼LoRAï¼Œæ¯å±‚éƒ½åµŒå…¥äº†æ–°çš„å‚æ•°ï¼ˆç§°ä¹‹ä¸ºDeep FTï¼‰ï¼Œä¸‹å›¾ä¸­å¼€æºçœ‹åˆ°p-tuning v2 é›†åˆäº†å¤šç§å¾®è°ƒæ–¹æ³•ã€‚

p-tuning v2 åœ¨å¤šç§ä»»åŠ¡ä¸Šä¸‹è¿›è¡Œå¾®è°ƒï¼Œä¹‹åå¯¹äºä¸åŒçš„ä»»åŠ¡å¦‚token classificationä¸sentence classificationæ·»åŠ äº†éšæœºåˆå§‹åŒ–çš„ä»»åŠ¡å¤´ï¼ˆAutoModelForTokenClassificationã€AutoModelForSequenceClassificationï¼‰ï¼Œè€Œéä½¿ç”¨è‡ªç„¶è¯­è¨€çš„æ–¹å¼ï¼Œå¯ä»¥è¯´V2æ˜¯é›†å¤§æˆè€…ã€‚
- ![](https://pic2.zhimg.com/80/v2-85f6abe44785882dd95943758e600e65_1440w.webp)

æ¯ä¸ªTransformerå±‚éƒ½åŠ å…¥äº†prompt tokenä½œä¸ºè¾“å…¥ï¼Œå¼•å…¥**å¤šä»»åŠ¡**å­¦ä¹ ï¼Œé’ˆå¯¹ä¸åŒä»»åŠ¡é‡‡ç”¨ä¸åŒçš„æç¤ºé•¿åº¦ã€‚å¹¶ä¸”å›å½’ä¼ ç»Ÿçš„**åˆ†ç±»æ ‡ç­¾**èŒƒå¼ï¼Œè€Œä¸æ˜¯**æ˜ å°„å™¨**ã€‚

ç‰¹ç‚¹ï¼š
- è§£å†³äº†Prompt Tuningæ— æ³•åœ¨å°æ¨¡å‹ä¸Šæœ‰æ•ˆæå‡çš„é—®é¢˜ã€‚
- ç§»é™¤äº†å¯¹æ¨¡å‹æ•ˆæœæ”¹è¿›è¾ƒå°çš„é‡å‚æ•°åŒ–çš„ç¼–ç å™¨ï¼ˆå¦‚ï¼šPrefix Tuningä¸­çš„MLPã€P-Tuningä¸­çš„LSTMï¼‰ã€‚
- å¯¹äºä¸€äº›å¤æ‚çš„ç¡¬åºåˆ—æ ‡è®°ä»»åŠ¡ï¼ˆå³åºåˆ—æ ‡æ³¨ï¼‰å–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚


ä»£ç æ ·ä¾‹ï¼š

```py
peft_config = PrefixTuningConfig(task_type="SEQ_CLS", num_virtual_tokens=20)
model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)
model = get_peft_model(model, peft_config)
```

## Adapter Tuning

è¯¥æ–¹æ³•è®¾è®¡äº†Adapterç»“æ„ï¼Œå¹¶å°†å…¶åµŒå…¥Transformerçš„ç»“æ„é‡Œé¢ï¼Œé’ˆå¯¹æ¯ä¸€ä¸ªTransformerå±‚ï¼Œå¢åŠ äº†ä¸¤ä¸ªAdapterç»“æ„ï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œå›ºå®šä½åŸæ¥é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°ä¸å˜ï¼Œåªå¯¹æ–°å¢çš„Adapterç»“æ„å’ŒLayer Norm å±‚è¿›è¡Œå¾®è°ƒã€‚

ç‰¹ç‚¹ï¼š
- é€šè¿‡åœ¨Transformerå±‚ä¸­åµŒå…¥Adapterç»“æ„ï¼Œåœ¨æ¨ç†æ—¶ä¼šé¢å¤–å¢åŠ æ¨ç†æ—¶é•¿ã€‚

Adapter Tuningï¼ˆé€‚é…å™¨å¾®è°ƒï¼‰

ä»€ä¹ˆæ˜¯Adapter Tuningï¼Ÿ
- Adapter Tuning åœ¨ä¿æŒæ¨¡å‹å‚æ•°æ•°é‡ç›¸å¯¹è¾ƒå°çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¢åŠ **å°‘é‡å¯è®­ç»ƒå‚æ•°**ï¼ˆå³é€‚é…å™¨ï¼‰æ¥æé«˜æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

Adapter Tuning æ ¸å¿ƒæ€æƒ³
- åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„ä¸­é—´å±‚ä¸­æ’å…¥å°çš„å¯è®­ç»ƒå±‚æˆ–â€œé€‚é…å™¨â€ã€‚
- è¿™äº›é€‚é…å™¨é€šå¸¸åŒ…æ‹¬ä¸€äº›å…¨è¿æ¥å±‚ã€éçº¿æ€§æ¿€æ´»å‡½æ•°ç­‰ï¼Œå®ƒä»¬è¢«è®¾è®¡ç”¨æ¥æ•è·ç‰¹å®šä»»åŠ¡çš„çŸ¥è¯†ï¼Œè€Œä¸éœ€è¦å¯¹æ•´ä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¤§è§„æ¨¡çš„å¾®è°ƒã€‚

è¯¦è§: [ä¸€æ–‡å½»åº•ææ‡‚Fine-tuning - å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆParameter-Efficient Fine-Tuningï¼‰](https://mp.weixin.qq.com/s/OFufH9hSFdLsntQkNwL4LA), å«å›¾è§£

### AdapterFusion

ä¸€ç§èåˆå¤šä»»åŠ¡ä¿¡æ¯çš„Adapterçš„å˜ä½“ï¼Œåœ¨ Adapter çš„åŸºç¡€ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œé€šè¿‡å°†å­¦ä¹ è¿‡ç¨‹åˆ†ä¸ºä¸¤é˜¶æ®µæ¥æå‡ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°ã€‚

### AdapterDrop

è¯¥æ–¹æ³•åœ¨ä¸å½±å“ä»»åŠ¡æ€§èƒ½çš„æƒ…å†µä¸‹ï¼Œå¯¹AdapteråŠ¨æ€é«˜æ•ˆçš„ç§»é™¤ï¼Œå°½å¯èƒ½çš„å‡å°‘æ¨¡å‹çš„å‚æ•°é‡ï¼Œæé«˜æ¨¡å‹åœ¨åå‘ä¼ æ’­ï¼ˆè®­ç»ƒï¼‰å’Œæ­£å‘ä¼ æ’­ï¼ˆæ¨ç†ï¼‰æ—¶çš„æ•ˆç‡ã€‚

ç‰¹ç‚¹ï¼š
- é€šè¿‡ä»è¾ƒä½çš„ Transformer å±‚åˆ é™¤å¯å˜æ•°é‡çš„Adaperæ¥æå‡æ¨ç†é€Ÿåº¦ã€‚ å½“å¯¹å¤šä¸ªä»»åŠ¡æ‰§è¡Œæ¨ç†æ—¶ï¼ŒåŠ¨æ€åœ°å‡å°‘äº†è¿è¡Œæ—¶çš„è®¡ç®—å¼€é”€ï¼Œå¹¶åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¿æŒäº†ä»»åŠ¡æ€§èƒ½ã€‚


## ã€2021.8.16ã€‘LoRA ä½ç§©é€‚é…

é€šè¿‡**ä½ç§©åˆ†è§£**æ¥æ¨¡æ‹Ÿå‚æ•°æ”¹å˜é‡ï¼Œä»¥æå°å‚æ•°é‡å®ç°å¤§æ¨¡å‹çš„**é—´æ¥**è®­ç»ƒã€‚

ç‰¹ç‚¹ï¼š
- å°†BAåŠ åˆ°Wä¸Šï¼Œæ¶ˆé™¤æ¨ç†å»¶è¿Ÿã€‚
- å¯æ’æ‹”å¼åˆ‡æ¢ä¸åŒä»»åŠ¡ã€‚
- è®¾è®¡è¾ƒå¥½ï¼Œç®€å•ä¸”æ•ˆæœå¥½ã€‚

LoRAå¾®è°ƒä¸å…¨é‡å¾®è°ƒç›¸æ¯”ï¼Œæ•ˆæœä¼šæ›´å·®ï¼Œä½†å›¢é˜Ÿå°†LoRAæ·»åŠ åˆ°æ‰€æœ‰çš„**çº¿æ€§å±‚**è§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚

2021å¹´ï¼Œè®ºæ–‡ [LoRA: Low-Rank Adaption of Large Language Models](https://arxiv.org/abs/2106.09685) é€šè¿‡**å†»ç»“**é¢„è®­ç»ƒæƒé‡ï¼Œå¹¶åˆ›å»º**æŸ¥è¯¢å’Œå€¼**å±‚çš„æ³¨æ„åŠ›çŸ©é˜µçš„ä½ç§©ç‰ˆæœ¬ï¼Œå¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- ä½ç§©çŸ©é˜µå‚æ•°**è¿œå°‘äº**åŸå§‹æ¨¡å‹ï¼Œå› æ­¤å¯ç”¨æ›´å°‘çš„ GPU å†…å­˜è¿›è¡Œå¾®è°ƒã€‚
- ä½é˜¶é€‚é…å™¨çš„å¾®è°ƒå–å¾—äº†ä¸å¾®è°ƒå®Œæ•´é¢„è®­ç»ƒæ¨¡å‹ç›¸å½“çš„ç»“æœã€‚

### LoRA åŸç†

æ ¸å¿ƒæ€æƒ³
- å†»ç»“é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œå°†å¯è®­ç»ƒçš„**ç§©åˆ†è§£çŸ©é˜µ**æ³¨å…¥ Transformer æ¶æ„æ¯ä¸€å±‚ï¼Œä»è€Œå¤§å¤§å‡å°‘äº†ä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒå‚æ•°é‡

LoRA å®ç°æµç¨‹ï¼š
- åŸå§‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ (PLM) æ—å¢åŠ ä¸€ä¸ªæ—è·¯ï¼Œåšä¸€ä¸ªå…ˆ**é™ç»´**å†**å‡ç»´**çš„æ“ä½œï¼Œæ¨¡æ‹Ÿ`æœ¬å¾ç§©` (intrinsic rank)ï¼›
- è®­ç»ƒæ—¶ï¼Œå›ºå®š PLM å‚æ•°ä¸å˜ï¼Œåªè®­ç»ƒé™ç»´çŸ©é˜µ A å’Œå‡ç»´çŸ©é˜µ Bï¼Œå³ä¼˜åŒ–å™¨åªä¼˜åŒ–å³è·¯çš„å‚æ•°ï¼›
- æ¨¡å‹çš„è¾“å…¥/è¾“å‡ºç»´åº¦ä¸å˜ï¼Œå·¦å³ä¸¤è¾¹å…±ç”¨æ¨¡å‹è¾“å…¥ï¼Œè¾“å‡ºæ—¶å°† PLM ä¸æ—è·¯çš„è¾“å‡ºå åŠ ï¼š`h=Wx+BAx`
- ç”¨é›¶å‡å€¼éšæœº**é«˜æ–¯åˆ†å¸ƒ**åˆå§‹åŒ– Aï¼Œç”¨å…¨é›¶çŸ©é˜µåˆå§‹åŒ– Bï¼ŒçŸ©é˜µ B çš„å…¨é›¶åˆå§‹åŒ–ï¼Œä½¿å¾—åœ¨è®­ç»ƒæœ€å¼€å§‹çš„ä¸€æ®µæ—¶é—´ï¼Œå³è·¯ç»“æœä¼šæ¥è¿‘äº0ï¼Œè¿™æ ·æ¨¡å—è¾“å‡ºå°±åŸºæœ¬ä¸Šæ¥è‡ªäºå·¦è·¯ï¼Œä¹Ÿå°±æ˜¯å¤§æ¨¡å‹åŸæœ‰å‚æ•°çš„è®¡ç®—ç»“æœï¼Œè¿™ä½¿å¾—æ¨¡å‹ä¼˜åŒ–çš„åˆå§‹ç‚¹å’ŒåŸå§‹çš„å¤§æ¨¡å‹ä¿æŒä¸€è‡´ã€‚


- æ€æƒ³ï¼šåœ¨åŸæ¨¡å‹æ—è¾¹å¢åŠ ä¸€ä¸ªæ—è·¯ï¼Œé€šè¿‡**ä½ç§©åˆ†è§£**ï¼ˆå…ˆé™ç»´å†å‡ç»´ï¼‰æ¨¡æ‹Ÿå‚æ•°çš„æ›´æ–°é‡ã€‚
- è®­ç»ƒï¼šåŸæ¨¡å‹å›ºå®šï¼Œåªè®­ç»ƒé™ç»´çŸ©é˜µAå’Œå‡ç»´çŸ©é˜µBã€‚
- æ¨ç†ï¼šå¯å°†BAåŠ åˆ°åŸå‚æ•°ä¸Šï¼Œä¸å¼•å…¥é¢å¤–çš„æ¨ç†å»¶è¿Ÿã€‚
- åˆå§‹åŒ–ï¼šAé‡‡ç”¨é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–ï¼ŒBåˆå§‹åŒ–ä¸ºå…¨0ï¼Œä¿è¯è®­ç»ƒå¼€å§‹æ—¶æ—è·¯ä¸º0çŸ©é˜µã€‚

å¯æ’æ‹”å¼çš„åˆ‡æ¢ä»»åŠ¡ï¼šå½“å‰ä»»åŠ¡W0+B1A1ï¼Œå°†loraéƒ¨åˆ†å‡æ‰ï¼Œæ¢æˆB2A2ï¼Œå³å¯å®ç°ä»»åŠ¡åˆ‡æ¢ã€‚



ç¤ºæ„å›¾
- ![](https://pic4.zhimg.com/80/v2-48e88e61040a94284cc0499be8ecda37_1440w.webp)

LoRAå¾®è°ƒå¯ç”¨ä¸åŒä½ç§©çŸ©é˜µé€‚é…ä¸åŒä»»åŠ¡ç±»å‹ï¼Œä¸”LLM åŸå§‹æƒé‡ä¸ç”¨å˜åŒ–ï¼›
- ![](https://pic2.zhimg.com/80/v2-0ba745c8fff2c7015d9463206c5be631_1440w.webp)

LoRAå°†ä¼šä½¿ç”¨ä½ç§©è¡¨ç¤ºæ¥ç¼–ç  `â–³W` ï¼ŒåŒæ—¶å®ç°è®¡ç®—é«˜æ•ˆå’Œå­˜å‚¨é«˜æ•ˆã€‚å½“é¢„è®­ç»ƒæ¨¡å‹æ˜¯175B GPT-3ï¼Œå¯è®­ç»ƒå‚æ•° `|0|` å¯ä»¥å°è‡³ `|W0|` çš„ 0.01%

æ•´ä½“ä¸ŠLoRAå¾®è°ƒæ•ˆæœç›¸å¯¹äºåŸºåº§æ¨¡å‹æœ‰è¾ƒå¤§çš„æå‡ï¼Œä½†æ˜¯ç›¸å¯¹äºå…¨å‚æ•°å¾®è°ƒæ–¹å¼æ¥è¯´æ•ˆæœä¸Šè¿˜æ˜¯ä½ä¸€ç‚¹ã€‚
- `Full fine-tune` > `LoRA` > `base model`
- ![](https://pic4.zhimg.com/80/v2-90f36dc2e8d97ccbe6bb20b941a9745b_1440w.webp)

å¯¹äº $\delta W_x$ è¿™éƒ¨åˆ†ï¼Œä¼šä¹˜ä¸Šä¸€ä¸ª scale ç³»æ•° <span style='color:red;font-size:300%'> $ Scale = \frac{\alpha}{r}$ </span>
- $\alpha$ ç›¸å¯¹äº r, ä¿æŒå¸¸æ•°å€å…³ç³»ã€‚è°ƒèŠ‚è¿™ä¸ª $\alpha$ å¤§è‡´, ç›¸å½“äº**è°ƒèŠ‚å­¦ä¹ ç‡**ï¼Œäºæ˜¯å¹²è„†å›ºå®šä¸ºå¸¸æ•°
- å®è·µä¸­ï¼Œrank r åº”è¯¥è®¾ä¸ºå¤šå°‘æ¯”è¾ƒåˆé€‚å‘¢ï¼Ÿå¯ä»¥å¾ˆä½ï¼Œä¸è¶…è¿‡8
- [å½“çº¢ç‚¸å­é¸¡ LoRAï¼Œæ˜¯å½“ä»£å¾®è°ƒ LLMs çš„æ­£ç¡®å§¿åŠ¿ï¼Ÿ](https://zhuanlan.zhihu.com/p/618894919)

LoRA ä¸€èˆ¬ä¼šåœ¨ Transformer æ¯å±‚ä¸­çš„ query_key_value éƒ¨åˆ†å¢åŠ æ—è·¯ï¼Œå…¶ä¸­ `r` ä¸ºçŸ©é˜µçš„ç§©ï¼Œåœ¨æ¨¡å‹è®­ç»ƒä¸­æ˜¯å¯è°ƒèŠ‚çš„å‚æ•°ï¼Œ`r << d`ï¼Œ`r` è¶Šå¤§ï¼Œå¯è®­ç»ƒå‚æ•°è¶Šå¤šã€‚
- å›¾è§[åŸæ–‡](https://mp.weixin.qq.com/s/yTX_bQEur8Nj6h_uGFJ31g)

LoRA ä¼˜åŠ¿: ä½¿ç”¨è¾ƒå°‘ GPU èµ„æºï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å¯¹å¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- å¼€æºç¤¾åŒºä¸­ï¼Œå¼€å‘è€…ä»¬ä½¿ç”¨ LoRA å¯¹ `Stable Diffusion` è¿›è¡Œå¾®è°ƒï¼Œå–å¾—äº†éå¸¸ä¸é”™çš„æ•ˆæœã€‚
- éšç€ ChatGPT çš„ç«çˆ†ï¼Œä¹Ÿæ¶Œç°å‡ºäº†è®¸å¤šä½¿ç”¨ LoRA å¯¹ LLM è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„å·¥ä½œã€‚

è¿™ç§æŠ€æœ¯å…è®¸ä½¿ç”¨å°éƒ¨åˆ†å†…å­˜æ¥å¾®è°ƒ LLMã€‚ç„¶è€Œä¹Ÿæœ‰ç¼ºç‚¹
- ç”±äºé€‚é…å™¨å±‚ä¸­çš„**é¢å¤–**çŸ©é˜µä¹˜æ³•ï¼Œ<span style='color:red'>å‰å‘å’Œåå‘ä¼ é€’çš„é€Ÿåº¦å¤§çº¦æ˜¯åŸæ¥çš„**ä¸¤å€**</span>ã€‚

LoRA æ˜¯ Parameter Efficient æ–¹æ³•ä¹‹ä¸€ã€‚
- è¿‡åº¦å‚æ•°åŒ–çš„æ¨¡å‹ä½äºä¸€ä¸ªä½**å†…åœ¨ç»´åº¦**ä¸Šï¼Œæ‰€ä»¥å‡è®¾åœ¨æ¨¡å‹é€‚åº”è¿‡ç¨‹ä¸­çš„æƒé‡å˜åŒ–ä¹Ÿå…·æœ‰è¾ƒä½çš„â€œå†…åœ¨ç­‰çº§â€ã€‚
- [LoRA](https://github.com/microsoft/LoRA)ä¸»è¦æ–¹æ³•ä¸º**å†»ç»“**ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹çš„çŸ©é˜µå‚æ•°ï¼Œå¹¶é€‰æ‹©ç”¨Aå’ŒBçŸ©é˜µæ¥æ›¿ä»£ï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡æ—¶åªæ›´æ–°Aå’ŒBã€‚
- ![](https://pic4.zhimg.com/80/v2-67cd3e1e603a5bb674463ddc4db38d57_1440w.webp)
- ![](https://pic2.zhimg.com/80/v2-f56b07afc29ccad77a6faffa130ab24d_1440w.webp)

ã€2023-5-27ã€‘LoRAå‹ç¼©æ¯”ï¼Œ[æ¸¯ç§‘å¤§å®éªŒæ•°æ®](https://lmflow.com/)

Extremely few parameters with LoRA
- LLaMA-33B ï¼ˆ65GBï¼‰-> 25M
- LLaMA-13Bï¼ˆ26GBï¼‰ -> 13M
- LLaMA-7B ï¼ˆ13.5GBï¼‰-> 8M


ã€2023-4-5ã€‘LoRAåŸç†è®²è§£ï¼Œ[LoRAï¼šè®­ç»ƒä½ çš„GPT](https://www.bilibili.com/video/BV17g4y1g7S6)

<iframe src="//player.bilibili.com/player.html?aid=824514848&bvid=BV17g4y1g7S6&cid=1084363572&page=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width='800' height='600'> </iframe>


```py
class LoraLayer:
    def __init__(
        self,
        r: int,
        lora_alpha: int,
        lora_dropout: float,
        merge_weights: bool,
    ):
        self.r = r
        self.lora_alpha = lora_alpha

        # Optional dropout
        if lora_dropout > 0.0:
            self.lora_dropout = nn.Dropout(p=lora_dropout)
        else:
            self.lora_dropout = lambda x: x

        # Mark the weight as unmerged
        # æ ‡è®°ä½ç§©åˆ†è§£éƒ¨åˆ†æ˜¯å¦å·²ç»åˆå¹¶è‡³é¢„è®­ç»ƒæƒé‡
        self.merged = False
        # æŒ‡å®šæ˜¯å¦è¦å°†ä½ç§©åˆ†è§£éƒ¨åˆ†åˆå¹¶è‡³é¢„è®­ç»ƒæƒé‡ä¸­
        self.merge_weights = merge_weights

        # æ˜¯å¦è¦ç¦ç”¨ä½ç§©åˆ†è§£çš„éƒ¨åˆ†ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™ä»…ä½¿ç”¨é¢„è®­ç»ƒæƒé‡éƒ¨åˆ†
        self.disable_adapters = False
```


ã€2024-1-29ã€‘è‡ªå¨æ–¯åº·æ˜Ÿå¤§å­¦éº¦è¿ªé€Šåˆ†æ ¡çš„ç»Ÿè®¡å­¦åŠ©ç†æ•™æˆSebastian Raschka [ä½¿ç”¨ LoRA å’Œ QLoRA å¾®è°ƒLLMï¼šæ•°ç™¾æ¬¡å®éªŒçš„è§è§£](https://zhuanlan.zhihu.com/p/679172768?utm_psn=1735254298701877248)æ€»ç»“çš„ç»éªŒ
- å¦‚ä½•èŠ‚çœå†…å­˜ã€é€‰æ‹©æœ€ä½³é…ç½®ç­‰é—®é¢˜ã€‚
- æ˜¯å¦åº”è¯¥ç”¨SGDå–ä»£AdamWï¼Œä½¿ç”¨è°ƒåº¦å™¨çš„æ½œåœ¨ä»·å€¼
  - AdamW å’Œ SGD ä¼˜åŒ–å™¨é€‰æ‹©æ²¡åŒºåˆ«
- å¦‚ä½•è°ƒæ•´LoRAçš„è¶…å‚æ•°ã€‚
  - æé«˜ræ—¶ï¼Œåˆé€‚çš„ alpha å€¼æ˜¯ 2*r
- åŸæ–‡é“¾æ¥ï¼š[Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments - Lightning AI](https://lightning.ai/pages/community/lora-insights/)

LoRA å°†æƒé‡çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªè¾ƒå°çš„æƒé‡çŸ©é˜µï¼Œä»¥æ›´å‚æ•°æœ‰æ•ˆçš„æ–¹å¼è¿‘ä¼¼å®Œå…¨ç›‘ç£å¾®è°ƒ

å®éªŒæ¨¡å‹
- é‡ç‚¹å…³æ³¨å°šæœªè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹ï¼š[phi-1.5 1.3B](https://arxiv.org/abs/2309.05463)ã€[Mistral 7B](https://arxiv.org/abs/2310.06825)ã€[Llama 2 7B](https://arxiv.org/abs/2307.09288)ã€ Llama 2 13B å’Œ[Falcon 40B](https://falconllm.tii.ae/)

GPU èµ„æº
- å•å¡A100 GPU

æ•ˆæœ
- Mistral 7B æ¨¡å‹åœ¨æ•°å­¦åŸºå‡†æµ‹è¯•éå¸¸å‡ºè‰²ã€‚
- phi-1.5 1.3B å‹å·ç”±äºå…¶ç›¸å¯¹è¾ƒå°çš„å°ºå¯¸ï¼Œåœ¨TruthfulQA MC2 æ€§èƒ½è¾ƒå¥½ã€‚
- ç”±äºæŸç§åŸå› ï¼ŒLlama 2 13B åœ¨ç®—æœ¯åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸ä½³ï¼Œè€Œè¾ƒå°çš„ Llama 2 7B åœ¨è¯¥é¢†åŸŸè¡¨ç°æ˜æ˜¾ä¼˜äºå®ƒã€‚

ç›®å‰æ¨æµ‹ phi-1.5 1.3B å’Œ Mistral 7B å¯èƒ½å·²ç»æ¥å—è¿‡åŸºå‡†æµ‹è¯•æ•°æ®çš„è®­ç»ƒï¼Œå› æ­¤ä¸ç”¨ã€‚

æ­¤å¤–ï¼Œå‰©ä½™æ¨¡å‹ä¸­æœ€å°çš„æ¨¡å‹å°†æä¾›æœ€å¤§çš„æ”¹è¿›ç©ºé—´ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„ç¡¬ä»¶è¦æ±‚ã€‚é‡ç‚¹å…³æ³¨ Llama 2 7Bã€‚
- ![](https://pic1.zhimg.com/80/v2-5b5947fd3087bfdc8280e1bcf89305c8_1440w.webp)

Lit-GPT ä¸­çš„ `â€“quantize` æ ‡å¿—ï¼ˆ4 ä½æ™®é€šæµ®ç‚¹ç±»å‹ï¼‰å¯ç”¨ QLoRA
- ![](https://pic3.zhimg.com/80/v2-76124bca16d8ae19e5f86c49189c2e06_1440w.webp)
- QLoRA éå¸¸èŠ‚çœå†…å­˜ï¼Œä½†ä¼šå¢åŠ è¿è¡Œæ—¶æˆæœ¬ã€‚
- QLoRA å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ç¡®å®è¾ƒå°

### LoRA å‚æ•°

LoRA å‚æ•°
- ![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage7.png)
- [finetune/lora.py](https://github.com/Lightning-AI/lit-gpt/blob/bf60124fa72a56436c7d4fecc093c7fc48e84433/finetune/lora.py#L38)
- QKV:
  - LoRA é»˜è®¤ä»…é’ˆå¯¹å¤šå¤´è‡ªæ³¨æ„åŠ›å—ä¸­çš„ Key å’Œ Query çŸ©é˜µå¯ç”¨
  - æ›´æ”¹é…ç½®ï¼Œå¯åŠ¨å€¼çŸ©é˜µã€æŠ•å½±å±‚å’Œçº¿æ€§å±‚
- è¿­ä»£æ¬¡æ•° epoch
  - è¿­ä»£æ¬¡æ•°çš„å¢åŠ ä¼šå¯¼è‡´æ•´ä½“æ€§èƒ½å˜å·®ã€‚
- `r`: æœ€é‡è¦çš„å‚æ•° Rï¼ŒçŸ©é˜µçš„ç§©/ç»´åº¦,ç›´æ¥å½±å“æ¨¡å‹å¤æ‚æ€§å’Œå®¹é‡
  - ä»…å¢åŠ  r æœ¬èº«å°±ä¼šä½¿ç»“æœå˜å¾—æ›´ç³Ÿ
  - è¾ƒ**é«˜**çš„â€œrâ€æ„å‘³ç€æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†å¯èƒ½å¯¼è‡´**è¿‡æ‹Ÿåˆ**
  - è€Œè¾ƒ**ä½**çš„â€œrâ€å¯ä»¥å‡å°‘è¿‡åº¦æ‹Ÿåˆï¼Œä½†ä¼šç‰ºç‰²è¡¨è¾¾èƒ½åŠ›ã€‚
  - å®éªŒï¼š r ä» 8 å¢åŠ åˆ° 16ï¼Œå‘ç°ä»…å¢åŠ  r æœ¬èº«å°±ä¼šä½¿ç»“æœå˜å¾—æ›´ç³Ÿ
- `alpha`: 
  - è¾ƒé«˜çš„â€œalphaâ€åŠ å¼ºä½ç§©ç»“æ„æˆ–æ­£åˆ™åŒ–
  - è€Œè¾ƒä½çš„â€œalphaâ€ä¼šå‡å°‘å…¶å½±å“ï¼Œä½¿æ¨¡å‹æ›´åŠ ä¾èµ–äºåŸå§‹å‚æ•°ã€‚
  - è°ƒæ•´â€œalphaâ€æœ‰åŠ©äºåœ¨æ‹Ÿåˆæ•°æ®å’Œé€šè¿‡æ­£åˆ™åŒ–æ¨¡å‹é˜²æ­¢è¿‡åº¦æ‹Ÿåˆä¹‹é—´å–å¾—å¹³è¡¡ã€‚
  - æé«˜ræ—¶ï¼Œé€‰æ‹©è¾ƒå¤§çš„ alpha å€¼è‡³å…³é‡è¦
  - ç»éªŒï¼šå¾®è°ƒ LLM æ—¶ï¼Œé€šå¸¸é€‰æ‹©ä¸¤å€äºRçš„ alphaï¼ˆæ³¨æ„ä¸æ‰©æ•£æ¨¡å‹æ—¶æœ‰æ‰€ä¸åŒï¼‰ï¼Œä»¥ QLoRA ä¸ºä¾‹ï¼Œr=256 å’Œ alpha=512 æ¨¡å‹æ•ˆæœæœ€ä½³
- ç«å±±å¼•æ“æ–¹èˆŸå¹³å°ï¼š 
  - r è°ƒå°æ—¶, éœ€è¦åŒæ­¥åŠ å¤§å­¦ä¹ ç‡, è¯¥å‚æ•°éœ€è¦è°¨æ…
  - alpha æ˜¯ç¼©æ”¾ç³»æ•°, `scale = alpha / rank`
  - warmupç›¸å…³å‚æ•°: steps éœ€è¦å¤šå°‘æ­¥â€œçƒ­èº«â€, step_rate çƒ­èº«æ•°æ®é›†çš„ç™¾åˆ†æ¯”
  - max batch tokens: å•workeræ¯ä¸ªbatchæœ€å¤§tokenæ•°
  - LR Scheduler type

```py
# Hyperparameters
learning_rate = 3e-4
batch_size = 128
micro_batch_size = 1
max_iters = 50000  # train dataset size
weight_decay = 0.01
lora_r = 8 # æœ€é‡è¦çš„å‚æ•° Rï¼ŒçŸ©é˜µçš„ç§©/ç»´åº¦,ç›´æ¥å½±å“æ¨¡å‹å¤æ‚æ€§å’Œå®¹é‡
lora_alpha = 16
lora_dropout = 0.05
lora_query = True # Q çŸ©é˜µå¯ç”¨
lora_key = False # K
lora_value = True # V çŸ©é˜µå¯ç”¨
lora_projection = False # æŠ•å½±å±‚æ˜¯å¦å¯ç”¨
lora_mlp = False # çº¿æ€§å±‚æ˜¯å¦å¯åŠ¨
lora_head = False # 
warmup_steps = 100
```

![](https://lightningaidev.wpengine.com/wp-content/uploads/2023/10/lora-expimage10.jpg)

### LoRA ä½¿ç”¨

LoRA å·²ç»è¢«ä½œè€…æ‰“åŒ…åˆ°äº†`loralib`ä¸­ã€‚
- `pip install loralib`

å¯ä»¥é€‰æ‹©ç”¨loralibä¸­å®ç°çš„å¯¹åº”å±‚æ¥æ›¿æ¢ä¸€äº›å±‚ã€‚
- ç›®å‰loralibåªæ”¯æŒ nn.Linearã€nn.Embedding å’Œ nn.Conv2dã€‚
- loralibè¿˜æ”¯æŒä¸€ä¸ª MergedLinearï¼Œç”¨äºå•ä¸ª nn.Linear ä»£è¡¨ä¸€ä¸ªä»¥ä¸Šçš„å±‚çš„æƒ…å†µï¼Œæ¯”å¦‚åœ¨ä¸€äº›å…³æ³¨ qkv æŠ•å½±çš„å®ç°ä¸­ï¼ˆself- attentionï¼‰
- ![](https://pic2.zhimg.com/80/v2-bcef352dc1adf7d6f2fad86e1fe892fd_1440w.webp)

```py
# ===== Before =====
layer = nn.Linear(in_features, out_features)

# ===== After ======
import loralib as lora
# Add a pair of low-rank adaptation matrices with rank r=16
layer = lora.Linear(in_features, out_features, r=16)
```

è¯¦è§åŸæ–‡ï¼š[å¾®è½¯LoRA: Low-Rank Adaptation of Large Language Models ä»£ç è§£è¯»](https://zhuanlan.zhihu.com/p/515954218)

æˆ– huggingface ä»£ç æ ·ä¾‹ï¼š

```py
peft_config = LoraConfig(task_type="SEQ_CLS", inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)
model = get_peft_model(model, peft_config)
```

### LoRA æ€è€ƒ

ã€2024-8-22ã€‘[å¤§æ¨¡å‹é¢ç»â€”â€”LoRAæœ€å…¨æ€»ç»“](https://mp.weixin.qq.com/s/d3WIiA3VDyyRPyWWkwHa3w)

ä¼˜ç‚¹
- 1ï¼‰ä¸€ä¸ªä¸­å¿ƒæ¨¡å‹æœåŠ¡å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ï¼Œ**èŠ‚çœå‚æ•°å­˜å‚¨é‡** 
- 2ï¼‰æ¨ç†é˜¶æ®µä¸å¼•å…¥é¢å¤–è®¡ç®—é‡ 
- 3ï¼‰ä¸å…¶å®ƒå‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•æ­£äº¤ï¼Œå¯æœ‰æ•ˆç»„åˆ 
- 4ï¼‰è®­ç»ƒä»»åŠ¡æ¯”è¾ƒç¨³å®šï¼Œæ•ˆæœæ¯”è¾ƒå¥½ 
- 5ï¼‰LoRA å‡ ä¹ä¸æ·»åŠ ä»»ä½•æ¨ç†å»¶è¿Ÿï¼Œå› ä¸ºé€‚é…å™¨æƒé‡å¯ä»¥ä¸åŸºæœ¬æ¨¡å‹åˆå¹¶


ç¼ºç‚¹
- LoRA å‚ä¸è®­ç»ƒçš„æ¨¡å‹å‚æ•°é‡ä¸å¤šï¼Œä¹Ÿå°±ç™¾ä¸‡åˆ°åƒä¸‡çº§åˆ«çš„å‚æ•°é‡ï¼Œæ‰€ä»¥æ•ˆæœæ¯”å…¨é‡å¾®è°ƒå·®å¾ˆå¤šã€‚
- æ•°æ®ä»¥åŠç®—åŠ›æ»¡è¶³çš„æƒ…å†µä¸‹ï¼Œè¿˜æ˜¯å¾®è°ƒçš„å‚æ•°è¶Šå¤šè¶Šå¥½



è®­ç»ƒç†è®º
1. ChatGLM-6B LoRA åçš„ æƒé‡å¤šå¤§ï¼Ÿ 
  - rank 8 target_module query_key_value æ¡ä»¶ä¸‹ï¼Œå¤§çº¦15Mã€‚
1. LoRA å¾®è°ƒæ–¹æ³•ä¸ºå•¥èƒ½åŠ é€Ÿè®­ç»ƒï¼Ÿ 
  - 1ï¼‰åªæ›´æ–°äº†**éƒ¨åˆ†å‚æ•°**ï¼šæ¯”å¦‚ LoRAåŸè®ºæ–‡å°±é€‰æ‹©åªæ›´æ–°Self Attentionçš„å‚æ•°ï¼Œå®é™…ä½¿ç”¨æ—¶è¿˜å¯ä»¥é€‰æ‹©åªæ›´æ–°éƒ¨åˆ†å±‚çš„å‚æ•°ï¼›
  - 2ï¼‰å‡å°‘äº†**é€šä¿¡æ—¶é—´**ï¼šç”±äºæ›´æ–°å‚æ•°é‡å˜å°‘äº†ï¼Œæ‰€ä»¥ï¼ˆå°¤å…¶æ˜¯å¤šå¡è®­ç»ƒæ—¶ï¼‰è¦ä¼ è¾“çš„æ•°æ®é‡ä¹Ÿå˜å°‘äº†ï¼Œä»è€Œå‡å°‘äº†ä¼ è¾“æ—¶é—´ï¼› 
  - 3ï¼‰é‡‡ç”¨äº†å„ç§**ä½ç²¾åº¦åŠ é€Ÿ**æŠ€æœ¯ï¼Œå¦‚FP16ã€FP8æˆ–è€…INT8é‡åŒ–ç­‰ã€‚
  - è¿™ä¸‰éƒ¨åˆ†åŸå› ç¡®å®èƒ½åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œç„¶è€Œå¹¶ä¸æ˜¯LoRAæ‰€ç‹¬æœ‰ï¼Œäº‹å®ä¸Šå‡ ä¹éƒ½æœ‰å‚æ•°é«˜æ•ˆæ–¹æ³•éƒ½å…·æœ‰è¿™äº›ç‰¹ç‚¹ã€‚LoRAçš„ä¼˜ç‚¹æ˜¯ä½ç§©åˆ†è§£å¾ˆç›´è§‚ï¼Œåœ¨ä¸å°‘åœºæ™¯ä¸‹è·Ÿå…¨é‡å¾®è°ƒçš„æ•ˆæœä¸€è‡´ï¼Œä»¥åŠåœ¨é¢„æµ‹é˜¶æ®µä¸å¢åŠ æ¨ç†æˆæœ¬ã€‚
1. LoRA è¿™ç§å¾®è°ƒæ–¹æ³•å’Œ**å…¨å‚æ•°**æ¯”èµ·æ¥æœ‰ä»€ä¹ˆåŠ£åŠ¿å—ï¼Ÿ
  - å¦‚æœæœ‰è¶³å¤Ÿè®¡ç®—èµ„æºä»¥åŠæœ‰10kä»¥ä¸Šæ•°æ®ï¼Œå»ºè®®**å…¨å‚æ•°å¾®è°ƒ**
  - lora åˆè¡·æ˜¯è§£å†³ä¸å¤Ÿè®¡ç®—èµ„æºçš„æƒ…å†µä¸‹å¾®è°ƒï¼Œåªå¼•å…¥äº†å°‘é‡å‚æ•°ï¼Œå°±å¯ä»¥åœ¨æ¶ˆè´¹çº§gpuä¸Šè®­ç»ƒ
  - ä½†lora é—®é¢˜: ä¸èƒ½èŠ‚çœè®­ç»ƒæ—¶é—´ï¼Œç›¸æ¯”äºå…¨é‡å¾®è°ƒï¼Œä»–è¦è®­ç»ƒæ›´ä¹…ï¼ŒåŒæ—¶å› ä¸ºå¯è®­ç»ƒå‚æ•°é‡å¾ˆå°ï¼Œåœ¨åŒæ ·å¤§é‡æ•°æ®è®­ç»ƒä¸‹ï¼Œæ¯”ä¸è¿‡å…¨é‡å¾®è°ƒã€‚
1. LORA åº”è¯¥ä½œç”¨äºTransformer å“ªä¸ªå‚æ•°çŸ©é˜µï¼Ÿ 
  - 1ï¼‰å°†æ‰€æœ‰å¾®è°ƒå‚æ•°éƒ½æ”¾åˆ°attentionçš„æŸä¸€ä¸ªå‚æ•°çŸ©é˜µçš„æ•ˆæœå¹¶ä¸å¥½ï¼Œå°†å¯å¾®è°ƒå‚æ•°å¹³å‡åˆ†é…åˆ° Wq å’Œ Wk çš„æ•ˆæœæœ€å¥½ï¼›
  - 2ï¼‰å³ä½¿æ˜¯ç§©ä»…å–4ä¹Ÿèƒ½åœ¨ âˆ†W ä¸­è·å¾—è¶³å¤Ÿçš„ä¿¡æ¯ã€‚
  - å®é™…æ“ä½œä¸­ï¼Œåº”å½“å°†å¯å¾®è°ƒå‚æ•°åˆ†é…åˆ°å¤šç§ç±»å‹æƒé‡çŸ©é˜µä¸­ï¼Œè€Œä¸åº”è¯¥ç”¨æ›´å¤§çš„ç§©å•ç‹¬å¾®è°ƒæŸç§ç±»å‹çš„æƒé‡çŸ©é˜µã€‚
1. LoRA å¾®è°ƒå‚æ•°é‡æ€ä¹ˆç¡®å®šï¼Ÿ 
  - LoRA æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„ç»“æœæ•°é‡å–å†³äºä½ç§©æ›´æ–°çŸ©é˜µçš„å¤§å°ï¼Œå…¶ä¸»è¦ç”±ç§© r å’ŒåŸå§‹æƒé‡çŸ©é˜µçš„å½¢çŠ¶ç¡®å®šã€‚å®é™…ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡é€‰æ‹©ä¸åŒçš„ lora_target å†³å®šè®­ç»ƒçš„å‚æ•°é‡ã€‚ 
  - ä»¥ LLama ä¸ºä¾‹ï¼š --lora_target q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj
1. Lora çŸ©é˜µæ€ä¹ˆåˆå§‹åŒ–ï¼Ÿä¸ºä»€ä¹ˆè¦åˆå§‹åŒ–ä¸ºå…¨0ï¼Ÿ
  - çŸ©é˜µBè¢«åˆå§‹åŒ–ä¸º0ï¼Œè€ŒçŸ©é˜µAæ­£å¸¸é«˜æ–¯åˆå§‹åŒ–ã€‚ 
  - å¦‚æœBï¼ŒAå…¨éƒ½åˆå§‹åŒ–ä¸º0ï¼Œé‚£ä¹ˆç¼ºç‚¹ä¸æ·±åº¦ç½‘ç»œå…¨0åˆå§‹åŒ–ä¸€æ ·ï¼Œå¾ˆå®¹æ˜“å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±(å› ä¸ºæ­¤æ—¶åˆå§‹æ‰€æœ‰ç¥ç»å…ƒçš„åŠŸèƒ½éƒ½æ˜¯ç­‰ä»·çš„)ã€‚ 
  - å¦‚æœBï¼ŒAå…¨éƒ¨é«˜æ–¯åˆå§‹åŒ–ï¼Œé‚£ä¹ˆåœ¨ç½‘ç»œè®­ç»ƒåˆšå¼€å§‹å°±ä¼šæœ‰æ¦‚ç‡ä¸ºå¾—åˆ°ä¸€ä¸ªè¿‡å¤§çš„åç§»å€¼Î” W ä»è€Œå¼•å…¥å¤ªå¤šå™ªå£°ï¼Œå¯¼è‡´éš¾ä»¥æ”¶æ•›ã€‚ 
  - å› æ­¤ï¼Œä¸€éƒ¨åˆ†åˆå§‹ä¸º0ï¼Œä¸€éƒ¨åˆ†æ­£å¸¸åˆå§‹åŒ–æ˜¯ä¸ºäº†åœ¨è®­ç»ƒå¼€å§‹æ—¶ç»´æŒç½‘ç»œçš„åŸæœ‰è¾“å‡º(åˆå§‹åç§»ä¸º0)ï¼Œä½†åŒæ—¶ä¹Ÿä¿è¯åœ¨çœŸæ­£å¼€å§‹å­¦ä¹ åèƒ½å¤Ÿæ›´å¥½çš„æ”¶æ•›ã€‚
1. Rank å¦‚ä½•é€‰å–ï¼Ÿ 
  - Rank å–å€¼å¸¸è§çš„æ˜¯8ï¼Œç†è®ºä¸Šè¯´Rankåœ¨4-8ä¹‹é—´æ•ˆæœæœ€å¥½ï¼Œå†é«˜å¹¶æ²¡æœ‰æ•ˆæœæå‡ã€‚
  - ä¸è¿‡è®ºæ–‡çš„å®éªŒæ˜¯é¢å‘ä¸‹æ¸¸å•ä¸€ç›‘ç£ä»»åŠ¡çš„ï¼Œå› æ­¤åœ¨æŒ‡ä»¤å¾®è°ƒä¸Šæ ¹æ®æŒ‡ä»¤åˆ†å¸ƒçš„å¹¿åº¦ï¼ŒRanké€‰æ‹©è¿˜æ˜¯éœ€è¦åœ¨8ä»¥ä¸Šçš„å–å€¼è¿›è¡Œæµ‹è¯•ã€‚
1. æ˜¯å¦å¯ä»¥é€å±‚è°ƒæ•´ LoRA æœ€ä¼˜rankï¼Ÿ 
  - ç†è®ºä¸Šï¼Œå¯ä¸ºä¸åŒå±‚é€‰æ‹©ä¸åŒçš„LoRA rankï¼Œç±»ä¼¼äºä¸ºä¸åŒå±‚è®¾å®šä¸åŒå­¦ä¹ ç‡ï¼Œä½†ç”±äºå¢åŠ äº†è°ƒä¼˜å¤æ‚æ€§ï¼Œå®é™…ä¸­å¾ˆå°‘æ‰§è¡Œã€‚
1. alpha å‚æ•° å¦‚ä½•é€‰å–ï¼Ÿ 
  - alpha å…¶å®æ˜¯ä¸ª**ç¼©æ”¾å‚æ•°**ï¼Œæœ¬è´¨å’Œlearning rateç›¸åŒï¼Œæ‰€ä»¥ä¸ºäº†ç®€åŒ–å¯ä»¥é»˜è®¤è®© alpha=rankï¼Œåªè°ƒæ•´lrï¼Œè¿™æ ·å¯ä»¥ç®€åŒ–è¶…å‚ã€‚
1. LoRA é«˜æ•ˆå¾®è°ƒå¦‚ä½•é¿å…`è¿‡æ‹Ÿåˆ`ï¼Ÿ
  - è¿‡æ‹Ÿåˆè¿˜æ˜¯æ¯”è¾ƒå®¹æ˜“å‡ºç°ã€‚å‡å°ræˆ–å¢åŠ æ•°æ®é›†å¤§å°å¯ä»¥å¸®åŠ©å‡å°‘è¿‡æ‹Ÿåˆï¼Œè¿˜å¯ä»¥å°è¯•å¢åŠ ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡ç‡æˆ–LoRAå±‚çš„dropoutå€¼ã€‚
1. å¦‚ä½•åœ¨å·²æœ‰ LoRA æ¨¡å‹ä¸Š**ç»§ç»­è®­ç»ƒ**ï¼Ÿ
  - ç†è§£æ­¤é—®é¢˜çš„æƒ…å½¢æ˜¯ï¼šå·²æœ‰loraæ¨¡å‹åªè®­ç»ƒäº†ä¸€éƒ¨åˆ†æ•°æ®ï¼Œè¦è®­ç»ƒå¦ä¸€éƒ¨åˆ†æ•°æ®çš„è¯ï¼Œæ˜¯åœ¨è¿™ä¸ªloraä¸Šç»§ç»­è®­ç»ƒå‘¢ï¼Œè¿˜æ˜¯è·Ÿbase æ¨¡å‹åˆå¹¶åå†å¥—ä¸€å±‚loraï¼Œæˆ–è€…ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªloraï¼Ÿ 
  - æŠŠä¹‹å‰çš„LoRAè·Ÿbase model åˆå¹¶åï¼Œç»§ç»­è®­ç»ƒå°±å¯ä»¥ï¼Œä¸ºäº†ä¿ç•™ä¹‹å‰çš„çŸ¥è¯†å’Œèƒ½åŠ›ï¼Œè®­ç»ƒæ–°çš„LoRAæ—¶ï¼ŒåŠ å…¥ä¸€äº›ä¹‹å‰çš„è®­ç»ƒæ•°æ®æ˜¯éœ€è¦çš„ã€‚æ¯æ¬¡éƒ½è¦é‡å¤´è®­ç»ƒçš„è¯æˆæœ¬æ¯”è¾ƒé«˜ã€‚
1. å“ªäº›å› ç´ ä¼šå½±å“**å†…å­˜ä½¿ç”¨**ï¼Ÿ 
  - å†…å­˜ä½¿ç”¨å—åˆ°æ¨¡å‹å¤§å°ã€æ‰¹é‡å¤§å°ã€LoRAå‚æ•°æ•°é‡ä»¥åŠæ•°æ®é›†ç‰¹æ€§çš„å½±å“ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨è¾ƒçŸ­çš„è®­ç»ƒåºåˆ—å¯ä»¥èŠ‚çœå†…å­˜ã€‚
1. LoRA æƒé‡æ˜¯å¦å¯ä»¥**åˆå…¥åŸæ¨¡å‹**ï¼Ÿ
  - å¯ä»¥ï¼Œå°†è®­ç»ƒå¥½çš„ä½ç§©çŸ©é˜µï¼ˆB*Aï¼‰+åŸæ¨¡å‹æƒé‡åˆå¹¶ï¼ˆç›¸åŠ ï¼‰ï¼Œè®¡ç®—å‡ºæ–°çš„æƒé‡ã€‚
1. LoRA æƒé‡æ˜¯å¦å¯ä»¥**åˆå¹¶**ï¼Ÿ 
  - å¯å°†å¤šå¥—LoRAæƒé‡åˆå¹¶ã€‚è®­ç»ƒä¸­ä¿æŒLoRAæƒé‡ç‹¬ç«‹ï¼Œå¹¶åœ¨å‰å‘ä¼ æ’­æ—¶æ·»åŠ ï¼Œè®­ç»ƒåå¯ä»¥åˆå¹¶æƒé‡ä»¥ç®€åŒ–æ“ä½œã€‚


### LoRA å®ç°

å®˜æ–¹notebookæ¡ˆä¾‹ï¼š[peft_lora_seq2seq](https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq.ipynb)

ä¾èµ–åŒ…ï¼š
- transformersæä¾›æ¨¡å‹åŠ è½½å’Œè®­ç»ƒï¼›
- peftæä¾›LoRAå®ç°ï¼›
- DeepSpeedæä¾›è®­ç»ƒåŠ é€Ÿã€‚

æ³¨æ„ï¼š
>peftåŒ…ç›®å‰è¿˜å¤„äºå¿«é€Ÿè¿­ä»£å½“ä¸­ï¼Œåç»­æ¥å£å¯èƒ½ä¼šæœ‰å¤§çš„å˜åŠ¨ï¼Œä¹Ÿå¯èƒ½å­˜åœ¨ä¸€äº›bugã€‚

å…³é”®ä¾èµ–åŒ…ç‰ˆæœ¬ï¼š

```sh
transformers==4.26.1
torch==1.13.1
deepspeed==0.8.2
peft==0.2.0

git clone https://github.com/microsoft/DeepSpeedExamples.git
cd DeepSpeedExamples/applications/DeepSpeed-Chat/
```

ds lora 

```sh
deepspeed --num_gpus 1 main.py \
   --data_path Dahoas/rm-static \
   --data_split 2,4,4 \
   --model_name_or_path facebook/opt-6.7b \
   --per_device_train_batch_size 8 \
   --per_device_eval_batch_size 8 \
   --max_seq_len 512 \
   --learning_rate 1e-3 \
   --weight_decay 0.1 \
   --num_train_epochs 2 \
   --gradient_accumulation_steps 16 \
   --lr_scheduler_type cosine \
   --num_warmup_steps 0 \
   --seed 1234 \
   --gradient_checkpointing \
   --zero_stage 0 \
   --lora_dim 128 \
   --lora_module_name decoder.layers. \
   --deepspeed \
   --output_dir $OUTPUT_PATH \
   &> $OUTPUT_PATH/training.log
```


#### è®­ç»ƒä»£ç 

å‡è®¾è®­ç»ƒä»£ç ä½äºtrain.pyã€‚

å¯¼å…¥ä¾èµ–åŒ…

```py
import os
import torch
import random
import datasets
import numpy as np
â€‹
from tqdm import tqdm
from typing import Dict
from torch.utils.data import DataLoader
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    Trainer
)
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
    get_peft_model_state_dict,
    set_peft_model_state_dict
)
â€‹
def set_random_seed(seed):
    if seed is not None and seed > 0:
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.random.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
â€‹
set_random_seed(1234)

# ----- è®¾ç½®å‚æ•° -----
# LoRAå‚æ•°
LORA_R = 8
LORA_ALPHA = 32
LORA_DROPOUT = 0.1
# è®­ç»ƒå‚æ•°
EPOCHS=3
LEARNING_RATE=5e-5
OUTPUT_DIR="./checkpoints"
BATCH_SIZE=4 # 2
GRADIENT_ACCUMULATION_STEPS=3
# å…¶ä»–å‚æ•°
MODEL_PATH = "bigscience/bloomz-7b1-mt"
DATA_PATH = "./data/belle_open_source_1M.train.json"
MAX_LENGTH = 512
PATTERN = "{}\n{}"
DS_CONFIG = "ds_zero2_config.json"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) # åŠ è½½tokenizer

# ---- åŠ è½½æ•°æ® -----
dataset = datasets.load_dataset("json", data_files=DATA_PATH)
# print(dataset["train"][0])
# tokenize åˆ†è¯
def tokenize(text: str, add_eos_token=True):
    result = tokenizer(
        text,
        truncation=True,
        max_length=MAX_LENGTH,
        padding=False,
        return_tensors=None)
    # åˆ¤æ–­æ˜¯å¦è¦æ·»åŠ eos_token
    if (result["input_ids"][-1] != tokenizer.eos_token_id
        and len(result["input_ids"]) < MAX_LENGTH
        and add_eos_token):
        result["input_ids"].append(tokenizer.eos_token_id)
        result["attention_mask"].append(1)
    result["labels"] = result["input_ids"].copy()
    return result
â€‹
def preprocess(example: Dict, train_on_inputs: bool = False):
    prompt = example["input"]
    response = example["target"]
    text = PATTERN.format(prompt, response)
    tokenized_inp = tokenize(text)
    # è‹¥train_on_inputsä¸ºFalseï¼Œåˆ™å°†labelä¸­ä¸inputç›¸å…³çš„tokenæ›¿æ¢ä¸º-100
    if not train_on_inputs:
        tokenized_prompt = tokenize(prompt,add_eos_token=False)
        prompt_tokens_len = len(tokenized_prompt["input_ids"])
        tokenized_inp["labels"] = [-100]*prompt_tokens_len + tokenized_inp["labels"][prompt_tokens_len:]
    return tokenized_inp
â€‹
train_data = dataset["train"].shuffle().map(preprocess, remove_columns=["id", "input", "target"])
print(train_data[0])

# ----- collate_fn -----
# pad_to_multiple_of=8è¡¨ç¤ºpaddingçš„é•¿åº¦æ˜¯8çš„å€æ•°
collate_fn = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True)
# åŠ è½½æ¨¡å‹
device_map = {"": int(os.environ.get("LOCAL_RANK") or 0)}
# device_mapæŒ‡å®šæ¨¡å‹åŠ è½½çš„GPU;troch_dtype=torch.float16è¡¨ç¤ºåŠç²¾åº¦åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.float16, device_map=device_map)
# ----- LoRAç›¸å…³ -----
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=LORA_R, # LoRAä¸­ä½ç§©è¿‘ä¼¼çš„ç§©
    lora_alpha=LORA_ALPHA, # è§ä¸Šæ–‡ä¸­çš„ä½ç§©çŸ©é˜µç¼©æ”¾è¶…å‚æ•°
    lora_dropout=LORA_DROPOUT, # LoRAå±‚çš„dropout
)
# ----- è½¬æ¢æ¨¡å‹ -----
model = get_peft_model(model, lora_config)
model.config.use_cache = False
old_state_dict = model.state_dict
model.state_dict = (
    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())
).__get__(model, type(model))
# æ‰“å°æ¨¡å‹ä¸­çš„å¯è®­ç»ƒå‚æ•°
model.print_trainable_parameters()
# ----- è®­ç»ƒå‚æ•° -----
args = TrainingArguments(
    output_dir=OUTPUT_DIR, # checkpointçš„å­˜å‚¨ç›®å½•
    per_device_train_batch_size=BATCH_SIZE, # å•è®¾å¤‡ä¸Šçš„batch size
    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, # æ¢¯åº¦ç´¯åŠ çš„stepæ•°
    warmup_steps=100,
    num_train_epochs=EPOCHS,
    learning_rate=LEARNING_RATE,
    fp16=True, # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    logging_steps=50,
    evaluation_strategy="no", # ä¸è¿›è¡Œè¯„ä¼°
    save_strategy="steps",
    save_steps=2000, # ä¿å­˜checkpointçš„stepæ•°
    save_total_limit=5, # æœ€å¤šä¿å­˜5ä¸ªcheckpoint
    deepspeed=DS_CONFIG
)
# æ¨¡å‹è®­ç»ƒ
trainer = Trainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=None,
    args=args,
    data_collator=collate_fn
)
trainer.train()
model.save_pretrained("best_model")
```

DeepSpeedé…ç½®æ–‡ä»¶
- DeepSpeedé…ç½®æ–‡ä»¶åä¸ºds_zero2_config.jsonã€‚

```py
{
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  "steps_per_print": 50,
  "gradient_clipping": 1.0,
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
            "device": "cpu"
    },
    "contiguous_gradients": true,
    "overlap_comm": true
  },
  "zero_allow_untested_optimizer": true,
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "hysteresis": 2,
    "min_loss_scale": 1
  },
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": "auto",
      "betas": "auto",
      "eps": "auto",
      "weight_decay": "auto"
    }
  },
  "activation_checkpointing": {
    "partition_activations": true,
    "contiguous_memory_optimization": true
  },
  "wall_clock_breakdown": false
}
```

**å¯åŠ¨**

```sh
deepspeed --include=localhost:0,1,2,3 train.py
```

#### LoRA æ¨ç†

æ¨ç†æ–‡ä»¶åä¸ºinference.py

åŸå§‹æ¨¡å‹å’Œloraæ¨¡å‹é¡ºåºå¤„ç†ï¼Œå†åˆå¹¶
- å…ˆåŠ è½½ base_model
- å†åŠ è½½ lora_model 
- æ¨ç†

```py
import torch
  
from peft import PeftModel # lora
from transformers import AutoModelForCausalLM, AutoTokenizer
â€‹# ---- åŸå§‹æ¨¡å‹ -----
BASE_MODEL = "bigscience/bloomz-7b1-mt"
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16, # åŠ è½½åŠç²¾åº¦
        device_map={"":0}, # æŒ‡å®šGPU 0
    )
model.eval()
â€‹# ---- LoRAæ¨¡å‹ -----
LORA_WEIGHTS = "best_model"
model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16)
model.half() # åŠç²¾åº¦
â€‹# ---- æ¨ç† -----
prompt = ""
inp = tokenizer(prompt, max_length=512, return_tensors="pt").to("cuda")
outputs = model.generate(input_ids=inp["input_ids"], max_new_tokens=256)
print(tokenizer.decode(outputs[0]))
```

[åŸæ–‡](https://zhuanlan.zhihu.com/p/618073170)


## LoRA è¿›åŒ–

ã€2024-5-24ã€‘

<!-- draw.io diagram -->
<div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers tags lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; modified=\&quot;2024-05-24T12:43:16.306Z\&quot; agent=\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\&quot; etag=\&quot;MZRa7dl-SX42qmgFS0G9\&quot; version=\&quot;24.4.4\&quot;&gt;\n  &lt;diagram name=\&quot;ç¬¬ 1 é¡µ\&quot; id=\&quot;YUrH7kkdw6S7EPocWAtV\&quot;&gt;\n    &lt;mxGraphModel dx=\&quot;1434\&quot; dy=\&quot;761\&quot; grid=\&quot;1\&quot; gridSize=\&quot;10\&quot; guides=\&quot;1\&quot; tooltips=\&quot;1\&quot; connect=\&quot;1\&quot; arrows=\&quot;1\&quot; fold=\&quot;1\&quot; page=\&quot;1\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;827\&quot; pageHeight=\&quot;1169\&quot; math=\&quot;0\&quot; shadow=\&quot;0\&quot;&gt;\n      &lt;root&gt;\n        &lt;mxCell id=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;CRyWcW9bKPYmjVe2kgWn-2\&quot; value=\&quot;LoRAè¿›åŒ–ä¹‹è·¯\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontStyle=0;fontSize=22;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;450\&quot; y=\&quot;30\&quot; width=\&quot;170\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;lHimWeaf7UQe36nZpLsc-30\&quot; value=\&quot;wqw547243068@163.com&amp;lt;br&amp;gt;2023-12-31\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;67.88\&quot; y=\&quot;410\&quot; width=\&quot;170\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;s2pv88CfUx30G5g510ko-26\&quot; value=\&quot;ã€2021-10-16ã€‘å¾®è½¯\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;40.50999999999999\&quot; y=\&quot;240\&quot; width=\&quot;130\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;1nzIP9oYbUBeoa8APZYR-3\&quot; value=\&quot;ä½ç§©åˆ†è§£æ¨¡æ‹Ÿå‚æ•°æ”¹å˜é‡ï¼ŒPEFTæ–¹æ³•ä¹‹ä¸€&amp;lt;div&amp;gt;ä»¥æå°å‚æ•°é‡å®ç°å¤§æ¨¡å‹é—´æ¥è®­ç»ƒ&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;7&amp;amp;nbsp; B: 13G -&amp;amp;gt; 8M&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;13B: 26G -&amp;amp;gt; 13M&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;33B: 65G -&amp;amp;gt; 25M&amp;lt;/div&amp;gt;\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; parent=\&quot;1\&quot; vertex=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;10\&quot; y=\&quot;300\&quot; width=\&quot;250\&quot; height=\&quot;90\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-4\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#999999;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;2hlaNrKACTqZJU9JD0af-1\&quot; target=\&quot;2hlaNrKACTqZJU9JD0af-2\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-1\&quot; value=\&quot;&amp;lt;font&amp;gt;LoRA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#0050ef;strokeColor=#001DBC;shadow=1;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;67.88\&quot; y=\&quot;270\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-2\&quot; value=\&quot;&amp;lt;font&amp;gt;AdaLoRA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#0050ef;strokeColor=#001DBC;shadow=1;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;308.75\&quot; y=\&quot;270\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-6\&quot; value=\&quot;æ ¹æ®é‡è¦è¯„åˆ†åŠ¨æ€åˆ†é…å‚æ•°é¢„ç®—&amp;lt;div&amp;gt;é‡è¦çŸ©é˜µé«˜ç§©ï¼Œæ¬¡è¦çŸ©é˜µä½ç§©&amp;lt;/div&amp;gt;\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;389.75\&quot; y=\&quot;270\&quot; width=\&quot;190\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-7\&quot; value=\&quot;&amp;lt;font&amp;gt;QLoRA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#0050ef;strokeColor=#001DBC;shadow=1;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;284\&quot; y=\&quot;380\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-8\&quot; value=\&quot;ã€2023-05-23ã€‘åç››é¡¿å¤§å­¦\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;344\&quot; y=\&quot;370\&quot; width=\&quot;170\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-9\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#999999;exitX=1;exitY=0.5;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;2hlaNrKACTqZJU9JD0af-1\&quot; target=\&quot;2hlaNrKACTqZJU9JD0af-7\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;153\&quot; y=\&quot;295\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;294\&quot; y=\&quot;295\&quot; as=\&quot;targetPoint\&quot; /&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;190\&quot; y=\&quot;285\&quot; /&gt;\n              &lt;mxPoint x=\&quot;190\&quot; y=\&quot;395\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-10\&quot; value=\&quot;4bité‡åŒ–ï¼Œåˆ†é¡µä¼˜åŒ–å™¨ï¼ŒåŒé‡é‡åŒ–&amp;lt;div&amp;gt;48Gæ˜¾å­˜GPUä¸Šè®­ç»ƒ65Bæ¨¡å‹ï¼Œå¯æ‰‹æœºä¸Šä½¿ç”¨&amp;lt;/div&amp;gt;\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;360\&quot; y=\&quot;390\&quot; width=\&quot;260\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-11\&quot; value=\&quot;&amp;lt;font&amp;gt;ReLoRA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#0050ef;strokeColor=#001DBC;shadow=1;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;364.75\&quot; y=\&quot;195\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-12\&quot; value=\&quot;ã€2023-08-21ã€‘é©¬è¨è¯¸å¡å¤§å­¦ã€Eleuther AI\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;344\&quot; y=\&quot;165\&quot; width=\&quot;250\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-13\&quot; value=\&quot;å åŠ å¤šä¸ªä½ç§©æ›´æ–°çŸ©é˜µ\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;440\&quot; y=\&quot;190\&quot; width=\&quot;140\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-14\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#999999;exitX=1;exitY=0.5;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;2hlaNrKACTqZJU9JD0af-1\&quot; target=\&quot;2hlaNrKACTqZJU9JD0af-11\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;153\&quot; y=\&quot;295\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;294\&quot; y=\&quot;295\&quot; as=\&quot;targetPoint\&quot; /&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;210\&quot; y=\&quot;285\&quot; /&gt;\n              &lt;mxPoint x=\&quot;210\&quot; y=\&quot;210\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-15\&quot; value=\&quot;&amp;lt;font&amp;gt;LongLoRA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#0050ef;strokeColor=#001DBC;shadow=1;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;494.75\&quot; y=\&quot;234\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-16\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#999999;exitX=1;exitY=0.5;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;2hlaNrKACTqZJU9JD0af-1\&quot; target=\&quot;2hlaNrKACTqZJU9JD0af-15\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;153\&quot; y=\&quot;295\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;375\&quot; y=\&quot;220\&quot; as=\&quot;targetPoint\&quot; /&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;230\&quot; y=\&quot;285\&quot; /&gt;\n              &lt;mxPoint x=\&quot;230\&quot; y=\&quot;250\&quot; /&gt;\n              &lt;mxPoint x=\&quot;485\&quot; y=\&quot;250\&quot; /&gt;\n              &lt;mxPoint x=\&quot;485\&quot; y=\&quot;249\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-17\&quot; value=\&quot;ã€2023-10-01ã€‘æ¸¯ä¸­æ–‡/MIT\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;559.75\&quot; y=\&quot;225\&quot; width=\&quot;170\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-18\&quot; value=\&quot;æ‰©å¤§çª—å£ï¼š4k -&amp;amp;gt; 32k\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;579.75\&quot; y=\&quot;240\&quot; width=\&quot;130\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-19\&quot; value=\&quot;&amp;lt;font&amp;gt;SLoRA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#0050ef;strokeColor=#001DBC;shadow=1;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;584.75\&quot; y=\&quot;314\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-20\&quot; value=\&quot;ã€2023-11-15ã€‘ä¼¯å…‹åˆ©/æ–¯å¦ç¦\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;570\&quot; y=\&quot;284\&quot; width=\&quot;180\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-21\&quot; value=\&quot;ä¸€ä¸ªGPUéƒ¨ç½²å¤šä¸ªLoRA&amp;lt;div&amp;gt;pageattention â†’ ç»Ÿä¸€åˆ†é¡µ&amp;lt;/div&amp;gt;\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;570\&quot; y=\&quot;344\&quot; width=\&quot;160\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-22\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#999999;exitX=1;exitY=0.5;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;2hlaNrKACTqZJU9JD0af-1\&quot; target=\&quot;2hlaNrKACTqZJU9JD0af-19\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;153\&quot; y=\&quot;295\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;294\&quot; y=\&quot;405\&quot; as=\&quot;targetPoint\&quot; /&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;240\&quot; y=\&quot;285\&quot; /&gt;\n              &lt;mxPoint x=\&quot;240\&quot; y=\&quot;330\&quot; /&gt;\n              &lt;mxPoint x=\&quot;580\&quot; y=\&quot;330\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-23\&quot; value=\&quot;&amp;lt;font&amp;gt;DoRA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#0050ef;strokeColor=#001DBC;shadow=1;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;799\&quot; y=\&quot;355\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-24\&quot; value=\&quot;ã€2024-03-01ã€‘NVIDIA Lab\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;864.25\&quot; y=\&quot;355\&quot; width=\&quot;170\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-25\&quot; value=\&quot;&amp;lt;span style=&amp;quot;color: rgb(51, 51, 51); font-family: Arial, &amp;amp;quot;Microsoft YaHei&amp;amp;quot;, é»‘ä½“, å®‹ä½“, sans-serif; font-size: 11px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;&amp;quot;&amp;gt;é¢„è®­ç»ƒæƒé‡åˆ†è§£ä¸º&amp;lt;/span&amp;gt;&amp;lt;strong style=&amp;quot;box-sizing: border-box; color: rgb(51, 51, 51); font-family: Arial, &amp;amp;quot;Microsoft YaHei&amp;amp;quot;, é»‘ä½“, å®‹ä½“, sans-serif; font-size: 11px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;&amp;quot;&amp;gt;å¹…åº¦&amp;lt;/strong&amp;gt;&amp;lt;span style=&amp;quot;color: rgb(51, 51, 51); font-family: Arial, &amp;amp;quot;Microsoft YaHei&amp;amp;quot;, é»‘ä½“, å®‹ä½“, sans-serif; font-size: 11px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;&amp;quot;&amp;gt;ï¼ˆmagnitudeï¼‰å’Œ&amp;lt;/span&amp;gt;&amp;lt;strong style=&amp;quot;box-sizing: border-box; color: rgb(51, 51, 51); font-family: Arial, &amp;amp;quot;Microsoft YaHei&amp;amp;quot;, é»‘ä½“, å®‹ä½“, sans-serif; font-size: 11px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;&amp;quot;&amp;gt;æ–¹å‘&amp;lt;/strong&amp;gt;&amp;lt;span style=&amp;quot;color: rgb(51, 51, 51); font-family: Arial, &amp;amp;quot;Microsoft YaHei&amp;amp;quot;, é»‘ä½“, å®‹ä½“, sans-serif; font-size: 11px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;&amp;quot;&amp;gt;ï¼ˆdirectionï¼‰&amp;lt;/span&amp;gt;&amp;lt;div&amp;gt;&amp;lt;span style=&amp;quot;color: rgb(51, 51, 51); font-family: Arial, &amp;amp;quot;Microsoft YaHei&amp;amp;quot;, é»‘ä½“, å®‹ä½“, sans-serif; font-size: 11px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;&amp;quot;&amp;gt;LoRAå¾®è°ƒæ–¹å‘çŸ©é˜µ&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;\&quot; style=\&quot;text;whiteSpace=wrap;html=1;fillColor=none;labelBackgroundColor=none;fontSize=11;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;802.75\&quot; y=\&quot;385\&quot; width=\&quot;293\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-36\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;2hlaNrKACTqZJU9JD0af-26\&quot; target=\&quot;2hlaNrKACTqZJU9JD0af-35\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-26\&quot; value=\&quot;&amp;lt;font&amp;gt;LoRAMOE&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#1ba1e2;strokeColor=#006EAF;shadow=1;fontColor=#ffffff;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;715.25\&quot; y=\&quot;135\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-27\&quot; value=\&quot;ã€2023-12-?ã€‘?\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;697.87\&quot; y=\&quot;75\&quot; width=\&quot;110\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-28\&quot; value=\&quot;ç»“åˆMoEæŠ€æœ¯ï¼Œtokenè½¯è·¯ç”±\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;520.0000000000001\&quot; y=\&quot;105\&quot; width=\&quot;170\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-31\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;2hlaNrKACTqZJU9JD0af-29\&quot; target=\&quot;2hlaNrKACTqZJU9JD0af-26\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-29\&quot; value=\&quot;&amp;lt;font&amp;gt;MoLoRA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#1ba1e2;strokeColor=#006EAF;shadow=1;fontColor=#ffffff;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;477.37\&quot; y=\&quot;135\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-30\&quot; value=\&quot;ã€2023-09-?ã€‘?\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;500\&quot; y=\&quot;85\&quot; width=\&quot;110\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-32\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#999999;exitX=1;exitY=0.5;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;2hlaNrKACTqZJU9JD0af-1\&quot; target=\&quot;2hlaNrKACTqZJU9JD0af-29\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;140\&quot; y=\&quot;290\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;375\&quot; y=\&quot;220\&quot; as=\&quot;targetPoint\&quot; /&gt;\n            &lt;Array as=\&quot;points\&quot;&gt;\n              &lt;mxPoint x=\&quot;180\&quot; y=\&quot;285\&quot; /&gt;\n              &lt;mxPoint x=\&quot;180\&quot; y=\&quot;150\&quot; /&gt;\n            &lt;/Array&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-33\&quot; value=\&quot;&amp;lt;font&amp;gt;MoV&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#1ba1e2;strokeColor=#006EAF;shadow=1;fontColor=#ffffff;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;481.37\&quot; y=\&quot;100\&quot; width=\&quot;32.63\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-34\&quot; value=\&quot;è§£å†³ç¾éš¾é—å¿˜&amp;lt;div&amp;gt;åŒä¸€ä½ç½®loraåˆ†ä¸¤ç»„&amp;lt;/div&amp;gt;\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;715.2500000000001\&quot; y=\&quot;95\&quot; width=\&quot;130\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-35\&quot; value=\&quot;&amp;lt;font&amp;gt;MOLA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#1ba1e2;strokeColor=#006EAF;shadow=1;fontColor=#ffffff;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;845.25\&quot; y=\&quot;135\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-37\&quot; value=\&quot;ã€2024-02-?ã€‘?\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;827.88\&quot; y=\&quot;85\&quot; width=\&quot;110\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-38\&quot; value=\&quot;ç¦»æ•£è·¯ç”±\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;847.8800000000001\&quot; y=\&quot;105\&quot; width=\&quot;70\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-39\&quot; value=\&quot;&amp;lt;font&amp;gt;PiSSA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#0050ef;strokeColor=#001DBC;shadow=1;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;999\&quot; y=\&quot;300\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-40\&quot; value=\&quot;ã€2024-04-12ã€‘åŒ—å¤§\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;989\&quot; y=\&quot;255\&quot; width=\&quot;130\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-41\&quot; value=\&quot;&amp;lt;font face=&amp;quot;Arial, Microsoft YaHei, é»‘ä½“, å®‹ä½“, sans-serif&amp;quot; color=&amp;quot;#333333&amp;quot;&amp;gt;Adapteråˆå§‹åŒ–æ–¹å¼ä¸åŒ&amp;lt;/font&amp;gt;\&quot; style=\&quot;text;whiteSpace=wrap;html=1;fillColor=none;labelBackgroundColor=none;fontSize=11;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;1009\&quot; y=\&quot;275\&quot; width=\&quot;120\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-42\&quot; value=\&quot;&amp;lt;font&amp;gt;LISA&amp;lt;/font&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#0050ef;strokeColor=#001DBC;shadow=1;fontColor=#FFFFFF;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;816.8800000000001\&quot; y=\&quot;300\&quot; width=\&quot;75.25\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-43\&quot; value=\&quot;ã€2024-03-28ã€‘é¦™æ¸¯ç†å·¥\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;fontColor=#006600;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;804\&quot; y=\&quot;259\&quot; width=\&quot;160\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;2hlaNrKACTqZJU9JD0af-44\&quot; value=\&quot;å§‹ç»ˆæ›´æ–°åº•å±‚embeddingå’Œé¡¶å±‚linearhead\&quot; style=\&quot;text;html=1;align=left;verticalAlign=middle;resizable=0;points=[];autosize=1;strokeColor=none;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;760\&quot; y=\&quot;275\&quot; width=\&quot;250\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n      &lt;/root&gt;\n    &lt;/mxGraphModel&gt;\n  &lt;/diagram&gt;\n&lt;/mxfile&gt;\n&quot;}"></div>
<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>



### å• LoRA

#### ã€2023-3-18ã€‘AdaLoRA

å¯¹LoRAçš„ä¸€ç§æ”¹è¿›ï¼Œæ ¹æ®**é‡è¦æ€§è¯„åˆ†**åŠ¨æ€åˆ†é…å‚æ•°é¢„ç®—ç»™æƒé‡çŸ©é˜µï¼Œå°†å…³é”®çš„å¢é‡çŸ©é˜µåˆ†é…é«˜ç§©ä»¥æ•æ‰æ›´ç²¾ç»†å’Œä»»åŠ¡ç‰¹å®šçš„ä¿¡æ¯ï¼Œè€Œå°†è¾ƒä¸é‡è¦çš„çŸ©é˜µçš„ç§©é™ä½ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶èŠ‚çœè®¡ç®—é¢„ç®—ã€‚

è®ºæ–‡
- [ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING](https://arxiv.org/pdf/2303.10512)

å‡ºå‘ç‚¹ï¼š
- åœ¨ä¸åŒå±‚ã€ä¸åŒ$$\textbf{W}$$ä¸Šæ·»åŠ LoRAï¼Œæ•ˆæœä¸åŒï¼Œé‚£ä¹ˆå¦‚ä½•åœ¨è§„å®šçš„æ€»ranké¢„ç®—ä¸‹ï¼Œè¾¾æˆæœ€ä¼˜æ•ˆæœã€‚ä¹Ÿå°±æ˜¯å¦‚ä½•ç»™ä¸åŒ$$\textbf{W}$$åˆ†é…ä¸åŒçš„rankè¿›è¡Œfinetune

é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­çš„ä¸åŒæƒé‡å‚æ•°å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„è´¡çŒ®ä¸åŒã€‚

å› æ­¤éœ€è¦æ›´åŠ æ™ºèƒ½åœ°åˆ†é…å‚æ•°é¢„ç®—ï¼Œä»¥ä¾¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ›´åŠ é«˜æ•ˆåœ°æ›´æ–°é‚£äº›å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®è¾ƒå¤§çš„å‚æ•°ã€‚
- é€šè¿‡**å¥‡å¼‚å€¼åˆ†è§£**å°†æƒé‡çŸ©é˜µåˆ†è§£ä¸º**å¢é‡çŸ©é˜µ**ï¼Œå¹¶æ ¹æ®**æ–°é‡è¦æ€§åº¦é‡**åŠ¨æ€åœ°è°ƒæ•´æ¯ä¸ªå¢é‡çŸ©é˜µä¸­å¥‡å¼‚å€¼çš„å¤§å°ã€‚
- è¿™æ ·å¯ä»¥ä½¿å¾—åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åªæ›´æ–°é‚£äº›å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®è¾ƒå¤§æˆ–å¿…è¦çš„å‚æ•°ï¼Œä»è€Œæé«˜äº†æ¨¡å‹æ€§èƒ½å’Œå‚æ•°æ•ˆç‡ã€‚

æ•ˆæœ
1. åœ¨ç›¸åŒæ€»rankä¸‹ï¼ŒadaLoRAæ•ˆæœå¥½äºLoRA
2. ä¸åŒå±‚ä¸åŒ$$\textbf{W}$$çš„rankï¼šæ›´åå¥½FFN1å’Œæ›´é«˜å±‚çš„W

ä»£ç æ ·ä¾‹ï¼š

```py
peft_config = AdaLoraConfig(peft_type="ADALORA", task_type="SEQ_2_SEQ_LM", r=8, lora_alpha=32, target_modules=["q", "v"],lora_dropout=0.01)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)
model = get_peft_model(model, peft_config)
```


#### ã€2023-5-23ã€‘QLoRA

ã€2023-5-23ã€‘åç››é¡¿å¤§å­¦å‘å¸ƒä¸€ç§é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼šQLoRAï¼Œåœ¨ä¿æŒå®Œæ•´çš„16ä½å¾®è°ƒä»»åŠ¡æ€§èƒ½ä¸‹ï¼Œå®ç°å•ä¸ª 48GB GPU ä¸Šå¾®è°ƒ 65B å‚æ•°é‡æ¨¡å‹ã€‚
- [QLoRA: Efficient Finetuning of Quantized LLMs](arxiv.org/abs/2305.14314)
- githubï¼š[QLoRA](https://github.com/artidoro/qlora)ï¼Œ[Demo](https://huggingface.co/spaces/uwnlp/guanaco-playground-tgi)
- å‚è€ƒï¼š[å¼€æºã€ŒåŸé©¼ã€çˆ†ç«ï¼ŒiPhoneéƒ½èƒ½å¾®è°ƒå¤§æ¨¡å‹äº†ï¼Œå¾—åˆ†é€¼è¿‘ChatGPTï¼](https://mp.weixin.qq.com/s/RakazI25dMJz0JUkdtbr0w)

QLoRA é€šè¿‡å†»ç»“çš„ 4-bit é‡åŒ–é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å‘ä½ç§©é€‚é…å™¨(LoRA) **åå‘ä¼ æ’­**æ¢¯åº¦ã€‚ä½¿ç”¨ 4-bit NormalFloat (NF4) é‡åŒ–ã€Double Quantizationã€Paged Optimizersã€æ‰€æœ‰ Linear å±‚æ’å…¥ adapter ç­‰æŠ€æœ¯ï¼ŒQLoRA åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å¤§å¤§èŠ‚çœäº†æ˜¾å­˜å ç”¨ã€‚

è¯´æ˜å¦‚ä¸‹ï¼š
- **4bit NormalFloat**ï¼ˆNF4ï¼‰ï¼šå¯¹äºæ­£æ€åˆ†å¸ƒæƒé‡è€Œè¨€ï¼Œä¸€ç§ä¿¡æ¯ç†è®ºä¸Šæœ€ä¼˜çš„æ–°æ•°æ®ç±»å‹ï¼Œè¯¥æ•°æ®ç±»å‹å¯¹äºæ­£æ€åˆ†å¸ƒæ•°æ®å¯ä»¥äº§ç”Ÿæ¯” 4 bit æ•´æ•°å’Œ 4bit æµ®ç‚¹æ•°æ›´å¥½çš„å®è¯ç»“æœã€‚
- **Double Quantization**ï¼šå¯¹ç¬¬ä¸€æ¬¡é‡åŒ–åçš„é‚£äº›å¸¸é‡å†è¿›è¡Œä¸€æ¬¡é‡åŒ–ï¼Œå‡å°‘å­˜å‚¨ç©ºé—´ã€‚
- **Paged Optimizers**ï¼šä½¿ç”¨ NVIDIA ç»Ÿä¸€å†…å­˜ç‰¹æ€§ï¼Œå®ç°äº† CPU å’Œ GPU ä¹‹é—´è‡ªåŠ¨çš„é¡µé¢è½¬æ¢ã€‚å½“ GPU å†…å­˜ä¸è¶³æ—¶ï¼ŒPaged Optimizers æŠ€æœ¯ä¼šè‡ªåŠ¨å°†ä¼˜åŒ–å™¨çŠ¶æ€è½¬ç§»åˆ° CPU å†…å­˜ï¼Œä»¥ç¡®ä¿ä¼˜åŒ–å™¨çš„æ­£å¸¸è¿è¡Œã€‚
- **All-Linear-Layer-Adapter**ï¼šåœ¨æ‰€æœ‰å…¨è¿æ¥å±‚éƒ½æ’å…¥ LoRA Adapterï¼Œå¢åŠ äº†è®­ç»ƒå‚æ•°ï¼Œèƒ½åŒ¹é…16ä½å…¨å‚æ•°å¾®è°ƒçš„æ€§èƒ½ã€‚

- å‡å°‘å†…å­˜ä½¿ç”¨é‡ï¼Œè¶³ä»¥åœ¨å•ä¸ª **48GB** GPU ä¸Šå¾®è°ƒ **65B** å‚æ•°æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™å®Œæ•´çš„ 16 ä½å¾®è°ƒä»»åŠ¡æ€§èƒ½ã€‚å…¶ä¸­æœ€å¥½çš„æ¨¡å‹ç§°ä¸º `Guanaco`ï¼Œåœ¨ `Vincuna` åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºä¹‹å‰å…¬å¼€å‘å¸ƒçš„æ¨¡å‹ï¼Œå¹¶ç¼©å°äº†åœ¨ ChatGPT ä¸Šçš„å·®è·ï¼Œè¾¾åˆ° ChatGPT æ€§èƒ½æ°´å¹³çš„ 99.3%ï¼ŒåŒæ—¶ä»…åœ¨å•ä¸ªä¸“ä¸š GPU ä¸Šå¾®è°ƒ 24 å°æ—¶ã€‚
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/1e63b20d24a648bfaffa5d4e86266b65~tplv-obj:787:744.image?_iz=97245&from=post&x-expires=1692748800&x-signature=SEttWjvsni1S2XkoNgW7RHdIqiI%3D)

Qä»£è¡¨**é‡åŒ–**ï¼ˆQuantizationï¼‰ï¼Œç”¨ä½ç²¾åº¦æ•°æ®ç±»å‹å»é€¼è¿‘ç¥ç»ç½‘ç»œä¸­çš„é«˜ç²¾åº¦æµ®ç‚¹æ•°ï¼Œä»¥æé«˜è¿ç®—æ•ˆç‡

QLoRA ç»“åˆäº† 4-bité‡åŒ– å’Œ LoRAï¼Œä»¥åŠå›¢é˜Ÿæ–°åˆ›çš„ä¸‰ä¸ªæŠ€å·§ï¼š
- æ–°æ•°æ®ç±»å‹ 4-bit NormalFloat
- åˆ†é¡µä¼˜åŒ–å™¨ï¼ˆPaged Optimizersï¼‰
- åŒé‡é‡åŒ–ï¼ˆDouble Quantizationï¼‰

æœ€ç»ˆï¼Œ QLoRA è®© 4-bitçš„åŸé©¼åœ¨æ‰€æœ‰åœºæ™¯å’Œè§„æ¨¡çš„æµ‹è¯•ä¸­åŒ¹é…16-bitçš„æ€§èƒ½ã€‚
- QLoRAçš„é«˜æ•ˆç‡è®©å›¢é˜Ÿåœ¨åç››é¡¿å¤§å­¦çš„å°å‹GPUé›†ç¾¤ä¸Šæ¯å¤©å¯ä»¥å¾®è°ƒLLaMA 100å¤šæ¬¡

ä¸¤ä¸ªå…³é”®ç»“è®ºï¼š
- <span style='color:blue'>æ•°æ®è´¨é‡ >> æ•°æ®æ•°é‡</span>
- æŒ‡ä»¤å¾®è°ƒæœ‰åˆ©äº**æ¨ç†**ï¼Œä½†ä¸åˆ©äº**èŠå¤©**

QLoRAå¯ä»¥ç”¨åœ¨æ‰‹æœºä¸Šï¼Œè®ºæ–‡å…±åŒä¸€ä½œTim Dettmersä¼°è®¡ä»¥ iPhone 12 Plusçš„ç®—åŠ›, æ¯ä¸ªæ™šä¸Šèƒ½å¾®è°ƒ300ä¸‡ä¸ªå•è¯çš„æ•°æ®é‡ã€‚

ç‰¹ç‚¹
- ç”¨ QLoRA å¾®è°ƒæ¨¡å‹ï¼Œå¯ä»¥æ˜¾è‘—é™ä½å¯¹äºæ˜¾å­˜çš„è¦æ±‚ã€‚
- åŒæ—¶ï¼Œæ¨¡å‹è®­ç»ƒçš„é€Ÿåº¦ä¼š**æ…¢äº**LoRAã€‚

#### ã€2023-8-7ã€‘LoRA-FA

LoRA-FAï¼Œé¦™æ¸¯ç§‘æŠ€å¤§å­¦
- è®ºæ–‡: [LORA-FA: MEMORY-EFFICIENT LOW-RANK ADAPTATION FOR LARGE LANGUAGE MODELS FINE-TUNING](https://arxiv.org/pdf/2308.03303)

åšæ³•
1. éšæœºåˆå§‹åŒ– A ï¼Œ B åˆå§‹åŒ–ä¸º$$\textbf{0}$$çŸ©é˜µ
2. freeze  A ï¼Œåªæ›´æ–°$$\textbf{B}$$ï¼Œéœ€è¦æ›´æ–°çš„å‚æ•°é™ä¸ºä¸€åŠ

æ•ˆæœä¹Ÿä¸LoRAç›¸å½“
- å‡å°‘è®­ç»ƒå‚æ•°ï¼Œä¸å‡å°‘è®¡ç®—é‡


#### ã€2023-8-21ã€‘ReLoRA

ã€2023-8-21ã€‘[LoRAç»§ä»»è€…ReLoRAç™»åœºï¼Œé€šè¿‡å åŠ å¤šä¸ªä½ç§©æ›´æ–°çŸ©é˜µå®ç°æ›´é«˜æ•ˆå¤§æ¨¡å‹è®­ç»ƒæ•ˆæœ](https://www.toutiao.com/article/7269582259458572834)
- è®ºæ–‡é“¾æ¥ï¼š[paper](https://arxiv.org/abs/2307.05695)
- ä»£ç ä»“åº“ï¼š[peft_pretraining](https://github.com/guitaricet/peft_pretraining)
- è¿‡å»åå¹´ä¸­æ·±åº¦å­¦ä¹ å‘å±•é˜¶æ®µä¸­çš„ä¸€ä¸ªæ ¸å¿ƒåŸåˆ™å°±æ˜¯ä¸æ–­çš„â€œå †å æ›´å¤šå±‚ï¼ˆstack more layersï¼‰. é‚£ä¹ˆç»§ç»­ä»¥å †å æ–¹å¼æ¥æå‡ä½ç§©é€‚åº”çš„è®­ç»ƒæ•ˆç‡

é©¬è¨è¯¸å¡å¤§å­¦æ´›å„å°”åˆ†æ ¡å°†`ReLoRA`åº”ç”¨åœ¨å…·æœ‰é«˜è¾¾350Må‚æ•°çš„Transformerä¸Šæ—¶ï¼Œå±•ç°å‡ºäº†ä¸å¸¸è§„ç¥ç»ç½‘ç»œè®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚

æ­¤å¤–ï¼ŒReLoRAçš„å¾®è°ƒæ•ˆç‡ä¼šéšç€æ¨¡å‹å‚æ•°**è§„æ¨¡å¢åŠ è€Œä¸æ–­æé«˜**ï¼Œè¿™ä½¿å¾—å…¶æœªæ¥æœ‰å¯èƒ½æˆä¸ºè®­ç»ƒè¶…å¤§è§„æ¨¡ï¼ˆé€šå¸¸è¶…è¿‡1Bå‚æ•°ï¼‰LLMsçš„æ–°å‹æ‰‹æ®µã€‚

è®ºæ–‡æå‡ºä¸€ç§åŸºäº**ä½ç§©æ›´æ–°**çš„ReLoRAæ–¹æ³•è®­ç»ƒå’Œå¾®è°ƒé«˜ç§©ç½‘ç»œï¼Œå…¶æ€§èƒ½ä¼˜äºå…·æœ‰ç›¸åŒå¯è®­ç»ƒå‚æ•°æ•°é‡çš„ç½‘ç»œï¼Œç”šè‡³èƒ½å¤Ÿè¾¾åˆ°ä¸è®­ç»ƒ100M+è§„æ¨¡çš„å®Œæ•´ç½‘ç»œç±»ä¼¼çš„æ€§èƒ½ï¼Œå¯¹æ¯”æ•ˆæœå¦‚å›¾æ‰€ç¤ºã€‚

ä»ä¸¤ä¸ªçŸ©é˜µä¹‹å’Œçš„ç§©å…¥æ‰‹
- çŸ©é˜µç›¸åŠ çš„åç§©çš„ä¸Šç•Œä¼šæ¯”è¾ƒç´§å‡‘
- å¯¹äºçŸ©é˜µ A, Bï¼Œæ»¡è¶³ Rank(A) < dim(A)ï¼ŒBåŒç†, ä½¿å¾—çŸ©é˜µä¹‹å’Œçš„ç§©é«˜äº A æˆ– B 

åˆ©ç”¨è¿™ä¸€ç‰¹æ€§åˆ¶å®šçµæ´»çš„å‚æ•°é«˜æ•ˆè®­ç»ƒæ–¹æ³•ï¼Œç„¶åä»LoRAç®—æ³•å¼€å§‹å…¥æ‰‹ï¼ŒLoRAå¯ä»¥å°†æ¨¡å‹æƒé‡çš„æ›´æ–°é‡ delta(W) åˆ†è§£ä¸ºä¸€ç»„ä½ç§©çŸ©é˜µä¹˜ç§¯ 

ReLoRAæ–¹æ³•åŒ…å«
- ï¼ˆ1ï¼‰åˆå§‹åŒ–å…¨ç§©è®­ç»ƒ
- ï¼ˆ2ï¼‰LoRA è®­ç»ƒ
- ï¼ˆ3ï¼‰å‚æ•°é‡æ–°å¯åŠ¨
- ï¼ˆ4ï¼‰é”¯é½¿çŠ¶å­¦ä¹ ç‡è°ƒåº¦ï¼ˆjagged learning rate scheduleï¼‰
- ï¼ˆ5ï¼‰ä¼˜åŒ–å™¨å‚æ•°éƒ¨åˆ†é‡ç½®ã€‚

ä½œè€…é€‰æ‹©ç›®å‰éå¸¸ç«çƒ­çš„`è‡ªå›å½’è¯­è¨€æ¨¡å‹`è¿›è¡Œå®éªŒï¼Œå¹¶ä¸”ä¿è¯æ¯ä¸ªå®éªŒæ‰€ä½¿ç”¨çš„GPUè®¡ç®—æ—¶é—´ä¸è¶…è¿‡8å¤©ã€‚


#### ã€2023-10-1ã€‘LongLoRA

ã€2023-10-1ã€‘[è´¾ä½³äºšéŸ©æ¾å›¢é˜Ÿæ–°ä½œï¼šä¸¤è¡Œä»£ç è®©å¤§æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å€å¢](https://www.toutiao.com/article/7284843466167796239)

åªè¦ä¸¤è¡Œä»£ç +11ä¸ªå°æ—¶å¾®è°ƒï¼Œå°±èƒ½æŠŠå¤§æ¨¡å‹**4k**çš„çª—å£é•¿åº¦æé«˜åˆ°**32k**ã€‚

è§„æ¨¡ä¸Šï¼Œæœ€é•¿å¯ä»¥æ‰©å±•åˆ°10ä¸‡tokenï¼Œä¸€å£æ°”å°±èƒ½è¯»å®Œé•¿ç¯‡å°è¯´çš„å¤šä¸ªç« èŠ‚æˆ–ä¸­çŸ­ç¯‡å°è¯´ã€‚

è´¾ä½³äºšéŸ©æ¾è”åˆå›¢é˜Ÿæå‡ºçš„è¿™ä¸ªåŸºäºLoRAçš„å…¨æ–°å¤§æ¨¡å‹**å¾®è°ƒ**æ–¹æ³•ï¼Œç™»ä¸Šäº†GitHubçƒ­æ¦œï¼Œå¼€æºä¸€å‘¨æ—¶é—´æ”¶è·1k+ starsã€‚è¿™ç§æ–¹å¼å«åš `LongLoRA` ï¼Œç”±æ¥è‡ªé¦™æ¸¯ä¸­æ–‡å¤§å­¦å’ŒMITçš„å…¨åäººå›¢é˜Ÿè”åˆå‡ºå“ã€‚

åœ¨ä¸€å°8ä¸ªA100ç»„æˆçš„å•æœºä¸Šï¼Œå¢å¤§çª—å£é•¿åº¦çš„é€Ÿåº¦æ¯”å…¨é‡å¾®è°ƒå¿«æ•°å€ã€‚
- [è®ºæ–‡åœ°å€](https://arxiv.org/abs/2309.12307)
- [GitHubé¡¹ç›®é¡µ](https://github.com/dvlab-research/LongLoRA)


#### ã€2024-1-16ã€‘VERA

é˜¿å§†æ–¯ç‰¹ä¸¹å¤§å­¦
- [VERA: VECTOR-BASED RANDOM MATRIX ADAPTATION](https://arxiv.org/pdf/2310.11454)

1. ä¸è®­ç»ƒABï¼Œè®­ç»ƒé¢å¤–çš„ä¸¤ä¸ªå‘é‡
2. ABåœ¨æ‰€æœ‰å±‚å…±äº«ï¼Œä¸”éƒ½éšæœºåˆå§‹åŒ–ï¼Œæ˜¾è‘—å‡å°‘éœ€è¦è®­ç»ƒçš„å‚æ•°é‡

æ•ˆæœ
- å¯å­¦ä¹ å‚æ•°é‡æ˜¾è‘—å°‘äºLoRAçš„æƒ…å†µä¸‹ï¼Œæ•ˆæœå’ŒLoRAåŸºæœ¬æŒå¹³

#### ã€2024-2-19ã€‘LoRA+

ã€2024-2-19ã€‘LoRA+
- è®ºæ–‡: [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/pdf/2402.12354)
ï¼ˆUCBï¼‰

ç»“è®º
- å½“Bçš„å­¦ä¹ ç‡å¤§äºAçš„å­¦ä¹ ç‡æ—¶æ•ˆæœæ›´å¥½
- Bçš„å­¦ä¹ ç‡æ˜¯Açš„ $$\sqrt{m/r}$$ å€æ—¶, æ•ˆæœæœ€å¥½(ç†è®ºå€¼ï¼Œå®éªŒæ—¶å€æ•°å½“ä½œè¶…å‚)

#### ã€2024-3-1ã€‘DoRA

ã€2024-3-1ã€‘[DoRAï¼šLoRAå†å‡çº§-å‚æ•°é«˜æ•ˆå¾®è°ƒ](https://zhuanlan.zhihu.com/p/684833295)
- [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/pdf/2402.09353)
- ä»£ç  [DoRA](https://github.com/NVlabs/DoRA)

Lora æœ¬è´¨ä¸ŠæŠŠå¤§çŸ©é˜µæ‹†æˆä¸¤ä¸ªå°çŸ©é˜µçš„ä¹˜æ³•
- ![](https://pic1.zhimg.com/80/v2-5a6c032445742aa6df093e6fec4dfd98_1440w.webp)

```py
class LoRALayer(nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha):
        super().__init__()
        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())
        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)
        self.B = nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha

    def forward(self, x):
        x = self.alpha * (x @ self.A @ self.B)
        return x

class LinearWithLoRAMerged(nn.Module):
    def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(
            linear.in_features, linear.out_features, rank, alpha
        )

    def forward(self, x):
        lora = self.lora.A @ self.lora.B # Combine LoRA matrices
        # Then combine LoRA with orig. weights
        combined_weight = self.linear.weight + self.lora.alpha*lora.T 
        return F.linear(x, combined_weight, self.linear.bias)
```

DoRAï¼ˆWeight-Decomposed Low-Rank Adaptationï¼‰ä¸»è¦æ€æƒ³
- å°†é¢„è®­ç»ƒæƒé‡åˆ†è§£ä¸º**å¹…åº¦**ï¼ˆmagnitudeï¼‰å’Œ**æ–¹å‘**ï¼ˆdirectionï¼‰ï¼Œå¹¶åˆ©ç”¨LoRAæ¥å¾®è°ƒæ–¹å‘çŸ©é˜µ
- ![](https://pic1.zhimg.com/80/v2-34cab3f7896975b3ea1032113441aff8_1440w.webp)
- å…¬å¼è§åŸæ–‡

```py
class LinearWithDoRAMerged(nn.Module):

    def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(
            linear.in_features, linear.out_features, rank, alpha
        )
        self.m = nn.Parameter(
            self.linear.weight.norm(p=2, dim=0, keepdim=True))

  # Code loosely inspired by    
  # https://github.com/catid/dora/blob/main/dora.py

    def forward(self, x):
        lora = self.lora.A @ self.lora.B
        numerator = self.linear.weight + self.lora.alpha*lora.T
        denominator = numerator.norm(p=2, dim=0, keepdim=True)
        directional_component = numerator / denominator
        new_weight = self.m * directional_component
        return F.linear(x, new_weight, self.linear.bias)
```

**LoRAé€šå¸¸ä¼šç­‰æ¯”ä¾‹å¢å‡å¹…åº¦å’Œæ–¹å‘ï¼ŒDoRAé€šè¿‡å°†é¢„è®­ç»ƒæƒé‡çŸ©é˜µåˆ†è§£ä¸ºå¹…åº¦å’Œæ–¹å‘ï¼Œèƒ½å¤Ÿæ›´æ¥è¿‘å…¨é‡å¾®è°ƒçš„æ•ˆæœ**ã€‚

- ä½¿ç”¨æ¯”LoRAæ›´å°‘çš„å‚æ•°ï¼Œæ•ˆæœè¿˜æ›´å¥½
- ![](https://pic3.zhimg.com/v2-2681727bb6226a5b1d3651b80ccbc52e_b.jpg)
- ä½¿ç”¨è¾ƒå°çš„rankï¼Œæ•ˆæœä¹Ÿå¾ˆå¥½
- ![](https://pic2.zhimg.com/80/v2-8e92d2e2f256e7d69226772e0083e9c5_1440w.webp)

1. DoRAåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæ•ˆæœå¥½äºLoRA
2. DoRAå¯¹ä¸åŒrank rå–å€¼æ›´é²æ£’

ç›¸ä¿¡DoRAåº”è¯¥å¾ˆå¿«ä¼šæˆä¸ºä¸€ç§æ™®éçš„å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ã€‚

èµ„æ–™ï¼š
- DoRA: Weight-Decomposed Low-Rank Adaptation
- Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch

#### ã€2024-3-28ã€‘LISA

LoRAï¼ˆLow-Rank Adaptationï¼‰

LoRAæ˜¯ä¸€ç§å¤§æ¨¡å‹å¾®è°ƒæŠ€æœ¯ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯åœ¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰åŸºç¡€ä¸Šï¼Œé€šè¿‡å¢åŠ ä¸€ä¸ªä½ç§©é€‚é…å™¨æ¥å®ç°å¾®è°ƒã€‚è¿™ç§æ–¹æ³•é€šè¿‡åœ¨åŸå§‹æ¨¡å‹çš„æ¯ä¸€å±‚æ³¨å…¥å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µï¼Œä»è€Œä»¥è¾ƒå°çš„å‚æ•°é‡å®ç°æ¨¡å‹çš„å¾®è°ƒã€‚

LoRAä¸»è¦ä¼˜åŠ¿åœ¨äºå‡å°‘äº†å¾®è°ƒè¿‡ç¨‹ä¸­éœ€è¦æ›´æ–°çš„å‚æ•°æ•°é‡ï¼Œä»è€Œé™ä½äº†å­˜å‚¨å’Œè®¡ç®—èµ„æºçš„éœ€æ±‚ã€‚ç„¶è€Œï¼ŒLoRAåœ¨æŸäº›ä»»åŠ¡ä¸Šå¯èƒ½æ— æ³•è¶…è¶Šå…¨å‚æ•°å¾®è°ƒçš„æ•ˆæœï¼Œä¸”å…¶ç†è®ºæ€§è´¨åˆ†æè¾ƒä¸ºå›°éš¾ã€‚

ã€2024-3-28ã€‘é¦™æ¸¯ç†å·¥ LISAï¼ˆLayerwise Importance Sampled AdamWï¼‰
- è®ºæ–‡: [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/pdf/2403.17919.pdf)

LISAæ˜¯ç”±UIUCè”åˆLMFlowå›¢é˜Ÿæå‡ºçš„å¦ä¸€ç§å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ã€‚
- ä¸LoRAä¸åŒï¼ŒLISAç®—æ³•æ ¸å¿ƒåœ¨äº**å§‹ç»ˆæ›´æ–°åº•å±‚embeddingå’Œé¡¶å±‚linearhead**ï¼ŒåŒæ—¶**éšæœºæ›´æ–°**å°‘æ•°ä¸­é—´çš„self-attentionå±‚ã€‚
- è¿™ç§æ–¹æ³•åœ¨å®éªŒä¸­æ˜¾ç¤ºå‡ºåœ¨æŒ‡ä»¤å¾®è°ƒä»»åŠ¡ä¸Šè¶…è¿‡LoRAç”šè‡³å…¨å‚æ•°å¾®è°ƒçš„æ•ˆæœã€‚
- LISAçš„ç©ºé—´æ¶ˆè€—ä¸LoRAç›¸å½“ç”šè‡³æ›´ä½ï¼Œä¸”ç”±äºå…¶æ¯æ¬¡ä¸­é—´åªä¼šæ¿€æ´»ä¸€å°éƒ¨åˆ†å‚æ•°ï¼Œå¯¹æ›´æ·±çš„ç½‘ç»œå’Œæ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯ï¼ˆGradientCheckpointingï¼‰ä¹Ÿå¾ˆå‹å¥½ï¼Œèƒ½å¤Ÿå¸¦æ¥æ›´å¤§çš„ç©ºé—´èŠ‚çœã€‚

æ­¤å¤–ï¼ŒLISAçš„æ”¶æ•›æ€§è´¨æ¯”LoRAæœ‰å¾ˆå¤§æå‡ï¼ˆ17~30%ï¼‰ï¼Œä¸”è®¡ç®—é€Ÿåº¦æ¯”LoRAå¿«å°†è¿‘50%ã€‚

å¯¹æ¯”æ€»ç»“
- **å¾®è°ƒæ•ˆæœ**ï¼šLISAåœ¨æŸäº›ä»»åŠ¡ä¸Šæ˜¾ç¤ºå‡ºæ¯”LoRAæ›´å¥½çš„å¾®è°ƒæ•ˆæœï¼Œç”šè‡³èƒ½è¶…è¶Šå…¨å‚æ•°å¾®è°ƒã€‚
- **èµ„æºæ¶ˆè€—**ï¼šLISAåœ¨ç©ºé—´æ¶ˆè€—ä¸Šä¸LoRAç›¸å½“ï¼Œä½†ç”±äºå…¶æ›´æ–°ç­–ç•¥ï¼Œå¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹æ›´åŠ èŠ‚çœèµ„æºã€‚
- **è®¡ç®—é€Ÿåº¦**ï¼šLISAçš„è®¡ç®—é€Ÿåº¦æ¯”LoRAå¿«ï¼Œå› ä¸ºå®ƒå‡å°‘äº†éœ€è¦æ›´æ–°çš„å‚æ•°æ•°é‡ã€‚
- **ç†è®ºåˆ†æ**ï¼šLISAçš„ç†è®ºæ€§è´¨ç›¸å¯¹å®¹æ˜“åˆ†æï¼Œå¯ä»¥ä½¿ç”¨ç°æœ‰çš„ä¼˜åŒ–é¢†åŸŸçš„æ•°å­¦å·¥å…·è¿›è¡Œåˆ†æã€‚
- **åº”ç”¨å‹å¥½æ€§**ï¼šLISAå¯¹æ›´æ·±çš„ç½‘ç»œå’Œæ¢¯åº¦æ£€æŸ¥ç‚¹æŠ€æœ¯æ›´åŠ å‹å¥½ï¼Œæœ‰åŠ©äºåœ¨èµ„æºå—é™çš„æƒ…å†µä¸‹è¿›è¡Œå¾®è°ƒã€‚

ç»¼ä¸Šæ‰€è¿°ï¼ŒLISAåœ¨ä¿æŒä¸LoRAç›¸å½“çš„èµ„æºæ¶ˆè€—çš„åŒæ—¶ï¼Œæä¾›äº†æ›´å¿«çš„è®¡ç®—é€Ÿåº¦å’Œæ›´å¥½çš„å¾®è°ƒæ•ˆæœï¼Œæ˜¯ä¸€ç§å…·æœ‰æ½œåŠ›çš„å¤§æ¨¡å‹å¾®è°ƒæŠ€æœ¯ã€‚



#### ã€2024-4-12ã€‘PiSSA

ã€2024-4-12ã€‘[æ”¹å˜LoRAçš„åˆå§‹åŒ–æ–¹å¼ï¼ŒåŒ—å¤§æ–°æ–¹æ³•PiSSAæ˜¾è‘—æå‡å¾®è°ƒæ•ˆæœ](https://www.jiqizhixin.com/articles/2024-04-12-7)

åŒ—äº¬å¤§å­¦çš„ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åä¸º PiSSA çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œä¸»æµæ•°æ®é›†ä¸Šéƒ½è¶…è¿‡äº†ç›®å‰å¹¿æ³›ä½¿ç”¨çš„ LoRA çš„å¾®è°ƒæ•ˆæœã€‚
- è®ºæ–‡: [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](https://arxiv.org/pdf/2404.02948.pdf)
- ä»£ç é“¾æ¥: [PiSSA](https://github.com/GraphPKU/PiSSA)

PiSSA åœ¨æ¨¡å‹æ¶æ„ä¸Šå’Œ LoRA å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯**Adapteråˆå§‹åŒ–æ–¹å¼**ä¸åŒã€‚[img](https://image.jiqizhixin.com/uploads/editor/cf318bea-5793-4f34-83a3-ecc1bc7d7773/640.png)
- `LoRA` ä½¿ç”¨é«˜æ–¯å™ªå£°åˆå§‹åŒ– Aï¼Œä½¿ç”¨ 0 åˆå§‹åŒ– Bã€‚
- `PiSSA` ç”¨ä¸»å¥‡å¼‚å€¼å’Œå¥‡å¼‚å‘é‡ (Principal Singular values and Singular vectors) æ¥åˆå§‹åŒ– Adapter æ¥åˆå§‹åŒ– A å’Œ Bã€‚
- ![](https://image.jiqizhixin.com/uploads/editor/cf318bea-5793-4f34-83a3-ecc1bc7d7773/640.png)

æ•ˆæœ
- PiSSA å¾®è°ƒæ•ˆæœæ˜¾è‘—è¶…è¶Šäº† LoRAï¼Œç”šè‡³è¶…è¶Šäº†å…¨å‚æ•°å¾®è°ƒ
- PiSSA æ¯” LoRA æ”¶æ•›æ›´å¿«ï¼Œæœ€ç»ˆæ•ˆæœæ›´å¥½ï¼Œå”¯ä¸€çš„ä»£ä»·ä»…æ˜¯éœ€è¦å‡ ç§’çš„ SVD åˆå§‹åŒ–è¿‡ç¨‹ã€‚


#### ã€2024-4-15ã€‘LoRA Dropout

LoRA Dropout, PKU
- è®ºæ–‡: [LoRA Dropout as a Sparsity Regularizer for Overfitting Control]()

1. è®­ç»ƒé˜¶æ®µå¯¹Aå’ŒBçŸ©é˜µè¿›è¡Œæ•´è¡Œæˆ–æ•´åˆ—çš„dropoutï¼Œè€Œä¸æ˜¯element-wiseçš„dropoutï¼Œè¿™æ ·å¯ä»¥ä½¿å¾—BAä¸­æœ‰å¤§é‡ä¸º0å€¼ï¼Œè¾¾åˆ°mask Wçš„æ•ˆæœ
2. inferenceæ—¶ï¼Œæ— æ³•åƒåŸå§‹dropouté‚£æ ·ä¹˜ä»¥(1-p)æ¥rescaleã€‚å› æ­¤éœ€å¤šæ¬¡ä½¿ç”¨dropoutï¼Œå–ensembleçš„ç»“æœï¼Œæ•ˆæœæœ‰æå‡(æ•ˆç‡é™ä½)

æ•ˆæœ
1. +dropoutåœ¨loraå’Œadaloraä¸Šæ•ˆæœå‡æ›´å¥½
2. Dropout rateå–0.6æ—¶æ•ˆæœæœ€å¥½


#### ã€2024-6-18ã€‘LoRA-drop

å“ˆå·¥å¤§
- è®ºæ–‡: [LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation](https://arxiv.org/pdf/2402.07721)

1. LoRAåœ¨ä¸åŒä½ç½®(é’ˆå¯¹ä¸åŒW)çš„ $$||\Delta Wx||$$ ä¸åŒï¼Œè®¤ä¸ºè¶Šå¤§çš„è¶Šé‡è¦
2. æŒ‰ä»¥ä¸‹æ­¥éª¤å°†ä¸é‡è¦çš„LoRAè¿›è¡Œæƒé‡å…±äº«ï¼Œç„¶åé‡æ–°è®­ç»ƒï¼Œå‡å°‘è®­ç»ƒå‚æ•°ï¼Œä¸å‡å°‘è®¡ç®—é‡


æ•ˆæœä¸loraå·®ä¸å¤šï¼Œå¯å‡å°‘loraçº¦ä¸€åŠçš„å†…å­˜å ç”¨



### å·¥ç¨‹ä¼˜åŒ–


#### ã€2023-11-15ã€‘S-LoRA å¤šæœåŠ¡éƒ¨ç½²

ã€2023-11-15ã€‘S-LoRAï¼šä¸€ä¸ªGPUè¿è¡Œæ•°åƒå¤§æ¨¡å‹æˆä¸ºå¯èƒ½

å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²éƒ½ä¼šé‡‡ç”¨ã€Œé¢„è®­ç»ƒ â€” å¾®è°ƒã€æ¨¡å¼ã€‚ä½†æ˜¯ï¼Œé’ˆå¯¹ä¼—å¤šä»»åŠ¡ï¼ˆå¦‚ä¸ªæ€§åŒ–åŠ©æ‰‹ï¼‰å¯¹ base æ¨¡å‹è¿›è¡Œå¾®è°ƒæ—¶ï¼Œè®­ç»ƒå’ŒæœåŠ¡æˆæœ¬ä¼šå˜å¾—éå¸¸é«˜æ˜‚ã€‚

ä½ç§©é€‚é…ï¼ˆLowRank Adaptationï¼ŒLoRAï¼‰æ˜¯ä¸€ç§å‚æ•°æ•ˆç‡é«˜çš„å¾®è°ƒæ–¹æ³•ï¼Œé€šå¸¸ç”¨äºå°† base æ¨¡å‹é€‚é…åˆ°å¤šç§ä»»åŠ¡ä¸­ï¼Œä»è€Œäº§ç”Ÿäº†å¤§é‡ä»ä¸€ä¸ª base æ¨¡å‹è¡ç”Ÿå‡ºæ¥çš„ LoRA é€‚é…ç¨‹åºã€‚

åªå¯¹é€‚é…å™¨æƒé‡è¿›è¡Œå¾®è°ƒï¼Œå°±èƒ½è·å¾—ä¸å…¨æƒé‡å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚è™½ç„¶è¿™ç§æ–¹æ³•å¯ä»¥å®ç°å•ä¸ªé€‚é…å™¨çš„ä½å»¶è¿Ÿæ¨ç†å’Œè·¨é€‚é…å™¨çš„ä¸²è¡Œæ‰§è¡Œï¼Œä½†åœ¨åŒæ—¶ä¸ºå¤šä¸ªé€‚é…å™¨æä¾›æœåŠ¡æ—¶ï¼Œä¼šæ˜¾è‘—é™ä½æ•´ä½“æœåŠ¡ååé‡å¹¶å¢åŠ æ€»å»¶è¿Ÿã€‚æ€»ä¹‹ï¼Œå¦‚ä½•å¤§è§„æ¨¡æœåŠ¡äºè¿™äº›å¾®è°ƒå˜ä½“çš„é—®é¢˜ä»æœªå¾—åˆ°è§£å†³ã€‚

UC ä¼¯å…‹åˆ©ã€æ–¯å¦ç¦ç­‰é«˜æ ¡çš„ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åä¸º S-LoRA çš„æ–°å¾®è°ƒæ–¹å¼ã€‚
- è®ºæ–‡åœ°å€ï¼š[https://arxiv.org/pdf/2311.03285.pdf](https://arxiv.org/pdf/2311.03285.pdf)
- é¡¹ç›®åœ°å€ï¼š[https://github.com/S-LoRA/S-LoRA](https://github.com/S-LoRA/S-LoRA)

S-LoRA ä¸“ä¸ºä¼—å¤š LoRA é€‚é…ç¨‹åºçš„å¯æ‰©å±•æœåŠ¡è€Œè®¾è®¡ï¼Œå®ƒå°†æ‰€æœ‰é€‚é…ç¨‹åºå­˜å‚¨åœ¨ä¸»å†…å­˜ä¸­ï¼Œå¹¶å°†å½“å‰è¿è¡ŒæŸ¥è¯¢æ‰€ä½¿ç”¨çš„é€‚é…ç¨‹åºå–åˆ° GPU å†…å­˜ä¸­ã€‚
- S-LoRA æå‡ºã€Œ**ç»Ÿä¸€åˆ†é¡µ**ã€ï¼ˆUnified Pagingï¼‰æŠ€æœ¯ï¼Œå³ä½¿ç”¨ç»Ÿä¸€çš„å†…å­˜æ± æ¥ç®¡ç†ä¸åŒç­‰çº§çš„åŠ¨æ€é€‚é…å™¨æƒé‡å’Œä¸åŒåºåˆ—é•¿åº¦çš„ KV ç¼“å­˜å¼ é‡ã€‚
  - PagedAttention æ‰©å±•ä¸º**ç»Ÿä¸€åˆ†é¡µ**ï¼ˆUnified Pagingï¼‰ï¼Œåè€…é™¤äº†ç®¡ç† KV ç¼“å­˜å¤–ï¼Œè¿˜ç®¡ç†é€‚é…å™¨æƒé‡ã€‚
- æ­¤å¤–ï¼ŒS-LoRA è¿˜é‡‡ç”¨äº†æ–°çš„**å¼ é‡å¹¶è¡Œ**ç­–ç•¥å’Œé«˜åº¦ä¼˜åŒ–çš„å®šåˆ¶ CUDA å†…æ ¸ï¼Œä»¥å®ç° LoRA è®¡ç®—çš„å¼‚æ„æ‰¹å¤„ç†ã€‚

S-LoRA åŒ…å«ä¸‰ä¸ªä¸»è¦åˆ›æ–°éƒ¨åˆ†ã€‚è®ºæ–‡ç¬¬ 4 èŠ‚ä»‹ç»äº†æ‰¹å¤„ç†ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åˆ†è§£äº† base æ¨¡å‹å’Œ LoRA é€‚é…å™¨ä¹‹é—´çš„è®¡ç®—ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜è§£å†³äº†éœ€æ±‚è°ƒåº¦çš„éš¾é¢˜ï¼ŒåŒ…æ‹¬é€‚é…å™¨é›†ç¾¤å’Œå‡†å…¥æ§åˆ¶ç­‰æ–¹é¢ã€‚è·¨å¹¶å‘é€‚é…å™¨çš„æ‰¹å¤„ç†èƒ½åŠ›ç»™å†…å­˜ç®¡ç†å¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ã€‚ç¬¬ 5 èŠ‚ï¼Œç ”ç©¶è€…å°† PagedAttention æ¨å¹¿åˆ° Unfied Pagingï¼Œæ”¯æŒåŠ¨æ€åŠ è½½ LoRA é€‚é…å™¨ã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨ç»Ÿä¸€çš„å†…å­˜æ± ä»¥åˆ†é¡µæ–¹å¼å­˜å‚¨ KV ç¼“å­˜å’Œé€‚é…å™¨æƒé‡ï¼Œå¯ä»¥å‡å°‘ç¢ç‰‡å¹¶å¹³è¡¡ KV ç¼“å­˜å’Œé€‚é…å™¨æƒé‡çš„åŠ¨æ€å˜åŒ–å¤§å°ã€‚æœ€åï¼Œç¬¬ 6 èŠ‚ä»‹ç»äº†æ–°çš„å¼ é‡å¹¶è¡Œç­–ç•¥ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è§£è€¦ base æ¨¡å‹å’Œ LoRA é€‚é…å™¨ã€‚

å¦‚æœå°† LoRA é€‚é…å™¨å­˜å‚¨åœ¨ä¸»å†…å­˜ä¸­ï¼Œæ•°é‡å¯èƒ½ä¼šå¾ˆå¤§ï¼Œä½†å½“å‰è¿è¡Œæ‰¹æ‰€éœ€çš„ LoRA é€‚é…å™¨æ•°é‡æ˜¯å¯æ§çš„ï¼Œå› ä¸ºæ‰¹å¤§å°å— GPU å†…å­˜çš„é™åˆ¶ã€‚ä¸ºäº†åˆ©ç”¨è¿™ä¸€ä¼˜åŠ¿ï¼Œç ”ç©¶è€…å°†æ‰€æœ‰çš„ LoRA é€‚é…å¡éƒ½å­˜å‚¨åœ¨ä¸»å†…å­˜ä¸­ï¼Œå¹¶åœ¨ä¸ºå½“å‰æ­£åœ¨è¿è¡Œçš„æ‰¹è¿›è¡Œæ¨ç†æ—¶ï¼Œä»…å°†è¯¥æ‰¹æ‰€éœ€çš„ LoRA é€‚é…å¡å–åˆ° GPU RAM ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯æœåŠ¡çš„é€‚é…å™¨æœ€å¤§æ•°é‡å—é™äºä¸»å†…å­˜å¤§å°ã€‚
- ![](https://pic1.zhimg.com/80/v2-02ad431025559544cb5e501d7d44ffc4_1440w.webp)

è¿™äº›åŠŸèƒ½ä½¿ S-LoRA èƒ½å¤Ÿä»¥è¾ƒå°å¼€é”€åœ¨å•ä¸ª GPU æˆ–å¤šä¸ª GPU ä¸Šä¸ºæ•°åƒä¸ª LoRA é€‚é…å™¨æä¾›æœåŠ¡ï¼ˆåŒæ—¶ä¸º 2000 ä¸ªé€‚é…å™¨æä¾›æœåŠ¡ï¼‰ï¼Œå¹¶å°†å¢åŠ çš„ LoRA è®¡ç®—å¼€é”€é™è‡³æœ€ä½ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒvLLM-packed éœ€è¦ç»´æŠ¤å¤šä¸ªæƒé‡å‰¯æœ¬ï¼Œå¹¶ä¸”ç”±äº GPU å†…å­˜é™åˆ¶ï¼Œåªèƒ½ä¸ºå°‘äº 5 ä¸ªé€‚é…å™¨æä¾›æœåŠ¡ã€‚

ä¸ HuggingFace `PEFT` å’Œ `vLLM`ï¼ˆä»…æ”¯æŒ LoRA æœåŠ¡ï¼‰ç­‰æœ€å…ˆè¿›çš„åº“ç›¸æ¯”ï¼Œ`S-LoRA` ååé‡æœ€å¤šå¯æé«˜ 4 å€ï¼ŒæœåŠ¡é€‚é…å™¨æ•°é‡å¯å¢åŠ å‡ ä¸ªæ•°é‡çº§ã€‚å› æ­¤ï¼ŒS-LoRA èƒ½å¤Ÿä¸ºè®¸å¤šç‰¹å®šä»»åŠ¡çš„å¾®è°ƒæ¨¡å‹æä¾›å¯æ‰©å±•çš„æœåŠ¡ï¼Œå¹¶ä¸ºå¤§è§„æ¨¡å®šåˆ¶å¾®è°ƒæœåŠ¡æä¾›äº†æ½œåŠ›ã€‚


### MoE+LoRA


#### LoRA + MoE

ã€2024-3-5ã€‘[å¤§æ¨¡å‹å¾®è°ƒæ–°èŒƒå¼ï¼šå½“LoRAé‡è§MoE](https://mp.weixin.qq.com/s/t_X8AHFgi-RHuviTuCYv0Q)

å¯¹æ¯”
- åŸå§‹ç‰ˆæœ¬çš„ LoRAï¼Œæƒé‡ç¨ å¯†ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½ä¼šæ¿€æ´»**æ‰€æœ‰å‚æ•°**ï¼›
- ä¸æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¡†æ¶ç»“åˆçš„ LoRAï¼Œæ¯ä¸€å±‚æ’å…¥å¤šä¸ªå¹¶è¡Œçš„ LoRA æƒé‡ï¼ˆå³ MoE ä¸­çš„å¤šä¸ªä¸“å®¶æ¨¡å‹ï¼‰ï¼Œè·¯ç”±æ¨¡å—ï¼ˆRouterï¼‰è¾“å‡ºæ¯ä¸ªä¸“å®¶çš„æ¿€æ´»æ¦‚ç‡ï¼Œä»¥å†³å®šæ¿€æ´»å“ªäº› LoRA æ¨¡å—ã€‚

ä¸ºäº†å…‹æœ**ç¨ å¯†æ¨¡å‹**çš„å‚æ•°æ•ˆç‡ç“¶é¢ˆï¼Œä»¥ Mistralã€DeepSeek MoE ä¸ºä»£è¡¨çš„æ··åˆä¸“å®¶ï¼ˆMixure of Expertsï¼Œç®€ç§° MoEï¼‰æ¨¡å‹æ¡†æ¶ã€‚

æ¨¡å‹æŸä¸ªæ¨¡å—ï¼ˆå¦‚ Transformer çš„æŸä¸ª FFN å±‚ï¼‰å­˜åœ¨å¤šç»„å½¢çŠ¶ç›¸åŒçš„æƒé‡ï¼ˆç§°ä¸ºä¸“å®¶ï¼‰ï¼Œå¦å¤–æœ‰ä¸€ä¸ª**è·¯ç”±æ¨¡å—**ï¼ˆRouterï¼‰æ¥å—åŸå§‹è¾“å…¥ã€è¾“å‡ºå„ä¸“å®¶çš„æ¿€æ´»æƒé‡ï¼Œæœ€ç»ˆçš„è¾“å‡ºä¸ºï¼š
- å¦‚æœæ˜¯**è½¯è·¯ç”±**ï¼ˆsoft routingï¼‰ï¼Œè¾“å‡ºå„ä¸“å®¶è¾“å‡ºçš„**åŠ æƒæ±‚å’Œ**ï¼›
- å¦‚æœæ˜¯**ç¦»æ•£è·¯ç”±**ï¼ˆdiscrete routingï¼‰ï¼Œå³ Mistralã€DeepDeek MoE é‡‡ç”¨çš„**ç¨€ç–æ··åˆä¸“å®¶**ï¼ˆSparse MoEï¼‰æ¶æ„,åˆ™å°† Top-Kï¼ˆK ä¸ºå›ºå®šçš„ è¶…å‚æ•°ï¼Œå³æ¯æ¬¡æ¿€æ´»çš„ä¸“å®¶ä¸ªæ•°ï¼Œå¦‚ 1 æˆ– 2ï¼‰ä¹‹å¤–çš„æƒé‡ç½®é›¶ï¼Œå†åŠ æƒæ±‚å’Œã€‚

MoE æ¶æ„ä¸­æ¯ä¸ªä¸“å®¶å‚æ•°çš„æ¿€æ´»ç¨‹åº¦å–å†³äºæ•°æ®å†³å®šçš„**è·¯ç”±æƒé‡**ï¼Œä½¿å¾—å„ä¸“å®¶çš„å‚æ•°èƒ½å„è‡ªå…³æ³¨å…¶æ‰€æ“…é•¿çš„æ•°æ®ç±»å‹ã€‚åœ¨ç¦»æ•£è·¯ç”±çš„æƒ…å†µä¸‹ï¼Œè·¯ç”±æƒé‡åœ¨ TopK ä¹‹å¤–çš„ä¸“å®¶ç”šè‡³ä¸ç”¨è®¡ç®—ï¼Œåœ¨ä¿è¯æ€»å‚æ•°å®¹é‡çš„å‰æä¸‹æå¤§é™ä½äº†æ¨ç†çš„è®¡ç®—ä»£ä»·ã€‚

æ¡ˆä¾‹
- MoVã€MoLORAã€LoRAMOE å’Œ MOLA ç­‰æ–°çš„ PEFT æ–¹æ³•ï¼Œç›¸æ¯”åŸå§‹ç‰ˆæœ¬çš„ LORA è¿›ä¸€æ­¥æå‡äº†å¤§æ¨¡å‹å¾®è°ƒçš„æ•ˆç‡ã€‚

è¯¦æƒ…
- `MoV` å’Œ `MoLORA`ï¼š
  - 2023 å¹´ 9 æœˆï¼Œé¦–ä¸ªç»“åˆ PEFT å’Œ MoE çš„å·¥ä½œï¼ŒMoV å’Œ MoLORA åˆ†åˆ«æ˜¯ `IA` å’Œ `LORA` çš„ MOE ç‰ˆæœ¬ï¼Œé‡‡ç”¨ token çº§åˆ«çš„**è½¯è·¯ç”±**ï¼ˆåŠ æƒåˆå¹¶æ‰€æœ‰ä¸“å®¶çš„è¾“å‡ºï¼‰ã€‚
  - å¯¹ 3B å’Œ 11B çš„ T5 å¤§æ¨¡å‹çš„ SFTï¼ŒMoV ä»…ä½¿ç”¨ä¸åˆ° 1% å¯è®­ç»ƒå‚æ•°é‡å°±å¯ä»¥è¾¾åˆ°å’Œå…¨é‡å¾®è°ƒç›¸å½“çš„æ•ˆæœï¼Œæ˜¾è‘—ä¼˜äºåŒç­‰å¯è®­ç»ƒå‚æ•°é‡è®¾å®šä¸‹çš„ LoRAã€‚
  - `MoLORA` è®ºæ–‡ï¼š [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://arxiv.org/abs/2309.05444)
  - ç»“è®º
    1. 15ä¸ªexpertæ•ˆæœèƒ½è¶…è¿‡full finetune
    2. expertä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼Œåé¢ä¼šæ”¶æ•›
    3. Soft routerå¥½äºtop-1/2ï¼Œå¯èƒ½æ˜¯ç”±äºtop-1/2è¿™ç§æœ¬èº«éš¾äºè®­ç»ƒï¼Œæ ·æœ¬è¿˜å°‘
    4. å¯¹äºmulti-taskæ•°æ®é›†ï¼Œä¸åŒexpertèƒ½focusåœ¨ä¸åŒtaskä¸Š
- `LoRAMOE`ï¼šLoRAä¸“å®¶åˆ†ç»„ï¼Œé¢„è®­ç»ƒçŸ¥è¯†è®°å¾—æ›´ç‰¢
  - é—®é¢˜ï¼šéšç€æ‰€ç”¨æ•°æ®é‡çš„å¢é•¿ï¼ŒSFT è®­ç»ƒä¼šå¯¼è‡´æ¨¡å‹å‚æ•°å¤§å¹…åº¦åç¦»é¢„è®­ç»ƒå‚æ•°ï¼Œé¢„è®­ç»ƒé˜¶æ®µå­¦ä¹ åˆ°çš„ä¸–ç•ŒçŸ¥è¯†ï¼ˆworld knowledgeï¼‰é€æ¸è¢«é—å¿˜ï¼Œè™½ç„¶æ¨¡å‹çš„æŒ‡ä»¤è·Ÿéšèƒ½åŠ›å¢å¼ºã€åœ¨å¸¸è§çš„æµ‹è¯•é›†ä¸Šæ€§èƒ½å¢é•¿ï¼Œä½†éœ€è¦è¿™äº›ä¸–ç•ŒçŸ¥è¯†çš„ QA ä»»åŠ¡æ€§èƒ½å¤§å¹…åº¦ä¸‹é™
  - 2023 å¹´ 12 æœˆï¼Œåœ¨ `MoLORA` åŸºç¡€ä¸Šï¼Œä¸ºè§£å†³å¾®è°ƒå¤§æ¨¡å‹æ—¶çš„ç¾éš¾é—å¿˜é—®é¢˜ï¼Œå°†åŒä¸€ä½ç½®çš„ LoRA ä¸“å®¶åˆ†ä¸ºä¸¤ç»„ï¼Œåˆ†åˆ«è´Ÿè´£ä¿å­˜é¢„è®­ç»ƒæƒé‡ä¸­çš„ä¸–ç•ŒçŸ¥è¯†å’Œå¾®è°ƒæ—¶å­¦ä¹ çš„æ–°ä»»åŠ¡ï¼Œå¹¶ä¸ºæ­¤ç›®æ ‡è®¾è®¡äº†æ–°çš„è´Ÿè½½å‡è¡¡ lossã€‚
  - [LoRAMoE: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment](https://arxiv.org/abs/2312.09979)
- `MOLA`ï¼šç»Ÿç­¹å¢æ•ˆï¼Œæ›´æ¥è¿‘è¾“å‡ºç«¯çš„é«˜å±‚éœ€è¦æ›´å¤šä¸“å®¶
  - é—®é¢˜: ä¸“å®¶ä¸ªæ•°è¿‡å¤šå®¹æ˜“å¯¼è‡´æ€§èƒ½ä¸‹é™
  - 2024 å¹´ 2 æœˆï¼Œä½¿ç”¨ç¦»æ•£è·¯ç”±ï¼ˆæ¯æ¬¡åªæ¿€æ´»è·¯ç”±æƒé‡ top-2 çš„ä¸“å®¶ï¼‰ï¼Œå¹¶å‘ç°åœ¨æ¯ä¸€å±‚è®¾ç½®åŒæ ·çš„ä¸“å®¶ä¸ªæ•°ä¸æ˜¯æœ€ä¼˜çš„ï¼Œå¢åŠ é«˜å±‚ä¸“å®¶æ•°ç›®ã€é™ä½åº•å±‚ä¸“å®¶æ•°ç›®ï¼Œèƒ½åœ¨å¯è®­ç»ƒå‚æ•°é‡ä¸å˜çš„å‰æä¸‹ï¼Œæ˜æ˜¾æå‡ LLaMa-2 å¾®è°ƒçš„æ•ˆæœã€‚
  - æ¨¡å‹çš„ä¸åŒå±‚æ·»åŠ ä¸åŒæ•°é‡çš„LoRA expertsï¼Œè¶Šé«˜å±‚ä½¿ç”¨æ›´å¤šexpertæ•ˆæœæ›´å¥½
  - è®ºæ–‡ [Higher Layers Need More LoRA Experts](https://arxiv.org/abs/2402.08562)
  - ç›¸åŒæ€»expertsæ•°çš„æƒ…å†µä¸‹ï¼Œå€’ä¸‰è§’ç»“æ„æ•ˆæœæœ€å¥½ã€‚è¶Šé«˜å±‚ç¦»outputè¶Šè¿‘ï¼Œå‚æ•°å°±è¶Šé‡è¦ï¼ˆéMOEæƒ…å†µä¸‹ï¼Œä¹Ÿæ˜¯æ›´é«˜å±‚çš„LoRAæ›´é‡è¦ï¼‰


### åºåˆ—å¼ LoRA

ç›¸å…³å·¥ä½œ
- Chain of LoRA
  - è®ºæ–‡: [Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning](https://arxiv.org/pdf/2401.04151)
- ReLoRA: 
  - è®ºæ–‡: [ReLoRA: High-Rank Training Through Low-Rank Updates](https://arxiv.org/pdf/2307.05695)
  - æ‹Ÿåˆæ›´é«˜ç§©çš„W
  - æ•ˆæœ: å®éªŒç»“æœå¥½äºLoRA
- PeriodicLoRA
  - è®ºæ–‡: [PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization](https://arxiv.org/pdf/2402.16141)
- Delta-LoRA
  - è®ºæ–‡: [DELTA-LORA: FINE-TUNING HIGH-RANK PARAMETERS WITH THE DELTA OF LOW-RANK MATRICES](https://arxiv.org/pdf/2309.02411)
  - æ¯”LoRAå¤šä¸€æ­¥ï¼šæ¯æ¬¡æ›´æ–°æ—¶ï¼Œé€šè¿‡$$\textbf{AB}$$è¿‘ä¼¼è®¡ç®—$$\Delta \textbf{W}$$ï¼Œæ¥å¯¹$$\textbf{W}$$è¿›è¡Œæ›´æ–°
  - å®éªŒç»“æœå¥½äºLoRA




## MAM Adapter

ä¸€ç§åœ¨ Adapterã€Prefix Tuning å’Œ LoRA ä¹‹é—´å»ºç«‹**è”ç³»**çš„ç»Ÿä¸€æ–¹æ³•ã€‚
- æœ€ç»ˆçš„æ¨¡å‹ MAM Adapter æ˜¯ç”¨äº FFN çš„å¹¶è¡Œ **Adapter** å’Œ **è½¯æç¤º**çš„ç»„åˆã€‚

ç‰¹ç‚¹ï¼š
- æ•´ä½“ä¸Šæ¥è¯´ï¼Œæœ€ç»ˆçš„æ¨¡å‹MAM Adapteræ•ˆæœä¼šä¼˜äºå•ä¸ªé«˜æ•ˆå¾®è°ƒæ–¹æ³•ã€‚

## UniPELT

ä¸€ç§å°†ä¸åŒçš„PELTæ–¹æ³•LoRAã€Prefix Tuningå’ŒAdapterä½œä¸ºå­æ¨¡å—ï¼Œå¹¶é€šè¿‡é—¨æ§æœºåˆ¶å­¦ä¹ æ¿€æ´»æœ€é€‚åˆå½“å‰æ•°æ®æˆ–ä»»åŠ¡çš„æ–¹æ³•ã€‚

ç‰¹ç‚¹ï¼š
- ç›¸å¯¹äºLoRAï¼ŒBitFitï¼ŒPrefix-tuningï¼Œè®­ç»ƒçš„å‚æ•°é‡æ›´å¤§ï¼›åŒæ—¶ï¼Œæ¨ç†æ›´è€—æ—¶ï¼›å¹¶ä¸”ï¼Œè¾“å…¥ä¼šå ç”¨é¢å¤–çš„åºåˆ—é•¿åº¦ã€‚
- å¤šç§ PELT æ–¹æ³•çš„æ··åˆæ¶‰åŠPLM çš„ä¸åŒéƒ¨åˆ†å¯¹æ¨¡å‹æœ‰æ•ˆæ€§å’Œé²æ£’æ€§éƒ½æœ‰å¥½å¤„ã€‚



# ç»“æŸ