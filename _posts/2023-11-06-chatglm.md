---
layout: post
title:  ChatGLM ç³»åˆ—
date:   2023-11-06 16:52:00
categories: å¤§æ¨¡å‹
tags: GPT ChatGLM 
excerpt: å¼€æºå¤§æ¨¡å‹ç¬”è®°
mathjax: true
permalink: /chatglm
---

* content
{:toc}


# ChatGLM 


## èµ„è®¯

[ChatGLM](https://chatglm.cn/)ï¼šå¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œå®˜æ–¹[Bç«™](https://search.bilibili.com/all?vt=83961745&keyword=chatglm)

ã€2023-6-3ã€‘[ä»GLM-130Båˆ°ChatGLMï¼šå¤§æ¨¡å‹é¢„è®­ç»ƒä¸å¾®è°ƒ](https://www.bilibili.com/video/BV1iu4y1Z7bv)

ã€2023-8-31ã€‘æ™ºè°±AIæ­£å¼ä¸Šçº¿é¦–æ¬¾AIåŠ©æ‰‹ï¼šæ™ºè°±æ¸…è¨€ï¼ŒåŸºäº ChatGLM2ï¼Œé‡‡ç”¨ç›‘ç£å¾®è°ƒï¼Œä»¥é€šç”¨å¯¹è¯å½¢å¼æä¾›æ™ºèƒ½åŒ–æœåŠ¡

æ–‡å¿ƒä¸€è¨€å’Œæ™ºè°±æ¸…è¨€ï¼ˆChatGLMï¼‰è¿™æ¬¡è¿å¤œå¼€æ”¾

## ChatGLM ä»‹ç»


### GLM æ—¶é—´çº¿

å‡ ä¸ªå…³é”®çš„é‡Œç¨‹ç¢‘ï¼š
- 20å¹´11æœˆï¼Œå¼€å§‹å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œåœ¨21å¹´5æœˆï¼Œåšå‡ºç™¾äº¿çš„GLMæ¨¡å‹
- 21å¹´åº•ï¼Œå¼€å§‹è®­ç»ƒGLM-130Båƒäº¿æ¨¡å‹ï¼Œ22å¹´8æœˆï¼ŒGLM-130Bè®­ç»ƒå®Œæˆ
- 22å¹´8æœˆ-23å¹´ï¼Œé€šè¿‡SFT+RLHFè®­ç»ƒChatGLM-130B/ChatGLM-6B

å›¾è§£
- ![](https://pic3.zhimg.com/80/v2-115efce61bdd214c9e387d3ad330589a_1440w.webp)



## ChatGLM ç”Ÿæ€


ChatGLM3 æ¨å‡ºäº†å¯åœ¨æ‰‹æœºä¸Šéƒ¨ç½²çš„ç«¯æµ‹æ¨¡å‹ `ChatGLM3-1.5B` å’Œ `ChatGLM3-3B`ï¼Œæ”¯æŒåŒ…æ‹¬vivoã€å°ç±³ã€ä¸‰æ˜Ÿåœ¨å†…çš„å¤šæ¬¾æ‰‹æœºä»¥åŠè½¦è½½å¹³å°ï¼Œç”šè‡³æ”¯æŒç§»åŠ¨å¹³å°ä¸ŠCPUèŠ¯ç‰‡çš„æ¨ç†ï¼Œé€Ÿåº¦å¯è¾¾20tokensæ¯ç§’(tokenæ˜¯è¯­è¨€æ¨¡å‹ä¸­ç”¨æ¥è¡¨ç¤ºå•è¯æˆ–çŸ­è¯­çš„ç¬¦å·)ã€‚


### ChatGLM ç¬¬ä¸‰æ–¹æ‰©å±•

ã€2023-5-19ã€‘[åŸºäºChatGLMçš„æ‰©å±•æ¨¡å‹](https://mp.weixin.qq.com/s/acwU0JEvuVuyQbNylsj8fQ)
- `Chinese-LangChain`: ä¸­æ–‡**langchain**é¡¹ç›®ï¼ŒåŸºäºChatGLM-6b+langchainå®ç°**æœ¬åœ°åŒ–çŸ¥è¯†åº“**æ£€ç´¢ä¸æ™ºèƒ½ç­”æ¡ˆç”Ÿæˆï¼Œå¢åŠ web searchåŠŸèƒ½ã€çŸ¥è¯†åº“é€‰æ‹©åŠŸèƒ½å’Œæ”¯æŒçŸ¥è¯†å¢é‡æ›´æ–°
- `bibliothecarius`: å¿«é€Ÿæ„å»ºæœåŠ¡ä»¥é›†æˆæ‚¨çš„**æœ¬åœ°æ•°æ®**å’ŒAIæ¨¡å‹ï¼Œæ”¯æŒChatGLMç­‰æœ¬åœ°åŒ–æ¨¡å‹æ¥å…¥ã€‚
- `langchain-ChatGLM`: åŸºäº langchain çš„ ChatGLM åº”ç”¨ï¼Œå®ç°åŸºäºå¯æ‰©å±•çŸ¥è¯†åº“çš„é—®ç­”
- `InstructGLM`: åŸºäºChatGLM-6Bè¿›è¡Œ**æŒ‡ä»¤å­¦ä¹ **ï¼Œæ±‡æ€»å¼€æºä¸­è‹±æ–‡æŒ‡ä»¤æ•°æ®ï¼ŒåŸºäº**Lora**è¿›è¡ŒæŒ‡ä»¤æ•°æ®å¾®è°ƒï¼Œå¼€æ”¾äº†Alpacaã€Belleå¾®è°ƒåçš„Loraæƒé‡ï¼Œä¿®å¤web_demoé‡å¤é—®é¢˜
- `ChatGLM-Efficient-Tuning`: åŸºäºChatGLM-6Bæ¨¡å‹è¿›è¡Œå®šåˆ¶åŒ–å¾®è°ƒï¼Œæ±‡æ€»10ä½™ç§æŒ‡ä»¤æ•°æ®é›†å’Œ3ç§å¾®è°ƒæ–¹æ¡ˆï¼Œå®ç°äº†4/8æ¯”ç‰¹é‡åŒ–å’Œæ¨¡å‹æƒé‡èåˆï¼Œæä¾›å¾®è°ƒæ¨¡å‹å¿«é€Ÿéƒ¨ç½²æ–¹æ³•ã€‚
- `ChatGLM-Finetuning`: åŸºäºChatGLM-6Bæ¨¡å‹ï¼Œè¿›è¡Œä¸‹æ¸¸å…·ä½“ä»»åŠ¡å¾®è°ƒï¼Œæ¶‰åŠFreezeã€Loraã€P-tuningç­‰ï¼Œå¹¶è¿›è¡Œå®éªŒæ•ˆæœå¯¹æ¯”ã€‚
- `ChatGLM-Tuning`: åŸºäº LoRA å¯¹ ChatGLM-6B è¿›è¡Œå¾®è°ƒã€‚ç±»ä¼¼çš„é¡¹ç›®è¿˜åŒ…æ‹¬ Humanable ChatGLM/GPT Fine-tuning




## GLM æ¨¡å‹

2022å¹´8æœˆï¼Œæ™ºè°±AIåŸºäºGLMæ¡†æ¶ï¼Œæ¨å‡º1300äº¿å‚æ•°çš„ä¸­è‹±åŒè¯­ç¨ å¯†æ¨¡å‹GLM-130Bï¼Œç»¼åˆèƒ½åŠ›ä¸GPT3ç›¸å½“ å†…å­˜èŠ‚çœ75%ï¼Œå¯åœ¨å•å°3090 (4)æˆ–å•å°2080(8)è¿›è¡Œæ— æŸæ¨ç† é«˜é€Ÿæ¨ç†ï¼Œæ¯”Pytorchæå‡7-8å€é€Ÿåº¦
- ä¸åŒäº BERTã€GPT-3 ä»¥åŠ T5 çš„æ¶æ„ï¼Œæ˜¯ä¸€ä¸ªåŒ…å«**å¤šç›®æ ‡å‡½æ•°**çš„è‡ªå›å½’é¢„è®­ç»ƒæ¨¡å‹ã€‚
- è®ºæ–‡ï¼š 
  - [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)
  - ã€2022-10-5ã€‘[GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)
- ä»£ç ï¼š[GLM](https://github.com/THUDM/GLM)

GLMçš„å‡ºå‘ç‚¹æ˜¯å°†3ç§ä¸»æµé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç»Ÿä¸€ï¼š
- `GPT`è®­ç»ƒç›®æ ‡æ˜¯**ä»å·¦åˆ°å³**çš„**æ–‡æœ¬ç”Ÿæˆ**
- `BERT`è®­ç»ƒç›®æ ‡æ˜¯å¯¹æ–‡æœ¬è¿›è¡Œ**éšæœºæ©ç **ï¼Œç„¶åé¢„æµ‹è¢«æ©ç çš„è¯
- `T5`åˆ™æ˜¯æ¥å—ä¸€æ®µæ–‡æœ¬ï¼Œä»å·¦åˆ°å³çš„ç”Ÿæˆå¦ä¸€æ®µæ–‡æœ¬
- ![img](https://picx.zhimg.com/v2-b674c5be5ec5ed96e22dbb507a0e897c_r.jpg?source=1940ef5c)

### è‡ªå›å½’å¡«ç©º

GLM é¢„è®­ç»ƒä»»åŠ¡æ˜¯ä¸€ç§**è‡ªå›å½’å¡«ç©º**ä»»åŠ¡(Autoregressive Blank Infilling)ï¼Œå’Œå¤§å¤šæ•°é¢„è®­ç»ƒä»»åŠ¡çš„è®¾è®¡æ€è·¯ä¸€æ ·ï¼Œé‡‡ç”¨â€œå…ˆç ´åï¼Œå†é‡å»ºâ€çš„æ–¹å¼ï¼Œå°†åŸå§‹æ–‡æœ¬çš„éƒ¨åˆ†è¿›è¡Œmaskï¼ˆ**å…ˆç ´å**ï¼‰ï¼Œå†å¯¹maskçš„éƒ¨åˆ†è¿›è¡Œé¢„æµ‹ï¼ˆ**å†é‡å»º**ï¼‰
- ä¸åŒï¼šè¢«maskçš„è¾“å…¥éƒ¨åˆ†ä½¿ç”¨å’Œbertç›¸åŒçš„**åŒå‘æ³¨æ„åŠ›**ï¼Œåœ¨ç”Ÿæˆé¢„æµ‹çš„ä¸€ä¾§ä½¿ç”¨çš„æ˜¯**è‡ªå›å½’çš„å•å‘æ³¨æ„åŠ›**

æ ¹æ®maskçš„é•¿åº¦ä¸åŒï¼Œå¯ä»¥åˆ†ä¸ºä¸‰ç§æ–¹å¼ï¼š**å•è¯**ï¼ˆMASKï¼‰ã€**å¥å­**ï¼ˆsMASKï¼‰ã€**æ–‡æ¡£**ï¼ˆgMASKï¼‰
- å®é™…ä½¿ç”¨ä¸­ï¼Œå¯ä»¥æ ¹æ®ä¸åŒçš„ä»»åŠ¡éœ€è¦ï¼Œè®¾ç½®ä¸åŒmaskæ–¹å¼çš„æ¯”ä¾‹ã€‚
- ä¾‹å¦‚ï¼Œå¦‚æœå¸Œæœ›æ¨¡å‹æœ‰æ›´å¼ºçš„ç”Ÿæˆèƒ½åŠ›ï¼Œå¯ä»¥æŠŠæ–‡æ¡£çº§åˆ«çš„gMASKçš„æ¯”ä¾‹è®¾ç½®åœ°æ¯”è¾ƒé«˜ã€‚
- GLM-130Bä¸­ï¼Œé‡‡ç”¨äº†70%æ–‡æ¡£çº§åˆ«çš„gMASKå’Œ30%å•è¯çº§åˆ«çš„MASK

![](https://pic1.zhimg.com/80/v2-9993b6bcde5e1f8437157e451e59d248_1440w.webp)


è°·æ­Œçš„UL2([UL2: UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131))ï¼Œå…¶ä¸­çš„é¢„è®­ç»ƒä»»åŠ¡å’ŒGLMé«˜åº¦ç›¸ä¼¼ï¼Œä½†æ˜¯æ™šäºGLMä¸€å¹´åæå‡º

### LayerNorm

LayerNorm ä¼šå½±å“è®­ç»ƒçš„ç¨³å®šæ€§
-   `Post-LN`ï¼ˆåŸå§‹çš„BERTï¼‰
-   `Pre-LN`ï¼š[On layer normalization in the transformer architecture](https://arxiv.org/abs/2002.04745)
-   `Sandwich-LN`: [Cogview: Mastering text-to-image generation via transformers](https://arxiv.org/abs/2105.13290)

é€šå¸¸è®¤ä¸ºç¨³å®šæ€§ä¸Š: `Sandwich-LN` > `Pre-LN` > `Post-LN`
- ![](https://pic1.zhimg.com/80/v2-2cb6030dbe040cbb04e36762377a1280_1440w.webp)

130B è§„æ¨¡å®éªŒ
- `DeepNorm` æ¯” `Sandwich-LN` æ›´ç¨³å®š

GLM-130Bæœ€ç»ˆé‡‡ç”¨ DeepNormï¼ˆ[Deepnet: Scaling transformers to 1,000 layers](https://arxiv.org/abs/2203.00555)ï¼‰



### Positional Embedding


ä½ç½®ç¼–ç åˆ†ä¸º**ç»å¯¹**ä½ç½®ç¼–ç å’Œ**ç›¸å¯¹**ä½ç½®ç¼–ç ã€‚ 

**ç»å¯¹**ä½ç½®ç¼–ç ä»£è¡¨ï¼š
-   ä¸‰è§’å¼ï¼šæœ€åˆçš„ä½ç½®ç¼–ç  [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
-   å¯å­¦ä¹ å¼ï¼š[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

**ç›¸å¯¹**ä½ç½®ç¼–ç ä»£è¡¨æ€§çš„æœ‰ï¼š
-   Googleï¼š[Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)
-   Transformer-XL: [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)

- ![](https://pic4.zhimg.com/80/v2-fe67d4318c3a12aa2fd53dae334a76bf_1440w.webp)
- ![](https://pic2.zhimg.com/80/v2-ab9c95a1f1f789fc57b053f619ec22a1_1440w.webp)


å¤§æ¨¡å‹ä¸­åº”ç”¨è¾ƒå¤šçš„ä½ç½®ç¼–ç ï¼š
-   `ALiBi`ï¼š[Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)
-   `RoPE`: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
-   GLM-130Bé‡‡ç”¨çš„æ˜¯`RoPE`ï¼ŒGLM-130Bå›¢é˜Ÿçš„è§‚ç‚¹æ˜¯è™½ç„¶RoPEå¤–æ¨æ€§èƒ½æœ‰é™ï¼Œä½†æ˜¯å¹¶ä¸åº”è¯¥æŠŠé•¿æ–‡æœ¬çš„å¤„ç†é—®é¢˜å®Œå…¨ä¾èµ–äºä½ç½®ç¼–ç çš„å¤–æ¨ï¼Œè€Œæ˜¯éœ€è¦ä»€ä¹ˆæ ·çš„é•¿åº¦å°±åœ¨ä»€ä¹ˆæ ·çš„context lengthä¸Šåšè®­ç»ƒã€‚



## GLM-130B

2022 å¹´ 8 æœˆï¼Œ`æ¸…åå¤§å­¦`è”åˆ[æ™ºè°±AI](https://www.zhipuai.cn/index.html)ï¼ˆå”æ°ã€ææ¶“å­ï¼Œå…¬å¸å:[åŒ—äº¬æ™ºè°±åç« ç§‘æŠ€æœ‰é™å…¬å¸](https://www.qcc.com/firm/3e6bd6d29872b6c3a14f72f4ef6b9197.html)ï¼‰ å‘ç ”ç©¶ç•Œå’Œå·¥ä¸šç•Œå¼€æ”¾äº†æ‹¥æœ‰ 1300 äº¿å‚æ•°çš„ä¸­è‹±åŒè¯­**åŒå‘**ç¨ å¯†æ¨¡å‹ `GLM-130B`
- æˆªè‡³2022å¹´7æœˆï¼Œå®ƒå·²ç»è®­ç»ƒäº†è¶…è¿‡4000äº¿ä¸ªæ–‡æœ¬æ ‡è®°ã€‚
- åº•å±‚æ¶æ„åŸºäº`é€šç”¨è¯­è¨€æ¨¡å‹`(GLM)ï¼Œåœ¨è¯­è¨€ç†è§£å’Œè¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸Šå‡å±•ç¤ºå‡ºå¼ºå¤§çš„æ€§èƒ½ã€‚

å®˜æ–¹githubå¯¹[ChatGLMçš„ä»‹ç»](https://github.com/THUDM/GLM-130B/blob/main/README_zh.md)

è¯¥æ¨¡å‹æœ‰ä¸€äº›ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼š
- åŒè¯­ï¼šåŒæ—¶æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ï¼›
- é«˜ç²¾åº¦ï¼ˆè‹±æ–‡ï¼‰ï¼šåœ¨å…¬å¼€çš„è‹±æ–‡è‡ªç„¶è¯­è¨€æ¦œå• LAMBADAã€MMLU å’Œ Big-bench-lite ä¸Šä¼˜äº GPT-3 175Bï¼ˆAPI: davinciï¼ŒåŸºåº§æ¨¡å‹ï¼‰ã€OPT-175B å’Œ BLOOM-176Bï¼›
- é«˜ç²¾åº¦ï¼ˆä¸­æ–‡ï¼‰ï¼šåœ¨ 7 ä¸ªé›¶æ ·æœ¬ CLUE æ•°æ®é›†å’Œ 5 ä¸ªé›¶æ ·æœ¬ FewCLUE æ•°æ®é›†ä¸Šæ˜æ˜¾ä¼˜äº ERNIE TITAN 3.0 260B å’Œ YUAN 1.0-245Bï¼›
- å¿«é€Ÿæ¨ç†ï¼šé¦–ä¸ªå®ç° INT4 é‡åŒ–çš„åƒäº¿æ¨¡å‹ï¼Œæ”¯æŒç”¨ä¸€å° 4 å¡ 3090 æˆ– 8 å¡ 2080Ti æœåŠ¡å™¨è¿›è¡Œå¿«é€Ÿä¸”åŸºæœ¬æ— æŸæ¨ç†ï¼›
- å¯å¤ç°æ€§ï¼šæ‰€æœ‰ç»“æœï¼ˆè¶…è¿‡ 30 ä¸ªä»»åŠ¡ï¼‰å‡å¯é€šè¿‡æˆ‘ä»¬çš„å¼€æºä»£ç å’Œæ¨¡å‹å‚æ•°å¤ç°ï¼›
- è·¨å¹³å°ï¼šæ”¯æŒåœ¨å›½äº§çš„æµ·å…‰ DCUã€åä¸ºæ˜‡è…¾ 910 å’Œç”³å¨å¤„ç†å™¨åŠç¾å›½çš„è‹±ä¼Ÿè¾¾èŠ¯ç‰‡ä¸Šè¿›è¡Œè®­ç»ƒä¸æ¨ç†ã€‚

æ›´å¤šèµ„æ–™
-   [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)
-   [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)
-   [GLM-130Bï¼šå¼€æºçš„åŒè¯­é¢„è®­ç»ƒæ¨¡å‹](https://keg.cs.tsinghua.edu.cn/glm-130b/zh/posts/glm-130b/)

### GLM-130B åŸç†

GLM-130B å°† `BERT` å’Œ `GPT` ç›®æ ‡è¿›è¡Œäº†ç»Ÿä¸€ï¼Œå¹¶ä¸æœ€è¿‘æå‡ºçš„ä¸€äº›æŠ€æœ¯è¿›è¡Œç»“åˆä»¥æå‡è¯­è¨€æ¨¡å‹çš„æ€§èƒ½è¡¨ç°ã€‚

2022å¹´11æœˆï¼Œæ–¯å¦ç¦å¤§å­¦å¤§æ¨¡å‹ä¸­å¿ƒå¯¹å…¨çƒ30ä¸ªä¸»æµå¤§æ¨¡å‹è¿›è¡Œäº†å…¨æ–¹ä½çš„è¯„æµ‹ï¼ŒGLM-130Bæ˜¯äºšæ´²**å”¯ä¸€**å…¥é€‰çš„å¤§æ¨¡å‹ã€‚åœ¨ä¸OpenAIã€Google Brainã€å¾®è½¯ã€è‹±ä¼Ÿè¾¾ã€Meta AIçš„å„å¤§æ¨¡å‹å¯¹æ¯”ä¸­ï¼Œè¯„æµ‹æŠ¥å‘Šæ˜¾ç¤ºGLM-130Båœ¨å‡†ç¡®æ€§å’Œå…¬å¹³æ€§æŒ‡æ ‡ä¸Šä¸GPT-3 175Bï¼ˆdavinciï¼‰æ¥è¿‘æˆ–æŒå¹³ï¼Œé²æ£’æ€§ã€æ ¡å‡†è¯¯å·®å’Œæ— åæ€§ä¼˜äºGPT-3 175Bã€‚

2022å¹´8æœˆï¼Œå‘ç ”ç©¶ç•Œå’Œå·¥ä¸šç•Œå¼€æ”¾äº†æ‹¥æœ‰1300äº¿å‚æ•°çš„ä¸­è‹±åŒè¯­ç¨ å¯†æ¨¡å‹ GLM-130B1ï¼Œè¯¥æ¨¡å‹æœ‰ä¸€äº›ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼š
- åŒè¯­ï¼š åŒæ—¶æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ã€‚
- é«˜ç²¾åº¦ï¼ˆè‹±æ–‡ï¼‰ï¼š åœ¨å…¬å¼€çš„è‹±æ–‡è‡ªç„¶è¯­è¨€æ¦œå• LAMBADAã€MMLU å’Œ Big-bench-lite ä¸Šä¼˜äº GPT-3 175Bï¼ˆAPI: davinciï¼ŒåŸºåº§æ¨¡å‹ï¼‰ã€OPT-175B å’Œ BLOOM-176Bã€‚
- é«˜ç²¾åº¦ï¼ˆä¸­æ–‡ï¼‰ï¼š åœ¨7ä¸ªé›¶æ ·æœ¬ CLUE æ•°æ®é›†å’Œ5ä¸ªé›¶æ ·æœ¬ FewCLUE æ•°æ®é›†ä¸Šæ˜æ˜¾ä¼˜äº ERNIE TITAN 3.0 260B å’Œ YUAN 1.0-245Bã€‚
- å¿«é€Ÿæ¨ç†ï¼š é¦–ä¸ªå®ç° INT4 é‡åŒ–çš„åƒäº¿æ¨¡å‹ï¼Œæ”¯æŒç”¨ä¸€å° 4 å¡ 3090 æˆ– 8 å¡ 2080Ti æœåŠ¡å™¨è¿›è¡Œå¿«é€Ÿä¸”åŸºæœ¬æ— æŸæ¨ç†ã€‚
- å¯å¤ç°æ€§ï¼š æ‰€æœ‰ç»“æœï¼ˆè¶…è¿‡ 30 ä¸ªä»»åŠ¡ï¼‰å‡å¯é€šè¿‡æˆ‘ä»¬çš„å¼€æºä»£ç å’Œæ¨¡å‹å‚æ•°å¤ç°ã€‚
- è·¨å¹³å°ï¼š æ”¯æŒåœ¨å›½äº§çš„æµ·å…‰ DCUã€åä¸ºæ˜‡è…¾ 910 å’Œç”³å¨å¤„ç†å™¨åŠç¾å›½çš„è‹±ä¼Ÿè¾¾èŠ¯ç‰‡ä¸Šè¿›è¡Œè®­ç»ƒä¸æ¨ç†ã€‚
- ![](https://pic1.zhimg.com/80/v2-791a1175a6346d7b3098c58b93b9ae5c_1440w.webp)

å‚è€ƒ ChatGPT çš„è®¾è®¡æ€è·¯ï¼Œ `ChatGLM` åœ¨åƒäº¿åŸºåº§æ¨¡å‹ GLM-130B ä¸­æ³¨å…¥äº†`ä»£ç é¢„è®­ç»ƒ`ï¼Œé€šè¿‡`æœ‰ç›‘ç£å¾®è°ƒ`ï¼ˆSupervised Fine-Tuningï¼‰ç­‰æŠ€æœ¯å®ç°äººç±»æ„å›¾å¯¹é½ã€‚

### GLM-130B è®­ç»ƒ

ã€2023-6-12ã€‘[å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç³»åˆ—æŠ€æœ¯ï¼šä»¥GLM-130Bä¸ºä¾‹](https://zhuanlan.zhihu.com/p/636329188?utm_psn=1705892296980123649)

#### è®­ç»ƒéš¾ç‚¹

è®­ç»ƒä¸­çš„å·¥ç¨‹æŒ‘æˆ˜

![](https://pic2.zhimg.com/80/v2-acffa8baa32e70157eabea44fbe480ed_1440w.webp)

-   é¦–å…ˆï¼Œå¤§æ¨¡å‹æœ€å¤§çš„è®­ç»ƒæŒ‘æˆ˜å°±æ˜¯å®ƒçš„è®­ç»ƒæˆæœ¬éå¸¸é«˜ï¼Œä½“ç°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨¡å‹çš„è®¡ç®—é‡éå¸¸å¤§

![](https://pic3.zhimg.com/80/v2-311053b0c41ed8bbe637bf0c2c71f09a_1440w.webp)

-   å…¶æ¬¡ï¼Œæ˜¯å†…å­˜çš„æŒ‘æˆ˜ï¼Œä½“ç°åœ¨å•ä¸ªå¤§æ¨¡å‹çš„å‚æ•°é‡æ—©å·²è¶…å‡ºå•å¼ æ˜¾å¡çš„æ˜¾å­˜å¤§å°ã€‚åŒæ—¶ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé™¤äº†æ¨¡å‹å‚æ•°å¤–ï¼Œä¼˜åŒ–å™¨çš„çŠ¶æ€ï¼ˆä¾‹å¦‚Adamä¼˜åŒ–å™¨é‡Œé¢çš„mã€vç­‰ç­‰ï¼‰ä¹Ÿéœ€è¦å ç”¨å¤§é‡å†…å­˜
-   æ¥ä¸‹æ¥å›´ç»•ä¸Šè¿°ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œä»‹ç»ä¸€ä¸‹å¤§æ¨¡å‹è®­ç»ƒçš„å¸¸ç”¨æŠ€æœ¯ï¼ˆå¹¶éå…¨éƒ¨æ˜¯GLM-130Bå®é™…é‡‡ç”¨çš„ï¼‰ã€‚

**æ··åˆç²¾åº¦è®­ç»ƒ**

![](https://pic2.zhimg.com/80/v2-07b31727f027787870595692f9a3f5c9_1440w.webp)

-   æ··åˆç²¾åº¦è®­ç»ƒå®é™…â€œæ··â€çš„æ˜¯16ä½æµ®ç‚¹æ•°å’Œ32ä½æµ®ç‚¹æ•°
-   ä»å³é¢nvidiaæä¾›çš„è¡¨æ ¼å¯ä»¥çœ‹åˆ°FP16 Tensor Coreå’ŒBFLOAT16 Tensor Coreæ¯”Tensor Float 32ï¼ˆTF32ï¼‰å¿«ä¸¤å€ï¼Œæ¯”FP32å¿«10å€ä»¥ä¸Š
-   16ä½æµ®ç‚¹æ•°æœ‰ä¸¤ç§æ ¼å¼ï¼šFP16/BF16ï¼ŒFP16æ¯”BF16ç²¾åº¦æ›´é«˜ï¼Œä½†æ˜¯è¡¨ç¤ºèŒƒå›´æ›´å°ã€‚åœ¨å®é™…è®­ç»ƒä¸­ï¼Œè¡¨ç¤ºèŒƒå›´æ›´åŠ é‡è¦ï¼ˆä¸ºäº†é˜²æ­¢ä¸Šæº¢å’Œä¸‹æº¢ï¼‰ï¼Œå› æ­¤æ›´å€¾å‘äºé€‰æ‹©BF16ã€‚ä½†æ˜¯è¦æ³¨æ„ï¼ŒBF16åªæ”¯æŒ3080ã€A100ã€TPU

![](https://pic4.zhimg.com/80/v2-56129aeac524866c6a8f561b67c041f3_1440w.webp)

-   è¿™é‡Œä»‹ç»çš„æ˜¯ï¼Œæ··åˆç²¾åº¦è®­ç»ƒçš„æ‰§è¡Œæµç¨‹ï¼Œä¸ŠåŠéƒ¨åˆ†å¯¹åº”çš„æ˜¯è®¡ç®—æŸå¤±å‡½æ•°å’Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼Œè¿™éƒ¨åˆ†é‡‡ç”¨çš„æ˜¯fp16ï¼Œä¸‹åŠéƒ¨åˆ†çš„ä¼˜åŒ–å™¨çŠ¶æ€åˆ™é‡‡ç”¨fp32æ ¼å¼
-   æƒ³è¿›ä¸€æ­¥äº†è§£çš„è¯å¯ä»¥çœ‹ä¸‹åŸè®ºæ–‡ï¼š [Mixed Precision Training](https://arxiv.org/abs/1710.03740)

![](https://pic3.zhimg.com/80/v2-138a7aad1be0cade5d9db34c5ab5b81a_1440w.webp)

-   è¿™é‡Œä»‹ç»çš„æ˜¯é‡‡ç”¨äº†æ··åˆç²¾åº¦è®­ç»ƒåï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„å†…å­˜å ç”¨
-   å¯ä»¥çœ‹åˆ°ï¼Œå¤§éƒ¨åˆ†çš„å­˜å‚¨å ç”¨éƒ½åœ¨æ¿€æ´»å‡½æ•°çš„ä¿å­˜å­˜å‚¨ä¸Š

**æ¿€æ´»å‡½æ•°é‡æ¼”**

![](https://pic3.zhimg.com/80/v2-fdbf4f93c8c6d2dfaa1989d90bd33662_1440w.webp)

-   å‰é¢æåˆ°çš„æ¿€æ´»å‡½æ•°å ç”¨å­˜å‚¨è¿‡å¤šçš„é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡æ¿€æ´»å‡½æ•°é‡æ¼”çš„æŠ€æœ¯æ¥è§£å†³

**æ•°æ®å¹¶è¡Œ**

![](https://pic1.zhimg.com/80/v2-707d60aa38c0d92e9252465417463d98_1440w.webp)

-   è¿™é‡Œä»‹ç»çš„æ•°æ®å¹¶è¡Œä¸­çš„å‚æ•°æœåŠ¡å™¨ï¼ˆParameter Serverï¼‰æ–¹æ¡ˆ

![](https://pic2.zhimg.com/80/v2-dca74be88fccf8a65863dbe7b0ceb01d_1440w.webp)

-   è¿™é‡Œä»‹ç»çš„æ˜¯å¦ä¸€ç§æ•°æ®å¹¶è¡Œæ–¹æ¡ˆï¼šAll-Reduce

![](https://pic4.zhimg.com/80/v2-849dc6eb3f79bb7477ce961bbe71a04b_1440w.webp)

-   è¿‘å¹´æ¥æ›´æµè¡Œçš„æ•°æ®å¹¶è¡Œæ–¹æ³•æ˜¯ZeROä¼˜åŒ–å™¨
-   è¿™é‡Œçœ‹ç›´æ’­çš„æ—¶å€™æ²¡å¬æ‡‚ï¼Œéœ€è¦å†çœ‹ä¸€ä¸‹åŸè®ºæ–‡ï¼š[ZeRO: Memory Optimization Towards Training A Trillion Parameter Models](https://arxiv.org/abs/1910.02054)

**æ¨¡å‹å¹¶è¡Œ**

![](https://pic4.zhimg.com/80/v2-071dd4a1c97a0c8b00edc860277aa8c7_1440w.webp)

-   éœ€è¦æ¨¡å‹å¹¶è¡Œè§£å†³ä¸äº†ä¸€å¼ æ˜¾å¡æ”¾ä¸ä¸‹å•ä¸ªæ¨¡å‹çš„é—®é¢˜
-   æ¨¡å‹å¹¶è¡Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯å¼ é‡å¹¶è¡Œï¼Œè¿™ä¸ªæ–¹æ³•å°†çŸ©é˜µåˆ‡åˆ†åˆ°ä¸åŒçš„æ˜¾å¡ä¸Šï¼Œåˆ†åˆ«è®¡ç®—ï¼Œæœ€åå†é€šè¿‡all-reduceæ“ä½œæŠŠè®¡ç®—ç»“æœåŒæ­¥å›æ¥ï¼Œæ˜¾ç„¶è¿™ç§æ–¹æ³•é€šä¿¡é‡æ˜¯æ¯”è¾ƒå¤§çš„ï¼Œå› æ­¤å®é™…æ›´å¤šåº”ç”¨åœ¨å•æœºå¤šå¡ï¼Œæœ‰nvlinkçš„åœºæ™¯ä¸‹
-   å¯¹äºæ›´å¤§çš„æ¨¡å‹ï¼Œåšæ¨¡å‹å¹¶è¡Œçš„æ–¹æ¡ˆæ˜¯æµæ°´çº¿å¹¶è¡Œ

![](https://pic1.zhimg.com/80/v2-1e92d9b7c13a5c4e9ffcc6d4d08ba148_1440w.webp)

-   è¿™é‡Œä»‹ç»çš„æ˜¯æµæ°´çº¿å¹¶è¡Œæœ€æœ´ç´ çš„å®ç°
-   å…ˆForwardï¼Œå†Backwardï¼Œæœ€åå†åšæ›´æ–°ï¼Œæ¯æ¬¡åªæœ‰1å¼ å¡åœ¨è¿ç®—
-   å…¶ä¸­çš„Bubble timeï¼ˆä¹Ÿå°±æ˜¯å›¾ä¸­ç©ºç™½åŒºåŸŸï¼‰æ¯”è¾ƒå¤§ï¼Œä¹Ÿå°±æ˜¯è¿™ç§æ–¹æ¡ˆä¸‹æ˜¾å¡çš„æ•´ä½“ä½¿ç”¨ç‡æ˜¯ä¸é«˜çš„

![](https://pic2.zhimg.com/80/v2-a7eae7e31d1ee8afe77f8975dbb0286d_1440w.webp)

-   GPipeæ˜¯ä¸€ç§æ”¹è¿›çš„æµæ°´çº¿å¹¶è¡Œæ–¹æ¡ˆï¼Œè¿›ä¸€æ­¥é™ä½äº†æ°”æ³¡å æ¯”

![](https://pic3.zhimg.com/80/v2-51292f9537a2558da99e9d7b1c4f88b6_1440w.webp)

-   ä½†æ˜¯GPipeçš„å³°å€¼å†…å­˜å ç”¨æ¯”è¾ƒå¤§

![](https://pic2.zhimg.com/80/v2-486ba739109a8941923e1978cc9c8021_1440w.webp)

-   æµæ°´çº¿å¹¶è¡Œç­–ç•¥ï¼š1F1B

![](https://pic1.zhimg.com/80/v2-0f7e70b8e69a0a45ad7017d9c1bc83dc_1440w.webp)

-   æµæ°´çº¿å¹¶è¡Œç­–ç•¥ï¼šInterleaved 1F1B


#### å¹¶è¡Œç­–ç•¥

GLM-130BåŒæ—¶ä½¿ç”¨äº†å¤šç§å¹¶è¡Œç­–ç•¥
- ![](https://pic4.zhimg.com/80/v2-c64f0f4c1ae09a2e6357a309f71daf03_1440w.webp)
- ![](https://pic4.zhimg.com/80/v2-d9bc6088d3295093f76cabb58632a0bb_1440w.webp)
- ![](https://pic1.zhimg.com/80/v2-a7ebf16851c7ec556a29fc2e48d4cb58_1440w.webp)


#### ç¨³å®šæ€§

### è®­ç»ƒä¸­çš„ç¨³å®šæ€§é—®é¢˜

![](https://pic3.zhimg.com/80/v2-f4e9caa45ad901ca4ee25d4f39a5f636_1440w.webp)

-   ç¨³å®šæ€§å¯èƒ½æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§çš„é—®é¢˜äº†
-   åœ¨Major Issuesä¸­å¯ä»¥çœ‹åˆ°å¤§éƒ¨åˆ†çš„é—®é¢˜éƒ½å’Œdisconverge/spikeæœ‰å…³

![](https://pic1.zhimg.com/80/v2-d5d18eab230721689deaf01fc0c87bec_1440w.webp)

-   è¿™å¼ å›¾é‡Œçš„ç»éªŒä»·å€¼åƒé‡‘


#### é‡åŒ–


![](https://pic1.zhimg.com/80/v2-f7827d82357e78cba6bf48a478426af4_1440w.webp)

-   æ¨¡å‹é‡åŒ–çš„ç›®æ ‡æ˜¯é™ä½æ¨ç†é˜¶æ®µçš„æ˜¾å­˜æˆæœ¬
-   ä¸Šå›¾ä¸­çš„ç­–ç•¥éƒ½æ²¡æœ‰é‡‡ç”¨

![](https://pic1.zhimg.com/80/v2-82ae5fb47cc343c71ffee77f4e3dc094_1440w.webp)

-   å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGLMç³»åˆ—çš„é‡åŒ–æ–¹æ¡ˆåªé™ä½äº†æ˜¾å­˜çš„å ç”¨ï¼Œä¸­é—´çš„è®¡ç®—é‡ï¼ˆæ¨ç†æ—¶é—´ï¼‰å¹¶ä¸ä¼šæœ‰æ˜æ˜¾ä¸‹é™ï¼Œå› ä¸ºä»ç„¶ä½¿ç”¨FP16è¿›è¡Œè®¡ç®—ã€‚

### GLM-130B è®²åº§

æ¸…åå¤§å­¦`æ›¾å¥¥æ¶µ`æŠ¥å‘Šâ€œä»GLM-130Båˆ°ChatGLMï¼šå¤§æ¨¡å‹é¢„è®­ç»ƒä¸å¾®è°ƒâ€ï¼Œæ•´ä¸ªæŠ¥å‘Šåˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†
- ç¬¬äºŒæ®µâ€œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ç³»åˆ—æŠ€æœ¯ï¼šä»¥GLM-130Bä¸ºä¾‹â€, GLM-130Bçš„è®­ç»ƒè¿‡ç¨‹

<iframe src="//player.bilibili.com/player.html?aid=826984653&bvid=BV1iu4y1Z7bv&cid=1151930492&page=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>



## ChatGLM-6B ã€2023-3-14ã€‘

ã€2023-3-14ã€‘ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº General Language Model (`GLM`) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚
- ç”±æ¸…åå¤§å­¦çŸ¥è¯†å·¥ç¨‹ (`KEG`) å®éªŒå®¤å’Œ`æ™ºè°±AI`å…¬å¸ä¸äº2023å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹
- ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šè¿›è¡Œ**æœ¬åœ°éƒ¨ç½²**ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚ 
- `ChatGLM-6B` ä½¿ç”¨äº†å’Œ `ChatGLM` ç›¸åŒçš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥`ç›‘ç£å¾®è°ƒ`ã€`åé¦ˆè‡ªåŠ©`ã€`äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ `ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ `ChatGLM-6B` è™½ç„¶è§„æ¨¡ä¸åŠåƒäº¿æ¨¡å‹ï¼Œä½†å¤§å¤§é™ä½äº†æ¨ç†æˆæœ¬ï¼Œæå‡äº†æ•ˆç‡ï¼Œå¹¶ä¸”å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚
- [æ¨¡å‹å¼€æºåœ°å€](https://github.com/THUDM/ChatGLM-6B), [huggingface](https://huggingface.co/THUDM/chatglm-6b/tree/main)
- finetuneä»£ç ï¼š[ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning)
- APIï¼š è°ƒç”¨æ–¹æ³•å‚è€ƒ[æ™ºè°±AI](https://open.bigmodel.cn/doc/api#sdkauth)ï¼Œ ChatGLM å•†ç”¨ [Issue](https://github.com/THUDM/ChatGLM-6B/issues/799)
- ã€2023-3-17ã€‘issue: [Cannot import name 'convert_file_size_to_int' from 'transformers.utils.hub'](https://github.com/THUDM/ChatGLM-6B/issues/123)

ChatGLM-6B æ¨¡å‹ç»“æ„ï¼š é‡‡ç”¨äº†prefix decoder-onlyçš„transformeræ¨¡å‹æ¡†æ¶ï¼Œåœ¨è¾“å…¥ä¸Šé‡‡ç”¨**åŒå‘**çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨è¾“å‡ºä¸Šé‡‡ç”¨**å•å‘**æ³¨æ„åŠ›æœºåˆ¶ã€‚

æ¨¡å‹ç»†èŠ‚å‡ ç‚¹æ”¹åŠ¨ï¼š
- embeddingå±‚**æ¢¯åº¦ç¼©å‡**ï¼šä¸ºäº†æå‡è®­ç»ƒç¨³å®šæ€§ï¼Œå‡å°äº†embeddingå±‚çš„æ¢¯åº¦ã€‚å…·ä½“åœ°ï¼Œ word_embedding = word_embedding * alpha + word_embedding.detatch() * (1-alpha) ï¼Œå…¶ä¸­ alpha=0.1 ï¼Œè¿™é‡Œdetach()å‡½æ•°çš„ä½œç”¨æ˜¯è¿”å›ä¸€ä¸ªæ–°çš„tensorï¼Œå¹¶ä»è®¡ç®—å›¾åˆ†ç¦»å‡ºæ¥ã€‚æ¢¯åº¦ç¼©å‡çš„æ•ˆæœç›¸å½“äºæŠŠembeddingå±‚çš„æ¢¯åº¦ç¼©å°äº†10å€ï¼Œå‡å°äº†æ¢¯åº¦çš„èŒƒæ•°ã€‚
- **layer normalization**ï¼šé‡‡ç”¨äº†åŸºäºDeep Normçš„post layer normã€‚
- **æ¿€æ´»å‡½æ•°**ï¼šé‡‡ç”¨äº†GeGLUæ¿€æ´»å‡½æ•°ã€‚ç›¸æ¯”äºæ™®é€šçš„FFNï¼Œä½¿ç”¨çº¿å½¢é—¨æ§å•å…ƒçš„GLUæ–°å¢äº†ä¸€ä¸ªæƒé‡çŸ©é˜µï¼Œå…±æœ‰ä¸‰ä¸ªæƒé‡çŸ©é˜µï¼Œä¸ºäº†ä¿æŒå‚æ•°é‡ä¸€è‡´ï¼Œä¸­é—´ç»´åº¦é‡‡ç”¨äº† 8d/3 ï¼Œè€Œä¸æ˜¯4dã€‚
- **ä½ç½®ç¼–ç **ï¼šå»é™¤äº†ç»å¯¹ä½ç½®ç¼–ç ï¼Œé‡‡ç”¨äº†æ—‹è½¬ä½ç½®ç¼–ç `RoPE`ã€‚

è®­ç»ƒç›®æ ‡: ChatGLM-6Bçš„è®­ç»ƒä»»åŠ¡æ˜¯**è‡ªå›å½’æ–‡æœ¬å¡«ç©º**ã€‚ç›¸æ¯”äºé‡‡ç”¨causal decoder-onlyç»“æ„çš„å¤§è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨prefix decoder-onlyç»“æ„çš„ChatGLM-6Bå­˜åœ¨ä¸€ä¸ªåŠ£åŠ¿ï¼šè®­ç»ƒæ•ˆç‡ä½ã€‚
- causal decoderç»“æ„ä¼šåœ¨æ‰€æœ‰çš„tokenä¸Šè®¡ç®—æŸå¤±ï¼Œè€Œprefix decoderåªä¼šåœ¨è¾“å‡ºä¸Šè®¡ç®—æŸå¤±ï¼Œè€Œä¸è®¡ç®—è¾“å…¥ä¸Šçš„æŸå¤±ã€‚
- ç›¸åŒæ•°é‡çš„è®­ç»ƒtokensçš„æƒ…å†µä¸‹ï¼Œprefix decoderè¦æ¯”causal decoderçš„æ•ˆæœå·®ï¼Œå› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­å®é™…ç”¨åˆ°çš„tokensæ•°é‡è¦æ›´å°‘ã€‚

å¦å¤–ï¼ŒChatGPTçš„æˆåŠŸå·²ç»è¯æ˜äº†causal decoderç»“æ„çš„å¤§è¯­è¨€æ¨¡å‹å¯ä»¥è·å¾—éå¸¸å¥½çš„few-shotå’Œzero-shotç”Ÿæˆèƒ½åŠ›ï¼Œé€šè¿‡æŒ‡ä»¤å¾®è°ƒå¯ä»¥è¿›ä¸€æ­¥æ¿€å‘æ¨¡å‹çš„èƒ½åŠ›ã€‚è‡³äºprefix decoderç»“æ„çš„å¤§è¯­è¨€æ¨¡å‹èƒ½å¦è·å¾—ç›¸å½“çš„few-shotå’Œzero-shotèƒ½åŠ›è¿˜ç¼ºå°‘è¶³å¤Ÿçš„éªŒè¯ã€‚

å…³äºtokenizerï¼ŒChatGLMåœ¨25GBçš„ä¸­è‹±åŒè¯­æ•°æ®ä¸Šè®­ç»ƒäº†SentencePieceä½œä¸ºtokenizerï¼Œè¯è¡¨å¤§å°ä¸º130528ã€‚

ChatGLM-6B å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š
- å……åˆ†çš„ä¸­è‹±åŒè¯­é¢„è®­ç»ƒï¼šChatGLM-6B åœ¨ 1:1 æ¯”ä¾‹çš„ä¸­è‹±è¯­æ–™ä¸Šè®­ç»ƒäº† 1T çš„ token é‡ï¼Œå…¼å…·åŒè¯­èƒ½åŠ›ã€‚
- ä¼˜åŒ–çš„æ¨¡å‹æ¶æ„å’Œå¤§å°ï¼šå¸å– GLM-130B è®­ç»ƒç»éªŒï¼Œä¿®æ­£äº†äºŒç»´ RoPE ä½ç½®ç¼–ç å®ç°ï¼Œä½¿ç”¨ä¼ ç»Ÿ FFN ç»“æ„ã€‚6Bï¼ˆ62 äº¿ï¼‰çš„å‚æ•°å¤§å°ï¼Œä¹Ÿä½¿å¾—ç ”ç©¶è€…å’Œä¸ªäººå¼€å‘è€…è‡ªå·±å¾®è°ƒå’Œéƒ¨ç½² ChatGLM-6B æˆä¸ºå¯èƒ½ã€‚
- è¾ƒä½çš„éƒ¨ç½²é—¨æ§›ï¼šFP16 åŠç²¾åº¦ä¸‹ï¼ŒChatGLM-6B éœ€è¦è‡³å°‘ **13 GB** çš„æ˜¾å­˜è¿›è¡Œæ¨ç†ï¼Œç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œè¿™ä¸€éœ€æ±‚å¯ä»¥è¿›ä¸€æ­¥é™ä½åˆ° **10GB**ï¼ˆINT8ï¼‰ å’Œ **6GB**ï¼ˆINT4ï¼‰ï¼Œä½¿å¾— ChatGLM-6B å¯ä»¥éƒ¨ç½²åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šã€‚
  - åœ¨ç°ä»£ GPU å’Œ TPU ä¸Šï¼Œtensor è®¡ç®—å¯ä»¥åœ¨ 16 ä½æµ®ç‚¹ä¸Šé«˜æ•ˆå®Œæˆã€‚ ä½†å¹¶éç®€å•å°† tensor çš„ dtype è®¾ç½®ä¸º torch.float16ã€‚å¯¹äºæŸäº›éƒ¨åˆ†ï¼Œå¦‚ **loss**ï¼Œä»ç„¶éœ€è¦ 32 ä½ç²¾åº¦ã€‚ 
  - åŠç²¾åº¦ä¼˜åŒ–èƒ½ä½¿å†…å­˜å ç”¨å‡åŠï¼Œæˆ–è€…è¯´èƒ½ä½¿æœ‰æ•ˆå†…å­˜ç¿»å€ã€‚
- æ›´é•¿çš„åºåˆ—é•¿åº¦ï¼šç›¸æ¯” GLM-10Bï¼ˆåºåˆ—é•¿åº¦ 1024ï¼‰ï¼ŒChatGLM-6B åºåˆ—é•¿åº¦è¾¾ **2048**ï¼Œæ”¯æŒæ›´é•¿å¯¹è¯å’Œåº”ç”¨ã€‚
- äººç±»æ„å›¾å¯¹é½è®­ç»ƒï¼šä½¿ç”¨äº†`ç›‘ç£å¾®è°ƒ`ï¼ˆSupervised Fine-Tuningï¼‰ã€`åé¦ˆè‡ªåŠ©`ï¼ˆFeedback Bootstrapï¼‰ã€`äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ `ï¼ˆReinforcement Learning from Human Feedbackï¼‰ç­‰æ–¹å¼ï¼Œä½¿æ¨¡å‹åˆå…·ç†è§£äººç±»æŒ‡ä»¤æ„å›¾çš„èƒ½åŠ›ã€‚è¾“å‡ºæ ¼å¼ä¸º markdownï¼Œæ–¹ä¾¿å±•ç¤ºã€‚

ä¸è¿‡ç”±äº ChatGLM-6B æ¨¡å‹çš„å®¹é‡è¾ƒå°ï¼Œä¸å¯é¿å…çš„å­˜åœ¨ä¸€äº›å±€é™å’Œä¸è¶³ï¼Œç›®å‰å·²çŸ¥å…¶å…·æœ‰ç›¸å½“å¤šçš„å±€é™æ€§ï¼Œå¦‚ï¼š äº‹å®æ€§/æ•°å­¦é€»è¾‘é”™è¯¯ï¼Œå¯èƒ½ç”Ÿæˆæœ‰å®³/æœ‰åè§å†…å®¹ï¼Œè¾ƒå¼±çš„ä¸Šä¸‹æ–‡èƒ½åŠ›ï¼Œè‡ªæˆ‘è®¤çŸ¥æ··ä¹±ï¼Œä»¥åŠå¯¹è‹±æ–‡æŒ‡ç¤ºç”Ÿæˆä¸ä¸­æ–‡æŒ‡ç¤ºå®Œå…¨çŸ›ç›¾çš„å†…å®¹ã€‚
- ç›¸å¯¹**è¾ƒå¼±**çš„æ¨¡å‹è®°å¿†å’Œè¯­è¨€èƒ½åŠ›ã€‚åœ¨é¢å¯¹è®¸å¤šäº‹å®æ€§çŸ¥è¯†ä»»åŠ¡æ—¶ï¼Œ`ChatGLM-6B` å¯èƒ½ä¼šç”Ÿæˆä¸æ­£ç¡®çš„ä¿¡æ¯ï¼Œä¹Ÿä¸å¤ªæ“…é•¿é€»è¾‘ç±»é—®é¢˜ï¼ˆå¦‚æ•°å­¦ã€ç¼–ç¨‹ï¼‰çš„è§£ç­”ã€‚
- å¯èƒ½ä¼šäº§ç”Ÿæœ‰å®³è¯´æ˜æˆ–æœ‰åè§çš„å†…å®¹ï¼š`ChatGLM-6B` åªæ˜¯ä¸€ä¸ªåˆæ­¥ä¸äººç±»æ„å›¾å¯¹é½çš„è¯­è¨€æ¨¡å‹ï¼Œå¯èƒ½ä¼šç”Ÿæˆæœ‰å®³ã€æœ‰åè§çš„å†…å®¹ã€‚
- è¾ƒå¼±çš„**å¤šè½®**å¯¹è¯èƒ½åŠ›ï¼šChatGLM-6B çš„ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›è¿˜ä¸å¤Ÿå……åˆ†ï¼Œåœ¨é¢å¯¹é•¿ç­”æ¡ˆç”Ÿæˆå’Œå¤šè½®å¯¹è¯çš„åœºæ™¯æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°ä¸Šä¸‹æ–‡ä¸¢å¤±å’Œç†è§£é”™è¯¯çš„æƒ…å†µã€‚
- å‚è€ƒï¼š[æ¸…åç³»åƒäº¿åŸºåº§å¯¹è¯æ¨¡å‹ChatGLMå¯åŠ¨å†…æµ‹ï¼Œå¼€æºå•å¡ç‰ˆæ¨¡å‹](https://www.jiqizhixin.com/articles/2023-03-15-3)
- ![](https://image.jiqizhixin.com/uploads/editor/f1e9f5c9-25eb-46fa-80da-aa38e9658cbc/640.png)

### ChatGLM-6B å®è·µ


#### ChatGLM-6B æ¥å…¥

```py
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print(response)
# ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
print(response)
```

#### ChatGLM-6B æœ¬åœ°éƒ¨ç½²

ã€2023-4-13ã€‘[æ¸…åChatGLM-6Bæ¨¡å‹æœ¬åœ°éƒ¨ç½²](https://mp.weixin.qq.com/s/PYFNUmR0119ucJvcHNDfmQ)

ç¡¬ä»¶è¦æ±‚
- æ— é‡åŒ–æƒ…å†µä¸‹ï¼Œæ˜¾å­˜åˆå§‹åŒ–åŸºæœ¬ä¸Šéƒ½éœ€è¦**13G**å†…å­˜ï¼Œ16G æ˜¾å­˜å¯èƒ½å¯¹è¯ä¸¤è½®å°±å†…å­˜çˆ†æ»¡äº†ï¼Œå»ºè®®ä½¿ç”¨é‡åŒ–æ¨¡å‹ã€‚
- ![](https://pic2.zhimg.com/80/v2-b030e7adc6b9522e998ac7522f6ee5cd_1440w.webp)
- [ChatGLM-6Bï¼šæ„å»ºæœ¬åœ°ç¦»çº¿çŸ¥è¯†åº“çš„ç»ä½³é€‰æ‹©](https://zhuanlan.zhihu.com/p/633445989)

```sh
git clone https://github.com/THUDM/ChatGLM-6B.git
pip install -r requirements.txt
```

ä½¿ç”¨

```py
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().quantize(4).cuda()
model = model.eval()
response, history = model.chat(tokenizer, "ä½ æ˜¯è°", history=[])
# The dtype of attention mask (torch.int64) is not bool
print(response)
# æˆ‘æ˜¯ä¸€ä¸ªåä¸º ChatGLM-6B çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ˜¯åŸºäºæ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œæ™ºè°± AI å…¬å¸äº 2023 å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¼€å‘çš„ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚
response, history = model.chat(tokenizer, "ä½ ä¼šä»€ä¹ˆ", history=history)
print(response)
# ---------
# é»˜è®¤çš„
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()
# æˆ‘çš„ INT4çš„
model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().quantize(4).cuda()
```


### ChatGLM2-6B -- ã€2023-6-25ã€‘å‡çº§

ã€2023-6-25ã€‘[ChatGLM2-6Bï¼šæ€§èƒ½å¤§å¹…æå‡ï¼Œ8-32kä¸Šä¸‹æ–‡ï¼Œæ¨ç†æé€Ÿ42%](https://mp.weixin.qq.com/s/_h9ls_gHIgHho1RBwUlhsA)
- CEvalæ¦œå•ï¼ŒChatGLM2 æš‚æ—¶ä½å±… Rank 0ï¼ŒChatGLM2-6B ä½å±… Rank 6
- æˆªè‡³6æœˆ25æ—¥ ChatGLM2 æ¨¡å‹ä»¥ 71.1 çš„åˆ†æ•°ä½å±… Rank 0 ï¼ŒChatGLM2-6B æ¨¡å‹ä»¥ 51.7 çš„åˆ†æ•°ä½å±… Rank 6ï¼Œæ˜¯æ¦œå•ä¸Šæ’åæœ€é«˜çš„å¼€æºæ¨¡å‹ã€‚

ChatGLM2-6Bçš„å®‰è£…è¯·å‚è€ƒ[å®˜æ–¹](https://github.com/THUDM/ChatGLM2-6B)

ChatGLM2-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ ChatGLM-6B çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†å¦‚ä¸‹æ–°ç‰¹æ€§ï¼š
- æ›´å¼ºå¤§çš„**æ€§èƒ½**ï¼šåŸºäº ChatGLM åˆä»£æ¨¡å‹çš„å¼€å‘ç»éªŒï¼Œæˆ‘ä»¬å…¨é¢å‡çº§äº† ChatGLM2-6B çš„åŸºåº§æ¨¡å‹ã€‚ChatGLM2-6B ä½¿ç”¨äº† GLM çš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡äº† 1.4T ä¸­è‹±æ ‡è¯†ç¬¦çš„é¢„è®­ç»ƒä¸äººç±»åå¥½å¯¹é½è®­ç»ƒï¼Œè¯„æµ‹ç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B åœ¨ MMLUï¼ˆ+23%ï¼‰ã€CEvalï¼ˆ+33%ï¼‰ã€GSM8Kï¼ˆ+571%ï¼‰ ã€BBHï¼ˆ+60%ï¼‰ç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å–å¾—äº†å¤§å¹…åº¦çš„æå‡ï¼Œåœ¨åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚
- **æ›´é•¿çš„ä¸Šä¸‹æ–‡**ï¼šåŸºäº FlashAttention æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†åŸºåº§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ç”± ChatGLM-6B çš„ 2K æ‰©å±•åˆ°äº† 32Kï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä½¿ç”¨ 8K çš„ä¸Šä¸‹æ–‡é•¿åº¦è®­ç»ƒï¼Œå…è®¸æ›´å¤šè½®æ¬¡çš„å¯¹è¯ã€‚ä½†å½“å‰ç‰ˆæœ¬çš„ ChatGLM2-6B å¯¹å•è½®è¶…é•¿æ–‡æ¡£çš„ç†è§£èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬ä¼šåœ¨åç»­è¿­ä»£å‡çº§ä¸­ç€é‡è¿›è¡Œä¼˜åŒ–ã€‚
- æ›´é«˜æ•ˆçš„**æ¨ç†**ï¼šåŸºäº Multi-Query Attention æŠ€æœ¯ï¼ŒChatGLM2-6B æœ‰æ›´é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ï¼šåœ¨å®˜æ–¹çš„æ¨¡å‹å®ç°ä¸‹ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº† 42%ï¼ŒINT4 é‡åŒ–ä¸‹ï¼Œ6G æ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ç”± 1K æå‡åˆ°äº† 8Kã€‚
- æ›´å¼€æ”¾çš„åè®®ï¼šChatGLM2-6B æƒé‡å¯¹**å­¦æœ¯ç ”ç©¶**å®Œå…¨å¼€æ”¾ï¼Œåœ¨è·å¾—å®˜æ–¹çš„ä¹¦é¢è®¸å¯åï¼Œäº¦å…è®¸å•†ä¸šä½¿ç”¨ã€‚å¦‚æœå‘ç°å¼€æºæ¨¡å‹å¯¹ä¸šåŠ¡æœ‰ç”¨ï¼Œæˆ‘ä»¬æ¬¢è¿æ‚¨å¯¹ä¸‹ä¸€ä»£æ¨¡å‹ ChatGLM3 ç ”å‘çš„æèµ ã€‚

ã€2023-7-14ã€‘[ChatGLM2-6Bï¼Œå…è´¹å•†ç”¨](https://mp.weixin.qq.com/s/pNMcR2c6kFV1TVaI8wzHRg)ï¼Œæ‰«ç ç™»è®°å³å¯

è‡ª 3 æœˆ 14 æ—¥å‘å¸ƒ ChatGLM-6B åŠ 6 æœˆ 25 æ—¥å‘å¸ƒ ChatGLM2-6B ä»¥æ¥ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹åœ¨ Huggingface ä¸Šçš„ä¸‹è½½é‡å·²ç»å…ˆåè¶…è¿‡äº† 300 ä¸‡å’Œ 120 ä¸‡ã€‚éå¸¸æ„Ÿè°¢å¤§å®¶å¯¹ ChatGLM æ¨¡å‹çš„æ”¯æŒã€‚ä¸ºäº†æ›´å¥½åœ°æ”¯æŒå›½äº§å¤§æ¨¡å‹å¼€æºç”Ÿæ€çš„ç¹è£å‘å±•ï¼Œç»æ™ºè°± AI åŠæ¸…å KEG å®éªŒå®¤å†³å®šï¼Œè‡ªå³æ—¥èµ· ChatGLM-6B å’Œ ChatGLM2-6B æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œå¹¶ä¸”åœ¨å®Œæˆä¼ä¸šç™»è®°è·å¾—æˆæƒåï¼Œå…è®¸å…è´¹å•†ä¸šä½¿ç”¨ã€‚

ç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B å¤šä¸ªç»´åº¦çš„èƒ½åŠ›éƒ½å–å¾—äº†æå‡ï¼Œå¯¹æ¯”ç¤ºä¾‹
- æ•°ç†é€»è¾‘
- çŸ¥è¯†æ¨ç†
- é•¿æ–‡æ¡£ç†è§£

#### éƒ¨ç½²

- [GitHub åœ°å€](https://github.com/THUDM/ChatGLM2-6B/tree/main)
- æ¨¡å‹æ–‡ä»¶ï¼š[Huggingface åœ°å€](https://huggingface.co/THUDM/chatglm2-6b)
- 7ä¸ªbinæ–‡ä»¶å­˜æ”¾æ¨¡å‹å‚æ•°ï¼Œ æ–‡ä»¶åï¼š `pytorch_model-0000{1~7}-of...`, æ¯ä¸ªæ–‡ä»¶è¿‘ 2G
- [ä¸€æ–‡æå®šChatGLM2-6Béƒ¨ç½²](https://zhuanlan.zhihu.com/p/647224135)

ï¼ˆ1ï¼‰ä¸‹è½½æ–¹å¼
- æ‰‹åŠ¨ä¸‹è½½ï¼š ä¸‹è½½å®Œæ¯•ä¸Šä¼ åˆ°ç§ŸèµGPUæœåŠ¡å™¨å°±è¡Œï¼Œå¯èƒ½æ¯”è¾ƒè´¹æµé‡
- git lfs å·¥å…·ï¼š ä¸‹è½½å¤§æ–‡ä»¶çš„å·¥å…·ï¼ˆå—ç½‘ç»œé™åˆ¶ ï¼Œå¯èƒ½éœ€è¦å¤šæ¬¡å°è¯•ï¼‰

```sh
git clone https://github.com/THUDM/ChatGLM-6B
# modelæ–‡ä»¶æœ€å¥½åƒæˆ‘è¿™æ ·æ”¾ç½®ï¼Œå¥½æ‰¾ä¸€äº›ï½
cd ChatGLM-6B
mkdir model
cd model

apt-get update
apt-get install git-lfs 
git-lfs install 
git lfs clone https://huggingface.co/THUDM/chatglm2-6b 
# ä¸‹è½½glm2 ä»£ç ã€å’Œæ¨¡å‹æ–‡ä»¶
# è¿æ¥ä¸ç¨³å®šï¼Œå¯èƒ½éœ€è¦å¤šcloneå‡ æ¬¡ï¼Œæˆ–è€…ç›´æ¥æœ¬æœºdownloadç„¶åä¸Šä¼ ï¼ˆps è¿˜æ˜¯è‡ªå·±uploadä¸‡æ— ä¸€å¤±ï¼‰

```

ï¼ˆ2ï¼‰ç¯å¢ƒéƒ¨ç½²

```sh
conda create -n chatglm2 python=3.10
conda activate chatglm2 
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple #è¿™é‡Œé…äº†ä¸‹è½½æºï¼Œæ›´å¿«ä¸€äº›ï¼
```

ï¼ˆ3ï¼‰ä¿®æ”¹ä»£ç 

ä¿®æ”¹[web_demo.py](https://github.com/THUDM/ChatGLM2-6B/blob/main/web_demo.py)é…ç½®ä¿¡æ¯
- model_path : åŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä»huggingfaceä¸Špull
- launch : é»˜è®¤ä¸ä¼šç”Ÿæˆå¯è®¿é—®çš„å…¬ç½‘urlé“¾æ¥


```py
from transformers import AutoModel, AutoTokenizer
import gradio as gr
import mdtex2html
from utils import load_model_on_gpus

#model_path = 'THUDM/chatglm2-6b'
model_path = './chatglm2-6b' # æœ¬åœ°æ¨¡å‹åœ°å€
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, trust_remote_code=True).cuda() # cpu -> gpu
# å¤šæ˜¾å¡æ”¯æŒï¼Œä½¿ç”¨ä¸‹é¢ä¸¤è¡Œä»£æ›¿ä¸Šé¢ä¸€è¡Œï¼Œå°†num_gpusæ”¹ä¸ºä½ å®é™…çš„æ˜¾å¡æ•°é‡
# from utils import load_model_on_gpus
# model = load_model_on_gpus("THUDM/chatglm2-6b", num_gpus=2) # å¤šå¡æ¨¡å¼
model = model.eval()

# ....
# demo.queue().launch(share=True, inbrowser=True) # old
demo.queue().launch(share=True, inbrowser=True,server_name='0.0.0.0', server_port=7860)) # new

```

ï¼ˆ4ï¼‰å¯åŠ¨æœåŠ¡
- gradio å…¬ç½‘urlæœ‰æ—¶å¤±è´¥ â†’ sshéš§é“ or å…¶å®ƒå¹³å°ï¼ˆå¦‚ [autoDL](https://www.autodl.com/docs/port/)ï¼‰

```py
python web_demo.py
```

- api æ–¹å¼: è¿è¡Œapi.pyæ–‡ä»¶ï¼Œé»˜è®¤éƒ¨ç½²åœ¨æœ¬åœ°çš„ 8000 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨

```sh
curl -X POST "http://127.0.0.1:8000" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "ä½ å¥½", "history": []}'
# {"response":"ä½ å¥½ ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚","history":[["ä½ å¥½","ä½ å¥½ ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],"status":200,"time":"2023-09-25 22:23:34"}
```

OpenAI æ ¼å¼çš„æµå¼ API éƒ¨ç½²
- [openai_api.py](https://github.com/THUDM/ChatGLM2-6B/blob/main/openai_api.py)

```py
import openai
if __name__ == "__main__":
    openai.api_base = "http://localhost:8000/v1"
    openai.api_key = "none"
    for chunk in openai.ChatCompletion.create(
        model="chatglm2-6b",
        messages=[
            {"role": "user", "content": "ä½ å¥½"}
        ],
        stream=True
    ):
        if hasattr(chunk.choices[0].delta, "content"):
            print(chunk.choices[0].delta.content, end="", flush=True)
```

#### çŸ¥è¯†æ³¨å…¥

ã€2023-7-11ã€‘[å•æ ·æœ¬å¾®è°ƒç»™ChatGLM2æ³¨å…¥çŸ¥è¯†](https://mp.weixin.qq.com/s/hANR9OVDVEZMMvK8uxtChA)
- å€ŸåŠ© AdaLoRAç®—æ³•ï¼Œä½¿ç”¨1æ¡æ ·æœ¬å¯¹ChatGLM2-6bå®æ–½å¾®è°ƒã€‚å‡ åˆ†é’Ÿå°±æˆåŠŸæ³¨å…¥äº†æœ‰å…³çŸ¥è¯†
- AdaLoRAæ˜¯LoRAæ–¹æ³•çš„ä¸€ç§å‡çº§ç‰ˆæœ¬ï¼Œä½¿ç”¨æ–¹æ³•ä¸LoRAåŸºæœ¬ä¸€æ ·ã€‚ä¸»è¦å·®å¼‚
  - LoRAä¸­ä¸åŒè®­ç»ƒå‚æ•°çŸ©é˜µçš„ç§©è¢«å›ºå®šã€‚
  - ä½†AdaLoRAä¸­ä¸åŒè®­ç»ƒå‚æ•°çŸ©é˜µçš„ç§©æ˜¯ä¼šåœ¨ä¸€å®šèŒƒå›´å†…è‡ªé€‚åº”è°ƒæ•´çš„ï¼Œé‚£äº›æ›´é‡è¦çš„è®­ç»ƒå‚æ•°çŸ©é˜µä¼šåˆ†é…åˆ°æ›´é«˜çš„ç§©ã€‚
  - AdaLoRAçš„æ•ˆæœä¼šå¥½äºLoRAã€‚

å¤‡æ³¨
- (1) åªéœ€è¦1æ¡æ ·æœ¬ï¼Œå¾ˆå°‘çš„è®­ç»ƒæ—¶é—´ï¼Œå°±å¯ä»¥é€šè¿‡å¾®è°ƒç»™LLMæ³¨å…¥çŸ¥è¯†ã€‚
- (2) LLM å¯ä»¥çœ‹åšç±»ä¼¼Key-Valueå½¢å¼çš„**çŸ¥è¯†æ•°æ®åº“**ï¼Œæ”¯æŒå¢åˆ æ”¹æŸ¥ã€‚é€šè¿‡å¾®è°ƒå¯ä»¥**å¢åˆ ä¿®æ”¹**çŸ¥è¯†ï¼Œé€šè¿‡**æ¡ä»¶ç”Ÿæˆ**å¯ä»¥æŸ¥è¯¢æå–çŸ¥è¯†ã€‚
- (3) LoRA å¾®è°ƒæ˜¯ä¸€ç§é«˜æ•ˆçš„èå…¥å­¦ä¹ ç®—æ³•ã€‚ç±»ä¼¼äººç±»æŠŠæ–°çŸ¥è¯†èå…¥ç°æœ‰çŸ¥è¯†ä½“ç³»çš„å­¦ä¹ è¿‡ç¨‹ã€‚å­¦ä¹ æ—¶æ— éœ€æ–°çŸ¥è¯†ç‰¹åˆ«å¤šçš„æ ·æœ¬ï¼Œå­¦ä¹ ååŸæœ‰çš„åºå¤§çŸ¥è¯†å’Œèƒ½åŠ›å¯ä»¥åŸºæœ¬ä¸å—å½±å“ã€‚

```py
from peft import get_peft_model, AdaLoraConfig, TaskType

#è®­ç»ƒæ—¶èŠ‚çº¦GPUå ç”¨
model.config.use_cache=False
model.supports_gradient_checkpointing = True  #
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

peft_config = AdaLoraConfig(
    task_type=TaskType.CAUSAL_LM, inference_mode=False,
    r=8,
    lora_alpha=32, lora_dropout=0.1,
    target_modules=["query", "value"]
)

peft_model = get_peft_model(model, peft_config)

peft_model.is_parallelizable = True
peft_model.model_parallel = True
peft_model.print_trainable_parameters()
```

éªŒè¯æ¨¡å‹

```py
from peft import PeftModel 
ckpt_path = 'single_chatglm2'
model_old = AutoModel.from_pretrained("chatglm2-6b",
                                  load_in_8bit=False, 
                                  trust_remote_code=True)
peft_loaded = PeftModel.from_pretrained(model_old,ckpt_path).cuda()
model_new = peft_loaded.merge_and_unload() #åˆå¹¶loraæƒé‡
chatglm = ChatGLM(model_new,tokenizer,max_chat_rounds=20) #æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯ä»¥ä»ä¹‹å‰å¯¹è¯ä¸Šä¸‹æ–‡æå–çŸ¥è¯†ã€‚
```

## ChatGLM3 

ã€2023-10-27ã€‘ChatGLM3 æ˜¯æ™ºè°±AIå’Œæ¸…åå¤§å­¦ KEG å®éªŒå®¤è”åˆå‘å¸ƒçš„æ–°ä¸€ä»£å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚`ChatGLM3-6B` æ˜¯ ChatGLM3 ç³»åˆ—ä¸­çš„å¼€æºæ¨¡å‹ï¼Œåœ¨ä¿ç•™äº†å‰ä¸¤ä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›ä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¸Šï¼ŒChatGLM3-6B å¼•å…¥äº†å¦‚ä¸‹ç‰¹æ€§ï¼š
1.  **æ›´å¼ºå¤§çš„åŸºç¡€æ¨¡å‹ï¼š** ChatGLM3-6B çš„åŸºç¡€æ¨¡å‹ ChatGLM3-6B-Base é‡‡ç”¨äº†æ›´å¤šæ ·çš„è®­ç»ƒæ•°æ®ã€æ›´å……åˆ†çš„è®­ç»ƒæ­¥æ•°å’Œæ›´åˆç†çš„è®­ç»ƒç­–ç•¥ã€‚åœ¨è¯­ä¹‰ã€æ•°å­¦ã€æ¨ç†ã€ä»£ç ã€çŸ¥è¯†ç­‰ä¸åŒè§’åº¦çš„æ•°æ®é›†ä¸Šæµ‹è¯„æ˜¾ç¤ºï¼Œ**ChatGLM3-6B-Base å…·æœ‰åœ¨ 10B ä»¥ä¸‹çš„åŸºç¡€æ¨¡å‹ä¸­æœ€å¼ºçš„æ€§èƒ½**ã€‚
2.  **æ›´å®Œæ•´çš„åŠŸèƒ½æ”¯æŒï¼š** ChatGLM3-6B é‡‡ç”¨äº†å…¨æ–°è®¾è®¡çš„ [Prompt æ ¼å¼](https://github.com/THUDM/ChatGLM3/blob/main/PROMPT.md)ï¼Œé™¤æ­£å¸¸çš„å¤šè½®å¯¹è¯å¤–ã€‚åŒæ—¶åŸç”Ÿæ”¯æŒ[å·¥å…·è°ƒç”¨](https://github.com/THUDM/ChatGLM3/blob/main/tool_using/README.md)ï¼ˆFunction Callï¼‰ã€ä»£ç æ‰§è¡Œï¼ˆCode Interpreterï¼‰å’Œ Agent ä»»åŠ¡ç­‰å¤æ‚åœºæ™¯ã€‚
3.  **æ›´å…¨é¢çš„å¼€æºåºåˆ—ï¼š** é™¤äº†å¯¹è¯æ¨¡å‹ [ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b) å¤–ï¼Œè¿˜å¼€æºäº†åŸºç¡€æ¨¡å‹ [ChatGLM3-6B-Base](https://huggingface.co/THUDM/chatglm3-6b-base)ã€é•¿æ–‡æœ¬å¯¹è¯æ¨¡å‹ [ChatGLM3-6B-32K](https://huggingface.co/THUDM/chatglm3-6b-32k)ã€‚ä»¥ä¸Šæ‰€æœ‰æƒé‡å¯¹å­¦æœ¯ç ”ç©¶**å®Œå…¨å¼€æ”¾**ï¼Œåœ¨å¡«å†™[é—®å·](https://open.bigmodel.cn/mla/form)è¿›è¡Œç™»è®°å**äº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨**ã€‚

[ChatGLM3 GitHub](https://github.com/THUDM/ChatGLM3)

é—®é¢˜
- [streamlitæ‰“ä¸å¼€](https://github.com/THUDM/ChatGLM3/issues/82)


## ChatGLM3-6b

ã€2023-11-7ã€‘[chatglm3-6b](https://huggingface.co/THUDM/chatglm3-6b/tree/main)

### æ–‡ä»¶ç»“æ„


```sh
# æ¨¡å‹æ–‡ä»¶
pytorch_model-00001-of-00007.bin

```


### é…ç½®


#### config.json



#### configuration_chatglm.py




### åˆ†è¯

#### tokenization_chatglm.py


```py
import json
import os
import torch
from typing import List, Optional, Union, Dict
from sentencepiece import SentencePieceProcessor
from transformers import PreTrainedTokenizer
from transformers.utils import logging, PaddingStrategy
from transformers.tokenization_utils_base import EncodedInput, BatchEncoding


class SPTokenizer:
    def __init__(self, model_path: str):
        # reload tokenizer
        assert os.path.isfile(model_path), model_path
        self.sp_model = SentencePieceProcessor(model_file=model_path)

        # BOS / EOS token IDs
        self.n_words: int = self.sp_model.vocab_size()
        self.bos_id: int = self.sp_model.bos_id()
        self.eos_id: int = self.sp_model.eos_id()
        self.pad_id: int = self.sp_model.unk_id()
        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()

        special_tokens = ["[MASK]", "[gMASK]", "[sMASK]", "sop", "eop", "<|system|>", "<|user|>", "<|assistant|>",
                          "<|observation|>"]
        self.special_tokens = {}
        self.index_special_tokens = {}
        for token in special_tokens:
            self.special_tokens[token] = self.n_words
            self.index_special_tokens[self.n_words] = token
            self.n_words += 1

    def tokenize(self, s: str):
        return self.sp_model.EncodeAsPieces(s)

    def encode(self, s: str, bos: bool = False, eos: bool = False) -> List[int]:
        assert type(s) is str
        t = self.sp_model.encode(s)
        if bos:
            t = [self.bos_id] + t
        if eos:
            t = t + [self.eos_id]
        return t

    def decode(self, t: List[int]) -> str:
        text, buffer = "", []
        for token in t:
            if token in self.index_special_tokens:
                if buffer:
                    text += self.sp_model.decode(buffer)
                    buffer = []
                text += self.index_special_tokens[token]
            else:
                buffer.append(token)
        if buffer:
            text += self.sp_model.decode(buffer)
        return text

    def decode_tokens(self, tokens: List[str]) -> str:
        text = self.sp_model.DecodePieces(tokens)
        return text

    def convert_token_to_id(self, token):
        """ Converts a token (str) in an id using the vocab. """
        if token in self.special_tokens:
            return self.special_tokens[token]
        return self.sp_model.PieceToId(token)

    def convert_id_to_token(self, index):
        """Converts an index (integer) in a token (str) using the vocab."""
        if index in self.index_special_tokens:
            return self.index_special_tokens[index]
        if index in [self.eos_id, self.bos_id, self.pad_id] or index < 0:
            return ""
        return self.sp_model.IdToPiece(index)


class ChatGLMTokenizer(PreTrainedTokenizer):
    vocab_files_names = {"vocab_file": "tokenizer.model"}

    model_input_names = ["input_ids", "attention_mask", "position_ids"]

    def __init__(self, vocab_file, padding_side="left", clean_up_tokenization_spaces=False, **kwargs):
        self.name = "GLMTokenizer"

        self.vocab_file = vocab_file
        self.tokenizer = SPTokenizer(vocab_file)
        self.special_tokens = {
            "<bos>": self.tokenizer.bos_id,
            "<eos>": self.tokenizer.eos_id,
            "<pad>": self.tokenizer.pad_id
        }
        super().__init__(padding_side=padding_side, clean_up_tokenization_spaces=clean_up_tokenization_spaces, **kwargs)

    def get_command(self, token):
        if token in self.special_tokens:
            return self.special_tokens[token]
        assert token in self.tokenizer.special_tokens, f"{token} is not a special token for {self.name}"
        return self.tokenizer.special_tokens[token]

    @property
    def unk_token(self) -> str:
        return "<unk>"

    @property
    def pad_token(self) -> str:
        return "<unk>"

    @property
    def pad_token_id(self):
        return self.get_command("<pad>")

    @property
    def eos_token(self) -> str:
        return "</s>"

    @property
    def eos_token_id(self):
        return self.get_command("<eos>")

    @property
    def vocab_size(self):
        return self.tokenizer.n_words

    def get_vocab(self):
        """ Returns vocab as a dict """
        vocab = {self._convert_id_to_token(i): i for i in range(self.vocab_size)}
        vocab.update(self.added_tokens_encoder)
        return vocab

    def _tokenize(self, text, **kwargs):
        return self.tokenizer.tokenize(text)

    def _convert_token_to_id(self, token):
        """ Converts a token (str) in an id using the vocab. """
        return self.tokenizer.convert_token_to_id(token)

    def _convert_id_to_token(self, index):
        """Converts an index (integer) in a token (str) using the vocab."""
        return self.tokenizer.convert_id_to_token(index)

    def convert_tokens_to_string(self, tokens: List[str]) -> str:
        return self.tokenizer.decode_tokens(tokens)

    def save_vocabulary(self, save_directory, filename_prefix=None):
        """
        Save the vocabulary and special tokens file to a directory.
        Args:
            save_directory (`str`):
                The directory in which to save the vocabulary.
            filename_prefix (`str`, *optional*):
                An optional prefix to add to the named of the saved files.
        Returns:
            `Tuple(str)`: Paths to the files saved.
        """
        if os.path.isdir(save_directory):
            vocab_file = os.path.join(
                save_directory, self.vocab_files_names["vocab_file"]
            )
        else:
            vocab_file = save_directory

        with open(self.vocab_file, 'rb') as fin:
            proto_str = fin.read()

        with open(vocab_file, "wb") as writer:
            writer.write(proto_str)

        return (vocab_file,)

    def get_prefix_tokens(self):
        prefix_tokens = [self.get_command("[gMASK]"), self.get_command("sop")]
        return prefix_tokens

    def build_single_message(self, role, metadata, message):
        assert role in ["system", "user", "assistant", "observation"], role
        role_tokens = [self.get_command(f"<|{role}|>")] + self.tokenizer.encode(f"{metadata}\n")
        message_tokens = self.tokenizer.encode(message)
        tokens = role_tokens + message_tokens
        return tokens

    def build_chat_input(self, query, history=None, role="user"):
        if history is None:
            history = []
        input_ids = []
        for item in history:
            content = item["content"]
            if item["role"] == "system" and "tools" in item:
                content = content + "\n" + json.dumps(item["tools"], indent=4, ensure_ascii=False)
            input_ids.extend(self.build_single_message(item["role"], item.get("metadata", ""), content))
        input_ids.extend(self.build_single_message(role, "", query))
        input_ids.extend([self.get_command("<|assistant|>")])
        return self.batch_encode_plus([input_ids], return_tensors="pt", is_split_into_words=True)

    def build_inputs_with_special_tokens(
            self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
    ) -> List[int]:
        """
        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
        adding special tokens. A BERT sequence has the following format:
        - single sequence: `[CLS] X [SEP]`
        - pair of sequences: `[CLS] A [SEP] B [SEP]`
        Args:
            token_ids_0 (`List[int]`):
                List of IDs to which the special tokens will be added.
            token_ids_1 (`List[int]`, *optional*):
                Optional second list of IDs for sequence pairs.
        Returns:
            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
        """
        prefix_tokens = self.get_prefix_tokens()
        token_ids_0 = prefix_tokens + token_ids_0
        if token_ids_1 is not None:
            token_ids_0 = token_ids_0 + token_ids_1 + [self.get_command("<eos>")]
        return token_ids_0

    def _pad(
            self,
            encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],
            max_length: Optional[int] = None,
            padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
            pad_to_multiple_of: Optional[int] = None,
            return_attention_mask: Optional[bool] = None,
    ) -> dict:
        """
        Pad encoded inputs (on left/right and up to predefined length or max length in the batch)
        Args:
            encoded_inputs:
                Dictionary of tokenized inputs (`List[int]`) or batch of tokenized inputs (`List[List[int]]`).
            max_length: maximum length of the returned list and optionally padding length (see below).
                Will truncate by taking into account the special tokens.
            padding_strategy: PaddingStrategy to use for padding.
                - PaddingStrategy.LONGEST Pad to the longest sequence in the batch
                - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)
                - PaddingStrategy.DO_NOT_PAD: Do not pad
                The tokenizer padding sides are defined in self.padding_side:
                    - 'left': pads on the left of the sequences
                    - 'right': pads on the right of the sequences
            pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.
                This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability
                `>= 7.5` (Volta).
            return_attention_mask:
                (optional) Set to False to avoid returning attention mask (default: set to model specifics)
        """
        # Load from model defaults
        assert self.padding_side == "left"

        required_input = encoded_inputs[self.model_input_names[0]]
        seq_length = len(required_input)

        if padding_strategy == PaddingStrategy.LONGEST:
            max_length = len(required_input)

        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):
            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of

        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length

        # Initialize attention mask if not present.
        if "attention_mask" not in encoded_inputs:
            encoded_inputs["attention_mask"] = [1] * seq_length

        if "position_ids" not in encoded_inputs:
            encoded_inputs["position_ids"] = list(range(seq_length))

        if needs_to_be_padded:
            difference = max_length - len(required_input)

            if "attention_mask" in encoded_inputs:
                encoded_inputs["attention_mask"] = [0] * difference + encoded_inputs["attention_mask"]
            if "position_ids" in encoded_inputs:
                encoded_inputs["position_ids"] = [0] * difference + encoded_inputs["position_ids"]
            encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input

        return encoded_inputs
```


### æ¨¡å‹ç»“æ„


#### modeling_chatglm.py

- [modeling_chatglm.py](https://huggingface.co/THUDM/chatglm3-6b/blob/main/modeling_chatglm.py)

```py
""" PyTorch ChatGLM model. """

import math
import copy
import warnings
import re
import sys

import torch
import torch.utils.checkpoint
import torch.nn.functional as F
from torch import nn
from torch.nn import CrossEntropyLoss, LayerNorm, MSELoss, BCEWithLogitsLoss
from torch.nn.utils import skip_init
from typing import Optional, Tuple, Union, List, Callable, Dict, Any
from copy import deepcopy

from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
    SequenceClassifierOutputWithPast,
)
from transformers.modeling_utils import PreTrainedModel
from transformers.utils import logging
from transformers.generation.logits_process import LogitsProcessor
from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList, GenerationConfig, ModelOutput

from .configuration_chatglm import ChatGLMConfig

# flags required to enable jit fusion kernels

if sys.platform != 'darwin':
    torch._C._jit_set_profiling_mode(False)
    torch._C._jit_set_profiling_executor(False)
    torch._C._jit_override_can_fuse_on_cpu(True)
    torch._C._jit_override_can_fuse_on_gpu(True)

logger = logging.get_logger(__name__)

_CHECKPOINT_FOR_DOC = "THUDM/ChatGLM"
_CONFIG_FOR_DOC = "ChatGLMConfig"

CHATGLM_6B_PRETRAINED_MODEL_ARCHIVE_LIST = [
    "THUDM/chatglm3-6b",
    # See all ChatGLM models at https://huggingface.co/models?filter=chatglm
]


def default_init(cls, *args, **kwargs):
    return cls(*args, **kwargs)


class InvalidScoreLogitsProcessor(LogitsProcessor):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        if torch.isnan(scores).any() or torch.isinf(scores).any():
            scores.zero_()
            scores[..., 5] = 5e4
        return scores


class PrefixEncoder(torch.nn.Module):
    """
    The torch.nn model to encode the prefix
    Input shape: (batch-size, prefix-length)
    Output shape: (batch-size, prefix-length, 2*layers*hidden)
    """

    def __init__(self, config: ChatGLMConfig):
        super().__init__()
        self.prefix_projection = config.prefix_projection
        if self.prefix_projection:
            # Use a two-layer MLP to encode the prefix
            kv_size = config.num_layers * config.kv_channels * config.multi_query_group_num * 2
            self.embedding = torch.nn.Embedding(config.pre_seq_len, kv_size)
            self.trans = torch.nn.Sequential(
                torch.nn.Linear(kv_size, config.hidden_size),
                torch.nn.Tanh(),
                torch.nn.Linear(config.hidden_size, kv_size)
            )
        else:
            self.embedding = torch.nn.Embedding(config.pre_seq_len,
                                                config.num_layers * config.kv_channels * config.multi_query_group_num * 2)

    def forward(self, prefix: torch.Tensor):
        if self.prefix_projection:
            prefix_tokens = self.embedding(prefix)
            past_key_values = self.trans(prefix_tokens)
        else:
            past_key_values = self.embedding(prefix)
        return past_key_values


def split_tensor_along_last_dim(
        tensor: torch.Tensor,
        num_partitions: int,
        contiguous_split_chunks: bool = False,
) -> List[torch.Tensor]:
    """Split a tensor along its last dimension.
    Arguments:
        tensor: input tensor.
        num_partitions: number of partitions to split the tensor
        contiguous_split_chunks: If True, make each chunk contiguous
                                 in memory.
    Returns:
        A list of Tensors
    """
    # Get the size and dimension.
    last_dim = tensor.dim() - 1
    last_dim_size = tensor.size()[last_dim] // num_partitions
    # Split.
    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
    # Note: torch.split does not create contiguous tensors by default.
    if contiguous_split_chunks:
        return tuple(chunk.contiguous() for chunk in tensor_list)

    return tensor_list


class RotaryEmbedding(nn.Module):
    def __init__(self, dim, original_impl=False, device=None, dtype=None):
        super().__init__()
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).to(dtype=dtype) / dim))
        self.register_buffer("inv_freq", inv_freq)
        self.dim = dim
        self.original_impl = original_impl

    def forward_impl(
            self, seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000
    ):
        """Enhanced Transformer with Rotary Position Embedding.
        Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/
        transformers/rope/__init__.py. MIT License:
        https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.
        """
        # $\Theta = {\theta_i = 10000^{\frac{2(i-1)}{d}}, i \in [1, 2, ..., \frac{d}{2}]}$
        theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=torch.float, device=device) / n_elem))

        # Create position indexes `[0, 1, ..., seq_len - 1]`
        seq_idx = torch.arange(seq_len, dtype=torch.float, device=device)

        # Calculate the product of position index and $\theta_i$
        idx_theta = torch.outer(seq_idx, theta).float()

        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)

        # this is to mimic the behaviour of complex32, else we will get different results
        if dtype in (torch.float16, torch.bfloat16, torch.int8):
            cache = cache.bfloat16() if dtype == torch.bfloat16 else cache.half()
        return cache

    def forward(self, max_seq_len, offset=0):
        return self.forward_impl(
            max_seq_len, self.dim, dtype=self.inv_freq.dtype, device=self.inv_freq.device
        )


@torch.jit.script
def apply_rotary_pos_emb(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:
    # x: [sq, b, np, hn]
    sq, b, np, hn = x.size(0), x.size(1), x.size(2), x.size(3)
    rot_dim = rope_cache.shape[-2] * 2
    x, x_pass = x[..., :rot_dim], x[..., rot_dim:]
    # truncate to support variable sizes
    rope_cache = rope_cache[:sq]
    xshaped = x.reshape(sq, -1, np, rot_dim // 2, 2)
    rope_cache = rope_cache.view(sq, -1, 1, xshaped.size(3), 2)
    x_out2 = torch.stack(
        [
            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],
            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],
        ],
        -1,
    )
    x_out2 = x_out2.flatten(3)
    return torch.cat((x_out2, x_pass), dim=-1)


class RMSNorm(torch.nn.Module):
    def __init__(self, normalized_shape, eps=1e-5, device=None, dtype=None, **kwargs):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.empty(normalized_shape, device=device, dtype=dtype))
        self.eps = eps

    def forward(self, hidden_states: torch.Tensor):
        input_dtype = hidden_states.dtype
        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)

        return (self.weight * hidden_states).to(input_dtype)


class CoreAttention(torch.nn.Module):
    def __init__(self, config: ChatGLMConfig, layer_number):
        super(CoreAttention, self).__init__()

        self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling
        self.attention_softmax_in_fp32 = config.attention_softmax_in_fp32
        if self.apply_query_key_layer_scaling:
            self.attention_softmax_in_fp32 = True
        self.layer_number = max(1, layer_number)

        projection_size = config.kv_channels * config.num_attention_heads

        # Per attention head and per partition values.
        self.hidden_size_per_partition = projection_size
        self.hidden_size_per_attention_head = projection_size // config.num_attention_heads
        self.num_attention_heads_per_partition = config.num_attention_heads

        coeff = None
        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)
        if self.apply_query_key_layer_scaling:
            coeff = self.layer_number
            self.norm_factor *= coeff
        self.coeff = coeff

        self.attention_dropout = torch.nn.Dropout(config.attention_dropout)

    def forward(self, query_layer, key_layer, value_layer, attention_mask):
        pytorch_major_version = int(torch.__version__.split('.')[0])
        if pytorch_major_version >= 2:
            query_layer, key_layer, value_layer = [k.permute(1, 2, 0, 3) for k in [query_layer, key_layer, value_layer]]
            if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:
                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,
                                                                                 is_causal=True)
            else:
                if attention_mask is not None:
                    attention_mask = ~attention_mask
                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,
                                                                                 attention_mask)
            context_layer = context_layer.permute(2, 0, 1, 3)
            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)
            context_layer = context_layer.reshape(*new_context_layer_shape)
        else:
            # Raw attention scores

            # [b, np, sq, sk]
            output_size = (query_layer.size(1), query_layer.size(2), query_layer.size(0), key_layer.size(0))

            # [sq, b, np, hn] -> [sq, b * np, hn]
            query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            # [sk, b, np, hn] -> [sk, b * np, hn]
            key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)

            # preallocting input tensor: [b * np, sq, sk]
            matmul_input_buffer = torch.empty(
                output_size[0] * output_size[1], output_size[2], output_size[3], dtype=query_layer.dtype,
                device=query_layer.device
            )

            # Raw attention scores. [b * np, sq, sk]
            matmul_result = torch.baddbmm(
                matmul_input_buffer,
                query_layer.transpose(0, 1),  # [b * np, sq, hn]
                key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]
                beta=0.0,
                alpha=(1.0 / self.norm_factor),
            )

            # change view to [b, np, sq, sk]
            attention_scores = matmul_result.view(*output_size)

            # ===========================
            # Attention probs and dropout
            # ===========================

            # attention scores and attention mask [b, np, sq, sk]
            if self.attention_softmax_in_fp32:
                attention_scores = attention_scores.float()
            if self.coeff is not None:
                attention_scores = attention_scores * self.coeff
            if attention_mask is None and attention_scores.shape[2] == attention_scores.shape[3]:
                attention_mask = torch.ones(output_size[0], 1, output_size[2], output_size[3],
                                            device=attention_scores.device, dtype=torch.bool)
                attention_mask.tril_()
                attention_mask = ~attention_mask
            if attention_mask is not None:
                attention_scores = attention_scores.masked_fill(attention_mask, float("-inf"))
            attention_probs = F.softmax(attention_scores, dim=-1)
            attention_probs = attention_probs.type_as(value_layer)

            # This is actually dropping out entire tokens to attend to, which might
            # seem a bit unusual, but is taken from the original Transformer paper.
            attention_probs = self.attention_dropout(attention_probs)
            # =========================
            # Context layer. [sq, b, hp]
            # =========================

            # value_layer -> context layer.
            # [sk, b, np, hn] --> [b, np, sq, hn]

            # context layer shape: [b, np, sq, hn]
            output_size = (value_layer.size(1), value_layer.size(2), query_layer.size(0), value_layer.size(3))
            # change view [sk, b * np, hn]
            value_layer = value_layer.view(value_layer.size(0), output_size[0] * output_size[1], -1)
            # change view [b * np, sq, sk]
            attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)
            # matmul: [b * np, sq, hn]
            context_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))
            # change view [b, np, sq, hn]
            context_layer = context_layer.view(*output_size)
            # [b, np, sq, hn] --> [sq, b, np, hn]
            context_layer = context_layer.permute(2, 0, 1, 3).contiguous()
            # [sq, b, np, hn] --> [sq, b, hp]
            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)
            context_layer = context_layer.view(*new_context_layer_shape)

        return context_layer


class SelfAttention(torch.nn.Module):
    """Parallel self-attention layer abstract class.
    Self-attention layer takes input with size [s, b, h]
    and returns output of the same size.
    """

    def __init__(self, config: ChatGLMConfig, layer_number, device=None):
        super(SelfAttention, self).__init__()
        self.layer_number = max(1, layer_number)

        self.projection_size = config.kv_channels * config.num_attention_heads

        # Per attention head and per partition values.
        self.hidden_size_per_attention_head = self.projection_size // config.num_attention_heads
        self.num_attention_heads_per_partition = config.num_attention_heads

        self.multi_query_attention = config.multi_query_attention
        self.qkv_hidden_size = 3 * self.projection_size
        if self.multi_query_attention:
            self.num_multi_query_groups_per_partition = config.multi_query_group_num
            self.qkv_hidden_size = (
                    self.projection_size + 2 * self.hidden_size_per_attention_head * config.multi_query_group_num
            )
        self.query_key_value = nn.Linear(config.hidden_size, self.qkv_hidden_size,
                                         bias=config.add_bias_linear or config.add_qkv_bias,
                                         device=device, **_config_to_kwargs(config)
                                         )

        self.core_attention = CoreAttention(config, self.layer_number)

        # Output.
        self.dense = nn.Linear(self.projection_size, config.hidden_size, bias=config.add_bias_linear,
                               device=device, **_config_to_kwargs(config)
                               )

    def _allocate_memory(self, inference_max_sequence_len, batch_size, device=None, dtype=None):
        if self.multi_query_attention:
            num_attention_heads = self.num_multi_query_groups_per_partition
        else:
            num_attention_heads = self.num_attention_heads_per_partition
        return torch.empty(
            inference_max_sequence_len,
            batch_size,
            num_attention_heads,
            self.hidden_size_per_attention_head,
            dtype=dtype,
            device=device,
        )

    def forward(
            self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True
    ):
        # hidden_states: [sq, b, h]

        # =================================================
        # Pre-allocate memory for key-values for inference.
        # =================================================
        # =====================
        # Query, Key, and Value
        # =====================

        # Attention heads [sq, b, h] --> [sq, b, (np * 3 * hn)]
        mixed_x_layer = self.query_key_value(hidden_states)

        if self.multi_query_attention:
            (query_layer, key_layer, value_layer) = mixed_x_layer.split(
                [
                    self.num_attention_heads_per_partition * self.hidden_size_per_attention_head,
                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,
                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,
                ],
                dim=-1,
            )
            query_layer = query_layer.view(
                query_layer.size()[:-1] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)
            )
            key_layer = key_layer.view(
                key_layer.size()[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)
            )
            value_layer = value_layer.view(
                value_layer.size()[:-1]
                + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)
            )
        else:
            new_tensor_shape = mixed_x_layer.size()[:-1] + \
                               (self.num_attention_heads_per_partition,
                                3 * self.hidden_size_per_attention_head)
            mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)

            # [sq, b, np, 3 * hn] --> 3 [sq, b, np, hn]
            (query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)

        # apply relative positional encoding (rotary embedding)
        if rotary_pos_emb is not None:
            query_layer = apply_rotary_pos_emb(query_layer, rotary_pos_emb)
            key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)

        # adjust key and value for inference
        if kv_cache is not None:
            cache_k, cache_v = kv_cache
            key_layer = torch.cat((cache_k, key_layer), dim=0)
            value_layer = torch.cat((cache_v, value_layer), dim=0)
        if use_cache:
            kv_cache = (key_layer, value_layer)
        else:
            kv_cache = None

        if self.multi_query_attention:
            key_layer = key_layer.unsqueeze(-2)
            key_layer = key_layer.expand(
                -1, -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1
            )
            key_layer = key_layer.contiguous().view(
                key_layer.size()[:2] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)
            )
            value_layer = value_layer.unsqueeze(-2)
            value_layer = value_layer.expand(
                -1, -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1
            )
            value_layer = value_layer.contiguous().view(
                value_layer.size()[:2] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)
            )

        # ==================================
        # core attention computation
        # ==================================

        context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)

        # =================
        # Output. [sq, b, h]
        # =================

        output = self.dense(context_layer)

        return output, kv_cache


def _config_to_kwargs(args):
    common_kwargs = {
        "dtype": args.torch_dtype,
    }
    return common_kwargs


class MLP(torch.nn.Module):
    """MLP.
    MLP will take the input with h hidden state, project it to 4*h
    hidden dimension, perform nonlinear transformation, and project the
    state back into h hidden dimension.
    """

    def __init__(self, config: ChatGLMConfig, device=None):
        super(MLP, self).__init__()

        self.add_bias = config.add_bias_linear

        # Project to 4h. If using swiglu double the output width, see https://arxiv.org/pdf/2002.05202.pdf
        self.dense_h_to_4h = nn.Linear(
            config.hidden_size,
            config.ffn_hidden_size * 2,
            bias=self.add_bias,
            device=device,
            **_config_to_kwargs(config)
        )

        def swiglu(x):
            x = torch.chunk(x, 2, dim=-1)
            return F.silu(x[0]) * x[1]

        self.activation_func = swiglu

        # Project back to h.
        self.dense_4h_to_h = nn.Linear(
            config.ffn_hidden_size,
            config.hidden_size,
            bias=self.add_bias,
            device=device,
            **_config_to_kwargs(config)
        )

    def forward(self, hidden_states):
        # [s, b, 4hp]
        intermediate_parallel = self.dense_h_to_4h(hidden_states)
        intermediate_parallel = self.activation_func(intermediate_parallel)
        # [s, b, h]
        output = self.dense_4h_to_h(intermediate_parallel)
        return output


class GLMBlock(torch.nn.Module):
    """A single transformer layer.
    Transformer layer takes input with size [s, b, h] and returns an
    output of the same size.
    """

    def __init__(self, config: ChatGLMConfig, layer_number, device=None):
        super(GLMBlock, self).__init__()
        self.layer_number = layer_number

        self.apply_residual_connection_post_layernorm = config.apply_residual_connection_post_layernorm

        self.fp32_residual_connection = config.fp32_residual_connection

        LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm
        # Layernorm on the input data.
        self.input_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,
                                             dtype=config.torch_dtype)

        # Self attention.
        self.self_attention = SelfAttention(config, layer_number, device=device)
        self.hidden_dropout = config.hidden_dropout

        # Layernorm on the attention output
        self.post_attention_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,
                                                      dtype=config.torch_dtype)

        # MLP
        self.mlp = MLP(config, device=device)

    def forward(
            self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True,
    ):
        # hidden_states: [s, b, h]

        # Layer norm at the beginning of the transformer layer.
        layernorm_output = self.input_layernorm(hidden_states)
        # Self attention.
        attention_output, kv_cache = self.self_attention(
            layernorm_output,
            attention_mask,
            rotary_pos_emb,
            kv_cache=kv_cache,
            use_cache=use_cache
        )

        # Residual connection.
        if self.apply_residual_connection_post_layernorm:
            residual = layernorm_output
        else:
            residual = hidden_states

        layernorm_input = torch.nn.functional.dropout(attention_output, p=self.hidden_dropout, training=self.training)
        layernorm_input = residual + layernorm_input

        # Layer norm post the self attention.
        layernorm_output = self.post_attention_layernorm(layernorm_input)

        # MLP.
        mlp_output = self.mlp(layernorm_output)

        # Second residual connection.
        if self.apply_residual_connection_post_layernorm:
            residual = layernorm_output
        else:
            residual = layernorm_input

        output = torch.nn.functional.dropout(mlp_output, p=self.hidden_dropout, training=self.training)
        output = residual + output

        return output, kv_cache


class GLMTransformer(torch.nn.Module):
    """Transformer class."""

    def __init__(self, config: ChatGLMConfig, device=None):
        super(GLMTransformer, self).__init__()

        self.fp32_residual_connection = config.fp32_residual_connection
        self.post_layer_norm = config.post_layer_norm

        # Number of layers.
        self.num_layers = config.num_layers

        # Transformer layers.
        def build_layer(layer_number):
            return GLMBlock(config, layer_number, device=device)

        self.layers = torch.nn.ModuleList([build_layer(i + 1) for i in range(self.num_layers)])

        if self.post_layer_norm:
            LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm
            # Final layer norm before output.
            self.final_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,
                                                 dtype=config.torch_dtype)

        self.gradient_checkpointing = False

    def _get_layer(self, layer_number):
        return self.layers[layer_number]

    def forward(
            self, hidden_states, attention_mask, rotary_pos_emb, kv_caches=None,
            use_cache: Optional[bool] = True,
            output_hidden_states: Optional[bool] = False,
    ):
        if not kv_caches:
            kv_caches = [None for _ in range(self.num_layers)]
        presents = () if use_cache else None
        if self.gradient_checkpointing and self.training:
            if use_cache:
                logger.warning_once(
                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                )
                use_cache = False

        all_self_attentions = None
        all_hidden_states = () if output_hidden_states else None
        for index in range(self.num_layers):
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer = self._get_layer(index)
            if self.gradient_checkpointing and self.training:
                layer_ret = torch.utils.checkpoint.checkpoint(
                    layer,
                    hidden_states,
                    attention_mask,
                    rotary_pos_emb,
                    kv_caches[index],
                    use_cache
                )
            else:
                layer_ret = layer(
                    hidden_states,
                    attention_mask,
                    rotary_pos_emb,
                    kv_cache=kv_caches[index],
                    use_cache=use_cache
                )
            hidden_states, kv_cache = layer_ret
            if use_cache:
                presents = presents + (kv_cache,)

        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        # Final layer norm.
        if self.post_layer_norm:
            hidden_states = self.final_layernorm(hidden_states)

        return hidden_states, presents, all_hidden_states, all_self_attentions


class ChatGLMPreTrainedModel(PreTrainedModel):
    """
    An abstract class to handle weights initialization and
    a simple interface for downloading and loading pretrained models.
    """

    is_parallelizable = False
    supports_gradient_checkpointing = True
    config_class = ChatGLMConfig
    base_model_prefix = "transformer"
    _no_split_modules = ["GLMBlock"]

    def _init_weights(self, module: nn.Module):
        """Initialize the weights."""
        return

    def get_masks(self, input_ids, past_key_values, padding_mask=None):
        batch_size, seq_length = input_ids.shape
        full_attention_mask = torch.ones(batch_size, seq_length, seq_length, device=input_ids.device)
        full_attention_mask.tril_()
        past_length = 0
        if past_key_values:
            past_length = past_key_values[0][0].shape[0]
        if past_length:
            full_attention_mask = torch.cat((torch.ones(batch_size, seq_length, past_length,
                                                        device=input_ids.device), full_attention_mask), dim=-1)
        if padding_mask is not None:
            full_attention_mask = full_attention_mask * padding_mask.unsqueeze(1)
        if not past_length and padding_mask is not None:
            full_attention_mask -= padding_mask.unsqueeze(-1) - 1
        full_attention_mask = (full_attention_mask < 0.5).bool()
        full_attention_mask.unsqueeze_(1)
        return full_attention_mask

    def get_position_ids(self, input_ids, device):
        batch_size, seq_length = input_ids.shape
        position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1)
        return position_ids

    def _set_gradient_checkpointing(self, module, value=False):
        if isinstance(module, GLMTransformer):
            module.gradient_checkpointing = value


class Embedding(torch.nn.Module):
    """Language model embeddings."""

    def __init__(self, config: ChatGLMConfig, device=None):
        super(Embedding, self).__init__()

        self.hidden_size = config.hidden_size
        # Word embeddings (parallel).
        self.word_embeddings = nn.Embedding(
            config.padded_vocab_size,
            self.hidden_size,
            dtype=config.torch_dtype,
            device=device
        )
        self.fp32_residual_connection = config.fp32_residual_connection

    def forward(self, input_ids):
        # Embeddings.
        words_embeddings = self.word_embeddings(input_ids)
        embeddings = words_embeddings
        # Data format change to avoid explicit tranposes : [b s h] --> [s b h].
        embeddings = embeddings.transpose(0, 1).contiguous()
        # If the input flag for fp32 residual connection is set, convert for float.
        if self.fp32_residual_connection:
            embeddings = embeddings.float()
        return embeddings


class ChatGLMModel(ChatGLMPreTrainedModel):
    def __init__(self, config: ChatGLMConfig, device=None, empty_init=True):
        super().__init__(config)
        if empty_init:
            init_method = skip_init
        else:
            init_method = default_init
        init_kwargs = {}
        if device is not None:
            init_kwargs["device"] = device
        self.embedding = init_method(Embedding, config, **init_kwargs)
        self.num_layers = config.num_layers
        self.multi_query_group_num = config.multi_query_group_num
        self.kv_channels = config.kv_channels

        # Rotary positional embeddings
        self.seq_length = config.seq_length
        rotary_dim = (
            config.hidden_size // config.num_attention_heads if config.kv_channels is None else config.kv_channels
        )

        self.rotary_pos_emb = RotaryEmbedding(rotary_dim // 2, original_impl=config.original_rope, device=device,
                                              dtype=config.torch_dtype)
        self.encoder = init_method(GLMTransformer, config, **init_kwargs)
        self.output_layer = init_method(nn.Linear, config.hidden_size, config.padded_vocab_size, bias=False,
                                        dtype=config.torch_dtype, **init_kwargs)
        self.pre_seq_len = config.pre_seq_len
        self.prefix_projection = config.prefix_projection
        if self.pre_seq_len is not None:
            for param in self.parameters():
                param.requires_grad = False
            self.prefix_tokens = torch.arange(self.pre_seq_len).long()
            self.prefix_encoder = PrefixEncoder(config)
            self.dropout = torch.nn.Dropout(0.1)

    def get_input_embeddings(self):
        return self.embedding.word_embeddings

    def get_prompt(self, batch_size, device, dtype=torch.half):
        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(device)
        past_key_values = self.prefix_encoder(prefix_tokens).type(dtype)
        past_key_values = past_key_values.view(
            batch_size,
            self.pre_seq_len,
            self.num_layers * 2,
            self.multi_query_group_num,
            self.kv_channels
        )
        # seq_len, b, nh, hidden_size
        past_key_values = self.dropout(past_key_values)
        past_key_values = past_key_values.permute([2, 1, 0, 3, 4]).split(2)
        return past_key_values

    def forward(
            self,
            input_ids,
            position_ids: Optional[torch.Tensor] = None,
            attention_mask: Optional[torch.BoolTensor] = None,
            full_attention_mask: Optional[torch.BoolTensor] = None,
            past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
            inputs_embeds: Optional[torch.Tensor] = None,
            use_cache: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            return_dict: Optional[bool] = None,
    ):
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        batch_size, seq_length = input_ids.shape

        if inputs_embeds is None:
            inputs_embeds = self.embedding(input_ids)

        if self.pre_seq_len is not None:
            if past_key_values is None:
                past_key_values = self.get_prompt(batch_size=batch_size, device=input_ids.device,
                                                  dtype=inputs_embeds.dtype)
            if attention_mask is not None:
                attention_mask = torch.cat([attention_mask.new_ones((batch_size, self.pre_seq_len)),
                                            attention_mask], dim=-1)

        if full_attention_mask is None:
            if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):
                full_attention_mask = self.get_masks(input_ids, past_key_values, padding_mask=attention_mask)

        # Rotary positional embeddings
        rotary_pos_emb = self.rotary_pos_emb(self.seq_length)
        if position_ids is not None:
            rotary_pos_emb = rotary_pos_emb[position_ids]
        else:
            rotary_pos_emb = rotary_pos_emb[None, :seq_length]
        rotary_pos_emb = rotary_pos_emb.transpose(0, 1).contiguous()

        # Run encoder.
        hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(
            inputs_embeds, full_attention_mask, rotary_pos_emb=rotary_pos_emb,
            kv_caches=past_key_values, use_cache=use_cache, output_hidden_states=output_hidden_states
        )

        if not return_dict:
            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=presents,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
        )

    def quantize(self, weight_bit_width: int):
        from .quantization import quantize
        quantize(self.encoder, weight_bit_width)
        return self


class ChatGLMForConditionalGeneration(ChatGLMPreTrainedModel):
    def __init__(self, config: ChatGLMConfig, empty_init=True, device=None):
        super().__init__(config)

        self.max_sequence_length = config.max_length
        self.transformer = ChatGLMModel(config, empty_init=empty_init, device=device)
        self.config = config
        self.quantized = False

        if self.config.quantization_bit:
            self.quantize(self.config.quantization_bit, empty_init=True)

    def _update_model_kwargs_for_generation(
            self,
            outputs: ModelOutput,
            model_kwargs: Dict[str, Any],
            is_encoder_decoder: bool = False,
            standardize_cache_format: bool = False,
    ) -> Dict[str, Any]:
        # update past_key_values
        model_kwargs["past_key_values"] = self._extract_past_from_model_output(
            outputs, standardize_cache_format=standardize_cache_format
        )

        # update attention mask
        if "attention_mask" in model_kwargs:
            attention_mask = model_kwargs["attention_mask"]
            model_kwargs["attention_mask"] = torch.cat(
                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1
            )

        # update position ids
        if "position_ids" in model_kwargs:
            position_ids = model_kwargs["position_ids"]
            new_position_id = position_ids[..., -1:].clone()
            new_position_id += 1
            model_kwargs["position_ids"] = torch.cat(
                [position_ids, new_position_id], dim=-1
            )

        model_kwargs["is_first_forward"] = False
        return model_kwargs

    def prepare_inputs_for_generation(
            self,
            input_ids: torch.LongTensor,
            past_key_values: Optional[torch.Tensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.Tensor] = None,
            use_cache: Optional[bool] = None,
            is_first_forward: bool = True,
            **kwargs
    ) -> dict:
        # only last token for input_ids if past is not None
        if position_ids is None:
            position_ids = self.get_position_ids(input_ids, device=input_ids.device)
        if not is_first_forward:
            if past_key_values is not None:
                position_ids = position_ids[..., -1:]
                input_ids = input_ids[:, -1:]
        return {
            "input_ids": input_ids,
            "past_key_values": past_key_values,
            "position_ids": position_ids,
            "attention_mask": attention_mask,
            "return_last_logit": True,
            "use_cache": use_cache
        }

    def forward(
            self,
            input_ids: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.Tensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
            past_key_values: Optional[Tuple[torch.FloatTensor]] = None,
            inputs_embeds: Optional[torch.Tensor] = None,
            labels: Optional[torch.Tensor] = None,
            use_cache: Optional[bool] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            return_dict: Optional[bool] = None,
            return_last_logit: Optional[bool] = False,
    ):
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        transformer_outputs = self.transformer(
            input_ids=input_ids,
            position_ids=position_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = transformer_outputs[0]
        if return_last_logit:
            hidden_states = hidden_states[-1:]
        lm_logits = self.transformer.output_layer(hidden_states)
        lm_logits = lm_logits.transpose(0, 1).contiguous()

        loss = None
        if labels is not None:
            lm_logits = lm_logits.to(torch.float32)

            # Shift so that tokens < n predict n
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

            lm_logits = lm_logits.to(hidden_states.dtype)
            loss = loss.to(hidden_states.dtype)

        if not return_dict:
            output = (lm_logits,) + transformer_outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return CausalLMOutputWithPast(
            loss=loss,
            logits=lm_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )

    @staticmethod
    def _reorder_cache(
            past: Tuple[Tuple[torch.Tensor, torch.Tensor], ...], beam_idx: torch.LongTensor
    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor], ...]:
        """
        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or
        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct
        beam_idx at every generation step.
        Output shares the same memory storage as `past`.
        """
        return tuple(
            (
                layer_past[0].index_select(1, beam_idx.to(layer_past[0].device)),
                layer_past[1].index_select(1, beam_idx.to(layer_past[1].device)),
            )
            for layer_past in past
        )

    def process_response(self, output, history):
        content = ""
        history = deepcopy(history)
        for response in output.split("<|assistant|>"):
            metadata, content = response.split("\n", maxsplit=1)
            if not metadata.strip():
                content = content.strip()
                history.append({"role": "assistant", "metadata": metadata, "content": content})
                content = content.replace("[[è®­ç»ƒæ—¶é—´]]", "2023å¹´")
            else:
                history.append({"role": "assistant", "metadata": metadata, "content": content})
                if history[0]["role"] == "system" and "tools" in history[0]:
                    content = "\n".join(content.split("\n")[1:-1])
                    def tool_call(**kwargs):
                        return kwargs
                    parameters = eval(content)
                    content = {"name": metadata.strip(), "parameters": parameters}
                else:
                    content = {"name": metadata.strip(), "content": content}
        return content, history

    @torch.inference_mode()
    def chat(self, tokenizer, query: str, history: List[Dict] = None, role: str = "user",
             max_length: int = 8192, num_beams=1, do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None,
             **kwargs):
        if history is None:
            history = []
        if logits_processor is None:
            logits_processor = LogitsProcessorList()
        logits_processor.append(InvalidScoreLogitsProcessor())
        gen_kwargs = {"max_length": max_length, "num_beams": num_beams, "do_sample": do_sample, "top_p": top_p,
                      "temperature": temperature, "logits_processor": logits_processor, **kwargs}
        inputs = tokenizer.build_chat_input(query, history=history, role=role)
        inputs = inputs.to(self.device)
        eos_token_id = [tokenizer.eos_token_id, tokenizer.get_command("<|user|>"),
                        tokenizer.get_command("<|observation|>")]
        outputs = self.generate(**inputs, **gen_kwargs, eos_token_id=eos_token_id)
        outputs = outputs.tolist()[0][len(inputs["input_ids"][0]):-1]
        response = tokenizer.decode(outputs)
        history.append({"role": role, "content": query})
        response, history = self.process_response(response, history)
        return response, history

    @torch.inference_mode()
    def stream_chat(self, tokenizer, query: str, history: List[Dict] = None, role: str = "user",
                    past_key_values=None,max_length: int = 8192, do_sample=True, top_p=0.8, temperature=0.8,
                    logits_processor=None, return_past_key_values=False, **kwargs):
        if history is None:
            history = []
        if logits_processor is None:
            logits_processor = LogitsProcessorList()
        logits_processor.append(InvalidScoreLogitsProcessor())
        eos_token_id = [tokenizer.eos_token_id, tokenizer.get_command("<|user|>"),
                        tokenizer.get_command("<|observation|>")]
        gen_kwargs = {"max_length": max_length, "do_sample": do_sample, "top_p": top_p,
                      "temperature": temperature, "logits_processor": logits_processor, **kwargs}
        if past_key_values is None:
            inputs = tokenizer.build_chat_input(query, history=history, role=role)
        else:
            inputs = tokenizer.build_chat_input(query, role=role)
        inputs = inputs.to(self.device)
        if past_key_values is not None:
            past_length = past_key_values[0][0].shape[0]
            if self.transformer.pre_seq_len is not None:
                past_length -= self.transformer.pre_seq_len
            inputs.position_ids += past_length
            attention_mask = inputs.attention_mask
            attention_mask = torch.cat((attention_mask.new_ones(1, past_length), attention_mask), dim=1)
            inputs['attention_mask'] = attention_mask
        history.append({"role": role, "content": query})
        for outputs in self.stream_generate(**inputs, past_key_values=past_key_values,
                                            eos_token_id=eos_token_id, return_past_key_values=return_past_key_values,
                                            **gen_kwargs):
            if return_past_key_values:
                outputs, past_key_values = outputs
            outputs = outputs.tolist()[0][len(inputs["input_ids"][0]):-1]
            response = tokenizer.decode(outputs)
            if response and response[-1] != "ï¿½":
                response, new_history = self.process_response(response, history)
                if return_past_key_values:
                    yield response, new_history, past_key_values
                else:
                    yield response, new_history

    @torch.inference_mode()
    def stream_generate(
            self,
            input_ids,
            generation_config: Optional[GenerationConfig] = None,
            logits_processor: Optional[LogitsProcessorList] = None,
            stopping_criteria: Optional[StoppingCriteriaList] = None,
            prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
            return_past_key_values=False,
            **kwargs,
    ):
        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]

        if generation_config is None:
            generation_config = self.generation_config
        generation_config = copy.deepcopy(generation_config)
        model_kwargs = generation_config.update(**kwargs)
        model_kwargs["use_cache"] = generation_config.use_cache
        bos_token_id, eos_token_id = generation_config.bos_token_id, generation_config.eos_token_id

        if isinstance(eos_token_id, int):
            eos_token_id = [eos_token_id]
        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None

        has_default_max_length = kwargs.get("max_length") is None and generation_config.max_length is not None
        if has_default_max_length and generation_config.max_new_tokens is None:
            warnings.warn(
                f"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. "
                "This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we"
                " recommend using `max_new_tokens` to control the maximum length of the generation.",
                UserWarning,
            )
        elif generation_config.max_new_tokens is not None:
            generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length
            if not has_default_max_length:
                logger.warn(
                    f"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(="
                    f"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. "
                    "Please refer to the documentation for more information. "
                    "(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)",
                    UserWarning,
                )

        if input_ids_seq_length >= generation_config.max_length:
            input_ids_string = "decoder_input_ids" if self.config.is_encoder_decoder else "input_ids"
            logger.warning(
                f"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to"
                f" {generation_config.max_length}. This can lead to unexpected behavior. You should consider"
                " increasing `max_new_tokens`."
            )

        # 2. Set generation parameters if not already defined
        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()

        logits_processor = self._get_logits_processor(
            generation_config=generation_config,
            input_ids_seq_length=input_ids_seq_length,
            encoder_input_ids=input_ids,
            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
            logits_processor=logits_processor,
        )

        stopping_criteria = self._get_stopping_criteria(
            generation_config=generation_config, stopping_criteria=stopping_criteria
        )
        logits_warper = self._get_logits_warper(generation_config)

        unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)
        scores = None
        while True:
            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
            # forward pass to get next token
            outputs = self(
                **model_inputs,
                return_dict=True,
                output_attentions=False,
                output_hidden_states=False,
            )

            next_token_logits = outputs.logits[:, -1, :]

            # pre-process distribution
            next_token_scores = logits_processor(input_ids, next_token_logits)
            next_token_scores = logits_warper(input_ids, next_token_scores)

            # sample
            probs = nn.functional.softmax(next_token_scores, dim=-1)
            if generation_config.do_sample:
                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
            else:
                next_tokens = torch.argmax(probs, dim=-1)
            # update generated ids, model inputs, and length for next step
            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
            model_kwargs = self._update_model_kwargs_for_generation(
                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder
            )
            unfinished_sequences = unfinished_sequences.mul(
                next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
            )
            if return_past_key_values:
                yield input_ids, outputs.past_key_values
            else:
                yield input_ids
            # stop when each sentence is finished, or if we exceed the maximum length
            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):
                break

    def quantize(self, bits: int, empty_init=False, device=None, **kwargs):
        if bits == 0:
            return

        from .quantization import quantize

        if self.quantized:
            logger.info("Already quantized.")
            return self

        self.quantized = True

        self.config.quantization_bit = bits

        self.transformer.encoder = quantize(self.transformer.encoder, bits, empty_init=empty_init, device=device,
                                            **kwargs)
        return self


class ChatGLMForSequenceClassification(ChatGLMPreTrainedModel):
    def __init__(self, config: ChatGLMConfig, empty_init=True, device=None):
        super().__init__(config)

        self.num_labels = config.num_labels
        self.transformer = ChatGLMModel(config, empty_init=empty_init, device=device)

        self.classifier_head = nn.Linear(config.hidden_size, config.num_labels, bias=True, dtype=torch.half)
        if config.classifier_dropout is not None:
            self.dropout = nn.Dropout(config.classifier_dropout)
        else:
            self.dropout = None
        self.config = config

        if self.config.quantization_bit:
            self.quantize(self.config.quantization_bit, empty_init=True)

    def forward(
            self,
            input_ids: Optional[torch.LongTensor] = None,
            position_ids: Optional[torch.LongTensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
            full_attention_mask: Optional[torch.Tensor] = None,
            past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
            inputs_embeds: Optional[torch.LongTensor] = None,
            labels: Optional[torch.LongTensor] = None,
            use_cache: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            return_dict: Optional[bool] = None,
    ) -> Union[Tuple[torch.Tensor, ...], SequenceClassifierOutputWithPast]:
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        transformer_outputs = self.transformer(
            input_ids=input_ids,
            position_ids=position_ids,
            attention_mask=attention_mask,
            full_attention_mask=full_attention_mask,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        hidden_states = transformer_outputs[0]
        pooled_hidden_states = hidden_states[-1]
        if self.dropout is not None:
            pooled_hidden_states = self.dropout(pooled_hidden_states)
        logits = self.classifier_head(pooled_hidden_states)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                if self.num_labels == 1:
                    loss = loss_fct(logits.squeeze().float(), labels.squeeze())
                else:
                    loss = loss_fct(logits.float(), labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels).float(), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits.float(), labels.view(-1, self.num_labels))

        if not return_dict:
            output = (logits,) + transformer_outputs[1:]
            return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )
```

### å…¶å®ƒ


#### é‡åŒ– quantization.py


## ChatGLM å¾®è°ƒ

ã€2023-6-2ã€‘ [chatglmçš„å¾®è°ƒæœ‰æ²¡æœ‰ä¿å§†å¼çš„æ•™ç¨‹](https://www.zhihu.com/question/595670355/answer/3038045480)

æ€ä¹ˆfinetuningï¼Ÿ
- `P-tuning v2`
  - [å®˜æ–¹ç¤ºä¾‹](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning)ï¼Œå•å¡3090æ²¡é—®é¢˜
  - P-Tuningï¼Œè®¾è®¡äº†ä¸€ç§è¿ç»­å¯å¾®çš„virtual tokenï¼ˆåŒPrefix-Tuningç±»ä¼¼ï¼‰
  - æ¸…åå¤§å­¦å‘å¸ƒçš„ä¸¤ç§å‚æ•°é«˜æ•ˆPromptå¾®è°ƒæ–¹æ³•ï¼š P-Tuningã€P-Tuning v2ï¼Œå¯ä»¥ç®€å•çš„å°†P-Tuningè®¤ä¸ºæ˜¯é’ˆå¯¹Prompt Tuningçš„æ”¹è¿›ï¼ŒP-Tuning v2è®¤ä¸ºæ˜¯é’ˆå¯¹Prefix Tuningçš„æ”¹è¿›ã€‚
- `Full parameter`
  - å…¨é‡å‚æ•°finetune, 8å¡3090 æ²¡è·‘èµ·æ¥
  - è®­ç»ƒèµ„æº: 8 x A100 40G æˆ–è€… 4 x A100 80G.
- `LoRA`
  - å®˜æ–¹æ¨èçš„å·¥ç¨‹: [æœ€ç®€å•ã€æœ€ä¾¿å®œçš„è®­ç»ƒthu-chatglm-6bæ¨¡å‹æ•™ç¨‹](https://github.com/yuanzhoulvpi2017/zero_nlp/tree/main/simple_thu_chatglm6b)
  - æˆ–è€…ï¼šä¸€ç§å¹³ä»·çš„chatgptå®ç°æ–¹æ¡ˆ, åŸºäºChatGLM-6B + LoRA, [ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning), æ•°æ®é›†: [alpaca](https://github.com/tatsu-lab/stanford_alpaca), æˆ–ç›´æ¥åœ¨[colab](https://colab.research.google.com/github/mymusise/ChatGLM-Tuning/blob/master/examples/finetune.ipynb)ä¸Šå°è¯•

### ChatGLM-Tuning

ã€2023-3-25ã€‘ChatGLM-Tuning
- ä¸€ç§å¹³ä»·çš„chatgptå®ç°æ–¹æ¡ˆï¼ŒåŸºäºæ¸…åçš„ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) + LoRA è¿›è¡Œfinetune.
- æ•°æ®é›†: [alpaca](https://github.com/tatsu-lab/stanford_alpaca)




# ç»“æŸ