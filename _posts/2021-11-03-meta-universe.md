---
layout: post
title:  "元宇宙-Meta-Universe"
date:   2021-11-03 09:10:00
categories: 新技术
tags: 元宇宙 扎克伯格 游戏 互联网 虚拟人 数字人 智能客服 强化学习 多模态 大模型
author : 鹤啸九天
excerpt: 元宇宙到底是个啥？2021年真的是元宇宙元年吗？
mathjax: true
permalink: /meta
---

* content
{:toc}



# 元宇宙

- [元宇宙是什么？以及我们对元宇宙未来的展望](https://www.bilibili.com/video/BV1BQ4y1f72i)
- <iframe src="//player.bilibili.com/player.html?aid=717037354&bvid=BV1BQ4y1f72i&cid=382613104&page=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>

- 【2021.10.29】[Facebook 在 11 分钟内透露了有关 元宇宙 的所有内容](https://www.bilibili.com/video/BV1Wu411o7Ee) CNET ，[youtube原视频](https://youtu.be/gElfIo6uw4g): 元宇宙确实有可能成为未来的一个趋势，代表着web3.0时代的一个符号，贯穿全文的核心就是Connection with people，人与人之间的联系。

<iframe src="//player.bilibili.com/player.html?aid=506352450&bvid=BV1Wu411o7Ee&cid=432560295&page=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>


- 【2022-1-13】[北大出品，最强元宇宙报告，200页干货](https://mp.weixin.qq.com/s/OuHXv30PCf_RoxHfdLCAzw)
- 【2021-10-31】[清华大学：2021元宇宙研究报告](https://www.163.com/dy/article/GNL04QMN0519KHAT.html)
- 【2022-1-28】Meta: You will own nothing, and you will be happy... 元宇宙：让你一无所有却难以自拔
  - ![](https://p3.toutiaoimg.com/large/tos-cn-i-qvj2lq49k0/cbd33023b2d342ab96037fa934106cb6)
- 【2022-10-26】[可视化数字人技术在 Soul 的应用](https://mp.weixin.qq.com/s/79toed_WdjFhdP6Enb0ITA),凭借自己的虚拟化身，基于自己的信息图谱或推荐，体验多样的沉浸式社交场景，在接近真实的体验中去交流娱乐，最终找到与自己志同道合的朋友建立社交关系。
- 【2023-5-9】周杰伦官宣数智人「周同学」，未来将举办元宇宙[演唱会](https://www.toutiao.com/w/1765376057469960), 周杰伦现身中国移动咪咕元宇宙总部，与中国移动达成元宇宙领域系列合作。在现场，周杰伦发布了中国移动与好莱坞特效团队 WETA 为其打造的超写实数智人「周同学」，并宣布「周同学」成为中国移动动感星推官、5G 元宇宙星际开拓官和 5G 视频彩铃推广大使。

## 数字人



### 数字人定义

什么是数字人？

首先要有三个要素
- 第一是具备人的**外观**；
- 第二是具备人的**行为**；
- 最后是具备人的**思想**。

为什么称为多模态呢？
- 数字人本身是AI集大成者，涉及视觉、音频、文本等多种模态

数字人作为AI能力集大成者，涉及计算机视觉、计算机图形学、语音处理、自然语言处理等技术，正在金融、政务、传媒、电商等领域应用越来越广。

数字人：
> 以数字形式存在于数字空间中，具有拟人或真人的外貌、行为和特点的虚拟人物，也称之为虚拟形象、数字虚拟人、虚拟数字人等。

数字人的核心技术主要包括计算机图形学、动作捕捉、图像渲染、AI等。数字人可以打造更完美的人设，为品牌带来正向价值。

互联网、金融、电商平台、消费品牌、汽车出行等领 域纷纷推出数字人，用于品牌营销、智能客服等方向。

从应用角度的数字人来分类，可以分为: `IP型数字人`（主要为3D数字人）和`服务型数字人`（真人分身）。

数字人的价值
- 数字人可以对外树立品牌形象，并进行流量经营；
- 对内可以提高效率，例如数字人分身直播带货，可以永久在线。

### 虚拟人历史

【2022-1-24】[虚拟偶像发展史：虚拟偶像，会有光明的未来吗？](https://www.163.com/news/article/FP9V5C3D00019OH3.html)

- 2007年作为世界首个真正意义上的虚拟偶像**初音未来**诞生, 奠定了虚拟偶像的养成型孵化模式。粉丝直接参与创造价值，并进行线上分享和传播
- 2012年，这种模式五年后，在中国本土化落地。洛天依的专属声库正式落地，我们迎来国内第一款虚拟形象到来。出道后，她的发展速度同样迅猛，留下了《权御天下》《刀剑如梦》等代表作，顺利登上湖南卫视、央视等主流舞台，取得了一定大众知名度。
- 2016年年末，虚拟主播绊爱亮相Youtube，凭借蠢萌设定和犀利画风圈粉众多。不同于初音未来、洛天依，绊爱除投稿视频内容外，还可以通过直播和用户进行即时交流，带来更深层的互动体验。
- 2018、2019年，在短短两三年时间中，虚拟主播队伍急速扩张，光在日本数量就已破万，在不同直播平台玩得风生水起，辉夜月、电脑少女小白等顶流代表强势崛起，共同开启“虚拟主播元年”。B站最先推出虚拟次元计划，孵化出国内初代虚拟UP主小希。其实仍是借鉴日本模式，没能展现出独特吸引力，市场反响平平。但随着新媒介渠道的拓展，技术不断的创新完善。国内有实力有技术的大厂阿里、百度、B站、虚谷未来科技纷纷进场。
- 2020年，虚拟偶像圈层耕耘多年的生态已臻成熟，早有全面爆发之势。随着短视频、电商直播的火热，虚拟偶像面临更广阔的大众舞台，在探索商业变现的无线可能中，也面临新的挑战。当下“虚拟偶像的江湖”仍在继续，且故事远比大众想象的精彩。

摆脱御宅文化的刻板标签，是虚拟偶像发展的一大趋势。在国内特定的文化土壤下，孵化出了一批极具本土属性的虚拟主播。
- 由蔡明老师扮演的白毛萝莉菜菜子，出道便迅速圈下40多万粉丝。她的首秀直播登上站内人气榜榜首，#蔡明 菜菜子#的微博话题空降热搜。除二次元粉，菜菜子成功撬动起用户增量市场，吸引了许多新用户围观、试吃。
  - ![](https://nimg.ws.126.net/?url=http%3A%2F%2Fcrawl.ws.126.net%2Ffaeab23464843aed50b2ed243d8a3185.png&thumbnail=660x2147483647&quality=80&type=jpg)
- 美妆垂直领域的IMMA酱，少儿教育领域的班长小艾都是通过差异化定位，独特识别的形象，在这条激烈赛道中黑马突围。班长小艾由数字王国旗下公司虚谷未来科技打造，定位是国内首位虚拟少儿阅读推广人。作为一名12岁的狮子座女生，小艾主要面向的是学前和小学低年级段的用户，通过分享学习、生活，引导和陪伴少儿健康成长
  - ![](https://nimg.ws.126.net/?url=http%3A%2F%2Fcrawl.ws.126.net%2F7c94e843684192fbe3dd3a77e93778b7.jpg&thumbnail=660x2147483647&quality=80&type=jpg)


### 数字人分类

数字人可以按照不同维度进行分类:
- **人物图形资源**的维度：2D和3D两大类
  - 外形上：2D真人、2D卡通、3D卡通、3D风格化、3D写实、3D超写实、3D高保真等多种。
- **技术驱动**的维度：真人驱动和AI驱动两种。
- **商业和功能**维度：内容/IP型、功能服务型和虚拟分身等三种。

![](https://picx.zhimg.com/v2-0e76a7f25bb9af7ab11dba7a4da4616f_1440w.jpg)

- 交互维度：交互型数字人和非交互型数字人。
  - 非交互型数字人：系统依据目标文本生成对应的人物语音及动画，并合成音视频呈现给用户。
  - 交互型数字人：根据驱动方式的不同，可分为智能驱动型和真人驱动型。


真人驱动 vs 算法驱动
- 智能驱动型数字人：通过智能系统自动读取并解析识别外界输入信息，根据解析结果决策数字人后续的输出文本，驱动人物模型生成相应的语音与动作来使数字人跟用户互动。这种人物模型是预先通过AI技术训练得到，可通过文本驱动生成语音和对应动画，业内将此模型称为TTSA(Text To Speech & Animation)人物模型。
- 真人驱动型数字人：真人根据视频监控系统传来的用户视频，与用户实时语音，同时通过动作捕捉采集系统将真人的表情、动作呈现在虚拟数字人形象上，从而与用户进行交互。

![](https://pic4.zhimg.com/v2-b70429816ef0fb336bb9bb0adced8915_1440w.jpg)

从应用角度来看，分为：服务型数字人、身份型数字人。
- 服务型数字人员具有功能性，可以取代真人进行服务，完成内容生产和一些简单的工作，降低现有服务型行业的成本。代表应用场景是数字主播、数字老师。
- 身份型数字人具有身份性，主要是虚拟的IP或者偶像呈现，可以为未来的虚拟化世界提供核心交互中介。代表应用场景是数字偶像。

【2025-8-6】[一文了解数字人](https://zhuanlan.zhihu.com/p/686813995)

### 案例


#### 方案

【2025-3-26】用大模型创建指定IP数字人视频，如 李白
- [DeepSeek](https://chat.deepseek.com/) 生成指定IP的提问
- 即梦创建数字人图像: 「即梦 AI」，[网页版](https://jimeng.jianying.com/ai-tool/image/generate)
- 讯飞星火创建数字分身
  - 讯飞星火点击“+”创建数字分身
  - 上传IP人物照片
- 发布智能体

更多[参考](https://mp.weixin.qq.com/s/-J6hGv82h0L5eKZDZ6Hijw)

#### nick 个人

【2024-5-22】b站: [我不是真人](https://www.bilibili.com/video/BV14t421M7Qc/?spm_id_from=333.999.0.0)

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1754631510&bvid=BV14t421M7Qc&cid=1544862927&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"></iframe>

#### 虚拟客服

[“黑科技”出乎所料，百度终端虚拟人3.0重磅发布](https://new.qq.com/omn/20210402/20210402A0DZSY00.html)

终端虚拟人3.0-虚拟客服智能大屏产品示意图
- ![](https://inews.gtimg.com/newsapp_bt/0/13367913184/1000)

2019年，通过百度智能云与浦发银行联合打造的金融数字人“小浦”就已亮相。百度基于人工智能、感知技术、数据驱动等技术，为金融服务构造了一个具有智能感知、自然交互和精准分析决策能力的数字世界的“人”。 作为国内首个银行“虚拟员工”，“小浦”具备3D形象，表情、动作、语气、语调逼真，不仅能实现视觉、听觉多感官的实时交流互动，还能利用深度学习技术学习专业领域知识，帮助员工完成标准业务流程。
- ![](https://inews.gtimg.com/newsapp_bt/0/13367915051/1000)

【2021-12-28】深圳地铁20号线应用虚拟人客服，[深圳地铁“新线路”今天开通，首次无人驾驶](https://new.qq.com/omn/20211228/20211228A0C1K100.html)
- 不仅列车全自动驾驶，连客服中心也换了，现在，坐地铁想要任何帮助，找智能客服中心的“小姐姐”没有错。补票、充值、问路...只要开口问，她就会非常温柔地回答你。害羞的朋友当然也可以直接点击选择指令。“刷脸过闸”、“刷手进闸”
- ![](https://inews.gtimg.com/newsapp_bt/0/14362548062/1000)
- ![](https://inews.gtimg.com/newsapp_match/0/14362548066/0)


#### 百度

百度智能云，app端，云屏幕、电话端、小度屏
- APP端
  - 数字智能客服：提供基于数字人形象的智能客服，配合图文/卡片/点选按钮等组件提供更高效的客户服务，有效提升客户体验。
  - 数字理财经理：基于金融知识库，数字人可化身数字理财经理，为用户提供财富体检，理财推荐等服务，有效提升用户的服务覆盖及转化。
  - 数字商品导购：基于商品知识库及语音交互能力，数字人可根据客户需求进行商品讲解及推荐，扮演数字商品导购员提供全新的线上购物体验。
  - 数字培训师：基于课程脚本，提供可与学员智能问答的数字人培训师，服务于培训、教学、演练等具体场景。
    - ![](https://bce.bdstatic.com/p3m/common-service/uploads/APP_6779ca6.JPG)
- 云屏
  - 数字展厅讲解员：让线下展厅大屏可以内置数字人服务，基于表情、动作、图文等多种展现形式，提供主动迎宾，展厅讲解、互动问答等能力，实现智能展厅的体验，有效提升展厅科技感。
  - 数字大堂经理：提供多种规格的数字人云屏，在线下营业厅等场景中提供数字大堂经理服务，可实现取号、叫号、咨询、业务办理等能力。
  - 数字前台：在酒旅场景中，提供多种规格的数字人云屏，打通已有的业务系统，顾客可在数字前台直接完成业务办理，有效提升效率、降低员工成本。
  - ![](https://bce.bdstatic.com/p3m/common-service/uploads/DP_6ce8e83.JPG)
- 电话端：视频彩铃，用户等待接通电话时可出现不同类型数字人营销视频，打造5G通话时代亮点。
  - ![](https://bce.bdstatic.com/p3m/common-service/uploads/DIANHUA_e88025a.JPG)
- 小度屏：陪伴数字人, 在小度屏中内置数字人助理，唤醒小度屏后与用户进行交流互动，可重点应用于儿童及老人陪伴等居家场景。
  - ![](https://bce.bdstatic.com/p3m/common-service/uploads/XDP_3f17497.PNG)


【2024-5-8】百度智能云推出新功能：文生数字人, [大模型卷爆数字人：一句话5分钟实现定制，跳舞主持带货都能hold住](https://mp.weixin.qq.com/s/E0S6i6dSNKWwAV9XtSmC7A)

清华大学《虚拟数字人研究报告2.0版》数据显示
- 从头部企业的布局来看，面向B端的数字人产品服务是市场的主要组成部分，占比达到79%。
- 而随着大模型技术对数字人应用模式的颠覆，不仅中小企业不用再对6位数的3D高精度数字人望而却步，C端的应用也将得以拓展。

#### 游戏角色

【2021-12-23】[江湖无常，寻味 AI 虚拟人物的执念与命运](https://rct.ai/zh-hans/blog/jianghu-days-pondering-the-destiny-of-ai-metabeings)
- 金庸先生在《天龙八部》中所说，一个人的一生诚然会很大程度上受**性格**影响，但也不完全由性格决定，多出来的那部分就是运，或者说是机遇，也就是人与环境发生交互的结果。
- `慕容复`想当西夏的驸马，但却未能如愿，这是**求而不得**；`虚竹`想当和尚，却反倒成了西夏驸马，这是**不求反得**。
- 无论是小说还是相关游戏，《天龙八部》中的人物角色几乎都怀着各自的执念与意图，体验着风起云涌的写意江湖。

《天龙八部 2》手游在 rct AI 自研的分布式 AI 平台 Deterrence 的加持下，玩家在游戏中的言行举止，不仅会让角色给玩家带来极具个性化的对话内容，还会通过对话内容直接影响到这些江湖儿女的行为方式，从而真正实现犹如鲜活生命般极具动态感的武侠世界。作为 Y Combinator W19 成员，rct AI 始终坚持运用 AIGC 打造真正的 Metaverse。在由 VentureBeat 和 InvestGame 发布的“全球游戏行业最活跃的风险投资基金”名单中榜首和次席的两只基金 Makers Fund 和 Galaxy Interactive 的支持下，rct AI 推出了全球首个分布式 AI 平台 Deterrence，现已成功接入多个由来自不同国家与地区的团队打造的数个 Metaverse。

[混沌球背后的核心技术](https://rct.ai/zh-hans/blog/the-key-technology-behind-morpheus-engine)

混沌球算法提升游戏交互体验
- 传统的叙事，无论是单线的故事，还是现在几乎所有的所谓 “交互式电影”，都仍然是基于 “**事件**” 作为叙事的基本单元，也就是什么事情发生了，然后什么事情发生了。传统的交互式数字娱乐内容，无非是让用户可以自由的从给定的两到三个选项中，选择不同的接下来会发生的事件，整个叙事仍然是基于预先定义好的路径来往前推进的。
- 而混沌球与传统的叙事方式完全不同，我们将 “事件” 替换为一个又一个明确定义了入口和出口的**黑盒**，在每一个切片的混沌球里，开始和结局（一个或者多个）是确定的，但是玩家每一次如何从开始到达结局，则是**混沌**的，是路径不明确的。这个路径只有当玩家不断的和虚拟世界里的虚拟人物 NPC 作出交互，这些 NPC 根据深度强化学习训练后的模型作出动态且实时的反应来推动剧情发展之后，才会被确定下来。这也是我们为什么命名为**混沌球**算法的原因。因此，做到真正的交互式叙事的关键，在于将叙事的中心，从故事本身，转移到故事里的所有可能参与者身上，由所有可能参与者的逻辑来共同推动和串联不同的剧情可能性。
- ![](https://rct.ai/static/images/88e5ceea1dd64e12803b3e411adf6e23.png)

仿真引擎工作方式
- ![](https://rct.ai/static/images/af5afdef214a4fe18d1a96f1dfea50b7.png)

#### 北京昌平

【2022-8-3】[真香现场，昌平区首个虚拟代言人“昌小平”上线！](https://www.sohu.com/a/573798721_121106842)
- ![](https://p6.itc.cn/q_70/images03/20220803/2275bc60ceb541fa9a1f0fe9566fd2d6.gif)
- “我们主要通过真人动作采集，全身实时动作捕捉系统，将人物动作完整映射到虚拟人的骨骼上，实现‘昌小平’的动作驱动。”二六三网络通信股份有限公司设计部总监李小刚告诉记者，虽然“昌小平”体现为一个数字人，但其背后有着一整套基于人工智能为主的现代科技力量。
- 基于元宇宙创作背景，在场景设计中，技术团队融合了昌平地图关键元素、长城和回天大脑数字场景，然后将数字人、场景、动作输入到虚幻引擎进行合成。目前虚幻引擎是全球最先进的实时3D创作工具，能全方位的拟真化处理，让“昌小平”更生动也更具表现力。
- “昌小平”由昌平区经信局总指导，为了呈现更加自然、真实的效果，“昌小平”背后的技术团队——二六三网络通信股份有限公司对动作、神态、声音、语调采集后，通过精准的处理和深度学习的算法，赋予其极强的智能体系。
- “整个项目中最核心的是需要解决实时的音视频互动。”据李小刚介绍，在制作过程中，“昌小平”的收信动效非常复杂，技术团队通过在虚幻引擎下不断的仿真调优、反复模拟，最终解决了音视频和动作最自然的匹配，实现了元宇宙“沉浸感、拟真感、低延迟”三大特点。
- “‘昌小平’是回天大脑2.0升级过程中的一个灵感，最初设想是通过生动的方式介绍昌平和回天大脑的相关情况，加强与社区、居民的互动。”昌平回天大脑联合工作组成员龙慧告诉记者，下一步，将运用人工智能技术，让“昌小平”更加“聪明”，实现线上线下大会直播、语音交互等，成为昌平突破次元壁与场景和观众、听众链接的新纽带，助力企业元宇宙应用。

#### oppo小布助手

【2022-6-22】[万玉龙：OPPO小布数字人的多场景应用实践](https://mp.weixin.qq.com/s/9ehS_Ndf5YVremHUC6fBTw)，[ppt资料](https://tool.lu/deck/z4/detail?slide=8)
- OPPO小布助手是国内首个月活破亿的手机语音助手，截止2021年12月，已累计覆盖2.5亿设备，月活用户突破1.3亿，月交互次数突破20亿。2021年9月底，小布助手发布了首个数字人形象，成为业界首个基于虚拟人多模态交互的手机智能助手，融合语音、NLP、视觉等多模态AI算法，结合端云一体的工程架构设计，为用户提供了多终端、全场景的智能交互体验。
- 小布助手是OPPO、OnePlus和Realme三个品牌的手机和IoT设备的内置智能助手，目前已经拥有语音、建议、指令、识屏和扫一扫五大能力。我们致力于将小布打造为“机智、有趣、温暖”的智能助手，为亿万用户提供“多设备、跨平台和多场景”的智慧服务体验。
- ![](https://www.xmtvip.com/img.php?imgUrl=https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPjMGMoLSXKLIVaunwib7tRCnzgNyuEk3tVoxsOibDaZwuFN6jmic4ThagHhCZ0ZUomaszcrv5icmUicVsQ/640?wx_fmt=png)


#### 创业公司

【2023-2-28】[AIGC助力“数字人自由”，新华智云靠什么？](https://mp.weixin.qq.com/s/7owRLtwvg6Rrvy4KP7Ifaw), 新华智云是国内较早投身数字人的科技公司之一，2019年便试水数字人，开创了新闻领域实时音频与AI真人形象合成的先河。2020年的地方两会报道中，有7省的两会报道使用新华智云虚拟主播。截至2023年2月，超过500家媒体、政府机构、金融机构、会议会展企业使用新华智云虚拟主播。得益于多年MGC（机器生产内容）和数字人技术储备，新华智云如今已具备AI全链路生产数字人的能力，AI训练出的数字，不仅外形逼真，类型多元，而且生产速度不断加快，成本日渐降低，应用场景越发全面。

基于深度学习模型、动作模拟、情感模拟等智能科技，只需采集2-5分钟的真人视频，AI最快训练1小时，即可生成形象逼真、表情到位、口型匹配的数字分身。
相比过去，现在数字人面部精细度更高，发音和口型更精准，肢体动作更自然，而且数据采集时间缩短75%，训练速度更是提高约100倍。越接近真人外形的数字人，越能提供更亲切、自然、高效的服务体验，让人产生信任。因此，2D仿真类数字人往往适合社交、媒体、金融、电商直播、教育等需要“多交流”“高互动”的场景。数字人可以“扮演”主持人、新闻主播、金融客服、导购员、讲解员、直播博主、老师等角色。在新华智云数字人制作平台中输入文字，一段由数字人播报和讲解的视频，就可以快速生成。


#### 柳叶熙

B站爆火数字人ip `柳叶熙`

柳叶熙：[数字红娘](https://www.bilibili.com/video/BV1kF41167hH)

<iframe src="//player.bilibili.com/player.html?aid=275585638&bvid=BV1kF41167hH&cid=1257563362&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>



#### imma

【2024-5-7】日本数字人[imma](https://aww.tokyo/vhuman/imma/), 覆盖各个主流平台 



### 数字人工具


#### 汇总

主流
- 剪映数字人、腾讯智影、synthesia、蝉镜、魔珐有言
- 有2D有3D，种类繁多。


数字人工具要点：[摘要](https://zhuanlan.zhihu.com/p/19953235955)

|名称|地址|介绍|优点|缺点|图|
|----|----|----|----|----|---|
|剪映数字人|剪映/Capcut|剪映软件一部分，输入文字，选择数字人，设置背景、景别和音色后可一键生成|使用简单，容易上手|数字人数量有限，无法进一步编辑形象细节，定制步骤复杂，语音不够自然，动作易重复，生成的视频无法调整，适用场景较少|![](https://pic3.zhimg.com/v2-139d255e2a1a0c7dc61ba50ea2ca3f8e_1440w.jpg)|
|腾讯智影|[智影](https://zenvideo.qq.com/)|多种风格的2D、3D数字人形象，可根据用户描述自动生成数字人形象，提供多种音色，集成视频编辑功能|数字人形象多样，可自动生成，语音能根据文本情感基调调整，综合性强|数字人形象不能自定义，动作种类单一、易重复，复杂动作生硬不自然，场景少，数字人与背景融合有“抠图感”，复杂场景表现欠佳|![](https://pic1.zhimg.com/v2-d4a62c4d2a296fc1a14a74ff35f5f22c_1440w.jpg)|
|Synthesia|[Synthesia](https://www.synthesia.io/)|数字人极具专业性，适合**企业商务演讲**、**产品介绍**场景，输入文本脚本可生成视频，可更改文字一键更新视频，添加字幕、音乐等|肢体语言丰富恰当，语音合成支持多种语言，能满足企业大规模、多样化视频制作需求|订阅费用较高，数字人形象多为外国人，适合海外数字人内容，对预算有限的入门创作者不友好|![](https://pica.zhimg.com/v2-0bd80ffa7b09b1253698a332007b5d4a_1440w.jpg) |
|蝉镜数字人|[蝉镜](https://www.chanjing.cc/refc/)|适合国内使用的2D数字人，内置模板丰富，输入文案可用内置数字人模板生成视频，支持形象定制，上传几分钟视频可克隆本人真身|内置模板丰富，支持形象定制|未提及| ![](https://pic2.zhimg.com/v2-a725bcbda02fa0f3471ded4b13bde565_1440w.jpg) |
|魔珐有言|[魔珐有言](https://www.youyan3d.com/)|专注于3D数字人，有上千个角色库，可自定义编辑3D数字人的五官、妆容、服装、配饰等，提供几百个专业级3D场景，能一站式完成视频创作，输入文字脚本或AI自动生成脚本可生成视频，生成后可多次调整|数字人可高度自定义，表情动作生动，语音风格多样、支持多语言，场景丰富专业，能自动生成专业镜头和运镜，可一站式完成视频创作，视频生成后可多次调整，适用多场景|未提及| ![](https://pic1.zhimg.com/v2-b892ff7884ebb4017adaf2613a05c6e2_1440w.jpg) |


数字人工具对比：
- 【2025-5-20】参考 [好用的Github开源AI数字人项目推荐](https://promptchoose.com/ai-tools/github-open-sources-ai-digital-human/)

| **项目名称** | **技术特点** | **应用场景** | **社区支持** | **学习门槛** | **多语言支持** | **性能需求** | **优点** | **缺点** |
|--------------|--------------------------------------------------------------------------|--------------------------------------|--------------|--------------|----------------|--------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **[EchoMimicV2](https://github.com/antgroup/echomimic_v2)** | 半身人体动画、音频-姿势同步、支持中英文、加速推理（9倍提速） | 虚拟主播、跨语言内容创作、影视动画 | 有（Gradio demo、Hugging Face模型） | 中（需CUDA 11.7+、手动配置环境） | 支持中英 | 高（A100/RTX4090D等高端GPU） | 画面质量高、跨语言兼容好、推理速度快，适合专业级半身动画生成 | 仅支持半身动画，对硬件要求较高 |
| **[hallo](https://github.com/fudan-generative-vision/hallo)** | 实时对话、 hierarchical 音频驱动合成、支持多模态输入 | 虚拟助手、虚拟偶像、实时交互场景 | 活跃（社区贡献Windows/ComfyUI支持） | 中（需手动下载多模型，依赖FFmpeg） | 仅英文（训练数据限制） | 中（A100 GPU） | 支持自定义训练、集成多种工具（如Whisper音频处理），适合个性化虚拟人搭建 | 暂不支持中文音频驱动，需科学上网下载模型 |
| **[AniPortrait](https://github.com/Zejun-Yang/AniPortrait)** | 单图生成动态头像、音频驱动表情、支持vid2vid面部重现 | 游戏角色、虚拟主播、社交媒体动态头像 | 有（Gradio demo、Hugging Face空间） | 中（需配置PyTorch 3D、多模型路径管理） | 未明确提及 | 中（CUDA 11.7+，支持A100） | 擅长面部表情细节还原，提供pose控制和加速推理选项，适合精细化表情动画 | 对输入图片角度要求高（需正面人脸），中文文档较少 |
| **[MOFA-Video](https://github.com/MyNiuuu/MOFA-Video)** | 3D人脸重建、轨迹+关键点混合控制、基于扩散模型 | 影视特效、VR/AR虚拟人、可控动画生成 | 有（ECCV 2024论文支持、Gradio demo） | 高（需理解扩散模型原理、复杂参数调节） | 未明确提及 | 高（CUDA 11.7+，依赖SVD_Xtend等模型） | 支持多模态控制（轨迹+音频+视频驱动），适合高保真面部动画 | 代码结构复杂，新手难以快速上手 |
| **[Fay](https://github.com/xszyou/Fay)** | 全栈开源框架、支持2.5D/3D数字人、多终端适配（单片机/App/网页） | 直播带货、在线教育、企业服务机器人 | 有（飞书文档、QQ群） | 中（需配置多模块，支持Docker部署） | 支持中文（自定义知识库） | 灵活（可部署于CPU/GPU，依赖LLM性能） | 低耦合设计，可自由替换ASR/TTS/LLM模块，适合企业级集成 | 核心功能需二次开发，界面交互较简陋 |
| **[duix.ai](https://github.com/GuijiAI/duix.ai)** | 移动端轻量级SDK、实时交互、低资源占用 | 手机应用、智能屏、移动客服机器人 | 有限（文档较简略，依赖官方支持） | 低（提供Android/iOS SDK，一键集成） | 支持中文（预设中文虚拟人模型） | 低（普通智能手机/边缘设备） | 专为移动端优化，支持离线运行，适合非技术用户快速部署 | 功能较基础，高度定制需付费服务 |
| **[Thin-Plate-Spline-Motion-Model](https://github.com/yoyo-nb/Thin-Plate-Spline-Motion-Model)** | 薄板样条变换、关键点插值、支持视频重建 | 学术研究、趣味动画生成、图像动态化实验 | 有（CVPR 2022论文支持、Colab demo） | 中（需理解数学模型，依赖PyTorch） | 未明确提及 | 中（Python 3.9+，支持GPU加速） | 原理透明，适合研究人员复现经典算法，提供详细训练流程 | 动画效果偏基础，缺乏实时交互功能 |
| **[FaceSwap](https://github.com/deepfakes/faceswap)** | 高精度人脸替换、支持视频/图像批量处理、多模型可选 | 影视特效、深度伪造检测研究、趣味换脸 | 活跃（论坛、Discord、大量第三方教程） | 高（需训练自定义模型，依赖TensorFlow） | 未明确提及 | 中高（需现代GPU，支持CUDA/ROCm） | 社区资源丰富，提供GUI工具，适合专业换脸需求 | 伦理争议大，官方强调禁止非授权使用 |
| **[DeepFaceLive](https://github.com/iperov/DeepFaceLive)** | 实时人脸替换、低延迟、支持直播/视频会议 | 实时直播、虚拟会议、娱乐化换脸 | 有（Discord、QQ群、预训练模型库） | 低（提供一键安装包，支持Windows/Linux） | 未明确提及 | 中（DirectX 12兼容显卡，推荐RTX 2070+） | 实时性强，对硬件要求友好，适合快速部署虚拟形象 | 换脸效果偏娱乐化，缺乏精细化控制选项 |
| **[Avatarify](https://github.com/alievk/avatarify)** | 实时面部表情迁移、StyleGAN生成虚拟头像、支持视频流输入 | 视频会议、虚拟主播、实时表情互动 | 有（Slack社区、Colab在线 demo） | 中（需配置Python环境，依赖PyTorch） | 未明确提及 | 中（需GPU支持，推荐NVIDIA显卡） | 表情捕捉细腻，支持生成不存在的虚拟人物，适合创意场景 | 对网络要求高（部分功能需联网下载模型），中文支持不足 |


#### 国外MetaHuman

- 【2022-1-6】元宇宙虚拟人物建模工具 [MetaHuman Creator](https://www.unrealengine.com/zh-CN/metahuman-creator) （源自3D创作公司[unrealengine](https://www.unrealengine.com/zh-CN/)）是一款基于云服务的应用，能帮助任何人在几分钟内创建照片级逼真的数字人类。韩国姑娘失业后通过此工具玩游戏主播，成立公司。[使用示例](https://www.ixigua.com/7047138696230931747)
  - ![](https://i0.hdslb.com/bfs/article/4dffc65ff80a035e9ffcbaa827da4f1683faa533.png@942w_531h_progressive.webp)
  - [分分钟打造超写实角色！MetaHuman这个黑科技怎么用？](http://www.gamelook.com.cn/2021/05/441678)
  - 渲染条件：一台可访问互联网的Windows或macOS计算机，以及一个Chrome、Edge（Chromium）、火狐或Safari网页浏览器。你还需要一个Epic Games账户。要下载MetaHuman则需要安装免费的虚拟现实引擎：[Quixel Bridge应用](https://quixel.com/bridge)。


#### 百度


##### 百度智能云

【2021-08-24】[简单、快捷、低成本的超写实虚拟人平台来了……](https://ai.baidu.com/support/news?action=detail&id=2575)

新发布的百度智能云“数字明星运营平台与灿星计划”，有可能就是这样的平台，它正是朝着让超写实虚拟人开发，变得更简单快捷、更低成本、更易运营这一方向去的。
- ![](https://baidu-ai-ar-1512380202189-8487.bj.bcebos.com/%E6%95%B0%E5%AD%97%E6%98%8E%E6%98%9F%E8%BF%90%E8%90%A5%E5%B9%B3%E5%8F%B0.mp4)
- [视频](https://baidu-ai-ar-1512380202189-8487.bj.bcebos.com/%E6%95%B0%E5%AD%97%E6%98%8E%E6%98%9F%E8%BF%90%E8%90%A5%E5%B9%B3%E5%8F%B0.mp4)

<video width="620" height="440" controls="controls" autoplay="autoplay">
  <source src="https://baidu-ai-ar-1512380202189-8487.bj.bcebos.com/%E6%95%B0%E5%AD%97%E6%98%8E%E6%98%9F%E8%BF%90%E8%90%A5%E5%B9%B3%E5%8F%B0.mp4" type="video/mp4" />
</video>

百度“数字明星运营平台”能做到： [百度智能云曦灵-智能数字人平台](https://cloud.baidu.com/product/baidudigitalhuman.html)
- 用 AI 智能的方式 快速打造数字虚拟偶像 
  - 打造的虚拟偶像既有 3D 超写实的、直接在真人基础上生成的，也有半写实或二维的、宠物化的，都能做到高效、快速生成。而且是一体化智能平台，既能打造 IP 也能运营 IP，和我期望的新平台非常接近。
  - 8月18号，和央视新闻联合举办的“百度世界2021”上，已经展示了几个典范案例，包括将央视著名主持人、真人明星龚俊都变成了数字虚拟人，以及 Q 版的火星车数字人等，展现了各种可能和场景。
- 用简单快捷的 4D 扫描+人工智能  直接打造超写实虚拟人；已有超写实虚拟人开发实在是太麻烦了：
  - 高精度原画设计；
  - 高写实 3D 建模；
  - 导入高水准的虚拟人引擎；
  - 高精度动捕；
  - 高精致的修帧渲染；
-  只需高精度照片或摄像头 就能生成超写实虚拟人

![](https://bce.bdstatic.com/p3m/common-service/uploads/JGT14_2c33b95.JPG)

开发过程：
- 第一步，用高精度 4D 扫描技术，对人体进行全方位扫描，捕捉真人的细腻表现。
- 第二步，用人工智能机器深度学习扫描数据，并用人工智能算法组合驱动丰富真实的实时面部表现，从而超高精度还原真人模特。
  - ![](https://ai.bdstatic.com/file/AD3144C562E7463889E2D41C8FAC72BB)

<video width="620" height="440" controls="controls" autoplay="autoplay">
  <source src="https://baidu-ai-ar-1512380202189-8487.bj.bcebos.com/%E5%8A%A8%E6%80%81%E7%B2%BE%E6%A8%A1.mp4" type="video/mp4" />
</video>

- 第三步，设定五个关键维度，即表情、口型、毛发、布料、身型，来打造超写实虚拟明星，尤其是加入了表情可变的口型合成算法，让用户观感更亲切自然，彻底解决口型和表情失真的问题。

- （1）真人
- （2）简单快捷办法：用标准人像**照片**或**摄像头**来生成。这种方法只需要高精度照片或视频。
- （3）比前两种方式更极致简单的方式：让普通人可以用一句话的简单描述，即可自动生成个性化 3D 虚拟数字形象。

兼容导入已有 3D/2D 模型
- ![](https://ai.bdstatic.com/file/E4FE8A6A05374A718D34482F12FAAD2F)
三种驱动模式：AI 驱动、拟合驱动和真人驱动，让你既可以不需要真人直接用 AI 智能驱动，也可以用真人在背后进行“中之人”演绎。
- ![](https://ai.bdstatic.com/file/9FCF771D426449B2A55330A8E13C559E)


##### Hallo

Hallo 由百度联合复旦大学、苏黎世联邦理工学院和南京大学共同研发的一款开源数字人项目，在音频驱动的肖像动画生成方面取得了显著进展。

开源数字人项目 Hallo
- [官网](https://fudan-generative-vision.github.io/hallo/)
- 论文 [Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation](https://arxiv.org/pdf/2406.08801)
- github: [hallo](https://github.com/fudan-generative-vision/hallo)
- 启动整合包[下载](https://pan.quark.cn/s/455b24f79fc6)


【2024-6-26】hallo 用于**人像图像动画**的分层**音频驱动视觉合成**的项目，在使用语音音频输入的驱动下，人像图像动画领域在生成逼真和动态的人像。
- 采用了端到端**扩散**范式，并引入了分层音频驱动的视觉合成模块，以提高音频输入和视觉输出之间的对齐精度，包括嘴唇、表情和姿势运动。
- 无缝集成了基于扩散的生成模型、基于UNet的降噪器、时间对齐技术和参考网络。
- 所提出的分层音频驱动的视觉合成提供了对表情和姿势多样性的自适应控制，从而实现了针对不同身份的更有效的个性化。
- 在图像和视频质量、唇形同步精度和运动多样性方面取得了明显的提高

此外，该项目支持与 ComfyUI 工具集成

软件功能：
- 音频同步视频：利用先进的音频分析技术，将语音与肖像图像完美结合，生成动态的面部动画，实现逼真的唇动同步效果。
- 面部表情生成：根据音频信号中的情感和语调变化，自动生成相应的面部表情，增强视频动画的表现力。
- 头部姿态控制：支持调整视频中的头部姿态，使动画更加自然，更好地反映音频内容的意图和情感。
- 时间一致性维护：确保动画中的动作和表情在时间上流畅过渡，避免不自然的变化。
- 动作多样性：支持生成多样化的动作和风格，如手势、眨眼等，丰富视频的表现力。

![](https://cdn.aondata.work/img/best_visual_results.jpg)

详见: [AI数字人项目-hallo一键启动整合包](https://noisevip.cn/18358.html)

配置：[部署方法](https://aiyy.info/hallo/)
- 操作系统：Windows 10/11 64位
- 显卡：至少12G显存的英伟达（NVIDIA）显卡（建议16G及以上，否则生成速度太慢）

效果
- [百度开源数字人项目Hallo，效果炸裂！](https://www.bilibili.com/video/BV1HS411A7xz)

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1905907943&bvid=BV1HS411A7xz&cid=1585823533&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"></iframe>


#### ER-NeRF


【2024-4-3】[实时流式数字人，代码开源](https://zhuanlan.zhihu.com/p/675131165)

目前数字人模型效果最好的是ernerf，其借鉴了nerf体渲染的思路，在输入维度上添加了音频特征，通过音频来影响渲染效果（控制嘴型）。 

开源代码[metahuman-stream](https://github.com/lipku/metahuman-stream)基于ernerf模型实现了实时流式数字人
- 支持**声音克隆**
- 支持**大模型对话**: LLM模型支持 Chatgpt,Qwen和GeminiPro。要在app.py中填入自己的api_key。
- 支持多种音频特征驱动：wav2vec、hubert
  - hubert提取音频特征
- 支持全身视频拼接

![](https://github.com/lipku/metahuman-stream/raw/main/assets/dataflow.png)

总体流程
- text输入可以来自websocket，实现数字人播报输入文字。
- 可接入chatgpt，将chatgpt的回答做为text输入，这样就实现了数字人实时对话效果。
- tts采用的免费edge tts，这个延时有点大。可以换成商用的tts模块，并且加入声音克隆，这样数字人效果更逼真。
- 提取音频特征（音频转embedding）用的wav2vec模型，相应的ernerf模型训练时也要用wav2vec来提取音频特征
- ernerf模型在这里面用的推理流程，根据输入的音频特征输出对应嘴型的image
- 用python做rtmp推流网上没什么现成的库，大部分都是启动ffmpeg命令行进程来实现，但在有音视频同时推流时这种方式行不通。这块卡了比较长时间，最后还是用c++调用ffmpeg api函数来实现rtmp推流

在没有text输入时，通过输入全0的音频来控制嘴型不动，并且保持视频的连续。

性能分析
- 帧率
  - 在Tesla T4显卡上测试整体fps为18左右，如果去掉音视频编码推流，帧率在20左右。用4090显卡可以达到40多帧/秒。
- 优化：新开一个线程运行音视频编码推流
- 延时: 整体延时5s多
  - （1）tts延时2s左右，目前用的edgetts，需要将每句话转完后一次性输入，可以优化tts改成流式输入
  - （2）wav2vec延时1s多，需要缓存50帧音频做计算，可以通过-m设置context_size来减少延时
  - （3）srs转发延时，设置srs服务器减少缓冲延时。


第三方效果展示 [metahuman-stream](https://github.com/lipku/metahuman-stream)
- 实时互动数字人musetalk效果, [b站](https://www.bilibili.com/video/BV1PM4m1y7Q2/)


<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1304657234&bvid=BV1PM4m1y7Q2&cid=1543054375&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  height="600" width="100%"></iframe>


【2024-4-24】 
- MacOS 部署失败,缺乏GPU
- Ubuntu GPU 部署：docker 启动失败

```sh
# 运行rtmpserver (srs)
docker run --rm -it -p 1935:1935 -p 1985:1985 -p 8080:8080 registry.cn-hangzhou.aliyuncs.com/ossrs/srs:5
# 启动数字人：
python app.py
# 如果访问不了huggingface，在运行前
export HF_ENDPOINT=https://hf-mirror.com
# 运行成功后，用vlc访问rtmp://serverip/live/livestream
# 用浏览器打开 http://serverip:8010/echo.html, 在文本框输入任意文字，提交。数字人播报该段文字
```


#### 腾讯 MuseTalk

腾讯音乐娱乐实验室开源了一个名为 MuseTalk 模型, 实时高品质唇形同步模型。
- 参考 [腾讯MuseTalk：实时音唇同步虚拟数字人模型](https://mp.weixin.qq.com/s/3hyVcBGBjiOGDk26yoodEg), 包含各种示例
- 【2024-4-27】[腾讯开源的数字人MuseTalk到底行不行？](https://mp.weixin.qq.com/s/M4AEOR2xBMHtrojZrvmkow)

MuseTalk 可与输入视频一起使用，例如由 MuseV 生成的视频，作为完整的虚拟数字人人解决方案。
- [腾讯MuseV：无限长度和高保真虚拟人视频生成，ComfyUI使用指南](https://mp.weixin.qq.com/s?__biz=MjM5NTM1NDcyOQ==&mid=2651625826&idx=1&sn=a5a76754c96b3946255ca8a7be7c8c09&scene=21#wechat_redirect)

MuseTalk 是一个**实时高品质**音频驱动的唇形同步模型，在 ft-mse-vae 的潜在空间中进行训练

该模型：
1. 能够根据输入的音频修改未知的面部动作，面部区域大小为 256 x 256。
2. 支持中文、英文和日文等多种语言的音频。
3. 在 NVIDIA Tesla V100 上支持超过 30fps 的实时推理。
4. 支持修改面部区域中心点，这对生成结果有 显著 影响。
5. 在 HDTF 数据集上训练的模型checkpoint。

关于 MuseV 和 MuseTalk 结合作为虚拟人生成的完整解决方案。建议首先使用 MuseV 生成一个视频（文本到视频、图像到视频或姿态到视频）。建议使用帧插值以增加帧率。然后，可以使用 MuseTalk 生成一个音唇同步视频。

MuseTalk 在潜在空间中进行训练，其中图像由冻结的 VAE 编码，音频由冻结的 whisper-tiny 模型编码。
- 生成网络的架构借鉴了 stable-diffusion-v1-4 的 UNet，其中音频嵌入通过交叉注意力与图像嵌入融合。

注：尽管MuseTalk使用的架构与 Stable Diffusion 非常相似，但 MuseTalk 的独特之处在于它不是一个扩散模型。相反，MuseTalk 是通过在潜在空间中单步修复来操作。

MuseTalk
- [在线使用](https://www.mindtechassist.com)，[数字分身效果展示及体验](https://www.mindtechassist.com/%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB(%E9%80%9A%E7%94%A8%E7%89%88))
- [MuseTalk GitHub](https://github.com/TMElyralab/MuseTalk)
- [ComfyUI-MuseV](https://github.com/chaojie/ComfyUI-MuseTalk)

场景
- 图片数字人：让照片开口说话
- 真人视频根据语音换嘴型

效果虽然还不是很满意，但总比 wav2lip 好

腾讯选择的路子： **对口型**， 死磕对口型，如果一个视频中人物的口型能对上，通过GPT产生文本，通过TTS产生声音。视频对口型。那么数字人技术就解决了！

MuseV自动化产生短视频和MuseTalk给视频中的人物对口型的功能，共同构成了一个完整的数字人视频解决方案。
- 1） 首先生产 无限长度视频
  - 源码：[musev](http://www.gitpp.com/museai/musev)
- 2）对口型
  - 源码：[musetalk](http://www.gitpp.com/museai/musetalk)



第三方效果展示 [metahuman-stream](https://github.com/lipku/metahuman-stream)
- 实时互动数字人musetalk效果, [b站](https://www.bilibili.com/video/BV1gm421N7vQ)

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1605146957&bvid=BV1gm421N7vQ&cid=1560200163&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"></iframe>



#### wav2lip


第三方效果展示 [metahuman-stream](https://github.com/lipku/metahuman-stream)
- wav2lip实时互动数字人实时互动数字人效果, [b站](https://www.bilibili.com/video/BV1Bw4m1e74P/)

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1105937791&bvid=BV1Bw4m1e74P&cid=1584848028&p=1&autoplay=0" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  height="600" width="100%"></iframe>



#### 本地部署

【2024-6-14】[小模型数字人，本地推理实时驱动，不用训练，不用GPU](https://www.bilibili.com/video/BV18z421b7FQ)





#### DUIX.ai


【2024-6-24】[开源的数字人AI女友，妈妈，我不要做舔狗了](https://mp.weixin.qq.com/s/xZmj4cinufJ4LJ183sD0PA), 开源数字人效果

AI女友类的数字人，很多大厂都做了类似软件，不过大多只上架了海外。

`硅基智能`推出的 DUIX.ai 让数字人技术变得触手可及。
- 不仅开源了数字人技术，还开源了 iOS 和安卓的手机应用 UI。
- 你可以稍作修改，然后提交上架赚钱了。

[DUIX.ai](DUIX.ai) 是一个开源的数字人平台。无论想做个 AI 男/女朋友、直播带货，还是生成短视频，DUIX.ai 都能满足你的需求。
- DUIX.ai 提供了一堆帅哥美女的开源数字人模型供大家下载使用, 免费,不用自己再花钱去制作数字人模型。

DUIX.ai
- 超级易用：无需复杂的技术知识，在 Android、iOS 甚至车载系统上都能一键部署。
- 性能强悍：50 帧/秒的流畅画面，比那个啥还要丝滑！
- 真人级表现：精准模拟人类动作、唇形和微表情，让你的数字人栩栩如生。
- 开放生态：可以自由接入各种大模型、语音识别和合成能力，打造专属数字人。
- 丰富模板：多个现成的数字人模板任你挑选，想要哪个就用哪个！

应用场景有哪些？
- 做个 AI 伴侣陪你聊天？没问题！
- 搞个数字主播带货？轻松搞定！
- 批量生成营销视频？小菜一碟！



#### YinMei

【2024-7-7】[AI 虚拟主播 AI-YinMei](https://mp.weixin.qq.com/s/h6kWY6U8IrVURtYN2zOVxg)

项目简介
- 支持 fastgpt 知识库聊天对话
- 支持 LLM 大语言模型的一整套解决方案：[fastgpt] + [one-api] + [Xinference]
- 支持对接 bilibili 直播间弹幕回复和进入直播间欢迎语
- 支持微软 edge-tts 语音合成
- 支持 Bert-VITS2 语音合成
- 支持 GPT-SoVITS 语音合成
- 支持表情控制 Vtuber Studio
- 支持绘画 stable-diffusion-webui 输出 OBS 直播间
- 支持绘画图片鉴黄 public-NSFW-y-distinguish
- 支持搜索和搜图服务 duckduckgo（需要魔法上网）
- 支持搜图服务 baidu 搜图（不需要魔法上网）
- 支持 AI 回复聊天框【html 插件】
- 支持 AI 唱歌 Auto-Convert-Music
- 支持歌单【html 插件】
- 支持跳舞功能
- 支持表情视频播放
- 支持摸摸头动作
- 支持砸礼物动作
- 支持唱歌自动启动伴舞功能
- 聊天和唱歌自动循环摇摆动作
- 支持多场景切换、背景音乐切换、白天黑夜自动切换场景
- 支持开放性唱歌和绘画，让 AI 自动判断内容
- 支持流式聊天，提速 LLM 回复与语音合成
- 对接 bilibili 开放平台弹幕【稳定性高】
- 支持 funasr 阿里语音识别系统


#### ReSyncer

【2024-8-6】 清华、南洋理工发布 最好的数字人 ReSyncer
- [主页](https://guanjz20.github.io/projects/ReSyncer/)
- [ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer](https://arxiv.org/pdf/2408.03284)
- ![](https://guanjz20.github.io/projects/ReSyncer/pipeline.png)

<iframe width="560" height="315" src="https://www.youtube.com/embed/ayyJSmv4Nzo?si=0dCmv0xKIdF9ebfl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


#### EchoMimic

【2024-7-11】蚂蚁金服 推出数字人 [EchoMimic](https://github.com/BadToBest/EchoMimic) 
- 论文 [EchoMimic: Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditioning]()

Python Environment Setup
- Tested System Environment: Centos 7.2/Ubuntu 22.04, Cuda >= 11.7
- Tested GPUs: A100(80G) / RTX4090D (24G) / V100(16G)
- Tested Python Version: 3.8 / 3.10 / 3.11

测试

```sh
# Download the Codes
git clone https://github.com/BadToBest/EchoMimic
cd EchoMimic
```


#### MetaStudio


【2023-8-11】[华为云MetaStudio多模态数字人进展及挑战介绍](https://mp.weixin.qq.com/s/0H6QKa1H8SN3TfABDhjNSw)

华为云 [MetaStudio](https://activity.huaweicloud.com/metastudio-szr.html) 在数字人领域当前的主要进展，包括： 2D数字人驱动、3D数字人建模、绑定、驱动、情感数字人生成等，同时介绍数字人领域的一些挑战。
- 真人形象1:1复刻，原声克隆，支持中英文，让每个人都拥有个性化的数字人

华为云在媒体领域的架构：
- 底层算力算子包括昇腾芯片，同时也兼容N卡；CPU有鲲鹏，也兼容x86。
- AI框架平台包括训练平台 ModelArts、深度学习框架MindSpore以及TensorFlow和Pytorch等。
- 基于ModelArts，我们有训练加速引擎和推理加速引擎等。
- 再往上一层是媒体引擎和盘古基础大模型。接着是媒体服务，包括云桌面、远程写作平台、数字人生产线等。

总体来讲，华为云可以提供数字人领域从底层到上层全栈服务。

三大服务：
- 数字人视频**制作**：无需拍摄，通过输入文字生成视频。
- 数字人视频**直播**：一天24小时不停直播带货，用算力换人力并超越人力，真正实现不受地域限制、不受模特资源限制的、不受语种限制的全球全天候直播。
- 数字人视频**交互**：结合ChatGPT等对话机器人，可以实现实时智能交互，通用问答、垂直知识库等都能解决，可用于虚拟讲师辅导等。

华为云提供全栈的数字人解决方案。主要包括`IP型数字人`和`服务型分身数字人`全方位的解决方案。在底层会提供包括建模、驱动、仿真、渲染等能力，并基于这些能力开放一些API，让联合伙伴可以根据行业进行应用。
- IP型3D数字人的照片建模、语音驱动、视频驱动等
- 分身数字人的形象训练、视频制作等；
- 同时还包括数字人资产管理相关的服务。

文本生成数字人，避免了绿幕录制的繁琐以及肖像权的争议，同时还可以随时通过文本进行数字人形象调整。

语音驱动单照片分身数字人的原理。
- 输入一张照片和语音，输出视频，首先通过`wav2lip`做预训练基础，再通过动作迁移的方式，把后台预制的视频迁移到照片上。

语音驱动分身情感数字人。除了中性表情之外，实现数字人积极和消极表情的输出。
- 整体逻辑是首先生成中性表情的数字人，再逐帧进行情绪编辑，另外引入牙齿生成模块控制牙齿清晰度。

华为3D数字人目前可实现单照片美型建模、单照片卡通建模以及光笼扫描写实数字人建模。
- 美型建模的技术流程是：输入人像图片后，会进行人像证件化的预处理，然后进行形状建模，再进行配件组装和皮肤生成，最终输出完整的3D模型。
- 传统超写实3D数字人建模，即3D分身。其成本非常高，传统方法需要光笼扫描，再进行几何重建，需要大量的人工参与，所以华为云在探索如何进行通过AI的方式加速这一流程。

目的就是为了让伙伴和开发者可以快速集成华为云的底层API接口

不同场景下数字人效果：
- 和真人对比，真假难辨；
- 同时支持移动场景，即可实现可走动的分身数字人；
- 基于分身数字人可以制作数字人名片，更加亲切。
- 一次训练，实现多语种驱动。通过录制5分钟中文演讲视频，就可以生成分身数字人，用于多语种的视频生成。

#### 【2024-10-*】TANGO

图片数字人：LivePortrait、AniPortrait、MuseTalk、EchoMimic等

核心原理： 通过 姿态/音频等 驱动单张图片，生成对应的视频。

上述图片数字人，称为 `2D 真人`，这些数字人偏娱乐属性，无法做到代替真人。而要想代替真人出镜，实现数字人口播，就必须上2.5D真人。

实时视频数字人方案 Ultralight Digital Human, 类似的2.5D真人开源项目还有 dh_live，实现流程基本一致，区别在底层模型不同，比如 dh_live 中，音频编码用的 LSTM 模型，生成模型用的DINet。

但上述2.5D真人方案的缺陷是：只能实现**对口型**，无法生成和音频协调的肢体动作！全身动作生成，一直都是急需攻克的难题。

即便是付费方案 Heygen，也只能实现面部表情和上半身动作生成。直到看到一个开源项目 - TANGO，瞄准了全身动作生成这一更具挑战性的目标。

【2024-10-*】TANGO 由东京大学和 CyberAgent AI Lab 联合开发，技术路径非常独特。
- 主页 [TANGO](https://pantomatrix.github.io/TANGO/)
- github [TANGO](https://github.com/CyberAgentAILab/TANGO)

项目亮点：
- ![](ttps://pic1.zhimg.com/50/v2-1021fc48e089ed25a6ee9e622c1bbf80_720w.jpg)

对照论文中的流程图，TANGO 核心技术：
- 图结构表示：节点代表视频帧；边表示帧之间的转换。子图检索: 可根据目标音频的时序特征，检索最佳的视频播放路径子集。
- AuMoCLIP：分层音频运动嵌入，通过对比学习创建一个隐式的层次化音频-动作联合嵌入空间，能够捕捉更细微的音频-动作关系，从而生成更自然、更流畅的动作序列；
- ACInterp：扩散插值网络，在现有扩散模型之上，参考运动模块确保生成的动作与参考视频保持一致。

![](https://pica.zhimg.com/80/v2-2fa7984fcd8d6db2d522819818dd68e9_720w.webp?source=1def8aca)

注意
- 非实时、非流式、非可交互

本地部署
- 至少确保 35G 磁盘空间，用于存放模型；
- 至少确保 6G 显存，用于模型推理

```sh
# 因项目依赖 wav2lip 和 FILM，因此在 TANGO 目录下把这两个项目也要拉取下来。git clone https://github.com/CyberAgentAILab/TANGO
cd TANGO
git clone https://github.com/justinjohn0306/Wav2Lip.git
git clone https://github.com/dajes/frame-interpolation-pytorch.git
# 然后，参考项目主页，安装好依赖后，即可一键启动 WebUI:
python app.py
# 项目默认会生成带有 TANGO 水印的视频,调用本地的 ffmpeg 将原视频和水印图片合成了新视频
# 不想要水印，可修改app.py中：
gr.Video(value="./datasets/cached_audio/demo1.mp4", label="Demo 0", , watermark="./datasets/watermark.png")
# 修改为
gr.Video(value="./datasets/cached_audio/demo1.mp4", label="Demo 0")
# 非本地主机可访问，需修改：
demo.launch(server_name="0.0.0.0", server_port=7860)
# 最终生成的视频没有音频，需要手动把音频合成进去：
/usr/bin/ffmpeg -i outputs/gradio/test_0/xxx.mp4 -i gen_audio.wav -c:v libx264 -c:a aac result_wav.mp4

```


#### Ultralight

【2024-10-29】[Ultralight Digital Human： 第一款完全开源的实时视频数字人](https://mp.weixin.qq.com/s/ZjytjhNOS6cmalqHA9-0VQ)
- 源码地址：[Ultralight-Digital-Human](https://github.com/anliyuan/Ultralight-Digital-Human)

一个能在移动设备上实时运行的数字人模型，第一个开源的如此**轻量级**的数字人模型


#### 京东 JoyHallo

【2024-10-14】[京东开源普通话数字人JoyHallo，一口流利标准普通话还会讲英语](https://mp.weixin.qq.com/s/nkuTPr4HWJJq2opiBwg09A)

音频驱动的视频生成领域，制作普通话视频面临着许多挑战。
- 首先，收集全面的普通话数据集非常困难；
- 其次，普通话的复杂口型动作使得模型训练比英语更具挑战性。

为了解决这个问题，JoyHallo 收集了来自`京东健康`国际公司员工的29小时普通话语音视频，形成了 `jdh-Hallo` 数据集。
- 这个数据集中包含了不同年龄和说话风格的人，涵盖了日常对话和专业医疗主题。

现有模型如 AnimateAnyone 和 Hallo 在英语视频生成方面表现优异，但由于缺乏高质量的**普通话数据集**以及普通话复杂的**唇部动作**，它们在普通话生成中表现不佳。

为了解决这一问题，引入了 JoyHallo 模型
- 采用了半解耦结构，以提高普通话唇部动作预测的准确性。
- 为了让 JoyHallo 模型适应普通话，采用了中文的 wav2vec2 模型来提取音频特征。

同时，提出了一种半解耦结构，旨在捕捉嘴唇、表情和姿态特征之间的关系。这种集成不仅提高了信息的利用效率，还加快了推理速度，提升了 14.3%。

JoyHallo 在生成英语视频方面依然表现出色，显示了其优秀的跨语言生成能力。


#### HeyGem


[HeyGem](heygem.ai)

本地部署
- [HeyGem 本地训练生成数字人化身](https://zhuanlan.zhihu.com/p/30347869667)

硬件要求
- CPU: 推荐 13代 Intel Core i5-13400F 或更高
- 内存: 32GB
- 显卡: NVIDIA RTX 4070 或同等性能显卡
- 硬盘空间:
- D盘: 至少30GB可用空间（用于存储数字人和项目数据）
- C盘: 至少100GB可用空间（用于存储服务镜像文件）

软件要求
- Windows 10 19042.1526 或更高版本
- NVIDIA显卡驱动（[下载链接](https://www.nvidia.cn/drivers/lookup/)）
- Node.js 18(Node.js )
- Docker ([下载链接](https://www.docker.com/products/docker-desktop/))


#### OmniTalker

【2025-4-9】[阿里 OmniTalker 开源：让文本『一开口』就声情并茂](https://zhuanlan.zhihu.com/p/1893081112558950019)

只需输入一段文字，就能立刻生成一个数字人，不仅口型与声音完美同步，连说话的风格、面部表情都能模仿得惟妙惟肖

现有方法
- 先把文字转成语音（TTS）
- 再根据语音驱动数字人的口型和表情。

这种“分步走”方式像一个蹩脚的翻译，不仅效率低、延迟高，还常常导致声音和画面对不上号（音画不同步），或者说话的语气和脸上的表情完全不搭（风格不匹配）。

这让生成的数字人看起来总有点“假”，缺乏灵魂。

阿里推出的 开源项目 OmniTalker 解决方案，最大的亮点在于采用了**端到端**的统一框架。
- 【2025-4-3】论文 [OmniTalker: Real-Time Text-Driven Talking Head Generation with In-Context Audio-Visual Style Replication](https://arxiv.org/pdf/2504.02433)
- Project Page: [omnitalker](https://humanaigc.github.io/omnitalker)
- 不再搞“分工合作”那一套，而是用一个更强大的“大脑”同时处理文本、生成语音、驱动视频。
- 当 OmniTalker “阅读”文本时，同时构思“该怎么说”（生成语音特征）和“该做什么表情、口型”（生成面部动态和头部姿态）。通过一个巧妙设计的音视频融合模块，声音和画面的信息还能相互“沟通”，确保最终输出时，口型对得上声音，表情配得上语气。
- ![](https://pica.zhimg.com/v2-9678816d6bc799de764e9b2a02cc0d12_1440w.jpg)

OmniTalker 另一个让人惊艳的功能是风格复制
- 只需要看小段目标人物说话的视频（参考视频），就能“学”会这个人的语音风格（比如语速、音调、口音）和面部风格（比如习惯性的微表情、头部动作）。而且，这是零样本（Zero-Shot）
- ![](https://pica.zhimg.com/v2-249939786b5e1074e220ca6db749fbda_1440w.jpg)

实验对比

tts 方法
- CosyVoice
- MaskGCT
- F5-TTS

声音驱动的数字人方法对比
- SadTalker
- AniTalker
- StyleTalk
- EchoMimic
- Hallo


效果: 快、轻、稳、准
- 速度够快：OmniTalker 的推理速度达到了 25 帧/秒 (FPS)，这意味着它可以实时生成视频内容，满足直播、实时交互等场景的需求。
- 模型轻巧：整个模型的参数量大约在 0.8B (8亿) 左右，这在当今动辄百亿参数的大模型时代，算得上是相对“轻量级”了，部署起来也更方便。
- 多语言、多情感：目前支持中英文的文本输入和转换，并且能够生成带有不同情感（如平静、开心、悲伤、愤怒等）的视频，让数字人的表达更丰富、更真实。
- 长视频也没问题：它还能生成较长时间的连续视频，对于虚拟主播、在线教育课程制作这类需要持续输出内容的场景非常友好。
- ![](https://pic4.zhimg.com/v2-95f4417e4461683ca23a0fe0898dcd73_1440w.jpg)


#### 【2025-7-16】Grok ani

【2025-7-16】复刻老马 Grok ani 

Github 开源贝拉 (Bella)：你的数字伴侣，正在唤醒
- 地址：[Bella](github.com/Jackywine/Bella)

项目作者@jackywine ，Bella 形象由豆包生成，演示视频由 Demoget （demoget.com/?atp=xGLCA4）制作



### 表情迁移

- 【2022-1-24】iphone 手机自带标签迁移功能，imessage里可以通过摄像头捕捉表情，并应用到动画形象上。个性化拟人表情：选取肤色、头饰、眼镜等等
  - ![](https://help.apple.com/assets/61606EE5D7F26F422E7EB450/61606EEAD7F26F422E7EB46F/zh_CN/c98ffa684190ba59c66227ff40372418.png)
  - [官方指南](https://support.apple.com/zh-cn/guide/iphone/iph37b0bfe7b/ios)
- 【2022-3-18】asr与面部表情同步生成，Talking Head, 通过语音信号和一张常见的2d真人照片或者卡通图片，来生成一段与输入音频同步的人脸动画视频.
  - 论文：[MakeItTalk: Speaker-Aware Talking-Head Animation](https://zhuanlan.zhihu.com/p/410570384)
  - ![](https://pic1.zhimg.com/80/v2-7f4b2961268cb1ba600f34600b3af920_1440w.jpg)

## 评论

### 负面

[知乎](https://zhuanlan.zhihu.com/p/414519905), 元宇宙就是互联网进入瓶颈后找的一个概念而已，说白了还是互联网游戏。人生是逃脱不了的；但游戏可以，一个账户玩的不想要了还能换，能换也是能追求补偿的方式，元宇宙里可以吗？元宇宙里大公司用AI创作赚钱，用脚本执行任务赚钱（如挖比特币）。元宇宙里马太效应只会更加明显。人怎么样都无法脱离现实生活，物质满足才有了精神需求，试问在有绘画、歌唱、钓鱼等一系列现实活动可选的情况下谁愿意去虚拟中体验？

【2022-1-14】元宇宙有两种基本粒子，一种是**傻子**，另一种是**骗子**，傻子不断围绕骗子高速旋转，这就形成了元宇宙的物质基础——元子，在外界高能概念的激发下，傻子跃升到高能态，经过一段时间后再跃迁为低能态，并同时向外界寄出**票子**。

### 正面




## 元宇宙简介

【2021-9-9】[最近大火的「元宇宙」是什么？](https://cloud.tencent.com/developer/article/1875296)

如果问当下最火的概念是什么，那必然是**元宇宙**。
 
元宇宙到底有多火，对互联网行业有多重要？从 Facebook 创始人兼首席执行官马克·扎克伯格近日的一段采访中可窥知一二。在 The Verge 的专访里，这家世界最大的社交平台掌舵者表示：希望在未来用 5 年左右的时间，将 Facebook 打造为一家元宇宙公司。
 
元宇宙概念的火爆还体现在，今年的 ChinaJoy 上有关元宇宙的发言屡见报端、连芯片巨头英伟达也忍不住“蹭热点”，等等。
 
那么问题来了，元宇宙到底是什么？我们离它还有多远？
 
### 什么是元宇宙？
 
`元宇宙`的英语是 Metaverse，Meta 表示“超越”、“元”， verse 表示“宇宙 universe”。
 
这个概念最早出现在 1992 年尼尔·斯蒂芬森的科幻小说《雪崩》当中，小说描绘了一个平行于现实世界的虚拟数字世界，在这里，人们用数字化身来控制并相互竞争以提高自己的地位，到现在看来，描述的还是超前的未来世界。
- ![](https://ask.qcloudimg.com/http-save/170434/319fcd583f1c8dc2f9f31a12c2e25ad3.jpeg?imageView2/2/w/1620)
 
2018 年斯皮尔伯格导演的科幻电影《头号玩家》，被认为是目前最符合《雪崩》中描述的“元宇宙”形态。在电影中，男主角带上 VR 头盔后，瞬间就能进入自己设计的另一个极其逼真的虚拟游戏世界——“绿洲”（Oasis）。在《头号玩家》设定的“绿洲”场景里，有一个完整运行的虚拟社会形态，包含各行各业的无数数字内容、数字产品等，虚拟人格可以在其中进行价值交换。
 
如果说这些“元宇宙”都还存在于小说和电影中，那么在今年 3 月被称作“元宇宙”第一股的 Roblox 成功在纽交所上市，则似乎意味着这个虚拟世界想走向现实。
 
这也被外界认为是今年元宇宙概念爆发的起点。
- ![](https://ask.qcloudimg.com/http-save/170434/f54f6cc2f80ffb7a9ad1cf4d6b3f5f6d.jpeg?imageView2/2/w/1620)
 
Roblox 成立于 2004 年，是一家在线游戏创作社区公司。2011 年上线 iOS，2014 年上线 Android。2019 年，Roblox 的社区玩家 MAU 过亿，累计有千万名创作者使用过 Roblox 提供的工具来开发游戏。相比于其他大多数游戏，Roblox 中的游戏能够自己定义角色，同时着重满足玩家社交需求的设计，也拥有一套游戏内的经济系统。
 
在 Roblox 的招股书里，对元宇宙有更具体的描述，这家公司认为一个真正的元宇宙产品应该拥有 8 个属性：身份、朋友、沉浸感、低延迟、多元化、随地、经济系统和文明。
 
具体属性解释如下：
*   身份：拥有一个虚拟身份，无论与现实身份有没有相关性。
*   朋友：在元宇宙当中拥有朋友，可以社交，无论在现实中是否认识。
*   沉浸感：能够沉浸在元宇宙的体验当中，忽略其他的一切。
*   低延迟：元宇宙中的一切都是同步发生的，没有异步性或延迟性。
*   多元化：元宇宙提供多种丰富内容，包括玩法、道具、美术素材等。
*   随地：可以使用任何设备登录元宇宙，随时随地沉浸其中。
*   经济系统：与任何复杂的大型游戏一样，元宇宙应该有自己的经济系统。
*   文明：元宇宙应该是一种虚拟的文明。
 
![](https://ask.qcloudimg.com/http-save/yehe-3889679/bfabf73460393aca9feaaf08796f39a7.png?imageView2/2/w/1620)

### 元宇宙，到底依赖哪些技术？

[“元宇宙” Metaverse 火了，这玩意到底是啥？](https://cloud.tencent.com/developer/article/1868182?from=article.detail.1875296)

话说回来，1992年就提出的元宇宙，时隔29年，怎么突然就火了呢？很简单，**时机**问题。天马行空的想象，提早了就是疯子，把握住时机的，才是天才。

如今，之所以元宇宙会火，和前几年VR/AR技术风靡不无关系。而且，云计算、芯片、5G和人工智能技术的高速发展，也刺激了元宇宙的概念复苏。

从技术的角度来看，我们（似乎）已经摸到了元宇宙的门槛，不再像以前那样遥不可及。

元宇宙的底层技术，具体包括哪些呢？

粗略来说，元宇宙包括了这么几个方面的技术：**芯片**技术、**网络通信**技术、**虚拟现实**技术（VR/AR/MR/XR）、**游戏**技术（游戏引擎、游戏代码、多媒体资源）、**AI**人工智能技术、**区块链**技术。

`元宇宙`是数字宇宙，是平行世界。所以说，元宇宙是建立在数字技术基础上的，和IT、CT技术密不可分。

要支持庞大的元宇宙运作，首先必须要有极其强大的算力和算法。
- 算力的根基是芯片，不用解释了吧？算法呢？是软件，更是长期的人才积累和生态经营。众人拾柴火焰高，只有更多的人才加入，投入更多的资源，才能一砖一瓦地把元宇宙搭建起来。

虚拟现实技术、游戏技术、AI人工智能，都属于IT范畴。站在CT通信的角度，元宇宙也是一个可以蹭的大热点。前面不是说“随地”嘛？没有通信，没有低延时的5G/6G/全光网，用户就不能接入元宇宙，只能在单机里自嗨，那还有啥意思？

最后还有一个区块链。区块链和元宇宙有啥关系？

当然有关系，前面不是说了元宇宙需要金融体系和虚拟货币吗？虚拟货币的背后，就是去中心化的区块链技术啊。不然你以为现在币圈的人为啥对元宇宙这么亢奋呢。目前国内出版的好几本元宇宙的书，作者都有币圈或链圈背景。

### 元宇宙概念为何在今年爆发？
 
![](https://ask.qcloudimg.com/http-save/170434/8fe22fb272b6fd42d6724a78b820ea70.jpeg?imageView2/2/w/1620)
 
Roblox 对元宇宙概念的阐述虽然更加具体，但要素众多，每个要素背后，还有着一连串的解释。这也说明这一概念的模糊性。
 
然而这不妨碍元宇宙成为一个好的故事。Roblox3 月份上市后，其市值达到 400 亿美元，相比 1 年前 40 亿美元的估值暴涨了 10 倍。App Annie 发布的全球热门游戏收入排名显示，7 月 Roblox 继续蝉联冠军宝座。\[4\]
 
恰恰是因为元宇宙概念目前没有一个简单、具体的定义，吸引互联网公司们进入这个赛道，以自己的方式和理解去塑造、定义元宇宙。
 
4 月中旬，全球知名的游戏公司 Epic Games 融资 10 亿美元用于“元宇宙”相关业务开发，创下“元宇宙”赛道最高融资纪录。5 月，苹果公司以 1 亿美元收购虚拟现实公司 NextVR，以增强其在娱乐和体育领域的 VR 实力。7 月 29 日，在 Facebook 的季度盈利数据发布后，马克·扎克伯格着重强调了“把 Facebook 转变为‘元宇宙’公司的雄心”，并激活虚拟社区计划 Horizon。8 月 12 日，英伟达自曝，在其 4 月举行的发布会上，CEO 黄仁勋的演讲中有 14 秒由数字合成的“假人”代为出镜，被网友认为是元宇宙的体现，但事实还相去甚远。
 
![](https://ask.qcloudimg.com/http-save/170434/deb4997652db823478d80328ed84e2eb.jpeg?imageView2/2/w/1620)
 
在国内，号称要打造全年龄段元宇宙世界的 MeteApp 公司，在 Roblox 上市后拿到了 SIG 海纳亚洲资本领投的 1 亿美元 C 轮融资。字节跳动对游戏引擎研发商、“中国版 Roblox”代码乾坤进行了近 1 亿人民币战略投资。
 
资本的追逐只是观察元宇宙的一个切片。冷静下来思考一个问题，为什么元宇宙会在 2021 年火爆？
 
在需求侧，疫情的发展不断蚕食减弱人们在物理世界的联系，也加速了数字世界的完善，人们在虚拟空间中留存和交互的时间更多，对虚拟世界的需求和服务更加开放和认可。而在技术侧，随着 VR/AR、5G、AI 等技术的发展，让曾经科幻小说中的场景一一实现，为元宇宙描绘出一个可见的触摸门槛的机会。
 
### 我们离元宇宙还有多远？
 
虽然元宇宙描绘的未来很美好，大量的优秀公司加入元宇宙的建设，但在很多业内人士看来，目前仍处于一个萌芽初始阶段。
 
以“元宇宙第一股”为例，Roblox 的游戏画面也较为简单，与元宇宙里“逼真的物理世界”相去甚远。
 
![](https://ask.qcloudimg.com/http-save/170434/a3bb84ea28c1f908f443ef95d6617711.jpeg?imageView2/2/w/1620)
 
华安证券在一份研报里表示，从产品形态上看，游戏是元宇宙的雏形，与元宇宙的成熟形态仍有较大差距。\[6\]
 
从雏形到成熟的进化，技术提升是必经之路。
- 首先，通过 AR、VR 等交互技术提升游戏的沉浸感。未来，基于 VR、AR 为代表的人机交互技术的发展，由更加拟真、高频的人机交互方式承载的虚拟开放世界游戏。
- 第二，通过 5G、云计算技术支撑大规模用户同时在线，提升游戏的可进入性。元 宇宙是大规模的参与式媒介，交互用户数量将达到亿级。5G 和云计算等底层技术的进 步和普及，是未来突破游戏可进入性限制的关键。
- 第三，通过算法、算力提升驱动渲染模式升级，提升游戏的可触达性。目前，3A 游戏采用传统的终端渲染模式，受限于个人计算机 [GPU](https://cloud.tencent.com/product/gpu?from=10680) 渲染能力，游戏的画面像素精细度距拟真效果仍有很大差距。
- 第四，通过区块链、AI 技术降低内容创作门槛，提升游戏的可延展性。目前游戏 UGC 创作领域编程门槛过高，创作的高定制化和易得性不可兼得，同时鲜有游戏具备闭环经济体。
 
技术只是第一关，要实现元宇宙还要迈过内容门槛，等真正成熟或将受市场、法律等因素的制约。
 
但作为真实物理世界补充和延展，元宇宙足够让人期待。让喜欢深度参与虚拟世界的人在数字场域遨游，喜欢现实物理世界的人在野外漫游，这或许就是元宇宙对于普通人的价值。
 
道阻且长，值得等待。
 
在搜集元宇宙的资料时，我们发现券商研究员、从业者、媒体等不同身份的人都表达了强调了元宇宙的重要性。华安证券在研报中表示“元宇宙是互联网的终极形态”；行业媒体竞核认为“Metaverse 是赋予技术生命能力的开始”；海外分析师表示，多个行业对元宇宙的投入将达到数万亿美元。
 
而透过元宇宙庞大的概念，看到其发展价值更为重要。研发工具 Beamable 公司创始人 Jon Radoff 于近期发文解析了元宇宙 Metaverse 的 7 层价值链，**体验**、**发现**、**创作者经济**、**空间计算**、**去中心化**、**人机交互**、**基础设施**。
- ![](https://ask.qcloudimg.com/http-save/170434/b9d14b449f696e13ba9a3cff855bac9f.jpeg?imageView2/2/w/1620)
 
其中体验层，Jon Radoff 认为元宇宙并不是 2D 或者 3D 形式，甚至都不一定是图形的形式存在，它更多的是物理空间、距离和物体之间不可阻挡的非物质化。比如，在一款游戏里，你可以梦想成为摇滚明星、绝地武士、赛车手或者任何能想象的角色。在物理空间举办的音乐会只能高价卖出前排的少数座位，但虚拟音乐会可以在每个人的周围产生一个个性化存在的平面，在这个平面上，你总能找到最好的座位。
 
这些提到的在线活动还涉及元宇宙体验的另一个方面：内容社区综合体。过去，消费者只是内容的消费者，现在，他们既是内容的创造者，又是内容的“放大器”，内容还可以再次产生内容。
 
发现层，体验催生出内容社区综合体的意义在于社区推动内容比大部分常规市场营销更高效。在元宇宙情境下，交换、交易、分享内容变得更容易而且更多元，这对所有创作者来说是增大曝光率的机会。
 
创作者经济，元宇宙里的体验会越来越现场化、社交化，并持续更新。到目前为止，元宇宙里的创作者都围绕 Roblox、Rec Room 和 Manticore 等集中式平台，在这些平台上，有一整套集成的工具、曝光率、社交网络和变现功能，赋予了许多人为其他人打造体验的能力。
- ![](https://ask.qcloudimg.com/http-save/170434/5e0dbdc330bd591794e425a37ec0992c.jpeg?imageView2/2/w/1620)
 
去中心化，虽然电影《头号玩家》里的绿洲被认为与元宇宙十分贴近，但元宇宙的理想结构与绿洲由单一团体控制的结构相反。这样避免了中心的统治地位，因为元宇宙是由很多人创造，因此也应该由很多人共同拥有。
 
综上，元宇宙的核心价值在于，它将成为一个拥有极致沉浸体验、丰富内容生态、超时空的社交体系、虚实交互的经济系统，能映射现实人类社会文明的超大型数字社区。
 
### 元宇宙将改变哪些行业？
 
从元宇宙的价值以及目前的技术情况来看，元宇宙最具现实意义的表现形式体现在泛娱乐行业，特别是游戏有望成为元宇宙概念下最早落地的场景。
 
目前市场上已经出现一系列基于游戏内核的沉浸式场景体验。美国著名歌手 Travis Scott 在游戏《堡垒之夜》中举办虚拟演唱会，全球 1230 万游戏玩家成为虚拟演唱会观众；加州大学伯克利分校在《Minecraft》重现校园，毕业生以虚拟形象线上场景参加毕业典礼；顶级 AI 学术会议 ACAI 在任天堂《动物森友会》上举行 20 年研讨会，演讲者在游戏中播放 PPT 发表讲话。
- ![](https://ask.qcloudimg.com/http-save/170434/0f94ee7991fe79067a9845c9e55d2af3.jpeg?imageView2/2/w/1620)
 
上文中我们提到，在线游戏创作社区 Roblox 因为现象级的内容创作生态带来的游戏自由度和用户活跃度，成为现阶段公认的元宇宙雏形。随着市场对元宇宙认识的加深，游戏之于元宇宙更大的意义在于提供展现方式，是元宇宙搭建虚拟世界的底层逻辑。
 
与此同时，元宇宙概念的火热也吸引了无数游戏厂商的入局。在今年的 ChinaJoy 上有业内人士认为，元宇宙或成中小游戏厂商新的创意阵地。这意味元宇宙为游戏行业带来了新机会，但这是否会引起内卷或使游戏主题趋于同质化，需要持续关注。
 
随着技术的不断成熟，元宇宙的下一发展阶段是在数字化的世界中去重构现实中的社交、消费等多个方面。
 
在社交领域，Facebook CEO 扎克伯格日前表示，Facebook 已经组建了专门研发元宇宙的团队，并表示未来五年要从社交公司变成元宇宙公司。而事实上 Facebook 布局的时间要更早。2014 年，Facebook 以 20 亿美元高价收购了虚拟现实公司 Oculus。2019 年，Facebook 发布了 VR 社交平台——Facebook Horizon。目前来看，Horizon 在相当程度上带有元宇宙的影子。比如在 Horizon 中，用户可以创建角色，和朋友聚会、娱乐，而且每个人都有可以定义自己的形象，建立自己的活动。
- ![](https://ask.qcloudimg.com/http-save/170434/35677d740c5e5a6e370c9b5597dbb00e.jpeg?imageView2/2/w/1620)
 
扎克伯格的想象中，元宇宙会像最初的定义那样，真正成为现实世界的虚拟映射，通过虚拟世界，能够将朋友或同事远程送到你身边，和你在虚拟环境中共处一室，获得更强的空间感。
 
社交 App Soul 此前在 IPO 时，也提出了“社交元宇宙”的概念。Soul 在招股书上表示，其之所以能向社交元宇宙靠拢，是因为自己创造的虚拟世界中，用户能够沉浸感强、始终在线地进行娱乐、社交、消费等，并且有用户创造大量的 UGC 内容，成为其快速持续生长的原动力。例如，在 Soul 中用户可以通过群聊派对讨论、听音乐、学习等，也可以 在 Soul 中玩狼人杀等游戏，甚至通过 Giftmoji 为自己或他人购买现实中的商品。
 
在消费领域，随着元宇宙的到来，用户的消费体验或将迎来新的一波交互体验的升级。目前，新氧已经实现为用户提供 AR 检测脸型的服务，通过手机扫描脸部推算出适合每位用户的妆容发型护肤品等。得物 App 的 AR 虚拟试鞋功能允许用户只需要挑选自己喜欢的鞋型和颜色并 AR 试穿，看到鞋子上脚的效果。在 AR、VR、可穿戴设备、触觉传感等技术的带动下，更加沉浸式的消费或将成为常态。它不局限于购买衣服、鞋子等基本消费，AR 房屋装修、远程看房、甚至模拟旅游景点都将成为可能。
 
### 元宇宙的法律和监管问题
 
元宇宙可能对现实世界与虚拟世界带来的变革足够让人欣喜，但对待一个新技术、新概念，提前思考其技术能力和伦理边界也必不可少。
 
未来当人们朝着元宇宙逐步迈进的时候，技术限制、金融服务监管、侵犯知识产权等诸多现实问题也将伴其同行。Norton Rose Fulbright 国际律师事务所对元宇宙可能会引发的法律和监管问题进行了设想。
- ![](https://ask.qcloudimg.com/http-save/170434/ed6eb9b605e91cf94c9d26c70657e050.jpeg?imageView2/2/w/1620)
 
首先是[数据安全](https://cloud.tencent.com/solution/data_protection?from=10680)。元宇宙涉及收集的个人数据的数量和种类丰富度将会是前所未有的，包括个人的生理反应、运动，甚至可能是脑波模式的信息。是否会有一个元宇宙的主要管理员收集并共享这些个人数据？如果用户的个人数据在元宇宙中被盗或被滥用，谁来负责？类目繁多的信息，应如何以及何时获取用户的同意？诸如此类问题，都是在元宇宙建设中应该考虑的。
 
其次是知识产权。元宇宙的内容创造是由多个人完成的，这就涉及到谁拥有知识产权的问题。共同著作权和共同所有权的规则本就是复杂的，在复杂的虚拟世界场景中，它们的应用将变得更加复杂，因为利益相关者极有可能会以团体的形式存在。同时，快速发展的元宇宙可能涉及元素的 “混搭”，以及将不同利益相关者拥有的知识产权结合在一起。知识产权许可中的传统风险分配以及使用条款将需要被重新审查。
 
此外，正如上文所说，元宇宙的一大价值在于创造新的营销形式，但如果营销的对象是儿童，如何获得许可成为问题。另外元宇宙的全球性和互操作性将不可避免地鼓励多个企业相互沟通和合作，以便为参与者提供更多的选择和更好的体验。但如果它们是竞争对手，元宇宙产品之间可能会引起反垄断问题。
 
虽然例举了元宇宙可能涉及的监管问题，但目前言之尚早。元宇宙带给人们更多是机会和期待，相信随着技术的推进，元宇宙的实现或将开启互联网的全真时代。

## 简介

2021年是元宇宙(MetaVerse)元年。堪比大航海时代的大迁徙，人类全面走进数字世界，开辟鸿蒙、创世而生。创造、生活、娱乐，乃至工作的数字时空，是为元宇宙。其中，需要重新思考**存在和虚无**、**肉体和精神**、**性善和性恶**、**自我和宇宙**的哲学命题，需要不断探索有限和无限、秩序与自由、自治与法治、经济与治理、伦理和文明的边界，需要全面融合区块链、AR、5G、大数据、人工智能、3D引擎等新技术，形成数字创造、数字资产、数字市场、数字货币、数字消费的新模式。

元宇宙是“心”的绽放，是“梦”的具象，是“我思故我在”的全息展现。内求于心，外形于物，物物相生，元宇宙成矣。

或许，<font color='red'>互联网的终极形态就是“元宇宙”。</font>

本报告共126页，包含理念篇、产业篇和风险篇共三大篇章：

- 一、理念篇：
  - 1、孕育元宇宙；
  - 2、元宇宙是什么？
  - 3、虚拟与现实的关系；
  - 4、元宇宙的理论框架；
  - 5、对元宇宙的几点展望。
- 二、产业篇：
  - 1、元宇宙的技术底座；
  - 2、元宇宙的产业生态；
  - 3、中美日韩元宇宙发展现状；
  - 4、面向企业的元宇宙；
  - 5、元宇宙指数体系。
- 三、风险篇：


## VR设备


### AR 眼镜


#### AR 眼镜评测

【2023-8-31】[2023年3款主流AR眼镜大横评，全网最真实的雷⻦Air Plus、Xreal Air、Rokid Max体验报告]()

3款AR眼镜 哪款更值得购买？
- 1、雷⻦Air Plus
- 2、Xreal Air
- 3、Rokid Max

最惊艳的还是雷⻦ Air Plus，尤其是在画面和声效的结合上，细腻的画面+绚丽色彩+震撼声效。

其它两款，虽然配置能跟得上需求，但在细节的表现上，反而就没有雷鸟Air Plus这样细节到位，大屏、清晰、震撼音效、畅玩游戏，加上丰富的生态支持，这不就是一款AR眼镜的理想型嘛~

眼镜+魔盒只要2000多，在家、出差、旅行都能用，对这种科技产品感兴趣的人来说，是一个非常有质价比的选择。



#### GPT-4+AR眼镜

【2023-3-27】[把GPT-4搞进AR眼镜，一秒生成回答内容，面试简直开挂好嘛](https://zhuanlan.zhihu.com/p/617347972)
- 斯坦福团队主要成员包括：Bryan Hau-Ping Chiang、Alix Cui和Varun Shenoy

GPT-4版AR眼镜 rizzGPT
- 基于OpenAI的自动语音识别工具Whisper收听对话，GPT-4聊天机器人实时生成自然响应，最终通过一款开源AR眼镜让用户在现实环境中了解对方的信息。
- 主体是一个圆镜片透明体，轻松挂在任何一款眼镜上面。
- ![](https://pic2.zhimg.com/80/v2-3dc5a6939497e90bc84101c843034931_1440w.webp)
- ![](https://pic3.zhimg.com/80/v2-cbdb0e3b76d355f0d93d2c35f6f2094e_1440w.webp)


#### 致敬未知

【2023-10-8】[对话吴德周：前华为系高管，老罗合伙人，开启AR创业新征程](https://mp.weixin.qq.com/s/054fnNNB_ub4fWu7YNQvFA)

扎根在智能硬件领域近二十多年的行业老兵吴德周, 完整的见证了智能手机市场的迅猛增长。从曾经的**华为荣耀**产品线总经理，到**锤子科技**合伙人，再到**字节跳动**的新石实验室负责人，吴德周职业生涯的每一步转变，都是在万众瞩目中做出的选择，他也是智能硬件领域关注度较高的高管之一。
- 2001年，刚刚从南京理工大学毕业的吴德周顺利进入华为，并且在短短三年时间内，就成为了部门黑马，并在2004年进入华为北京研究所研发手机。
- 在华为，他曾和团队研发了华为第一款手机U626、华为第一款滑盖机V810、华为第一款触摸屏手机U7510、华为第一款类智能机U7520、华为第一款美国运营商T-Mobile定制手机U7519、第一代荣耀手机U8860，第一款WindowsPhone W1等。其中，在担任过原华为荣耀产品线总经理，吴德周主导开发的第一代荣耀手机荣耀手机U8860，销量超100万台。
- 2016年，受锤子科技的创始人兼CEO罗永浩邀请，吴德周离开了华为，来到锤子科技担任COO、产品线和硬件研发副总裁等职位，主要负责锤子科技的产品线以及硬件研发，研发出Smartisan M系列、坚果Pro和坚果R系列产品。
- 2016年，当时的锤子科技CEO罗永浩也曾提过锤子科技要做VR，并随后展开VR团队的招聘，但显然这个项目后续并未浮现出什么水花。命运的齿轮转动多年后，罗永浩带领着其主攻AR OS系统的细红线和吴德周创办的致敬未知再次重逢在AR赛道，这也是颇有缘分的一件趣事
- 当锤子科技被字节跳动收购后，吴德周在和字节跳动创始人张一鸣深入交流后，也同意加入**新石实验室**。回顾那次谈话时，吴德周说：“当时，字节跳动计划将新石实验室作为字节跳动硬件的中台，并且作为相对对立的部门，拥有一定的产品决策权。”在新石实验室里，他也曾主导研发了几款智能教育硬件产品，这几款产品在行业内的成果不温不火。
- “双减”政策的重锤敲下后，**新石实验室**逐步被“边缘化”，并且当字节跳动尝试用互联网产品的思维来做硬件产品时，吴德周感受到了其中的违和感，并于2021年3月选择离开字节跳动。
- 令人意外的是，离开字节跳动后的吴德周，并未继续选择智能硬件赛道，而是选择加入一家小公司鲨纹科技担任全球CEO一职，研究过鲨纹物理抗菌技术方向，并认为医疗电子的小型化是未来重要的发展方向之一。
- 2022年8月11日，吴德周创办的`致敬未知`成立，这一年，他47岁，终于自己从0开始创办了一家AR眼镜创企。

致敬未知共有三位联合创始人，其中有两位是吴德周的好朋友。现有创业团队人员主要来自于两大部分。
- 一部分员工来自于此前曾和他参与到智能手机的团队成员
- 还有一部分成员是最早进入VR/AR市场的团队成员，这些颇有智能硬件经验的团队成员也加速了致敬未知产品研发的节奏。

致敬未知的研发人员占比达到80%。

2023年7月，致敬未知刚刚完成了自己的天使轮融资，获得1000万美金，折合人民币约7291万元，该轮融资由阿里领投。

这一次，选择AR赛道，是吴德周进行了充分的产业调研以及技术思考后得出的结果。“无论是智能手机，还是AR眼镜，本质上我们都是要打造让用户满意的产品。”
- 手机正在成为大家离不开的设备，而AR眼镜成为下一个计算平台，其市场将会比手机规模更大。
- 从智能手机转到XR赛道，吴德周明显感受到投资者看重的公司要点有所不同，由于智能手机是一个较为成熟的产业，因此投资者可能更看重出货量、市场份额等公司数据，但由于**XR行业**还处在刚刚起步的阶段，投资者则更看向产品设计、技术方向、团队研发实力等，投资方带来的战略资源共享也能为AR眼镜创企带来一定助力。
- 凭借着自己对智能硬件的理解，以及此前积累的产业经验，吴德周创办的致敬未知在成立不到一年时间里就拿到了由阿里领投的1000万美元融资，并成功推出了自家首款消费级AR眼镜`ARknovv A1`。
- ARknovv是AR+Know两个单词的组合，他认为现在人们能够看到的世界只有1/2，通过AR眼镜实现虚拟世界加现实世界的方式，才是完整的世界。

AI的发展和AR眼镜结合是绝配

AR眼镜的爆发可能还需要三个阶段
- 第一阶段，他希望AR能够回顾AR的本质，能够让用户每天都能够使用它，真正**可戴出门**；
- 第二阶段，结合AR眼镜硬件和AIGC的结合，将ARknovv打造为一个**创造分享平台**；
- 第三阶段，打造人人都需要的**AR眼镜**。

ARknovv A1采用了耐德佳的自由曲面光学方案，拥有55°的透视FOV、入眼亮度达到1000nits，且重量仅68g。
- 在外观设计上，ARknovv A1的镜片采用了电致变色技术，拥有四档调节控制光线。 “之前在飞机上曾经看过相关技术，我就觉得非常适合用在AR眼镜，可以帮助AR眼镜开展室内室外更多的应用场景。我认为电致变色技术未来将成为AR眼镜的标配。”吴德周说道
- 在采访时，有幸提前体验了ARknovv A1的工程机版本。当场体验了ARknovv A1的三大AR功能：**AR相册**、**AR导航**、**现实大爆炸**功能，以及3米外80寸大屏显示。
- 其中，**AR相册**功能结合AIGC工具，通过AI算法可以快速把第一视角看到的2D风景变换成手绘、原画、油画风3D图。用户可以通过AR相册去拍照去创作，去观看3D电影。AR相册的风格化创作功能，是基于开源绘图大模型Stable Diffusion的AI图生图功能，直接风格化相关照片，生成手绘、原画、油画风的AI画面。
- 体验AR导航的功能时，我重点体验了两个场景，一是通过摄像头去识图、识物、识景，比如识别到景点、公园、便利店的招牌，AR眼镜将会直接跳出相关的信息介绍、用户评价等，你可以进一步查看有关店面的消息。

ARknovv A1系列还搭载了车载AR导航功能，在行车状态下，ARknovv A1的HUD导航显示只会占据前方视野下一个小角落以保证行车安全问题。


#### Rokid Glasses

【2024-12-16】唠点元宇宙 [rokid glasses](https://glasses.rokid.com/) AR眼镜，说是明年量产
- 眼镜搭载多模态AI大模型

- 好眼镜: 美学设计
- 好耳机: 接听电话、听音乐降噪
- 好相机: 抓拍美好时刻、一键分享
- 好助手: AI多模态识别

功能
- AI问答搜索
- AI识物
- AI拍照答题
- AI多语种翻译
- AI实时导航
- AI闪记
- AI快速回复
- AI健康提醒


#### Mentra

[Mentra](https://mentra.glass/)

通过运行 Mentra Link 的 Mentra Mach1 镜头来看。
- 上方：未知单词实时翻译。
- 下方：第二语言实时字幕

![](https://mentra.glass/wp-content/uploads/2024/11/LLSG_demo_picture.png)

1. App
  - Mentra Link helps you learn languages faster via proactive AI on smart glasses.
1. AugmentOS
  - AugmentOS.org is the smart glasses super app and app store, an ecosystem that smart glasses run on. “The Android for smart glasses.”
1. Hardware
  - Mentra Mach1 smart glasses and puck enable all-day Mentra Link use.


# 结束
















