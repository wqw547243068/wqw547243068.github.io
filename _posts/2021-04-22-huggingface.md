---
layout: post
title:  Huggingface Transformersåº“ ä½¿ç”¨ç¬”è®°
date:   2021-04-22 16:52:00
categories: æ·±åº¦å­¦ä¹  æŠ€æœ¯å·¥å…·
tags: NLP Transformer bert gpt tensorflow pytorch datasets
excerpt: è·Ÿé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸€èµ·æˆé•¿å£®å¤§çš„åˆ›ä¸šå…¬å¸
mathjax: true
permalink: /huggingface
---

* content
{:toc}


# Huggingface

![](https://huggingface.co/front/assets/huggingface_logo-noborder.svg)
- ![logo](https://img-blog.csdnimg.cn/20200904202104322.png)
- [demo](https://transformer.huggingface.co/)

## Hugging face ç®€ä»‹

[Hugging Face](https://huggingface.co/) æ˜¯ä¸€å®¶æ€»éƒ¨ä½äºçº½çº¦çš„èŠå¤©æœºå™¨äººåˆåˆ›æœåŠ¡å•†ï¼Œå¼€å‘çš„åº”ç”¨åœ¨é’å°‘å¹´ä¸­é¢‡å—æ¬¢è¿ï¼Œç›¸æ¯”äºå…¶ä»–å…¬å¸ï¼ŒHugging Faceæ›´åŠ æ³¨é‡äº§å“å¸¦æ¥çš„æƒ…æ„Ÿä»¥åŠç¯å¢ƒå› ç´ ã€‚

ä½†æ›´ä»¤å®ƒå¹¿ä¸ºäººçŸ¥çš„æ˜¯Hugging Faceä¸“æ³¨äºNLPæŠ€æœ¯ï¼Œæ‹¥æœ‰å¤§å‹çš„å¼€æºç¤¾åŒºã€‚å°¤å…¶æ˜¯åœ¨githubä¸Šå¼€æºçš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼Œé¢„è®­ç»ƒæ¨¡å‹åº“ `Transformers`ï¼Œå·²è¢«ä¸‹è½½è¶…è¿‡ä¸€ç™¾ä¸‡æ¬¡ï¼Œgithubä¸Šè¶…è¿‡24000ä¸ªstarã€‚

[Transformers](https://github.com/huggingface/transformers) æä¾›äº†NLPé¢†åŸŸå¤§é‡state-of-artçš„ é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç»“æ„çš„æ¨¡å‹å’Œè°ƒç”¨æ¡†æ¶ã€‚
- è®ºæ–‡: [Transformers: State-of-the-art Natural Language Processing](https://arxiv.org/pdf/1910.03771)

PyTorchå®ç°äº†ä»è¯­è¨€ä¸­è¯†åˆ«æƒ…ç»ªæƒ…æ„Ÿåè®½çš„DeepMojiæ¨¡å‹ï¼šhttps://github.com/huggingface/torchMoji

ã€2022-9-7ã€‘æ³¨å†Œè´¦æˆ·å[ç”³è¯·token](https://huggingface.co/settings/tokens)æ‰èƒ½ä¸‹è½½æ¨¡å‹

ã€2024-1-25ã€‘[huggingfaceå®˜æ–¹ä»‹ç»åŠæµ‹è¯•é¢˜](https://huggingface.co/learn/nlp-course/zh-CN/chapter1/3?fw=pt)

## Transformers åº“

Transformersåº“ [GitHub](https://github.com/huggingface/transformers)
- [huggingface å¿«é€Ÿä¸Šæ‰‹](https://zhuanlan.zhihu.com/p/610171544)

### ä»‹ç»

- æœ€åˆçš„åç§°æ˜¯ `pytorch-pretrained-bert`ï¼Œéšç€BERTä¸€èµ·åº”è¿è€Œç”Ÿã€‚
- Google 2018å¹´10æœˆåº•, å¼€æºäº†[BERT](https://github.com/google-research/bert) çš„tensorflowå®ç°ã€‚å½“æ—¶ï¼ŒBERTä»¥å…¶å¼ºåŠ²çš„æ€§èƒ½ï¼Œå¼•èµ·NLPerçš„å¹¿æ³›å…³æ³¨ã€‚
- å‡ ä¹ä¸æ­¤åŒæ—¶ï¼Œ`pytorch-pretrained-bert` ä¹Ÿå¼€å§‹äº†ç¬¬ä¸€æ¬¡æäº¤ã€‚
  - `pytorch-pretrained-bert` ç”¨å½“æ—¶å·²æœ‰å¤§é‡æ”¯æŒè€…çš„pytorchæ¡†æ¶å¤ç°äº†BERTçš„æ€§èƒ½ï¼Œå¹¶æä¾›é¢„è®­ç»ƒæ¨¡å‹çš„ä¸‹è½½ï¼Œä½¿æ²¡æœ‰è¶³å¤Ÿç®—åŠ›çš„å¼€å‘è€…ä»¬ä¹Ÿèƒ½å¤Ÿåœ¨å‡ åˆ†é’Ÿå†…å°±å®ç° state-of-art-fine-tuningã€‚
- 2019å¹´7æœˆ16æ—¥ï¼Œåœ¨repoä¸Šå·²ç»æœ‰äº†åŒ…æ‹¬ BERTï¼ŒGPTï¼ŒGPT-2ï¼ŒTransformer-XLï¼ŒXLNETï¼ŒXLMåœ¨å†…å…­ä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè¿™æ—¶å€™åå­—å†å« pytorch-pretrained-bert å°±ä¸åˆé€‚äº†ï¼Œäºæ˜¯æ”¹æˆäº†`pytorch-transformers`ï¼ŒåŠ¿åŠ›èŒƒå›´æ‰©å¤§äº†ä¸å°‘ã€‚
- 2019å¹´6æœˆ, Tensorflow2çš„betaç‰ˆå‘å¸ƒï¼ŒHuggingfaceä¹Ÿé—»é£è€ŒåŠ¨ã€‚ä¸ºäº†ç«‹äºä¸è´¥ä¹‹åœ°ï¼Œåˆå®ç°äº†TensorFlow 2.0å’ŒPyTorchæ¨¡å‹ä¹‹é—´çš„æ·±å±‚äº’æ“ä½œæ€§ï¼Œå¯ä»¥åœ¨TF2.0/PyTorchæ¡†æ¶ä¹‹é—´éšæ„è¿ç§»æ¨¡å‹ã€‚
- 2019å¹´9æœˆ, å‘å¸ƒäº†2.0.0ç‰ˆæœ¬ï¼ŒåŒæ—¶æ­£å¼æ›´åä¸º `transformers` ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œtransformers æä¾›äº†è¶…è¿‡100ç§è¯­è¨€çš„ï¼Œ32ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œç®€å•ï¼Œå¼ºå¤§ï¼Œé«˜æ€§èƒ½ï¼Œæ˜¯æ–°æ‰‹å…¥é—¨çš„ä¸äºŒé€‰æ‹©ã€‚

Huggingfaceåå­—æ¼”è¿›

<div class="mermaid">
    flowchart LR
    %% èŠ‚ç‚¹é¢œè‰²
    classDef red fill:#f02;
    classDef green fill:#5CF77B;
    classDef blue fill:#6BE0F7;
    classDef orange fill:#F7CF6B;
    classDef grass fill:#C8D64B;
    %%èŠ‚ç‚¹å…³ç³»å®šä¹‰
    B(2018å¹´,BERT):::grass-.->|2018,è¯ç”Ÿ,pytorchç‰ˆ|A(pytorch-pretrained-bert):::blue
    T1(TensorFlow 1.0) -.->B
    E(Elmo):::orange-->|åŒå‘|B
    G(GPT):::orange-->|transformer|B
    T1-.->E
    T1-.->G
    A -->|2019å¹´7æœˆ,æ¨¡å‹æ‰©å……åˆ°6ä¸ª| A1(pytorch-transformers):::green
    T2(Tensorflow 2.0)-.->|æ”¯æŒTF|A1
    T1-->|2019å¹´6æœˆ|T2
    A1 -->|2019å¹´9æœˆ,æ›´å| A2(transformers):::green
    A2 -->|æ‰©å……,32ç§æ¨¡å‹,100+ç§è¯­è¨€| A3(æ–°ç‰ˆtransformers):::green
</div>

### å®‰è£…

å®‰è£…ï¼š
- transformers åŒ…æ‰€éœ€çš„ tensorflow ç‰ˆæœ¬è‡³å°‘ä¸º2.2.0ï¼Œè€Œè¯¥ç‰ˆæœ¬å¯¹åº”çš„CUDAç‰ˆæœ¬å¯èƒ½ä¸åŒï¼Œå¦‚ç¬”è€…ä½¿ç”¨çš„2.4.0ç‰ˆæœ¬tensorflowå¯¹åº”çš„CUDAæ˜¯11ç‰ˆæœ¬

```shell
pip install transformers==2.2.0
pip install tensorflow
pip install numpy
# tfç¯å¢ƒ
pip install tensorflow-gpu==2.4.0
# pytorchç¯å¢ƒ
pip install torch
# æˆ– pytorch+transformersä¸€èµ·å®‰è£…
pip install transformers[torch]
# æˆ– TensorFlow+transformersä¸€èµ·å®‰è£…
pip install transformers[tf-cpu]
# æˆ–æºç å®‰è£…
pip install git+https://github.com/huggingface/transformers
# æŒ‡å®šæº
pip install transformers --trusted-host pypi.tuna.tsinghua.edu.cn
```

æµ‹è¯•ï¼š

```python
import transformers
transformers.__version__
import pipeline
print(pipeline('sentiment-analysis')('I hate you'))"
```


### æ¡†æ¶ç†è§£


æºç 
- [transformers/__init__.py](https://github.com/huggingface/transformers/blob/main/src/transformers/__init__.py)
- [è§£è¯»](https://juejin.cn/post/7350510977052229668)

#### Auto æ ¸å¿ƒç±»

transformers å¼€æºåº“æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬3ä¸ªï¼š
- `Conï¬guration`ï¼š**é…ç½®**ç±»ï¼Œç»§æ‰¿è‡ª`PretrainedConï¬g`ï¼Œä¿å­˜modelæˆ–tokenizerçš„è¶…å‚æ•°ï¼Œä¾‹å¦‚è¯å…¸å¤§å°ï¼Œéšå±‚ç»´åº¦æ•°ï¼Œdropout rateç­‰ã€‚é…ç½®ç±»ä¸»è¦å¯ç”¨äºå¤ç°æ¨¡å‹ã€‚
- `Tokenizer`ï¼š**åˆ‡è¯**ç±»ï¼Œç»§æ‰¿è‡ª`PreTrainedTokenizer`ï¼Œä¸»è¦å­˜å‚¨è¯å…¸ï¼ˆ**from_pretrained()**éƒ¨åˆ†ï¼‰ï¼Œtokenåˆ°indexæ˜ å°„å…³ç³»ç­‰ã€‚
  - ä¸‰ä»¶äº‹æƒ…ï¼šâ‘ åˆ†è¯ã€â‘¡æ‰©å±•è¯æ±‡è¡¨ã€â‘¢è¯†åˆ«å¹¶å¤„ç†ç‰¹æ®Štokenã€‚
  - model-specificçš„ç‰¹æ€§ï¼Œå¦‚ç‰¹æ®Štokenï¼Œ`[SEP]`, `[CLS]`ç­‰å¤„ç†ï¼Œtokençš„typeç±»å‹å¤„ç†ï¼Œè¯­å¥æœ€å¤§é•¿åº¦ç­‰
  - å› æ­¤**tokenizeré€šå¸¸å’Œæ¨¡å‹æ˜¯ä¸€å¯¹ä¸€é€‚é…**ã€‚æ¯”å¦‚BERTæ¨¡å‹æœ‰BertTokenizerã€‚
  - Tokenizer å®ç°æ–¹å¼æœ‰å¤šç§ï¼Œå¦‚ word-level, character-levelæˆ–è€…subword-levelï¼Œå…¶ä¸­subword-levelåŒ…æ‹¬Byte-Pair-Encodingï¼ŒWordPieceã€‚subword-levelçš„æ–¹æ³•ç›®å‰æ˜¯transformer-based modelsçš„ä¸»æµæ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³OOVé—®é¢˜ï¼Œå­¦ä¹ è¯ç¼€ä¹‹é—´çš„å…³ç³»ç­‰ã€‚
  - Tokenizerä¸»è¦ä¸ºäº†å°†åŸå§‹çš„è¯­æ–™ç¼–ç æˆé€‚é…æ¨¡å‹çš„è¾“å…¥ã€‚
- `Model`: **æ¨¡å‹**ç±»ã€‚å°è£…äº†é¢„è®­ç»ƒæ¨¡å‹çš„è®¡ç®—å›¾è¿‡ç¨‹ï¼Œéµå¾ªç€ç›¸åŒçš„èŒƒå¼ï¼Œå¦‚æ ¹æ®token idsè¿›è¡Œembedding matrixæ˜ å°„ï¼Œç´§æ¥ç€å¤šä¸ªself-attentionå±‚åšç¼–ç ï¼Œæœ€åä¸€å±‚task-specificåšé¢„æµ‹ã€‚
  - é™¤æ­¤ä¹‹å¤–ï¼ŒModelè¿˜å¯ä»¥åšä¸€äº›çµæ´»æ‰©å±•ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼Œä¾‹å¦‚åœ¨é¢„è®­ç»ƒå¥½çš„Baseæ¨¡å‹åŸºç¡€ä¸Šï¼Œæ·»åŠ task-specific headsã€‚
  - æ¯”å¦‚ï¼Œlanguage model headsï¼Œsequence classiï¬cation headsç­‰ã€‚åœ¨ä»£ç åº“ä¸­é€šå¸¸å‘½åä¸ºï¼ŒXXX**ForSequenceClassification** or XXX**ForMaskedLM**ï¼Œå…¶ä¸­XXXæ˜¯æ¨¡å‹çš„åç§°ï¼ˆå¦‚Bertï¼‰ï¼Œ ç»“å°¾æ˜¯é¢„è®­ç»ƒä»»åŠ¡çš„åç§° (MaskedLM) æˆ–ä¸‹æ¸¸ä»»åŠ¡çš„ç±»å‹(SequenceClassification)ã€‚


transformer é¢å¤–å°è£…äº†`AutoConfig`, `AutoTokenizer`,`AutoModel`
- é€šè¿‡**æ¨¡å‹å‘½å**å°±èƒ½å®šä½æ‰€å±çš„å…·ä½“ç±»
- æ¯”å¦‚ â€™bert-base-casedâ€™ï¼Œè¦åŠ è½½BERTæ¨¡å‹ç›¸å…³çš„é…ç½®ã€åˆ‡è¯å™¨å’Œæ¨¡å‹ã€‚

<!-- draw.io diagram -->
<div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers tags lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; modified=\&quot;2024-05-09T07:13:51.016Z\&quot; agent=\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36\&quot; etag=\&quot;5qBzsLmuwwd9dOJxDsPq\&quot; version=\&quot;24.3.1\&quot;&gt;\n  &lt;diagram id=\&quot;Lw-1uFHNzwHmlxUDpAkU\&quot; name=\&quot;ç¬¬ 1 é¡µ\&quot;&gt;\n    &lt;mxGraphModel dx=\&quot;1242\&quot; dy=\&quot;761\&quot; grid=\&quot;1\&quot; gridSize=\&quot;10\&quot; guides=\&quot;1\&quot; tooltips=\&quot;1\&quot; connect=\&quot;1\&quot; arrows=\&quot;1\&quot; fold=\&quot;1\&quot; page=\&quot;1\&quot; pageScale=\&quot;1\&quot; pageWidth=\&quot;827\&quot; pageHeight=\&quot;1169\&quot; math=\&quot;0\&quot; shadow=\&quot;0\&quot;&gt;\n      &lt;root&gt;\n        &lt;mxCell id=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;1\&quot; parent=\&quot;0\&quot; /&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-7\&quot; value=\&quot;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;dashed=1;dashPattern=1 1;strokeWidth=2;fillColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;216\&quot; y=\&quot;170\&quot; width=\&quot;395\&quot; height=\&quot;60\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-8\&quot; value=\&quot;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;dashed=1;dashPattern=1 1;strokeWidth=2;fillColor=#f5f5f5;fontColor=#333333;strokeColor=#666666;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;170\&quot; y=\&quot;300\&quot; width=\&quot;400\&quot; height=\&quot;60\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-1\&quot; value=\&quot;æ¨¡å‹ç±»&amp;lt;br&amp;gt;Model\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#d80073;strokeColor=none;shadow=1;fontColor=#ffffff;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;338\&quot; y=\&quot;310\&quot; width=\&quot;70\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-2\&quot; value=\&quot;åˆ†è¯ç±»&amp;lt;br&amp;gt;Tokenizer\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#a20025;strokeColor=#6F0000;shadow=1;fontColor=#ffffff;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;480\&quot; y=\&quot;310\&quot; width=\&quot;70\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-3\&quot; value=\&quot;æ•°æ®é›†&amp;lt;br&amp;gt;Dataset\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#1ba1e2;strokeColor=#006EAF;shadow=1;fontColor=#ffffff;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;625\&quot; y=\&quot;310\&quot; width=\&quot;70\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-10\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#999999;entryX=0.5;entryY=1;entryDx=0;entryDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-4\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-5\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-24\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;dashed=1;dashPattern=1 1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-4\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-23\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-25\&quot; value=\&quot;ç»§æ‰¿\&quot; style=\&quot;edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];\&quot; vertex=\&quot;1\&quot; connectable=\&quot;0\&quot; parent=\&quot;eTGUqoqL8qtCVFqHRuqP-24\&quot;&gt;\n          &lt;mxGeometry x=\&quot;0.0571\&quot; y=\&quot;1\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint as=\&quot;offset\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-4\&quot; value=\&quot;é…ç½®ç±»&amp;lt;br&amp;gt;Configuration\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;shadow=1;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;185\&quot; y=\&quot;310\&quot; width=\&quot;90\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-19\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-5\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-17\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-5\&quot; value=\&quot;&amp;lt;div&amp;gt;æµæ°´çº¿&amp;lt;/div&amp;gt;Pipeline\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=#6c8ebf;shadow=1;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;258\&quot; y=\&quot;180\&quot; width=\&quot;90\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-20\&quot; value=\&quot;\&quot; style=\&quot;edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-6\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-18\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-6\&quot; value=\&quot;&amp;lt;div&amp;gt;è®­ç»ƒ&amp;lt;/div&amp;gt;Trainer\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=#6c8ebf;shadow=1;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;468\&quot; y=\&quot;180\&quot; width=\&quot;90\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-9\&quot; value=\&quot;Transformersåº“æ¡†æ¶\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=16;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;348\&quot; y=\&quot;50\&quot; width=\&quot;170\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-11\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#999999;entryX=0.5;entryY=1;entryDx=0;entryDy=0;exitX=0.5;exitY=0;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-1\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-5\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;253\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;300\&quot; y=\&quot;250\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-12\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#999999;entryX=0.5;entryY=1;entryDx=0;entryDy=0;exitX=0.5;exitY=0;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-2\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-5\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;363\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;313\&quot; y=\&quot;250\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-13\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#999999;entryX=0.5;entryY=1;entryDx=0;entryDy=0;exitX=0.678;exitY=-0.05;exitDx=0;exitDy=0;exitPerimeter=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-4\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-6\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;373\&quot; y=\&quot;330\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;323\&quot; y=\&quot;260\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-14\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#999999;entryX=0.5;entryY=1;entryDx=0;entryDy=0;exitX=0.5;exitY=0;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-1\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-6\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;383\&quot; y=\&quot;340\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;333\&quot; y=\&quot;270\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-15\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#999999;entryX=0.5;entryY=1;entryDx=0;entryDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-2\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-6\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;393\&quot; y=\&quot;350\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;343\&quot; y=\&quot;280\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-16\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#999999;entryX=0.5;entryY=1;entryDx=0;entryDy=0;exitX=0.5;exitY=0;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-3\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-6\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;403\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;353\&quot; y=\&quot;290\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-17\&quot; value=\&quot;æ¨ç†é˜¶æ®µ\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#d0cee2;strokeColor=#56517e;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;268\&quot; y=\&quot;110\&quot; width=\&quot;70\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-18\&quot; value=\&quot;è®­ç»ƒ/è¯„ä¼°é˜¶æ®µ\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#d0cee2;strokeColor=#56517e;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;472\&quot; y=\&quot;110\&quot; width=\&quot;82\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-21\&quot; value=\&quot;æ ¸å¿ƒç»„ä»¶\&quot; style=\&quot;text;html=1;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;170\&quot; y=\&quot;270\&quot; width=\&quot;70\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-22\&quot; value=\&quot;&amp;lt;b&amp;gt;&amp;lt;font color=&amp;quot;#333333&amp;quot;&amp;gt;åŠŸèƒ½&amp;lt;/font&amp;gt;&amp;lt;/b&amp;gt;: ä¿å­˜modelã€tokenizerè¶…å‚,ä¾¿äºå¤ä¹ æ¨¡å‹\&quot; style=\&quot;text;whiteSpace=wrap;fillColor=none;html=1;fontColor=#0066CC;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;123\&quot; y=\&quot;360\&quot; width=\&quot;152\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-23\&quot; value=\&quot;&amp;lt;span style=&amp;quot;color: rgb(0, 0, 0); text-align: left;&amp;quot;&amp;gt;PretrainedConï¬g&amp;lt;/span&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#f5f5f5;fontColor=#333333;strokeColor=#666666;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;10\&quot; y=\&quot;315\&quot; width=\&quot;110\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-26\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;dashed=1;dashPattern=1 1;exitX=0.5;exitY=1;exitDx=0;exitDy=0;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-28\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-1\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;354\&quot; y=\&quot;350\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-27\&quot; value=\&quot;ç»§æ‰¿\&quot; style=\&quot;edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];\&quot; vertex=\&quot;1\&quot; connectable=\&quot;0\&quot; parent=\&quot;eTGUqoqL8qtCVFqHRuqP-26\&quot;&gt;\n          &lt;mxGeometry x=\&quot;0.0571\&quot; y=\&quot;1\&quot; relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint as=\&quot;offset\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-28\&quot; value=\&quot;&amp;lt;div style=&amp;quot;text-align: left;&amp;quot;&amp;gt;&amp;lt;span style=&amp;quot;background-color: initial;&amp;quot;&amp;gt;&amp;lt;font color=&amp;quot;#000000&amp;quot;&amp;gt;PreTrainedTokenizer&amp;lt;/font&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/div&amp;gt;\&quot; style=\&quot;rounded=1;whiteSpace=wrap;html=1;fillColor=#f5f5f5;fontColor=#333333;strokeColor=#666666;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;238\&quot; y=\&quot;400\&quot; width=\&quot;122\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-31\&quot; value=\&quot;&amp;lt;b&amp;gt;&amp;lt;font color=&amp;quot;#333333&amp;quot;&amp;gt;åŠŸèƒ½&amp;lt;/font&amp;gt;&amp;lt;/b&amp;gt;: åˆ†è¯ã€æ‰©å±•è¯è¡¨ã€è¯†åˆ«ç‰¹æ®Šå­—ç¬¦\&quot; style=\&quot;text;whiteSpace=wrap;fillColor=none;html=1;fontColor=#0066CC;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;320\&quot; y=\&quot;360\&quot; width=\&quot;152\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-32\&quot; value=\&quot;&amp;lt;b&amp;gt;&amp;lt;font color=&amp;quot;#333333&amp;quot;&amp;gt;åŠŸèƒ½&amp;lt;/font&amp;gt;&amp;lt;/b&amp;gt;: å°è£…æ¨¡å‹è®¡ç®—å›¾\&quot; style=\&quot;text;whiteSpace=wrap;fillColor=none;html=1;fontColor=#0066CC;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;480\&quot; y=\&quot;360\&quot; width=\&quot;152\&quot; height=\&quot;40\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-33\&quot; value=\&quot;AutoConfig\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;fontStyle=1;textShadow=1;labelBorderColor=none;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;185.5\&quot; y=\&quot;470\&quot; width=\&quot;90\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-34\&quot; value=\&quot;AutoModel\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#d80073;strokeColor=#A50040;fontStyle=1;textShadow=1;labelBorderColor=none;fontColor=#ffffff;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;328.5\&quot; y=\&quot;470\&quot; width=\&quot;90\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-35\&quot; value=\&quot;AutoTokenizer\&quot; style=\&quot;rounded=0;whiteSpace=wrap;html=1;fillColor=#a20025;strokeColor=#6F0000;fontStyle=1;textShadow=1;labelBorderColor=none;fontColor=#ffffff;\&quot; vertex=\&quot;1\&quot; parent=\&quot;1\&quot;&gt;\n          &lt;mxGeometry x=\&quot;470.5\&quot; y=\&quot;470\&quot; width=\&quot;90\&quot; height=\&quot;30\&quot; as=\&quot;geometry\&quot; /&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-36\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#B3B3B3;exitX=0.5;exitY=1;exitDx=0;exitDy=0;fontColor=#009900;dashed=1;dashPattern=1 1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-4\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-33\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;253\&quot; y=\&quot;320\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;313\&quot; y=\&quot;230\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-37\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#B3B3B3;exitX=0.5;exitY=1;exitDx=0;exitDy=0;fontColor=#009900;entryX=0.5;entryY=0;entryDx=0;entryDy=0;dashed=1;dashPattern=1 1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-1\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-34\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;240\&quot; y=\&quot;360\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;240\&quot; y=\&quot;460\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n        &lt;mxCell id=\&quot;eTGUqoqL8qtCVFqHRuqP-38\&quot; value=\&quot;\&quot; style=\&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=2;strokeColor=#B3B3B3;exitX=0.5;exitY=1;exitDx=0;exitDy=0;fontColor=#009900;entryX=0.5;entryY=0;entryDx=0;entryDy=0;dashed=1;dashPattern=1 1;\&quot; edge=\&quot;1\&quot; parent=\&quot;1\&quot; source=\&quot;eTGUqoqL8qtCVFqHRuqP-2\&quot; target=\&quot;eTGUqoqL8qtCVFqHRuqP-35\&quot;&gt;\n          &lt;mxGeometry relative=\&quot;1\&quot; as=\&quot;geometry\&quot;&gt;\n            &lt;mxPoint x=\&quot;250\&quot; y=\&quot;370\&quot; as=\&quot;sourcePoint\&quot; /&gt;\n            &lt;mxPoint x=\&quot;250\&quot; y=\&quot;470\&quot; as=\&quot;targetPoint\&quot; /&gt;\n          &lt;/mxGeometry&gt;\n        &lt;/mxCell&gt;\n      &lt;/root&gt;\n    &lt;/mxGraphModel&gt;\n  &lt;/diagram&gt;\n&lt;/mxfile&gt;\n&quot;}"></div>
<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>

é€šå¸¸ä¸Šæ‰‹æ—¶éƒ½ä¼šç”¨Autoå°è£…ç±»æ¥åŠ è½½åˆ‡è¯å™¨å’Œæ¨¡å‹ã€‚


#### ç¤ºä¾‹

```py
# åŠ è½½ä¸ä¿å­˜åˆ†è¯å™¨
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
tokenizer.save_pretrained("./models/bert-base-cased/")
# åŠ è½½ä¸ä¿å­˜æ¨¡å‹
from transformers import AutoModel
# æ‰€æœ‰å­˜å‚¨åœ¨ HuggingFace Model Hub ä¸Šçš„æ¨¡å‹éƒ½å¯ä»¥é€šè¿‡ Model.from_pretrained() æ¥åŠ è½½æƒé‡ï¼Œå‚æ•°å¯ä»¥æ˜¯ checkpoint çš„åç§°ï¼Œä¹Ÿå¯ä»¥æ˜¯æœ¬åœ°è·¯å¾„ï¼ˆé¢„å…ˆä¸‹è½½çš„æ¨¡å‹ç›®å½•ï¼‰
model = AutoModel.from_pretrained("bert-base-cased")
model.save_pretrained("./models/bert-base-cased/") # ä¿å­˜æ¨¡å‹

inputs = tokenizer(["æ¥åˆ°ç¾ä¸½çš„å¤§è‡ªç„¶ï¼Œæˆ‘ä»¬å‘ç°"], return_tensors="pt")
# {'input_ids': tensor([[    1, 68846, 68881, 67701, 67668, 98899, 91935]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}

gen_kwargs = {"max_length": 128, "top_p": 0.8, "temperature": 0.8, "do_sample": True, "repetition_penalty": 1.1}
output = model.generate(**inputs, **gen_kwargs)
# decode the new tokens
output = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)
print(output)
```

#### æ¨¡å‹æ‰©å±•

Transformer-based Pre-trained models

åŸºç¡€ Transformeræ¶æ„åŠ ä¸Šä¸åŒçš„headæ¨¡å—ç»„æˆï¼Œéƒ¨åˆ†ä¾‹å­å¦‚ä¸‹ï¼š

åŸºç¡€æ¨¡å‹ï¼šåªè¾“å‡ºéšå±‚çŠ¶æ€
- *`Model` (retrieve the hidden states)ï¼šåŸºç¡€æ¨¡å‹ï¼Œåªè¾“å‡º**éšçŠ¶æ€**
- *`ForCausalLM`ï¼šå¸¸è§„è¯­è¨€æ¨¡å‹(è‡ªå›å½’)ï¼Œå…¸å‹çš„æœ‰GPTç³»åˆ—

æ‰©å±•æ¨¡å‹ï¼šå¢åŠ å±‚ï¼Œé€‚é…ä¸‹æ¸¸åº”ç”¨
- *`ForMaskedLM`ï¼š**æ©ç **è¯­è¨€æ¨¡å‹ï¼Œå…¸å‹çš„æœ‰BERTã€RoBERTaã€DeBERTa
- *`ForMultipleChoice`ï¼šå¤šé¡¹é€‰æ‹©æ¨¡å‹
- *`ForQuestionAnswering`ï¼šé—®ç­”æ¨¡å‹ï¼Œä¸€èˆ¬æ˜¯æŠ½å–å¼é—®ç­”
- *`ForSequenceClassification`ï¼šåºåˆ—åˆ†ç±»æ¨¡å‹
- *`ForTokenClassification`ï¼štokenåˆ†ç±»æ¨¡å‹ï¼Œå¦‚å‘½åå®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–

å…³ç³»
- ![](https://qiankunli.github.io/public/upload/machine/transformers_pipelines.png)

å¼€æºåº“å®ç°çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬äº†BERTï¼ŒGPT2ï¼ŒXLNetï¼ŒRoBERTaï¼ŒALBERTï¼ŒELECTRAï¼ŒT5ç­‰å®¶å–»æˆ·æ™“çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚


```py
CONFIG_MAPPING = OrderedDict(
    [
        ("retribert", RetriBertConfig,),
        ("t5", T5Config,),
        ("mobilebert", MobileBertConfig,),
        ("distilbert", DistilBertConfig,),
        ("albert", AlbertConfig,),
        ("camembert", CamembertConfig,),
        ("xlm-roberta", XLMRobertaConfig,),
        ("marian", MarianConfig,),
        ("mbart", MBartConfig,),
        ("bart", BartConfig,),
        ("reformer", ReformerConfig,),
        ("longformer", LongformerConfig,),
        ("roberta", RobertaConfig,),
        ("flaubert", FlaubertConfig,),
        ("bert", BertConfig,),
        ("openai-gpt", OpenAIGPTConfig,),
        ("gpt2", GPT2Config,),
        ("transfo-xl", TransfoXLConfig,),
        ("xlnet", XLNetConfig,),
        ("xlm", XLMConfig,),
        ("ctrl", CTRLConfig,),
        ("electra", ElectraConfig,),
        ("encoder-decoder", EncoderDecoderConfig,),
    ]
```

å‚è€ƒ
- ã€2020-7-5ã€‘[Transformersæºç é˜…è¯»å’Œå®è·µ](http://xtf615.com/2020/07/05/transformers/)


ä»£ç ç¤ºä¾‹

```py
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]
tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
print(tokens)
#{
#    'input_ids': tensor([
#        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,2607,  2026,  2878,  2166,  1012,   102],
#        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0, 0,     0,     0,     0,     0,     0]
#    ]),     
#    'attention_mask': tensor([
#        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
#        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
#    ])
#}
output = model(**tokens)
print(output)
# SequenceClassifierOutput(
#    loss=None, 
#    logits=tensor([[-1.5607,  1.6123],[-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), 
#    hidden_states=None, 
#    attentions=None
#)
```


### ç”Ÿæ€ç³»ç»Ÿ

HuggingFaceä¸»å¹²åº“ï¼š
- Transformer æ¨¡å‹åº“
- Tokenizer åˆ†è¯åº“ï¼šå°†sequenceè½¬å˜ä¸ºä¸€ä¸ªidåºåˆ—
- Hub ç¤¾åŒº
- Datasets æ•°æ®é›†åº“ï¼šä¸‹è½½/é¢„å¤„ç†
- Evaluate è¯„ä¼°
- Accelerate åŠ é€Ÿåº“ï¼ˆè½¯ä»¶ï¼‰
- Optimum åŠ é€Ÿå¡ï¼ˆç¡¬ä»¶ï¼‰
- Diffusers æ‰©æ•£æ¨¡å‹
- Timm 
- PEFT
- Chat-UI
- Safetensors
- starcoder
- SetFit

ã€2023-6-14ã€‘HuggingFace Ecosystem ç”Ÿæ€ç³»ç»Ÿè¿›å±• [bç«™è§†é¢‘](https://www.bilibili.com/video/BV1qk4y1n7qa)
- ![](https://p3.toutiaoimg.com/large/tos-cn-i-qvj2lq49k0/7205980da6854039a68a8cd0c5404967)

æ—¶é—´çº¿
- 2022-7ï¼ŒBLOOMå¼€æºï¼Œè¯„æµ‹æ•ˆæœä¸åŠGPT-3
- 2022-8ï¼ŒStable Diffusion
- 2022-11ï¼ŒChatGPTå‘å¸ƒ
- 2023-2ï¼ŒMETA FAIRå¼€æºLLaMA
- å‘å¸ƒ LLM Leaderboeard
- å¼€æº HuggingChatï¼Œä»¥åŠ Chat-UI
- å‘å¸ƒ BigCodrï¼Œä»¥åŠ StarCoderï¼ŒåŸºäºgithubè®¸å¯ä»£ç ï¼Œå¯ä»¥é€šè¿‡opt-outå·¥å…·å‰”é™¤è‡ªå·±çš„ä»£ç 
- 2023-5ï¼Œå‘å¸ƒ StarChatï¼Œè¾…åŠ©ç¼–ç¨‹


## hf ä½¿ç”¨æ–¹æ³•

å¤šç§æ¨¡å¼
1. pipeline æ–¹å¼: ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸è®­ç»ƒ â€”â€” æœ€ç®€å•
1. AutoXXX æ–¹å¼: ä½¿ç”¨å·²æœ‰æ¨¡å‹, çµæ´»æ€§å¢åŠ 
1. finetune: å¾®è°ƒæ–¹å¼ Trainerã€tensorflowã€pytorch


### pipelineæ–¹å¼

ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

æœ€ç®€å•ï¼Œä¸è¿›è¡Œfinetuneï¼Œç›´æ¥å®Œæˆä»»åŠ¡ï¼Œbertæä¾›äº†pipelineçš„åŠŸèƒ½

pipeline å‚æ•°
- task ä¸‹æ¸¸ä»»åŠ¡
- model é¢„è®­ç»ƒæ¨¡å‹
- config å¯¹åº”æ¨¡å‹çš„å…·ä½“é…ç½®ã€‚
- tokenizer åˆ†è¯å™¨ã€‚
- frameworkï¼šptæˆ–è€…tfç”¨äºæŒ‡å®šæ¨¡å‹ä½¿ç”¨torchè¿˜æ˜¯tensorflowç‰ˆçš„
- use_fast: æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–åçš„åˆ†è¯å™¨

```py
pipeline(task: str, model: Optional = None, \
    config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, \
    tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, NoneType] = None, \
    framework: Union[str, NoneType] = None, revision: Union[str, NoneType] = None,  \
    use_fast: bool = True, model_kwargs: Dict[str, Any] = {}, **kwargs) \
-> transformers.pipelines.base.Pipeline
```


ç¤ºä¾‹

```py
from transformers import pipeline

classifier = pipeline(task='sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")
classifier('We are very happy to show you the   Transformers library.')
```

### AutoXXX æ–¹å¼

ç”¨ nn.module + class æ–¹å¼æ„å»ºå®Œå¯ä»¥è®­ç»ƒçš„modelã€‚

pipeline å°è£…ä»£ç ä¸»è¦ç”¨ autoXXX å®ç°ï¼Œè¿™ç§æ–¹å¼å’Œç°æœ‰çš„torchä»¥åŠtf.kerasçš„æ¡†æ¶ç»“åˆèµ·æ¥ï¼Œæœ¬è´¨ä¸ŠæŠŠè¿™äº›é¢„è®­ç»ƒæ¨¡å‹å½“ä½œä¸€ä¸ªå¤§å‹çš„modelï¼Œç›¸å¯¹äºpipelineæ¥è¯´ï¼Œå°è£…çš„ç¨‹åº¦å°ä¸€ç‚¹

automodelforXXX å®é™…ä¸Šå¸®ä½ æŠŠä¸‹æ¸¸å¯¹åº”çš„ä»»åŠ¡å±‚æ­å»ºå¥½

```py
self.model = BertForSequenceClassification.from_pretrained(pretrain_Model_path,config=config)
# ==== auto æ¨¡å¼ ====
# pytorch
self.model = AutoModelForSequenceClassification.from_pretrained(pretrain_Model_path,config=config)
# tensorflow
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
# tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

å®Œæ•´ä»£ç 

```py
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased-finetuned-mrpc")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased-finetuned-mrpc")

classes = ["not paraphrase", "is paraphrase"]
sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"

paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="pt")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="pt")
paraphrase_classification_logits = model(**paraphrase).logits
not_paraphrase_classification_logits = model(**not_paraphrase).logits
paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]
not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]
# Should be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")
# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
```


### finetune


pytorch ç¤ºä¾‹

```py
from transformers import TrainingArguments, Trainer
import numpy as np
import evaluate
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)

# training_args = TrainingArguments(output_dir="test_trainer")
training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")

metric = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()
```


## æ•°æ®

æ•°æ®é›†å·¥å…·åŒ… huggingface datasets 


### tokenizer

å›¾è§£
- ![](https://qiankunli.github.io/public/upload/machine/attention_mask.jpg)

é»˜è®¤æƒ…å†µä¸‹ï¼ŒAutoTokenizer ç±»é¦–å…ˆåŠ è½½ Fast tokenizer
- Fast é€‚åˆæµ·é‡æ•°æ®
- å°‘é‡æ•°æ®æ—¶ï¼Œä¸¤è€…é€Ÿåº¦å·®å¼‚ä¸å¤§

Fast tokenizer å’Œ Slow tokenizer çš„åŒºåˆ«ï¼š
- Slow tokenizer æ˜¯åœ¨ Transformer åº“ä¸­ç”¨Pythonç¼–å†™çš„ã€‚
- Fast tokenizer æ˜¯åœ¨ Tokenizers åº“ä¸­ç”¨Rustç¼–å†™çš„

æ”¯æŒ tensorflow å’Œ pytorch, å‚æ•° return_tensors = pt/tf

```py
# pytorch
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)

# tensorflow
tf_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)
```


### load_dataset å‡½æ•°


```py
datasets.load_dataset(
	path: str,
    name: Optional[str] = None,
    data_dir: Optional[str] = None,
    data_files: Optional[Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]]] = None,
    split: Optional[Union[str, Split]] = None,
    cache_dir: Optional[str] = None,
    features: Optional[Features] = None,
    download_config: Optional[DownloadConfig] = None,
    download_mode: Optional[DownloadMode] = None,
    ignore_verifications: bool = False,
    keep_in_memory: Optional[bool] = None,
    save_infos: bool = False,
    revision: Optional[Union[str, Version]] = None,
    use_auth_token: Optional[Union[bool, str]] = None,
    task: Optional[Union[str, TaskTemplate]] = None,
    streaming: bool = False,
    **config_kwargs
    )
```

å‡½æ•°è¯´æ˜
- `load_dataset`å‡½æ•°ä»Hugging Face Hubæˆ–è€…æœ¬åœ°æ•°æ®é›†æ–‡ä»¶ä¸­åŠ è½½ä¸€ä¸ªæ•°æ®é›†ã€‚å¯ä»¥é€šè¿‡ [datasets](https://huggingface.co/datasets) æˆ–è€… `datasets.list_datasets()` è·å–æ‰€æœ‰å¯ç”¨æ•°æ®é›†ã€‚
- å‚æ•° `path` è¡¨ç¤ºæ•°æ®é›†çš„åå­—æˆ–è€…è·¯å¾„ã€‚å¯ä»¥æ˜¯ä¸€ä¸ªæ•°æ®é›†çš„åå­—ï¼Œæ¯”å¦‚"imdb"ã€â€œglueâ€ï¼›ä¹Ÿå¯ä»¥æ˜¯é€šç”¨çš„äº§ç”Ÿæ•°æ®é›†æ–‡ä»¶çš„è„šæœ¬ï¼Œæ¯”å¦‚"json"ã€â€œcsvâ€ã€â€œparquetâ€ã€â€œtextâ€ï¼›æˆ–è€…æ˜¯åœ¨æ•°æ®é›†ç›®å½•ä¸­çš„è„šæœ¬ï¼ˆ.py)æ–‡ä»¶ï¼Œæ¯”å¦‚â€œglue/glue.pyâ€ã€‚
- å‚æ•° `name` è¡¨ç¤ºæ•°æ®é›†ä¸­çš„å­æ•°æ®é›†ï¼Œå½“ä¸€ä¸ªæ•°æ®é›†åŒ…å«å¤šä¸ªæ•°æ®é›†æ—¶ï¼Œå°±éœ€è¦è¿™ä¸ªå‚æ•°ã€‚æ¯”å¦‚"glue"æ•°æ®é›†ä¸‹å°±åŒ…å«"sst2"ã€â€œcolaâ€ã€"qqp"ç­‰å¤šä¸ªå­æ•°æ®é›†ï¼Œæ­¤æ—¶å°±éœ€è¦æŒ‡å®šnameæ¥è¡¨ç¤ºåŠ è½½å“ªä¸€ä¸ªå­æ•°æ®é›†ã€‚
- å‚æ•° `data_dir` è¡¨ç¤ºæ•°æ®é›†æ‰€åœ¨çš„ç›®å½•ï¼Œå‚æ•°data_filesè¡¨ç¤ºæœ¬åœ°æ•°æ®é›†æ–‡ä»¶ã€‚
- å‚æ•° `split` å¦‚æœä¸ºNoneï¼Œåˆ™è¿”å›ä¸€ä¸ªDataDictå¯¹è±¡ï¼ŒåŒ…å«å¤šä¸ªDataSetæ•°æ®é›†å¯¹è±¡ï¼›å¦‚æœç»™å®šçš„è¯ï¼Œåˆ™è¿”å›å•ä¸ªDataSetå¯¹è±¡ã€‚
- å‚æ•° `cache_dir` è¡¨ç¤ºç¼“å­˜æ•°æ®çš„ç›®å½•
  - é»˜è®¤ä¸º"`~/.cache/huggingface/datasets`"ã€‚
- å‚æ•° `keep_in_memory` è¡¨ç¤ºæ˜¯å¦å°†æ•°æ®é›†ç¼“å­˜åœ¨å†…å­˜ä¸­ï¼ŒåŠ è½½ä¸€æ¬¡åï¼Œå†æ¬¡åŠ è½½å¯ä»¥æé«˜åŠ è½½é€Ÿåº¦ã€‚
- å‚æ•° `revision` è¡¨ç¤ºåŠ è½½æ•°æ®é›†çš„è„šæœ¬çš„ç‰ˆæœ¬ã€‚


### è¿œç¨‹æ•°æ®é›†

```py
import datasets

dataset = datasets.load_dataset("imdb") # imdb æ•°æ®é›†
# åŠ è½½glueä¸‹çš„colaå­æ•°æ®é›†
dataset = datasets.load_dataset("glue", name="cola") 
# csvè„šæœ¬åŠ è½½æœ¬åœ°çš„test.tsvæ–‡ä»¶ä¸­çš„æ•°æ®é›†
dataset = datasets.load_dataset("csv", data_dir="./test", data_files="test.tsv")
# æœ¬åœ°glue.pyè„šæœ¬æ–‡ä»¶åŠ è½½è¿œç¨‹colaæ•°æ®é›†
dataset_1 = datasets.load_dataset("../dataset/glue/glue.py", name="cola")
# ä¸ä¸Šä¸€ä¸ªç­‰ä»·
dataset_2 = datasets.load_dataset("../dataset/glue", name="cola") 

```


### æœ¬åœ°æ•°æ®é›†

ã€2024-4-22ã€‘é”™è¯¯

```sh
  File "/usr/local/lib/python3.9/dist-packages/datasets/load.py", line 1780, in dataset_module_factory
    raise ConnectionError(f"Couldn't reach '{path}' on the Hub ({type(e).__name__})")
ConnectionError: Couldn't reach 'wikitext' on the Hub (ConnectTimeout)
```

æœåŠ¡å™¨è®¿é—®ä¸äº†å¤–ç½‘ï¼Œå¦‚ä½•è¯»å–æœ¬åœ°æ•°æ®é›†ï¼Ÿ
1. é¦–å…ˆï¼Œä¸‹è½½å¹¶å­˜å‚¨æ•°æ®
2. ç„¶åï¼ŒæŠŠæ•°æ®é›†ä¸Šä¼ åˆ°æŒ‡å®šæœåŠ¡å™¨åœ°å€ï¼Œå¹¶è¿›è¡Œæœ¬åœ°åŠ è½½

```py
import datasets

local_path = '.' # æœ¬åœ°ç¼“å­˜ç›®å½•
all_data = datasets.load_dataset('imdb')
# å­é›†
dataset = load_dataset("Salesforce/dialogstudio", "TweetSumm") 
# ç¼“å­˜åˆ°æœ¬åœ°, ç›®å½•å imdb, å†æ¬¡æ‰§è¡Œä¼šæŠ¥é”™
# ValueError: Invalid pattern: '**' can only be an entire path component
all_data = datasets.load_dataset('imdb', cache_dir=local_path) 
# åˆ’åˆ†è®­ç»ƒé›†ã€æµ‹è¯•é›†
train_data, test_data = datasets.load_dataset('imdb', split =['train', 'test'], cache_dir=local_path)
# é€šè¿‡csvè„šæœ¬åŠ è½½æœ¬åœ°çš„test.tsvæ–‡ä»¶ä¸­çš„æ•°æ®é›†
dataset = datasets.load_dataset("csv", data_dir="./test", data_files="test.tsv")

# æ‰‹å·¥ä¿å­˜åˆ°æœ¬åœ°
all_data.save_to_disk('my_imdb')
all_data.to_csv('my_imdb')
all_data.to_json('my_imdb')
# åŠ è½½æœ¬åœ°æ•°æ®é›†
new_data = datasets.load_from_disk('my_imdb')
```

ã€2024-5-8ã€‘å†æ¬¡æ‰§è¡ŒæŠ¥é”™

```sh
ValueError: Invalid pattern: '**' can only be an entire path component
```

åŸå› 
- æœ¬åœ°æœ‰æ•°æ®é›†åŒåç›®å½•ï¼Œæ”¹åå³å¯


æ³¨æ„ï¼š
- ä¿å­˜æ•°æ®é›†æ‰€ç”¨æœºå™¨ä¸Š datasetsç‰ˆæœ¬å’Œä½¿ç”¨æœ¬åœ°æ•°æ®é›†çš„datasetsçš„**ç‰ˆæœ¬è¦ä¸€è‡´**æ‰è¡Œï¼Œä¸ç„¶å¯èƒ½ä¼šå‡ºç°æ•°æ®é›†åŠ è½½é”™è¯¯çš„æƒ…å†µã€‚

```py
dataset = load_dataset("json", data_dir='data', data_files="data/train_dataset.json", split="train")
```


### å†…å­˜æ•°æ®é›†

å†…å­˜åŠ è½½æ•°æ®

æ”¯æŒä»å†…å­˜ä¸­åŠ è½½å­—å…¸æˆ–è€… DafaFrameï¼ˆpandasï¼‰æ•°æ®ç»“æ„çš„æ•°æ®ï¼Œå…·ä½“æ“ä½œç¤ºä¾‹å¦‚ä¸‹ï¼š

```py
# ä»å­—å…¸å¯¼å…¥æ•°æ® 
from datasets import Dataset 

my_dict = {"a": [1, 2, 3]} 
dataset = Dataset.from_dict(my_dict) # ä»dataFrameå¯¼å…¥æ•°æ® 

import pandas as pd 
df = pd.DataFrame({"a": [1, 2, 3]}) 
dataset = Dataset.from_pandas(df)
```

### æ•°æ®å¤„ç†


#### æ•°æ®æŸ¥çœ‹

åŠ è½½å®Œæ•°æ®å, çœ‹çœ‹æœ‰é‚£äº›å†…å®¹
- æ•´ä¸ªæ•°æ®é›†åˆ’åˆ†æˆäº†å¤šä¸ªæ•°æ®å­é›†ï¼ŒåŒ…å«trainï¼Œvalidä»¥åŠtesté›†ã€‚
- æ¯ä¸ªarrow_datasetéƒ½æœ‰å¤šå°‘æ¡æ•°æ®
- è¿™äº›æ•°æ®çš„featureæ˜¯ä»€ä¹ˆ

| æ•°æ®æ ¼å¼ | å‡½æ•° |
| --- | --- |
| Arrow | save_to_disk() |
| CSV | to_csv() |
| JSON | to_json() |

ç®€å•ä¸¤è¡Œä»£ç å¯¼å…¥æ•°æ®ï¼Œç„¶åæ‰“å°å‡ºæ¥çœ‹ä¸€ä¸‹ï¼›

```py
from datasets import load_dataset 

# datasets = load_dataset('cail2018') 
datasets = load_dataset('imdb') 
print(datasets)  # æŸ¥çœ‹æ•°æ®çš„ç»“æ„
datasets['train'] # type: datasets.arrow_dataset.Dataset
# DatasetDict({
#     train: Dataset({
#         features: ['text', 'label'],
#         num_rows: 25000
#     })
#     test: Dataset({
#         features: ['text', 'label'],
#         num_rows: 25000
#     })
#     unsupervised: Dataset({
#         features: ['text', 'label'],
#         num_rows: 50000
#     })
# })

datasets = load_dataset('cail2018',split='exercise_contest_test') # å¦‚æœçŸ¥é“æ•°æ®ç»“æ„ï¼Œåœ¨loadçš„æ—¶å€™å°±å¯ä»¥ç”¨splitåªloadè¿›æ¥ä¸€éƒ¨åˆ†æ•°æ®ï¼›
# ä»æ•°æ®é›†é‡Œé¢å–æ•°æ®
datasets_sample = datasets[ "exercise_contest_train" ].shuffle(seed=42).select(range(1000))
# è¿™é‡Œå°±æ˜¯ä»cail2018è¿™ä¸ªæ•°æ®é›†é‡Œé¢çš„ï¼Œexercise_contest_trainè¿™éƒ¨åˆ†æ•°æ®ï¼ŒéšæœºæŠ½å–1000ä¸ªæ•°æ®
# ä»è¿™ä¸ªé‡Œé¢åˆ‡ç‰‡å–æ•°å¦‚ä¸‹æ‰€ç¤ºï¼Œè§„å¾‹å’Œnpæˆ–è€…dataframeçš„æ•°æ®ç»“æ„å½¢å¼æ˜¯ä¸€æ ·çš„ã€‚
print(datasets_sample[10:15] )
```


#### æ•°æ®è½¬æ¢

```py
from datasets import load_dataset
datasets = load_dataset('cail2018')
print(datasets)  # æŸ¥çœ‹æ•°æ®çš„ç»“æ„


def add_prefix(example):
    example["fact"] = "æ¡ˆä»¶è¯¦æƒ…: " + example["fact"]
    return example
# shuffle æ‰“ä¹±
datasets_sample = datasets[ "exercise_contest_train" ].shuffle(seed= 42 ).select( range ( 1000 ))
# map æ˜ å°„: é€å…ƒç´ å¤„ç†
datasets_sample = datasets_sample.map(add_prefix)
print(datasets_sample[:3] )
# filter è¿‡æ»¤
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
# sort æ’åº
datasets_sample = datasets_sample.sort('punish_of_money') # æŒ‰ç…§è¢«ç½šé‡‘é¢æ’åºï¼Œæ˜¯ä»å¤§åˆ°å°çš„ï¼Œè¿™ä¸ªæ’åºä¼¼ä¹æ²¡æ³•æ”¹ï¼Œçœ‹äº†ä¸‹å‚æ•°æ²¡æ‰¾åˆ°æ”¹æˆä»å°åˆ°å¤§çš„ã€‚ã€‚ã€‚ã€‚
# set_format æ ¼å¼è½¬åŒ–: [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow'] None é»˜è®¤
datasets_sample.set_format("pandas") # è½¬æ¢ä¸ºpandasçš„dataFrameç»“æ„ï¼Œè¿™å¤„ç†èµ·æ¥è¿˜ä¸æ˜¯æ‰‹æ‹¿æŠŠæçš„


# ç”Ÿæˆæ–°åˆ—
from datasets import load_dataset , Dataset
dataset = Dataset.from_dict({"a": [0, 1, 2]})
dataset = dataset.map(lambda batch: {"b": batch["a"]*2})  
# è¿™é‡Œç»™æ•°æ®datasetäº§ç”Ÿä¸€ä¸ªæ–°çš„åˆ—bï¼Œè¯·æ³¨æ„å¤„ç†çš„æ—¶å€™è¦æ³¨æ„ï¼Œæ–°çš„åˆ—é•¿åº¦å¿…é¡»å’ŒåŸæ¥ä¸€è‡´ï¼›
```


#### å¦‚ä½•åŠ è½½å¤§æ•°æ®

åŠ è½½è¶…å¤§å‹çš„è¯­æ–™ï¼Œå ç”¨å†…å­˜æ˜¯åŠ è½½è¯­æ–™çš„å‡ å€ã€‚æ¯”å¦‚gpt-2è®­ç»ƒçš„40Gè¯­æ–™ï¼Œå¯èƒ½ä¼šè®©å†…å­˜çˆ†æ‰ã€‚

huggingfaceè®¾è®¡äº†ä¸¤ä¸ªæœºåˆ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç¬¬ä¸€ä¸ªæ˜¯å°†æ•°æ®é›†è§†ä¸ºâ€œå†…å­˜æ˜ å°„â€æ–‡ä»¶ï¼Œç¬¬äºŒä¸ªæ˜¯â€œæµå¼ä¼ è¾“â€è¯­æ–™åº“ã€‚
- **å†…å­˜æ˜ å°„**ï¼šé€šè¿‡Apache Arrowå†…å­˜æ ¼å¼å’Œpyarrowåº“å®ç°çš„ï¼Œhuggingfaceå·²ç»è‡ªå·±å¤„ç†å¥½äº†ï¼Œç½‘ç«™ä¸Šå®˜æ–¹æµ‹è¯•çš„æƒ…å†µå¤§æ¦‚æ˜¯0.3gb/sã€‚
- **æµå¼ä¼ è¾“**ï¼šå› ä¸ºå¾ˆå¤šè¯­æ–™åº“éå¸¸çš„å¤§ï¼ˆæ¯”å¦‚pileå¤šè¾¾800å¤šGï¼‰ï¼Œä¸‹è½½åˆ°æœ¬åœ°ç¡¬ç›˜è¿˜æ˜¯æœ‰äº›åƒä¸æ¶ˆå‘€ï¼Œå› æ­¤huggingfaceè®¾ç½®äº†æµå¼ä¼ è¾“ï¼Œç±»ä¼¼äºè§†é¢‘ç½‘ç«™çš„æ“ä½œï¼Œæœ¬åœ°æœ‰ä¸ªç¼“å†²åŒºå¤§å°å›ºå®šï¼Œç„¶åä¸åœçš„è¿­ä»£æ–°æ•°æ®ã€‚å‡è®¾ç¼“å†²åŒºæ•°æ®ä¸€å…±10000æ¡ï¼Œå½“ä½ å¤„ç†ç¬¬ä¸€æ¡çš„æ—¶å€™ï¼Œä»–å°±åŠ è½½ç¬¬10001æ¡æ•°æ®ã€‚

ç¤ºä¾‹ä»£ç ï¼š
- åªéœ€è¦è®¾ç½® streaming= True å³å¯ï¼Œè¿™ä¸ªloadä¸Šæ¥çš„æ•°æ®æ˜¯ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡ï¼Œä¹‹åçš„å¤„ç†ä¸å‰é¢ä»‹ç»çš„ä¸€æ ·ã€‚

```py
pubmed_dataset_streamed = load_dataset( "json" , data_files=data_files, split= "train" , streaming= True )
```


## æ¨¡å‹


huggingface transformers æ¡†æ¶ä¸»è¦æœ‰ä¸‰ä¸ªç±»
- model ç±»
- configuration ç±»
- tokenizer ç±»

æ‰€æœ‰ç›¸å…³ç±»éƒ½è¡ç”Ÿè‡ªè¿™ä¸‰ä¸ªç±»ï¼Œéƒ½æœ‰`from_pretained()`æ–¹æ³•å’Œ`save_pretrained()`æ–¹æ³•ã€‚


### æ¨¡å‹ä¿¡æ¯

ä» hf ä¸‹è½½çš„æ¨¡å‹ï¼Œå¸¸è§æ–‡ä»¶
- é…ç½®æ–‡ä»¶ `config.json`
- è¯å…¸æ–‡ä»¶ `vocab.json`
- é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶
  - å¦‚æœç”¨ pytorch, åˆ™ä¿å­˜ `pytorch_model.bin`
  - å¦‚æœç”¨ tensorflow 2ï¼Œåˆ™ä¿å­˜ `tf_model.h5`

é¢å¤–çš„æ–‡ä»¶
- merges.txtã€special_tokens_map.jsonã€added_tokens.jsonã€tokenizer_config.jsonã€sentencepiece.bpe.modelç­‰

è¿™å‡ ç±»æ˜¯tokenizeréœ€è¦ä½¿ç”¨çš„æ–‡ä»¶

ä»¥ GPT-2 æ¨¡å‹ä¸ºä¾‹

```sh
# é»˜è®¤ä¸‹è½½ç›®å½•
ls ~/.cache/huggingface/hub/models--gpt2/*
# 3 ä¸ªå­ç›®å½•: blobs, refs, snapshots
~/.cache/huggingface/hub/models--gpt2/blobs:
# 10c66461e4c109db5a2196bff4bb59be30396ed8                         
# 3dc481ecc3b2c47a06ab4e20dba9d7f4b447bdf3
# 248dfc3911869ec493c76e65bf2fcf7f615828b0254c12b473182f0f81d3a707  # 523M
~/.cache/huggingface/hub/models--gpt2/refs:
# main
~/.cache/huggingface/hub/models--gpt2/snapshots:
# 607a30d783dfa663caf39e06633721c8d4cfcd7e
# å­ç›®å½•ä¸‹æœ‰3ä¸ªæ–‡ä»¶: config.json, generation_config.json, model.safetensors
```

æ ¼å¼è½¬æ¢

```sh
mkdir -p Helsinki-NLP/opus-mt-zh-en
cd Helsinki-NLP/opus-mt-zh-en/
cp ~/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-zh-en/blobs/0ab361451ecc57b6223303c7b52e216ff40dc7e6 source.spm
cp ~/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-zh-en/blobs/3be15dddf54535a2257b485f32c8f9226352d5c4 vocab.json
cp ~/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-zh-en/blobs/60000ab989b1eec84f7b0299368f9dd498cdab61 tokenizer_config.json
cp ~/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-zh-en/blobs/710dcdf966ec0aa5b3d991a35264c7cb174ccf14 config.json
cp ~/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-zh-en/blobs/878ae3c6ca6afea7971e3be0b18debdd0d41e3ea target.spm
cp ~/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-zh-en/blobs/a43af728d2ddefe1ed73656ce004be6391c02e0a generation_config.json
cp ~/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-zh-en/blobs/9d8ceb91d103ef89400c9d9d62328b4858743cf8924878aee3b8afc594242ce0 pytorch_model.bin
```


### æ¨¡å‹å¯¼å…¥ 

å¯¼å…¥æ–¹æ³• 
- é»˜è®¤è‡ªåŠ¨ä»è¿œç¨‹ä¸‹è½½æ¨¡å‹
  - å‰æï¼šèƒ½è”ç½‘
  - é»˜è®¤ä¿å­˜è·¯å¾„ï¼š`~/.cache/huggingface/hub/`
- å¯ä»¥æœ¬åœ°å¯¼å…¥

```py
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
```

ã€2024-5-8ã€‘transformers ä¸­æœ‰å“ªäº›åŒ…å¯ç”¨ï¼Ÿ
- è§å®˜æ–¹æºç  [transformers/__init__.py](https://github.com/huggingface/transformers/blob/main/src/transformers/__init__.py)


å¦‚æœä¼ å…¥çš„æ˜¯ç›®å½•, åˆ™ä»ä¸­æ‰¾ vocab.jsonã€pytorch_model.binã€tf_model.h5ã€merges.txtã€special_tokens_map.jsonã€added_tokens.jsonã€tokenizer_config.jsonã€sentencepiece.bpe.modelç­‰è¿›è¡ŒåŠ è½½ã€‚

#### è¿œç¨‹å¯¼å…¥

æ–¹æ³•
- ä½¿ç”¨repo idä¸‹è½½åˆ°ç¼“å­˜å¹¶åŠ è½½
  - é»˜è®¤ä¸‹è½½åˆ° `~/.cache/huggingface/hub`/models--Helsinki-NLP--opus-mt-zh-en
  - ç¼“å­˜ç›®å½•: `blobs`, `refs`, `snapshots`
  - å…¶ä¸­ snapshots åŒ…å«å¤šä¸ªmd5å­—ç¬¦ä¸²çš„ç›®å½•, å¯¹åº”ä¸åŒç‰ˆæœ¬, å…¶ä¸­ä»»æ„ä¸€ä¸ªåŒ…å«å®é™…æ¨¡å‹æ–‡ä»¶ä¿¡æ¯(config.json,generation_config.json, tokenizer_config.json, pytorch_model.binç­‰)
- æœ¬åœ°è·¯å¾„ï¼Œé¿å…è®¿é—® http://huggingface.coï¼Œä»è€Œè¿…é€ŸåŠ è½½æ¨¡å‹

```py
from transformers import BertTokenizer, BertModel

# â‘  ä½¿ç”¨repo idä¸‹è½½åˆ°ç¼“å­˜å¹¶åŠ è½½
model_name = 'Helsinki-NLP/opus-mt-zh-en'
model_name = 'bert-base-chinese'
model_path = '/tmp/model'

# æ¯æ¬¡æ‰§è¡Œæ—¶éƒ½ä¸‹è½½è¿œç¨‹æ¨¡å‹
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

tokenizer.save_pretrained(model_path)
model.save_pretrained(model_path)

# æ¯æ¬¡æ‰§è¡Œæ—¶å…ˆæ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨ï¼Œå†è¿œç¨‹ä¸‹è½½
tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=model_path)
model = BertModel.from_pretrained(model_name, cache_dir=model_path)
# åªä»æœ¬åœ°åŠ è½½ï¼Œæ²¡æœ‰å°±æŠ¥é”™
tokenizer = BertTokenizer.from_pretrained(model_name, local_files_only=True)
model = BertModel.from_pretrained(model_name, local_files_only=True)
```



#### æœ¬åœ°å¯¼å…¥

æ— æ³•è”ç½‘æ—¶ï¼Œè¯»å–é¢„è®­ç»ƒæ¨¡å‹ä¼šå¤±è´¥

è§£æ³•
- ä¸‹è½½æ¨¡å‹ï¼šhuggingface å®˜ç½‘ [Files and versions]() ä¸‹è½½å‡ ä¸ªæ–‡ä»¶
  - æ¨¡å‹`é…ç½®æ–‡ä»¶`
    - `config.json` 
  - pytorch`æ¨¡å‹æ–‡ä»¶`
    - `pytorch_model.bin` 
  - tokenizer `åˆ†è¯æ–‡ä»¶`
    - `tokenizer.json` 
    - `tokenizer_config.json`
    - `vocab.txt`
- æœ¬åœ°å¯¼å…¥
  - æ”¹æˆ**æœ¬åœ°ç›®å½•**
  - é¢å¤–è¯»å– config ä¿¡æ¯

AutoTokenizer 
- åªä¼šä»ä¼ å…¥è·¯å¾„æ‰¾ tokenizer_config.json æ–‡ä»¶
- æ‰¾åˆ°åï¼Œæ‰€æœ‰çš„åŠ è½½å†…å®¹éƒ½ä»¥ tokenizer_config.json ä¸­å†…å®¹ä¸ºå‡†ï¼Œè¿™é‡Œçš„â€œauto_mapâ€å‘Šè¯‰åŠ è½½å™¨è¦å»å“ªé‡Œæ‰¾å¯¹åº”çš„tokenizerç±»ï¼Œå‰åŠæ®µçš„è·¯å¾„æ ‡è®°çš„å°±æ˜¯å»å“ªé‡Œæ‰¾`.py`æ–‡ä»¶ï¼Œä½¿ç”¨`--`åˆ†å‰²åé¢çš„å°±æ˜¯æŒ‡çš„å¯¹åº”çš„pythonæ–‡ä»¶ä¸­çš„Tokenizerç±»

```py
from transformers import BertTokenizer, BertModel

# config æ–‡ä»¶
config = BertConfig.from_json_file("bert-base-chinese/config.json")
# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
# model = BertModel.from_pretrained('bert-base-chinese')
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese/') # æ­¤å¤„ä¸ºæœ¬åœ°æ–‡ä»¶å¤¹ï¼ŒåŒ…å« tokenizer_config.json æ–‡ä»¶
model = BertModel.from_pretrained("bert-base-chinese/pytorch_model.bin", config=config)
```

ã€2024-5-14ã€‘æœ¬åœ°æ¨¡å‹åŠ è½½å®è·µ

```py
from datasets import load_dataset, load_from_disk
from transformers import AutoConfig, AutoTokenizer, DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification

local_dir = '/mnt/bn/flow-algo-cn/wangqiwen'

# æ•°æ®é›†åŠ è½½
# raw_datasets = load_dataset("glue", "mrpc") # è¿œç¨‹æ•°æ®é›†
local_data = f'{local_dir}/data/my_imdb'
local_model = f'{local_dir}/model/bigbird'
print(f'æœ¬åœ°æ•°æ®ç›®å½•: {local_data}\næœ¬åœ°æ¨¡å‹ç›®å½•:{local_model}')

raw_datasets = load_from_disk(local_data) # æœ¬åœ°æ•°æ®é›†

# model_name = "bert-base-uncased"
model_name = 'bigbird-roberta-base'
model_path = f'{local_model}/{model_name}'
model_file = f"{model_path}/pytorch_model.bin"

# è¿œç¨‹æ¨¡å‹
# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=local_model)
# model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=local_model)

# åŠ è½½æœ¬åœ°æ¨¡å‹: é…ç½®æ–‡ä»¶ã€åˆ†è¯å™¨ã€æ¨¡å‹æ–‡ä»¶
config = AutoConfig.from_pretrained(f"{local_model}/{model_name}/config.json") # é…ç½®æ–‡ä»¶
tokenizer = AutoTokenizer.from_pretrained(model_path) # åˆ†è¯å™¨, tokenizer_config.json
model = AutoModelForSequenceClassification.from_pretrained(model_file, config=config) # æœ¬åœ°æ¨¡å‹
```


### æ¨¡å‹ä¸‹è½½

- åœ¨[huggingfaceæ¨¡å‹åº“](https://huggingface.co/models)é‡Œé€‰æ‹©éœ€è¦çš„é¢„è®­ç»ƒæ¨¡å‹å¹¶ä¸‹è½½ã€‚
  - ä¾‹å¦‚ï¼Œç‚¹å‡» `bert-base-uncased` ä»¥åç‚¹ `Files and versions` æ‰‹åŠ¨ä¸‹è½½ã€‚
  - åªè¦ç‚¹å‡»å¯¹åº”æ–‡ä»¶çš„ä¸‹è½½ï¼ˆâ†“ï¼‰, ç„¶è€Œè¦é€šè¿‡from_pretrainedæ–¹æ³•åŠ è½½ï¼Œè¿˜éœ€è¦æŠŠæ¨¡å‹æ–‡ä»¶åæ”¹æˆpytorch_model.bin
- è¿™æ ·ä¸‹è½½çš„æ¨¡å‹**æœ‰æŸ**ï¼Œåç»­æ— æ³•ä½¿ç”¨ï¼Œå› æ­¤æœ€å¥½æ˜¯é€šè¿‡gitä¸‹è½½

è¿™ç§æ–¹æ³•éº»çƒ¦
- Git lfs æ–¹æ¡ˆè¦ç®€æ´å¾—å¤š -- <span style='color:red'>ä¼˜é›…ä½†ä¸çµæ´»</span>
  - é—®é¢˜ï¼šä¼šä¸‹è½½ä»“åº“**æ‰€æœ‰**æ–‡ä»¶ï¼Œå¤§å¤§å»¶é•¿æ¨¡å‹ä¸‹è½½æ—¶é—´
- HuggingFace Hub: <span style='color:green'>ç²¾å‡†ä¸‹è½½</span>

å‚è€ƒ
- å®˜æ–¹æä¾›çš„ä¸‹è½½[æ–¹æ³•](https://huggingface.co/docs/hub/models-downloading)
- [ã€Hugging Faceã€‘å¦‚ä½•ä»hubä¸­ä¸‹è½½æ–‡ä»¶](https://automanbro.blog.csdn.net/article/details/133658587)
- [å¦‚ä½•ä¼˜é›…çš„ä¸‹è½½huggingface-transformersæ¨¡å‹](https://zhuanlan.zhihu.com/p/475260268)

#### (0) è‡ªåŠ¨ä¸‹è½½

æ¨¡å‹æ–‡ä»¶å¯¼å…¥
- é»˜è®¤ä¿å­˜è·¯å¾„ï¼š`~/.cache/huggingface/hub/`

```py
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# ---- ç¤ºä¾‹ -----
import transformers

MODEL_PATH = "./transformr_files/bert-base-uncased/"
# a.é€šè¿‡è¯å…¸å¯¼å…¥åˆ†è¯å™¨
tokenizer = transformers.BertTokenizer.from_pretrained(f"{MODEL_PATH}/bert-base-uncased-vocab.txt") 
# b. å¯¼å…¥é…ç½®æ–‡ä»¶
model_config = transformers.BertConfig.from_pretrained(MODEL_PATH)
# ã€2023-2-22ã€‘é»˜è®¤ä¿å­˜è·¯å¾„ï¼š~/.cache/huggingface/hub/
tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir='./transformers/')	# cache_dirè¡¨ç¤ºå°†é¢„è®­ç»ƒæ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹ï¼Œè€Œä¸æ˜¯é»˜è®¤è·¯å¾„

# ä¿®æ”¹é…ç½®
model_config.output_hidden_states = True
model_config.output_attentions = True
# é€šè¿‡é…ç½®å’Œè·¯å¾„å¯¼å…¥æ¨¡å‹
model = transformers.BertModel.from_pretrained(MODEL_PATH,config = model_config)
```

#### (1) HuggingFace Hub ç²¾å‡†ä¸‹è½½

é€šè¿‡ huggingface hub ä¸‹è½½æ¨¡å‹æ—¶ï¼Œæ¨¡å‹æ–‡ä»¶æ˜¯**blobç¼–ç **
- å› ä¸º huggingface_hub é»˜è®¤ä¸‹è½½ä»¥ç³»ç»Ÿ**å…¨å±€ç¼“å­˜**å½¢å¼ä¿å­˜ã€‚
- åªéœ€è¦é€šè¿‡ä¿®æ”¹ `snapshot_download(local_dir_use_symlinks=False)` å³å¯ä»¥å¾—åˆ°æ­£å¸¸çš„æ–‡ä»¶å½¢å¼

hf_hub_download

```py
#! pip install huggingface_hub

from huggingface_hub import hf_hub_download
import joblib

REPO_ID = "YOUR_REPO_ID"
FILENAME = "sklearn_model.joblib"

model = joblib.load(
    hf_hub_download(repo_id=REPO_ID, filename=FILENAME)
)

from huggingface_hub import hf_hub_download
# ä¸‹è½½å•ä¸ªæ–‡ä»¶
hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json")
hf_hub_download(repo_id="google/fleurs", filename="fleurs.py", repo_type="dataset")
# ä¸‹è½½ç‰¹å®šç‰ˆæœ¬
hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="v1.0")
hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="test-branch")
hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="refs/pr/3")
hf_hub_download(repo_id="lysandre/arxiv-nlp", filename="config.json", revision="877b84a8f93f2d619faa2a6e514a32beef88ab0a")
```

snapshot_download
- `snaphot_download` æ–¹æ³•æä¾›äº†`allow_regex`å’Œ`ignore_regex`ä¸¤ä¸ªå‚æ•°ï¼Œå‰è€…æ˜¯å¯¹æŒ‡å®šçš„åŒ¹é…é¡¹è¿›è¡Œä¸‹è½½ï¼Œåè€…æ˜¯å¿½ç•¥æŒ‡å®šçš„åŒ¹é…é¡¹ï¼Œä¸‹è½½å…¶ä½™éƒ¨åˆ†ã€‚

```py
# ä¸‹è½½æ•´ä¸ªä»£ç åº“
from huggingface_hub import snapshot_download
#    snapshot_download() é»˜è®¤ä¸‹è½½æœ€æ–°çš„ä¿®è®¢ç‰ˆæœ¬
snapshot_download(repo_id="lysandre/arxiv-nlp")
snapshot_download(repo_id="google/fleurs", repo_type="dataset")
snapshot_download(repo_id="lysandre/arxiv-nlp", revision="refs/pr/1") # ç‰¹å®šç‰ˆæœ¬
snapshot_download(repo_id="lysandre/arxiv-nlp", allow_patterns="*.json") # æŒ‡å®šè¦ä¸‹è½½çš„æ–‡ä»¶ç±»å‹
snapshot_download(repo_id="lysandre/arxiv-nlp", ignore_patterns=["*.msgpack", "*.h5"]) # å¿½ç•¥å“ªäº›æ–‡ä»¶
# å®ä¾‹ï¼š è¿‡æ»¤æŸäº›æ–‡ä»¶
snapshot_download(repo_id="bert-base-chinese")
snapshot_download(repo_id="bert-base-chinese", ignore_regex=["*.h5", "*.ot", "*.msgpack"])
```

ä½¿ç”¨ [huggingface_hub](https://github.com/huggingface/huggingface_hub) å·¥å…·åˆ›å»ºã€åˆ é™¤ã€æ›´æ–°å’Œç´¢å¼•æ¨¡å‹åº“


```py
# python -m pip install huggingface_hub
from huggingface_hub import hf_hub_download
import joblib

REPO_ID = "YOUR_REPO_ID" # åŒ model_name
FILENAME = "sklearn_model.joblib"

hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")
# æˆ–
model = joblib.load(
    hf_hub_download(repo_id=REPO_ID, filename=FILENAME)
)
# ä½¿ç”¨
from transformers import AutoConfig
config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")
```

#### (2) huggingface-cli å•æ–‡ä»¶ä¸‹è½½

huggingface-cli å‘½ä»¤ç›´æ¥ä»Hubä¸‹è½½æ–‡ä»¶ã€‚
- å†…éƒ¨ä½¿ç”¨ `hf_hub_download()` å’Œ `snapshot_download()` åŠ©æ‰‹ï¼Œå¹¶å°†è¿”å›è·¯å¾„æ‰“å°åˆ°ç»ˆç«¯
- æ–‡ä»¶å°†è¢«ä¸‹è½½åˆ°ç”±`HF_HOME`ç¯å¢ƒå˜é‡å®šä¹‰çš„ç¼“å­˜ç›®å½•ä¸­ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œåˆ™ä¸º`~/.cache/huggingface/hub`ï¼‰


```sh
# æŸ¥çœ‹å¯ç”¨å‚æ•°
huggingface-cli download --help
# ä¸‹è½½å•ä¸ªæ–‡ä»¶
huggingface-cli download gpt2 config.json
#~/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
huggingface-cli download google/gemma-7b-it-pytorch
# [2024-5-6] merlin ä¸Šæ‰§è¡Œå¤±ï¼Œè´¥403, mac æœ¬åœ°æˆåŠŸ
huggingface-cli download google/bigbird-roberta-large --local-dir=. --quiet
# æŒ‡å®šèº«ä»½
huggingface-cli download gpt2 config.json --token=hf_****
#/home/wauplin/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json
# åŒäº‹ä¸‹è½½å¤šä¸ªæ–‡ä»¶ï¼Œå¹¶è¿›åº¦æ¡æ˜¾ç¤º
huggingface-cli download gpt2 config.json model.safetensors
# è¿›åº¦æ¡é™éŸ³
huggingface-cli download gpt2 config.json model.safetensors --quiet
# æŒ‡å®šç›®å½•
huggingface-cli download gpt2 config.json --cache-dir=./cache
# ä¸‹è½½åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ï¼Œè€Œä¸å¸¦ç¼“å­˜ç›®å½•ç»“æ„ï¼Œåˆ™å¯ç”¨ --local-dir 
huggingface-cli download gpt2 config.json --local-dir=./models/gpt2
# æŒ‡å®šä¸åŒç±»å‹çš„ä»“åº“æˆ–ç‰ˆæœ¬æ¥ä¸‹è½½ï¼Œå¹¶ä½¿ç”¨globæ¨¡å¼åŒ…å«/æ’é™¤è¦ä¸‹è½½çš„æ–‡ä»¶
huggingface-cli download bigcode/the-stack --repo-type=dataset --revision=v1.2 --include="data/python/*" --exclude="*.json" --exclude="*.zip"
```


#### (3) git lfs ä¼˜é›…ä½†ä¸çµæ´»

```shell
# macä¸‹
brew install git-lfs
apt get install git-lfs # ubuntu
git lfs install
git clone https://huggingface.co/bert-base-chinese
# æˆ–
git lfs clone https://huggingface.co/stabilityai/sd-vae-ft-mse
# git clone https://huggingface.co/lmsys/vicuna-13b-delta-v0
# git clone git@hf.co:bigscience/bloom
# git clone https://huggingface.co/LinkSoul/Chinese-Llama-2-7b
GIT_LFS_SKIP_SMUDGE=1 # åªä¸‹è½½ pointer æ–‡ä»¶ï¼Œä¸ä¸‹å¤§æ–‡ä»¶
# åœ¨å½“å‰ç›®å½•æ–°å»ºä¸€ä¸ª models æ–‡ä»¶å¤¹ç”¨æ¥å­˜æ”¾å¤§æ¨¡å‹
# åªä¸‹è½½ç‰¹å®šæ–‡ä»¶
git lfs clone --include="*.bin" [HF_REPO]
```

#### (4) hf_transfer 

hf_transferæ˜¯ä¸€ä¸ªåŸºäºRustå¼€å‘çš„åº“ï¼Œç”¨äºåŠ é€Ÿä¸Hubçš„æ–‡ä»¶ä¼ è¾“
- å®‰è£…è¯¥åŒ… `pip install hf_transfer`
- å¹¶å°†`HF_HUB_ENABLE_HF_TRANSFER=1`è®¾ç½®ä¸ºç¯å¢ƒå˜é‡



#### æ¨¡å‹ä¸åŒç‚¹

[å…³äºtransformersåº“ä¸­ä¸åŒæ¨¡å‹çš„Tokenizer](https://zhuanlan.zhihu.com/p/121787628)

ä¸åŒPLMåŸå§‹è®ºæ–‡å’Œtransformersåº“ä¸­æ•°æ®çš„ç»„ç»‡æ ¼å¼ã€‚å…¶å®ï¼ŒåƒRobertaï¼ŒXLMç­‰æ¨¡å‹çš„ä¸­< s>, < /s>æ˜¯å¯ä»¥ç­‰ä»·äºBertä¸­çš„[CLS], [SEP]çš„ï¼Œåªä¸è¿‡ä¸åŒä½œè€…çš„ä¹ æƒ¯ä¸åŒã€‚

```shell
# Bert
å•å¥ï¼š[CLS] A [SEP]
å¥å¯¹ï¼š[CLS] A [SEP] A [SEP]
# Roberta
å•å¥ï¼š<s> A </s>
å¥å¯¹ï¼š<s> A </s> </s> B </s>
# Albert
å•å¥ï¼š[CLS] A [SEP]
å¥å¯¹ï¼š[CLS] A [SEP] B [SEP]
# XLNet
å•å¥ï¼š[A] <sep> <cls>
å¥å¯¹ï¼šA <sep> B <sep> <cls>
# XLM
å•å¥ï¼š<s> A </s>
å¥å¯¹ï¼š<s> A </s> B </s>
# XLM-Roberta
å•å¥ï¼š<s> A </s>
å¥å¯¹ï¼š<s> A </s> </s> B </s>
# Bart
å•å¥ï¼š<s> A </s>
å¥å¯¹ï¼š<s> A </s> </s> B </s>
```

transformersåº“ä¸­RobertaTokenizerå’ŒBertTokenizerçš„ä¸åŒ
- transformersåº“ä¸­`RobertaTokenizer`éœ€è¦**åŒæ—¶è¯»å–vocab_fileå’Œmerges_fileä¸¤ä¸ªæ–‡ä»¶**ï¼Œä¸åŒäº`BertTokenizer`åªéœ€è¦è¯»å–vocab_fileä¸€ä¸ªè¯æ–‡ä»¶ã€‚ä¸»è¦åŸå› æ˜¯ä¸¤ç§æ¨¡å‹é‡‡ç”¨çš„ç¼–ç ä¸åŒï¼š
- Berté‡‡ç”¨çš„æ˜¯**å­—ç¬¦**çº§åˆ«çš„BPEç¼–ç ï¼Œç›´æ¥ç”Ÿæˆè¯è¡¨æ–‡ä»¶ï¼Œå®˜æ–¹è¯è¡¨ä¸­åŒ…å«**3w**å·¦å³çš„å•è¯ï¼Œæ¯ä¸ªå•è¯åœ¨è¯è¡¨ä¸­çš„ä½ç½®å³å¯¹åº”Embeddingä¸­çš„ç´¢å¼•ï¼ŒBerté¢„ç•™äº†100ä¸ª\[unused]ä½ç½®ï¼Œä¾¿äºä½¿ç”¨è€…å°†è‡ªå·±æ•°æ®ä¸­é‡è¦çš„tokenæ‰‹åŠ¨æ·»åŠ åˆ°è¯è¡¨ä¸­ã€‚
- Robertaé‡‡ç”¨çš„æ˜¯**byte**çº§åˆ«çš„BPEç¼–ç ï¼Œå®˜æ–¹è¯è¡¨åŒ…å«**5w**å¤šçš„byteçº§åˆ«çš„tokenã€‚merges.txtä¸­å­˜å‚¨äº†æ‰€æœ‰çš„tokenï¼Œè€Œvocab.jsonåˆ™æ˜¯ä¸€ä¸ªbyteåˆ°ç´¢å¼•çš„æ˜ å°„ï¼Œé€šå¸¸é¢‘ç‡è¶Šé«˜çš„byteç´¢å¼•è¶Šå°ã€‚æ‰€ä»¥è½¬æ¢çš„è¿‡ç¨‹æ˜¯ï¼Œå…ˆå°†è¾“å…¥çš„æ‰€æœ‰tokensè½¬åŒ–ä¸ºmerges.txtä¸­å¯¹åº”çš„byteï¼Œå†é€šè¿‡vocab.jsonä¸­çš„å­—å…¸è¿›è¡Œbyteåˆ°ç´¢å¼•çš„è½¬åŒ–ã€‚

ç”±äºä¸­æ–‡çš„ç‰¹æ®Šæ€§ä¸å¤ªé€‚åˆé‡‡ç”¨byteçº§åˆ«çš„ç¼–ç ï¼Œæ‰€ä»¥å¤§éƒ¨åˆ†å¼€æºçš„ä¸­æ–‡Robertaé¢„è®­ç»ƒæ¨¡å‹ä»ç„¶é‡‡ç”¨çš„æ˜¯**å•å­—è¯è¡¨**ï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨BertTokenizerè¯»å–å³å¯ï¼Œ ä¸éœ€è¦ä½¿ç”¨RobertaTokenizerã€‚

### æ¨¡å‹ä¿å­˜


```python
tokenizer.save_pretrained(save_directory) # ä¿å­˜è¯è¡¨
model.save_pretrained(save_directory) # ä¿å­˜æ¨¡å‹
```

#### Safetensors

Safetensors æ˜¯ä¸€ç§ç”¨äºåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œæ¨¡å‹çš„æ–‡ä»¶æ ¼å¼ã€‚ å®‰å…¨æ€§ã€å¿«é€ŸåŠ è½½å’Œå…¼å®¹æ€§ç­‰ä¼˜ç‚¹ã€‚ 
- å°†æ¨¡å‹è½¬æ¢ä¸ºSafetensorsæ–‡ä»¶æ ¼å¼ï¼Œå¯åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šé«˜æ•ˆåœ°åŠ è½½å’Œè¿è¡Œæ¨¡å‹ï¼ŒåŒæ—¶ä¿æŠ¤æ¨¡å‹çš„å®ç°å’Œé€»è¾‘


Hugging Face å¼€å‘ Safetensorsçš„æ–°åºåˆ—åŒ–æ ¼å¼
- ç®€åŒ–å’Œç²¾ç®€å¤§å‹å¤æ‚å¼ é‡çš„å­˜å‚¨å’ŒåŠ è½½ã€‚

å¼ é‡æ˜¯æ·±åº¦å­¦ä¹ ä¸­ä½¿ç”¨çš„ä¸»è¦æ•°æ®ç»“æ„ï¼Œå…¶å¤§å°ä¼šç»™æ•ˆç‡å¸¦æ¥æŒ‘æˆ˜ã€‚
- Safetensorsç»“åˆä½¿ç”¨é«˜æ•ˆçš„åºåˆ—åŒ–å’Œå‹ç¼©ç®—æ³•æ¥å‡å°‘å¤§å‹å¼ é‡çš„å¤§å°ï¼Œä½¿å…¶æ¯”pickleç­‰å…¶ä»–åºåˆ—åŒ–æ ¼å¼æ›´å¿«ã€æ›´é«˜æ•ˆã€‚
- ä¸ä¼ ç»ŸPyTorchåºåˆ—åŒ–æ ¼å¼ `pytorch_model.bin` å’Œ `model.safetensors` ç›¸æ¯”ï¼ŒSafetensorsåœ¨CPUä¸Šçš„é€Ÿåº¦å¿«**76.6å€**ï¼Œåœ¨GPUä¸Šçš„é€Ÿåº¦å¿«**2å€**ã€‚

Safetensors API é€‚ç”¨äº: Pytorchã€Tensorflowã€PaddlePaddleã€Flaxå’ŒNumpy

å®‰è£…

```sh
pip install safetensors
```

åˆ›å»ºæ¨¡å‹

```py
from torch import nn

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.a = nn.Linear(100, 100)
        self.b = self.a

    def forward(self, x):
        return self.b(self.a(x))


model = Model()
print(model.state_dict())
```

æ¨¡å‹å¯¼å…¥å¯¼å‡º

```py
from safetensors.torch import load_model, save_model

save_model(model, "model.safetensors")

load_model(model, "model.safetensors")
print(model.state_dict())
# OrderedDict([('a.weight', tensor([[-0.0913, 0.0470, -0.0209, ..., -0.0540, -0.0575, -0.0679], [ 0.0268, 0.0765, 0.0952, ..., -0.0616, 0.0146, -0.0343], [ 0.0216, 0.0444, -0.0347, ..., -0.0546, 0.0036, -0.0454], ...,

```

å¼ é‡å¯¼å…¥ã€å¯¼å‡º

```py
import torch
from safetensors.torch import save_file, load_file

tensors = {
   "weight1": torch.zeros((1024, 1024)),
   "weight2": torch.zeros((1024, 1024))
}
save_file(tensors, "new_model.safetensors")
load_file("new_model.safetensors")
```


### GPU

ã€2023-2-22ã€‘[Efficient Training on Multiple GPUs](https://huggingface.co/docs/transformers/perf_train_gpu_many)

The following is the brief description of the main concepts that will be described later in depth in this document.
- DataParallel (`DP`) - the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step.
- TensorParallel (`TP`) - each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single gpu, each shard of the tensor resides on its designated gpu. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is what one may call horizontal parallelism, as the splitting happens on horizontal level.
- PipelineParallel (`PP`) - the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are places on a single gpu. Each gpu processes in parallel different stages of the pipeline and working on a small chunk of the batch.
- Zero Redundancy Optimizer (`ZeRO`) - Also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesnâ€™t need to be modified. It also supports various offloading techniques to compensate for limited GPU memory.
- Sharded `DDP` - is another name for the foundational ZeRO concept as used by various other implementations of ZeRO.

Before diving deeper into the specifics of each concept we first have a look at the rough decision process when training large models on a large infrastructure.


### æ¨ç†åŠ é€Ÿ

ã€2022-1-21ã€‘[è®© Transformer çš„æ¨ç†é€Ÿåº¦æé«˜ 4.5 å€ï¼Œè¿™ä¸ªå° trick è¿˜èƒ½ç»™ä½ çœåå‡ ä¸‡](https://mp.weixin.qq.com/s/fYxFwBvfQFPTqMZL6UI5WQ)
- NLPæ˜æ˜Ÿå…¬å¸Hugging Faceå‘å¸ƒäº†ä¸€ä¸ªå«åšInfinityçš„äº§å“ï¼Œå¯ä»¥ä»¥1mså»¶æ—¶å®ŒæˆTransformerçš„æ¨ç†ï¼Œæ€§èƒ½ç›¸å½“é«˜äº†ã€‚ä½†æœ‰ç‚¹è´µâ€”â€”1å¹´è‡³å°‘è¦åå‡ ä¸‡å— ï¼ˆ2ä¸‡ç¾å…ƒï¼‰
- æœ‰æ²¡æœ‰åˆ«çš„åŠæ³•ï¼ŸTransformer-deployï¼šå¼€æºçš„ã€â€œä¸è´¹å¹ç°ä¹‹åŠ›â€å°±å¯ä»¥è¾¾åˆ°Infinityä¸€äº›å…¬å…±åŸºå‡†çš„é‚£ç§ã€‚å¹¶ä¸”ç°åœ¨ï¼Œé€šè¿‡åœ¨è¯¥æ–¹æ³•ä¸Šæ–½åŠ ä¸€ä¸ªå°trickï¼ˆGPUé‡åŒ–ï¼ˆquantizationï¼‰ï¼‰ï¼Œå°†Transformerçš„æ¨ç†é€Ÿåº¦æé«˜4.5å€ï¼
  - ç”¨ä¸€è¡Œå‘½ä»¤ä¼˜åŒ–å’Œéƒ¨ç½²Hugging Faceä¸Šçš„Transformeræ¨¡å‹ï¼Œå¹¶æ”¯æŒå¤§å¤šæ•°åŸºäºTransformerç¼–ç å™¨çš„æ¨¡å‹ï¼Œæ¯”å¦‚Bertã€Robertaã€miniLMã€Camembertã€Albertã€XLM-Rã€Distilbertç­‰ã€‚
  - Transformer-deployæ¨ç†æœåŠ¡å™¨ç”¨çš„æ˜¯Nvidia Tritonã€‚æ¨ç†å¼•æ“ä¸ºMicrosoft ONNX Runtimeï¼ˆç”¨äºCPUå’ŒGPUæ¨ç†ï¼‰å’ŒNvidia TensorRTï¼ˆä»…é™ GPUï¼‰ã€‚å¦‚æœæƒ³åœ¨GPUä¸Šè·å¾—ä¸€æµçš„æ€§èƒ½ï¼ŒNvidia Triton+Nvidia TensorRTè¿™æ ·çš„ç»„åˆæ— ç–‘æ˜¯æœ€ä½³é€‰æ‹©ã€‚è™½ç„¶TensorRTç”¨èµ·æ¥æœ‰ç‚¹éš¾ï¼Œä½†å®ƒç¡®å®èƒ½æ¯”ç”¨Pytorchå¿«5ï½10å€ã€‚
  - åœ¨å®é™…æ€§èƒ½æµ‹è¯•ä¸­ï¼ŒTransformer-deployåœ¨batch sizeä¸º1ã€tokenåˆ†åˆ«ä¸º16å’Œ128çš„è¾“å…¥åºåˆ—ä¸­çš„æ¨ç†é€Ÿåº¦ï¼Œéƒ½æ¯”ä»˜è´¹çš„Hugging Face Infinityè¦å¿«ï¼šTransformer-deployåœ¨tokenä¸º16æ—¶è¦1.52msï¼ŒInfinityåˆ™éœ€è¦1.7msï¼›tokenä¸º128æ—¶éœ€è¦1.99msï¼ŒInfinityåˆ™éœ€è¦2.5msã€‚

### pipeline

pipeline APIå¯ä»¥å¿«é€Ÿä½“éªŒ Transformersã€‚å®ƒå°†æ¨¡å‹çš„é¢„å¤„ç†ã€åå¤„ç†ç­‰æ­¥éª¤åŒ…è£…èµ·æ¥ï¼Œç›´æ¥å®šä¹‰å¥½ä»»åŠ¡åç§°åè¾“å‡ºæ–‡æœ¬ï¼Œå¾—åˆ°ç»“æœã€‚è¿™æ˜¯ä¸€ä¸ªé«˜çº§çš„APIï¼Œå¯ä»¥é¢†ç•¥åˆ°transformers è¿™ä¸ªåº“çš„å¼ºå¤§ä¸”å‹å¥½ã€‚

ç”¨ [pipeline API](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/pipelines#pipelines)ï¼Œè¾“å…¥ä»»åŠ¡åç§°ï¼Œé»˜è®¤ä¼šé€‰æ‹©ç‰¹å®šå·²ç»å­˜å¥½çš„æ¨¡å‹æ–‡ä»¶ï¼Œç„¶åä¼šè¿›è¡Œä¸‹è½½å¹¶ä¸”ç¼“å­˜ã€‚

#### pipeline æµç¨‹

æ¥æ”¶æ–‡æœ¬åï¼Œé€šå¸¸æœ‰ä¸‰æ­¥ï¼š**preprocess** ï¼ˆTokenizerï¼‰ -> **fit model**ï¼ˆè®­ç»ƒæ¨¡å‹ï¼‰ -> **postprocessing** 
- ï¼ˆ1ï¼‰è¾“å…¥æ–‡æœ¬è¢«é¢„å¤„ç†æˆæœºå™¨å¯ä»¥ç†è§£çš„æ ¼å¼
  - å°†è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼ˆTokenizerï¼‰
    - å˜æˆï¼šwordsï¼Œsubwordsï¼Œæˆ–è€…symbolsï¼Œè¿™äº›ç»Ÿç§°ä¸ºtoken
  - å°†æ¯ä¸ªtokenæ˜ å°„ä¸ºä¸€ä¸ªinteger
  - ä¸ºè¾“å…¥æ·»åŠ æ¨¡å‹éœ€è¦çš„ç‰¹æ®Šå­—ç¬¦ã€‚
- ï¼ˆ2ï¼‰è¢«å¤„ç†åçš„è¾“å…¥è¢«ä¼ å…¥æ¨¡å‹ä¸­
- ï¼ˆ3ï¼‰æ¨¡å‹çš„é¢„æµ‹ç»“æœç»è¿‡åå¤„ç†ï¼Œå¾—åˆ°äººç±»å¯ä»¥ç†è§£çš„ç»“æœ

![](https://pic2.zhimg.com/v2-d9b23d02a7e5e1988ba8f902d7da9c0d_r.jpg)

æ³¨æ„ï¼š
- æ‰€æœ‰çš„é¢„å¤„ç†é˜¶æ®µï¼ˆPreprocessingï¼‰ï¼Œéƒ½è¦**ä¸æ¨¡å‹é¢„è®­ç»ƒé˜¶æ®µä¿æŒä¸€è‡´**ï¼Œæ‰€ä»¥è¦ä»Model Hub ä¸­ä¸‹è½½é¢„å¤„ç†çš„ä¿¡æ¯ã€‚
- ç”¨ AutoTokenizer çš„ from_pretrained æ–¹æ³•è¿›è¡Œtokenizer çš„åŠ è½½ï¼Œé€šè¿‡æŠŠtokenizer çš„checkpoint å¯¼å…¥ï¼Œå®ƒå¯ä»¥è‡ªåŠ¨è·å–tokenizeréœ€è¦çš„æ•°æ®å¹¶è¿›è¡Œç¼“å­˜ï¼ˆä¸‹æ¬¡æ— éœ€ä¸‹è½½ï¼‰ã€‚

```py
from transformers import AutoTokenizer
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint) # åŠ è½½åˆ†è¯å™¨
model = AutoModel.from_pretrained(checkpoint) # åŠ è½½æ¨¡å‹

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
# ----- æ–‡æœ¬idåŒ– -----
# æŒ‡å®šè¿”å›çš„å¼ é‡ç±»å‹ï¼ˆPyTorchã€TensorFlow æˆ–æ™®é€š NumPyï¼‰ï¼Œç”¨ return_tensors å‚æ•°
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs) # è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé”®çš„å­—å…¸ï¼Œinput_idså’Œattention_mask
# ----- æ¨¡å‹ ------
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
# è¾“å‡º torch.Size([2, 16, 768])

```

è¿”å›

```
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```

![](https://pic3.zhimg.com/80/v2-f51f9dae359ec191229b35028d0897ca_1440w.webp)

Transformers ä¸­æœ‰è®¸å¤šä¸åŒçš„æ¶æ„å¯ç”¨ï¼Œæ¯ä¸€ç§æ¶æ„éƒ½å›´ç»•ç€å¤„ç†ç‰¹å®šä»»åŠ¡è€Œè®¾è®¡ï¼Œæ¸…å•ï¼š
* Model (retrieve the hidden states)
* ForCausalLM
* ForMaskedLM
* ForMultipleChoice
* ForQuestionAnswering
* ForSequenceClassification
* ForTokenClassification
* and others

3ï¼‰Post-Processing
- æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„åŸå§‹éæ ‡å‡†åŒ–åˆ†æ•°ã€‚è¦è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå®ƒä»¬éœ€è¦ç»è¿‡ä¸€ä¸ªSoftMaxå±‚ï¼ˆæ‰€æœ‰ Transformers æ¨¡å‹éƒ½è¾“å‡º logitsï¼Œå› ä¸ºç”¨äºè®­ç»ƒçš„æŸè€—å‡½æ•°ä¸€èˆ¬ä¼šå°†æœ€åçš„æ¿€æ´»å‡½æ•°(å¦‚SoftMax)ä¸å®é™…æŸè€—å‡½æ•°(å¦‚äº¤å‰ç†µ)èåˆ 

```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```



#### pipeline ä»»åŠ¡

ç›®å‰æ”¯æŒçš„pipeline å¦‚ä¸‹ï¼š
- feature-extraction (get the vector representation of a text) ç‰¹å¾æŠ½å–
- fill-mask æ©ç å›å¤
- ner (named entity recognition) å‘½åå®ä½“è¯†åˆ«
- question-answering é—®ç­”
- sentiment-analysis æƒ…æ„Ÿåˆ†æ
- summarization æ–‡æœ¬æ‘˜è¦
- text-generation æ–‡æœ¬ç”Ÿæˆ
- translation æœºå™¨ç¿»è¯‘
- zero-shot-classification é›¶æ ·æœ¬åˆ†ç±»

æœ€æ–°pipelineç±»å‹ï¼šè¯¦è§[å®˜ç½‘ä»‹ç»](https://huggingface.co/transformers/main_classes/pipelines.html)
- AudioClassificationPipeline
- AutomaticSpeechRecognitionPipeline
- ConversationalPipeline
- FeatureExtractionPipeline
- FillMaskPipeline
- ImageClassificationPipeline
- ObjectDetectionPipeline
- QuestionAnsweringPipeline
- SummarizationPipeline
- TableQuestionAnsweringPipeline
- TextClassificationPipeline
- TextGenerationPipeline
- Text2TextGenerationPipeline
- TokenClassificationPipeline
- TranslationPipeline
- ZeroShotClassificationPipeline

æ‰€æœ‰çš„APIéƒ½å¯ä»¥é€šè¿‡ æœç´¢ï¼Œå¹¶ä¸”åœ¨çº¿æµ‹è¯•

#### ä»»åŠ¡æ¨¡å‹

ä¸»è¦çš„æ¨¡å‹ï¼š
- è‡ªå›å½’ï¼šGPT2ã€Transformer-XLã€XLNet
- è‡ªç¼–ç ï¼šBERTã€ALBERTã€RoBERTaã€ELECTRA
- Seq2Seqï¼šBARTã€Pegasusã€T5

å„ç§ä»»åŠ¡çš„ä»£è¡¨æ¨¡å‹

| Model	 | Examples	| Tasks |
|---|---|---|
| Encoder ç¼–ç å™¨æ¨¡å‹ | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa	| Sentence classification, named entity recognition, extractive question answering <br>é€‚åˆéœ€è¦ç†è§£å®Œæ•´å¥å­çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å¥å­åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ï¼ˆä»¥åŠæ›´ä¸€èˆ¬çš„å•è¯åˆ†ç±»ï¼‰å’Œæå–å¼é—®ç­” |
| Decoder è§£ç å™¨æ¨¡å‹ | CTRL, GPT, GPT-2, Transformer XL	| Text generation <br>è§£ç å™¨æ¨¡å‹çš„é¢„è®­ç»ƒé€šå¸¸å›´ç»•é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚è¿™äº›æ¨¡å‹æœ€é€‚åˆæ¶‰åŠæ–‡æœ¬ç”Ÿæˆçš„ä»»åŠ¡ |
| Encoder-decoder åºåˆ—åˆ°åºåˆ—æ¨¡å‹ | BART, T5, Marian, mBART | Summarization, translation, generative question answering <br>åºåˆ—åˆ°åºåˆ—æ¨¡å‹æœ€é€‚åˆå›´ç»•æ ¹æ®ç»™å®šè¾“å…¥ç”Ÿæˆæ–°å¥å­çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ‘˜è¦ã€ç¿»è¯‘æˆ–ç”Ÿæˆå¼é—®ç­”ã€‚|

#### Text classification

é»˜è®¤checkpoint æ˜¯ distilbert-base-uncased-finetuned-sst-2-english

```python
from transformers import pipeline

#checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
#tokenizer = AutoTokenizer.from_pretrained(checkpoint)
classifier = pipeline("sentiment-analysis")
# æŒ‡å®šæ¨¡å‹ï¼Œç¡¬ä»¶ç¯å¢ƒ
pipe = pipeline("sentiment-analysis", model=model_name, device=0)
# å•å¥
classifier("I've been waiting for a HuggingFace course my whole life.")
# å¤šå¥
classifier([
    "I've been waiting for a HuggingFace course my whole life.", 
    "I hate this so much!"
])
```


```python
## ------------ PYTORCH CODE ------------ 
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = "bert-base-cased-finetuned-mrpc"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

classes = ["not paraphrase", "is paraphrase"]

sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"

# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to
# the sequence, as well as compute the attention masks.
paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="pt")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="pt")

paraphrase_classification_logits = model(**paraphrase).logits
not_paraphrase_classification_logits = model(**not_paraphrase).logits

paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]
not_paraphrase_results = torch.softmax(not_paraphrase_classification_logits, dim=1).tolist()[0]

# Should be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")

# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")

## ------------ TENSORFLOW CODE ------------ 
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
import tensorflow as tf

model_name = "bert-base-cased-finetuned-mrpc"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFAutoModelForSequenceClassification.from_pretrained(model_name)

classes = ["not paraphrase", "is paraphrase"]

sequence_0 = "The company HuggingFace is based in New York City"
sequence_1 = "Apples are especially bad for your health"
sequence_2 = "HuggingFace's headquarters are situated in Manhattan"

# The tokenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to
# the sequence, as well as compute the attention masks.
paraphrase = tokenizer(sequence_0, sequence_2, return_tensors="tf")
not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors="tf")

paraphrase_classification_logits = model(paraphrase).logits
not_paraphrase_classification_logits = model(not_paraphrase).logits

paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]
not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]

# Should be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%")

# Should not be paraphrase
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%")
```


#### Zero-shot classification

æ–‡æœ¬åˆ†ç±»æ ‡æ³¨å¾€å¾€éå¸¸è€—æ—¶ï¼Œhuggingface æä¾›äº†0æ ·æœ¬åˆ†ç±»çš„pipelineï¼Œ ç”¨æˆ·åªéœ€è¦ä¼ å…¥æ–‡æœ¬å†…å®¹ï¼Œä»¥åŠå¯èƒ½çš„åˆ†ç±»æ ‡ç­¾ï¼Œå°±å¯ä»¥å¾—åˆ°æ¯ä¸ªæ ‡ç­¾çš„æ¦‚ç‡ï¼Œè¿™æ ·å­å¯ä»¥æä¾›æ ‡æ³¨äººå‘˜å‚è€ƒç»“æœï¼Œå¤§å¤§æé«˜æ ‡æ³¨æ•ˆç‡ã€‚

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification")
classifier(
    "This is a course about the Transformers library",
    candidate_labels=["education", "politics", "business"],
)
{'sequence': 'This is a course about the Transformers library',
 'labels': ['education', 'business', 'politics'],
 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}
```

#### Text generation

[å®˜æ–¹ generate æ–¹æ³•è§£é‡Š](https://huggingface.co/docs/transformers/main_classes/text_generation)

æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œæ˜¯æŒ‡ä½ è¾“å…¥å¼€å¤´çš„è¯æœ¯ï¼ˆpromptï¼‰ï¼Œç„¶åè®©æœºå™¨è‡ªåŠ¨å¸®ä½ ç”Ÿæˆå®Œå‰©ä¸‹çš„å¥å­ã€‚Text generation ä¸­åŒ…å«äº†ä¸€äº›éšæœºå› å­ï¼Œå› æ­¤æ¯æ¬¡ç”Ÿæˆçš„ç»“æœéƒ½å¯èƒ½ä¸åŒã€‚

```python
from transformers import pipeline

generator = pipeline("text-generation")
generator("In this course, we will teach you how to")
[{'generated_text': 'In this course, we will teach you how to understand and use '
                    'data flow and data interchange when handling user data. We '
                    'will be working with one or more of the most commonly used '
                    'data flows â€” data flows of various types, as seen by the '
                    'HTTP'}]
```

ä½ å¯ä»¥è®¾ç½®å‚æ•° num_return_sequences é€‰æ‹©è¿”å›çš„ç»“æœä¸ªæ•°ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ max_length é™åˆ¶æ¯æ¬¡è¿”å›çš„ç»“æœå¥å­çš„é•¿åº¦.

å¹¶ä¸”æ¨¡å‹é€‰æ‹©å¯ä»¥é€šè¿‡ model è®¾ç½®ï¼Œè¿™è¾¹é€‰æ‹© distilgpt2

```python
from transformers import pipeline

generator = pipeline("text-generation", model="distilgpt2")
generator(
    "In this course, we will teach you how to",
    max_length=30,
    num_return_sequences=2,
)
[{'generated_text': 'In this course, we will teach you how to manipulate the world and '
                    'move your mental and physical capabilities to your advantage.'},
 {'generated_text': 'In this course, we will teach you how to become an expert and '
                    'practice realtime, and with a hands on experience on both real '
                    'time and real'}]
```

#### Mask filling

æ©ç æ¢å¤æ˜¯å°†ä¸€ä¸ªå¥å­ä¸­éšæœºé®æ©<mask>çš„è¯ç»™æ¢å¤å›æ¥ï¼Œtop_k æ§åˆ¶äº†æ¦‚ç‡æœ€å¤§çš„ top k ä¸ªè¯è¢«è¿”å›ã€‚

ä¾‹å¦‚ï¼š

```python
from transformers import pipeline

unmasker = pipeline("fill-mask")
unmasker("This course will teach you all about <mask> models.", top_k=2)
[{'sequence': 'This course will teach you all about mathematical models.',
  'score': 0.19619831442832947,
  'token': 30412,
  'token_str': ' mathematical'},
 {'sequence': 'This course will teach you all about computational models.',
  'score': 0.04052725434303284,
  'token': 38163,
  'token_str': ' computational'}]
```

#### Named entity recognition

å‘½åå®ä½“æ˜¯è¢«æ˜¯æŒ‡å¦‚ä½•å°†æ–‡æœ¬ä¸­çš„å®ä½“ï¼Œä¾‹å¦‚ï¼špersons, locations, or organizationsï¼Œè¯†åˆ«å‡ºæ¥çš„ä»»åŠ¡ï¼š

```python
from transformers import pipeline

ner = pipeline("ner", grouped_entities=True)
ner("My name is Sylvain and I work at Hugging Face in Brooklyn.")
[{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18}, 
 {'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, 
 {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}
]
```

æ³¨æ„è¿™è¾¹è®¾ç½®äº† grouped_entities=Trueï¼Œè¿™å°±å‘Šè¯‰æ¨¡å‹ï¼Œå°†åŒä¸€ä¸ªentityçš„éƒ¨åˆ†ï¼Œèšåˆèµ·æ¥ï¼Œä¾‹å¦‚è¿™è¾¹çš„ â€œHuggingâ€ and â€œFaceâ€ æ˜¯ä¸€ä¸ªå®ä½“organizationï¼Œæ‰€ä»¥å°±æŠŠå®ƒç»™èšåˆèµ·æ¥ã€‚

åœ¨æ•°æ®é¢„å¤„ç†çš„éƒ¨åˆ†ï¼Œ Sylvain è¢«æ‹†è§£ä¸º4 pieces: S, ##yl, ##va, and ##in. è¿™è¾¹åå¤„ç†ä¹Ÿä¼šå°†è¿™äº›ç»™èšåˆèµ·æ¥ã€‚

#### Question answering

é˜…è¯»ç†è§£çš„é—®é¢˜ï¼Œæ˜¯é€šè¿‡æ–‡æœ¬å†…å®¹ï¼Œä»¥åŠæå‡ºçš„é—®é¢˜ï¼Œå¾—åˆ°ç­”æ¡ˆï¼š

```python
from transformers import pipeline

question_answerer = pipeline("question-answering")
question_answerer(
    question="Where do I work?",
    context="My name is Sylvain and I work at Hugging Face in Brooklyn"
)
{'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}
```

#### Summarization

æ‘˜è¦é—®é¢˜ï¼Œæ˜¯å°†é•¿æ–‡æœ¬çš„è¿›è¡Œå¥å­çš„å‹ç¼©ï¼Œå¾—åˆ°ç®€ç»ƒçš„å¥å­è¡¨è¾¾ã€‚

```python
from transformers import pipeline

summarizer = pipeline("summarization")
summarizer("""
    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.

    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
""")
[{'summary_text': ' America has changed dramatically during recent years . The '
                  'number of engineering graduates in the U.S. has declined in '
                  'traditional engineering disciplines such as mechanical, civil '
                  ', electrical, chemical, and aeronautical engineering . Rapidly '
                  'developing economies such as China and India, as well as other '
                  'industrial countries in Europe and Asia, continue to encourage '
                  'and advance engineering .'}]
```

è·Ÿtext generation ä»»åŠ¡ä¸€æ ·ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è®¾ç½®å‚æ•°ï¼š max_length or a min_length ï¼Œé™åˆ¶æ–‡æœ¬çš„é•¿åº¦ã€‚

#### Translation

æ–‡æœ¬ç¿»è¯‘ï¼Œä½ å¯ä»¥åœ¨ Model Hub ä¸­ï¼Œæ‰¾åˆ°ç‰¹å®šçš„ç¿»è¯‘æ¨¡å‹ï¼Œä¾‹å¦‚æ³•ç¿»è‹±çš„æ¨¡å‹ï¼Œ Helsinki-NLP/opus-mt-fr-enï¼š

```python
from transformers import pipeline

translator = pipeline("translation", model="Helsinki-NLP/opus-mt-fr-en")
translator("Ce cours est produit par Hugging Face.")
[{'translation_text': 'This course is produced by Hugging Face.'}]
```

### generate

æ–‡æœ¬ç”Ÿæˆæ–¹æ³• generate 

Each framework has a generate method for text generation implemented in their respective GenerationMixin class:
- PyTorch `generate`() is implemented in `GenerationMixin`.
- TensorFlow `generate`() is implemented in `TFGenerationMixin`.
- Flax/JAX `generate`() is implemented in `FlaxGenerationMixin`.

Regardless of your framework of choice, you can parameterize the generate method with a GenerationConfig class instance. Please refer to this class for the complete list of generation parameters, which control the behavior of the generation method.

<div class="mermaid">
    flowchart LR
    %% èŠ‚ç‚¹é¢œè‰²
    classDef red fill:#f02;
    classDef green fill:#5CF77B;
    classDef blue fill:#6BE0F7;
    classDef orange fill:#F7CF6B;
    classDef grass fill:#C8D64B;
    %%èŠ‚ç‚¹å…³ç³»å®šä¹‰
    A(GenerationMixin):::green -.->|PyTorch|M(generateæ–¹æ³•):::orange
    B(TFGenerationMixin):::green -.->|TensorFlow|M
    C(FlaxGenerationMixin):::green -.->|Flax/JAX|M
    G(GernarationConfig):::blue -.->|read|M
    M-->O(è‡ªå®šä¹‰ç”Ÿæˆä»£ç ):::grass
</div>

#### è¾“å…¥æ ¼å¼

Special tokens that can be used at generation time
- `pad_token_id` (int, optional) â€” å¡«å……å­—ç¬¦ The id of the padding token. 
- `bos_token_id` (int, optional) â€” å¼€å§‹å­—ç¬¦ The id of the beginning-of-sequence token.
- `eos_token_id` (Union[int, List[int]], optional) â€” ç»“æŸå­—ç¬¦ The id of the end-of-sequence token. Optionally, use a list to set multiple end-of-sequence tokens.

Generation parameters exclusive to encoder-decoder models ç¼–ç -è§£ç æ¨¡å‹ç‹¬æœ‰å‚æ•°
- `encoder_no_repeat_ngram_size` (int, optional, defaults to 0) â€” If set to int > 0, all ngrams of that size that occur in the encoder_input_ids cannot occur in the `decoder_input_ids`.
- `decoder_start_token_id` (int, optional) â€” If an encoder-decoder model starts decoding with a different token than bos, the id of that token.

#### è¾“å‡ºå‚æ•°

Parameters that define the output variables of `generate`
- `num_return_sequences`(int, optional, defaults to 1) â€” The number of independently computed returned sequences for each element in the batch. è¿”å›å¥å­æ•°ç›®
- `output_attentions` (bool, optional, defaults to False) â€” Whether or not to return the attentions tensors of all attention layers. See attentions under returned tensors for more details. æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›å€¼
- `output_hidden_states` (bool, optional, defaults to False) â€” Whether or not to return the hidden states of all layers. See hidden_states under returned tensors for more details. æ‰€æœ‰æ›¾çš„éšå±‚çŠ¶æ€
- `output_scores` (bool, optional, defaults to False) â€” Whether or not to return the prediction scores. See scores under returned tensors for more details. é¢„æµ‹åˆ†å€¼
- `return_dict_in_generate` (bool, optional, defaults to False) â€” Whether or not to return a [ModelOutput](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/output#transformers.utils.ModelOutput) instead of a plain tuple.


#### è§£ç ç­–ç•¥

generate è§£ç å‚æ•°
- `do_sample`: æ˜¯å¦é‡‡æ ·, é»˜è®¤ False(å¯¹åº”è´ªå¿ƒè§£ç )
- `num_beams`: æŸå®½ï¼Œint, é»˜è®¤ 1ï¼ˆä¸ç”¨beam searchï¼‰,é›†æŸæœç´¢å‚æ•°, 
- `num_beam_groups`: int, é»˜è®¤ 1, ä¸€ç»„æŸå®½ï¼Œé€šè¿‡ä¸åŒbeamå–å€¼è·å–æ›´å¥½çš„å¤šæ ·æ€§
- `penalty_alpha`: float, æƒ©ç½šå› å­ï¼Œç”¨äºcontrastive search decodingï¼Œå¹³è¡¡æ¨¡å‹ç½®ä¿¡åº¦ä¸é€€åŒ–æƒ©ç½š
- `use_cache`: é»˜è®¤True, æ˜¯å¦ä½¿ç”¨ä¸Šä¸€ä¸ªK/Væ³¨æ„åŠ›ï¼Œç”¨äºè§£ç æé€Ÿ

æ–¹æ³•ä½¿ç”¨: can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.
- `greedy decoding` è´ªå¿ƒè§£ç : [greedy search](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search), è§¦å‘æ¡ä»¶: `num_beams=1` and `do_sample=False`
- `contrastive search` å¯¹æ¯”æœç´¢: [contrastive_search](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationMixin.contrastive_search), è§¦å‘æ¡ä»¶: `penalty_alpha>0`` and `top_k>1`
- `multinomial sampling` å¤šé¡¹å¼é‡‡æ ·: [sample](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationMixin.sample), è§¦å‘æ¡ä»¶: `num_beams=1` and `do_sample=True`
- `beam-search decoding` é›†æŸè§£ç : [beam search](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationMixin.beam_search), è§¦å‘æ¡ä»¶: `num_beams>1` and `do_sample=False`
- `beam-search multinomial sampling` é›†æŸå¤šé¡¹å¼é‡‡æ ·: [beam sample](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationMixin.beam_sample), è§¦å‘æ¡ä»¶: `num_beams>1` and `do_sample=True`
- `diverse beam-search decoding` DBSè§£ç : `DBS` [group_beam_search](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationMixin.group_beam_search), è§¦å‘æ¡ä»¶: `num_beams>1` and `num_beam_groups>1`
  - DBSè®ºæ–‡å®ç°ï¼Œå°†æŸå®½å‡åˆ†æˆå‡ ç»„ï¼Œå°ç»„å†…æ‰§è¡Œå¸¸è§„bs, è§£ç æ—¶è€ƒè™‘ä¸å‰é¢åºåˆ—çš„å·®å¼‚æ€§ï¼Œæ•ˆæœæ¯”diverseRLå¥½ï¼Œ[è¯¦è§](https://wqw547243068.github.io/text-generation?#2018-diverse-beam-search-dbs)
- `constrained beam-search decoding` å¯¹æ¯”é›†æŸè§£ç : [constrained_beam_search](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationMixin.constrained_beam_search), è§¦å‘æ¡ä»¶: `constraints!=None` or `force_words_ids!=None`

ç†è®ºä¸Šï¼Œè§£ç ç­–ç•¥æœ‰å‡ ç§
- `è´ªå¿ƒæœç´¢`ï¼šgreedy search
- `é›†æŸæœç´¢`ï¼šbeam search
- `å…¨å±€æœç´¢`ï¼šåˆç§°æš´åŠ›æœç´¢, brute search
- ![](https://pica.zhimg.com/80/v2-ef3522dfec91840dcad6642981722b18_1440w.webp?source=1940ef5c)

ç»“åˆé‡‡æ ·æ–¹æ³•ï¼Œå¯ä»¥è¡ç”Ÿå‡ºå¤šç§è§£ç ç­–ç•¥
- generation.GenerationMixin

|generateç±»å‹|è§£ç ç­–ç•¥|å‚æ•°`do_sample`|å‚æ•°`num_beams`|å…¶å®ƒå‚æ•°| è§¦å‘æ¡ä»¶ |
|---|---|----|----|----|--------|
| `greedy decoding` | greedy_search `è´ªå¿ƒè§£ç ` | False | 1 | - | `num_beams=1` and `do_sample=False` |
| `contrastive search` | contrastive_search | - | - | - | `penalty_alpha>0` and `top_k>1` |
| `multinomial sampling` | sample `å¤šæ ·å¼é‡‡æ ·` | true | 1 | - |`num_beams=1` and `do_sample=True` |
| `beam-search decoding` | beam_search `é›†æŸè§£ç ` | False | >1 | - | `num_beams>1` and `do_sample=False` |
| `beam-search multinomial sampling` | beam_sample `é›†æŸè§£ç `+`å¤šæ ·å¼é‡‡æ ·` | True | >1 | - | `num_beams>1` and `do_sample=True` |
| `diverse beam-search decoding` | group_beam_search `å¤šæ ·æ€§é›†æŸè§£ç ` | - | >1 | - | `num_beams>1` and `num_beam_groups>1` |
| `constrained beam-search decoding` | constrained_beam_search `å—é™é›†æŸè§£ç ` | å—é™beam search | - | - |`constraints!=None` or `force_words_ids!=None` |

å‚æ•°ï¼š
- [text_generation](https://huggingface.co/docs/transformers/main_classes/text_generation)å‡½æ•°è¯´æ˜
- generation[æºç ](https://github.com/huggingface/transformers/tree/v4.26.1/src/transformers/generation)

#### å“ªç§è§£ç æ–¹æ³•æœ€å¥½ï¼Ÿ

æ²¡æœ‰ä¸€ä¸ªæ™®é "æœ€ä½³"çš„è§£ç æ–¹æ³•ã€‚å“ªç§æ–¹æ³•æœ€å¥½å–å†³äºç”Ÿæˆä»»åŠ¡çš„æ€§è´¨ã€‚
- å¦‚æœæƒ³æ‰§è¡Œ**ç²¾ç¡®**çš„ä»»åŠ¡ï¼Œå¦‚è¿›è¡Œç®—æœ¯è¿ç®—æˆ–æä¾›ä¸€ä¸ªç‰¹å®šé—®é¢˜çš„ç­”æ¡ˆï¼Œé‚£ä¹ˆ**é™ä½æ¸©åº¦**æˆ–ä½¿ç”¨**ç¡®å®šæ€§**æ–¹æ³•ï¼Œå¦‚`è´ªå©ªæœç´¢`ä¸`æŸæœç´¢`ç›¸ç»“åˆï¼Œä»¥ä¿è¯å¾—åˆ°æœ€å¯èƒ½çš„ç­”æ¡ˆã€‚
- å¦‚æœæƒ³ç”Ÿæˆ**æ›´é•¿**çš„æ–‡æœ¬ï¼Œç”šè‡³æœ‰ç‚¹**åˆ›é€ æ€§**ï¼Œé‚£ä¹ˆæ”¹ç”¨**æŠ½æ ·æ–¹æ³•**ï¼Œå¹¶**æé«˜æ¸©åº¦**ï¼Œæˆ–è€…ä½¿ç”¨top-kå’Œæ ¸æŠ½æ ·çš„æ··åˆæ–¹æ³•ã€‚

ä½œè€…ï¼š[è‡´Great](https://www.zhihu.com/question/415657741/answer/2430106609)


### Demoå‘å¸ƒï¼ˆspaceï¼‰

ã€2022-10-8ã€‘[Spaces](https://huggingface.co/spaces) ï¼šDiscover amazing ML apps made by the community! å±•ç¤ºå„ç§DEMO
- [Hugging Face Spaces](https://huggingface.co/spaces) will host the interface on its servers and provide you with a link you can share.
- æ›´å¤šç”¨æ³•ï¼Œå‚è€ƒå¦ä¸€ç¯‡æ—¥å¿—ï¼š[Pythonä¸‹çš„æ¨¡å‹å¿«é€Ÿéƒ¨ç½²](https://wqw547243068.github.io/python?#%E6%A8%A1%E5%9E%8B%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2)


## transformers BERT æºç 

å‚è€ƒï¼š
- [BERTæºç è¯¦è§£ï¼ˆä¸€ï¼‰â€”â€”HuggingFace Transformersæœ€æ–°ç‰ˆæœ¬æºç è§£è¯»](https://zhuanlan.zhihu.com/p/360988428)
- [BERTæºç è¯¦è§£ï¼ˆäºŒï¼‰â€”â€”HuggingFace Transformersæœ€æ–°ç‰ˆæœ¬æºç è§£è¯»](https://zhuanlan.zhihu.com/p/363014957)

1. BERT Tokenizationåˆ†è¯æ¨¡å‹ï¼ˆBertTokenizerï¼‰ï¼ˆè¯·çœ‹ä¸Šç¯‡ï¼‰
2. BERT Modelæœ¬ä½“æ¨¡å‹ï¼ˆBertModelï¼‰ï¼ˆè¯·çœ‹ä¸Šç¯‡ï¼‰
  - 2.1 BertEmbeddings
  - 2.2 BertEncoder
    - 2.2.1 BertLayer
      - 2.2.1.1 BertAttention
        - 2.2.1.1 BertSelfAttention
        - 2.2.1.2 BertSelfOutput
      - 2.2.1.2 BertIntermediate
      - 2.2.1.3 BertOutput
    - 2.2.2 BertPooler
3. BERT-based Modelsåº”ç”¨æ¨¡å‹
  - 3.1 BertForPreTraining
  - 3.2 BertForSequenceClassification
  - 3.3 BertForMultiChoice
  - 3.4 BertForTokenClassification
  - 3.5 BertForQuestionAnswering
4. BERTè®­ç»ƒä¸ä¼˜åŒ–
  - 4.1 Pre-Training
  - 4.2 Fine-Tuning
    - 4.2.1 AdamW
    - 4.2.2 Warmup

### BERT å¿«é€Ÿè°ƒç”¨

5è¡Œä»£ç 

```py
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

inputs = tokenizer("Hello world!")
outputs = model(**inputs)
```


### Tokenizationï¼ˆBertTokenizerï¼‰

å’ŒBERTæœ‰å…³çš„Tokenizerä¸»è¦å†™åœ¨/models/bert/tokenization_bert.pyå’Œ/models/bert/tokenization_bert_fast.py ä¸­ã€‚è¿™ä¸¤ä»½ä»£ç åˆ†åˆ«å¯¹åº”åŸºæœ¬çš„BertTokenizerï¼Œä»¥åŠä¸è¿›è¡Œtokenåˆ°indexæ˜ å°„çš„BertTokenizerFastï¼Œè¿™é‡Œä¸»è¦è®²è§£ç¬¬ä¸€ä¸ªã€‚

BertTokenizer æ˜¯åŸºäº`BasicTokenizer`å’Œ`WordPieceTokenizer` çš„åˆ†è¯å™¨ï¼š
- `BasicTokenizer`è´Ÿè´£å¤„ç†çš„ç¬¬ä¸€æ­¥â€”â€”æŒ‰æ ‡ç‚¹ã€ç©ºæ ¼ç­‰åˆ†å‰²å¥å­ï¼Œå¹¶å¤„ç†æ˜¯å¦ç»Ÿä¸€å°å†™ï¼Œä»¥åŠæ¸…ç†éæ³•å­—ç¬¦ã€‚ç»§æ‰¿è‡ª class BertTokenizer(PreTrainedTokenizer):
  - å¯¹äºä¸­æ–‡å­—ç¬¦ï¼Œé€šè¿‡é¢„å¤„ç†ï¼ˆåŠ ç©ºæ ¼ï¼‰æ¥æŒ‰å­—åˆ†å‰²ï¼›
  - åŒæ—¶å¯ä»¥é€šè¿‡never_splitæŒ‡å®šå¯¹æŸäº›è¯ä¸è¿›è¡Œåˆ†å‰²ï¼›
  - è¿™ä¸€æ­¥æ˜¯å¯é€‰çš„ï¼ˆé»˜è®¤æ‰§è¡Œï¼‰ã€‚
- `WordPieceTokenizer`åœ¨è¯çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å°†è¯åˆ†è§£ä¸ºå­è¯ï¼ˆsubwordï¼‰ ã€‚
  - subwordä»‹äºcharå’Œwordä¹‹é—´ï¼Œæ—¢åœ¨ä¸€å®šç¨‹åº¦ä¿ç•™äº†è¯çš„å«ä¹‰ï¼Œåˆèƒ½å¤Ÿç…§é¡¾åˆ°è‹±æ–‡ä¸­å•å¤æ•°ã€æ—¶æ€å¯¼è‡´çš„è¯è¡¨çˆ†ç‚¸å’Œæœªç™»å½•è¯çš„OOVï¼ˆOut-Of-Vocabularyï¼‰é—®é¢˜ï¼Œå°†è¯æ ¹ä¸æ—¶æ€è¯ç¼€ç­‰åˆ†å‰²å‡ºæ¥ï¼Œä»è€Œå‡å°è¯è¡¨ï¼Œä¹Ÿé™ä½äº†è®­ç»ƒéš¾åº¦ï¼›
  - ä¾‹å¦‚ï¼Œtokenizerè¿™ä¸ªè¯å°±å¯ä»¥æ‹†è§£ä¸ºâ€œtokenâ€å’Œâ€œ##izerâ€ä¸¤éƒ¨åˆ†ï¼Œæ³¨æ„åé¢ä¸€ä¸ªè¯çš„â€œ##â€è¡¨ç¤ºæ¥åœ¨å‰ä¸€ä¸ªè¯åé¢ã€‚

BertTokenizer æœ‰ä»¥ä¸‹å¸¸ç”¨æ–¹æ³•ï¼š
- `from_pretrained`ï¼šä»åŒ…å«è¯è¡¨æ–‡ä»¶ï¼ˆvocab.txtï¼‰çš„ç›®å½•ä¸­åˆå§‹åŒ–ä¸€ä¸ªåˆ†è¯å™¨ï¼›
- `tokenize`ï¼šå°†æ–‡æœ¬ï¼ˆè¯æˆ–è€…å¥å­ï¼‰åˆ†è§£ä¸ºå­è¯åˆ—è¡¨ï¼›
- `convert_tokens_to_ids`ï¼šå°†å­è¯åˆ—è¡¨è½¬åŒ–ä¸ºå­è¯å¯¹åº”**ä¸‹æ ‡**çš„åˆ—è¡¨ï¼›
- `convert_ids_to_tokens` ï¼šä¸ä¸Šä¸€ä¸ªç›¸åï¼›
- `convert_tokens_to_string`ï¼šå°†subwordåˆ—è¡¨æŒ‰â€œ##â€æ‹¼æ¥å›è¯æˆ–è€…å¥å­ï¼›
- `encode`ï¼š
  - å¯¹äº**å•ä¸ªå¥å­**è¾“å…¥ï¼Œåˆ†è§£è¯å¹¶åŠ å…¥ç‰¹æ®Šè¯å½¢æˆâ€œ\[CLS], x, \[SEP]â€çš„ç»“æ„å¹¶è½¬æ¢ä¸ºè¯è¡¨å¯¹åº”ä¸‹æ ‡çš„åˆ—è¡¨ï¼›
  - å¯¹äº**ä¸¤ä¸ªå¥å­**è¾“å…¥ï¼ˆå¤šä¸ªå¥å­åªå–å‰ä¸¤ä¸ªï¼‰ï¼Œåˆ†è§£è¯å¹¶åŠ å…¥ç‰¹æ®Šè¯å½¢æˆâ€œ\[CLS], x1, \[SEP], x2, \[SEP]â€çš„ç»“æ„å¹¶è½¬æ¢ä¸ºä¸‹æ ‡åˆ—è¡¨ï¼›
- `decode`ï¼šå¯ä»¥å°†encodeæ–¹æ³•çš„è¾“å‡ºå˜ä¸ºå®Œæ•´å¥å­ã€‚

æ„å»ºtransformeråˆ†è¯å™¨æ—¶, é€šå¸¸ç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ª **merges.txt** å’Œ **vocab.json** æ–‡ä»¶ã€‚[transformersåˆ†è¯æ–¹æ³•](https://zhuanlan.zhihu.com/p/424565138)
- merges.txtç”¨äºæŠŠæ–‡æœ¬è½¬æ¢ä¸ºå•è¯
- é€šè¿‡vocab.jsonæ–‡ä»¶å¤„ç†è¿™äº›å•è¯ï¼Œè¯¥æ–‡ä»¶åªæ˜¯ä¸€ä¸ªä»å•è¯åˆ°å•è¯IDçš„æ˜ å°„æ–‡ä»¶ï¼š

æ¨¡å‹åœ¨ä½¿ç”¨ä¹‹å‰éœ€è¦è¿›è¡Œ**åˆ†è¯**å’Œ**ç¼–ç **ï¼Œæ¯ä¸ªæ¨¡å‹éƒ½ä¼šè‡ªå¸¦`åˆ†è¯å™¨`ï¼ˆtokenizerï¼‰ï¼Œç†Ÿæ‚‰åˆ†è¯å™¨çš„ä½¿ç”¨å°†ä¼šæé«˜æ¨¡å‹æ„å»ºçš„æ•ˆç‡ã€‚
- string åŸå§‹å­—ç¬¦ä¸²ï¼š :hello world!"
- tokens å•è¯ä¸²ï¼š [ "hello", "world", "!"]
- token ids ä¸²ï¼š [ 7592, 2088, 999]
- ![](https://pic4.zhimg.com/80/v2-b1d35ce62b42d3416f8abe01a073f883_1440w.webp)

```py
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = 'hello world!'
# å­—ç¬¦ä¸²è½¬æ¢ä¸ºå•è¯åˆ—è¡¨
token_ids = tokenizer(text) 
# å•è¯åˆ—è¡¨è½¬æ¢ä¸ºå•è¯IDåˆ—è¡¨
token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)) 
# ä¸€æ­¥åˆ°ä½ï¼šå­—ç¬¦ä¸² â†’ idåˆ—è¡¨
# å¦‚ï¼š [101, 7592, 2088, 999, 102]
token_ids = tokenizer.encode(text)
tokens = tokenizer.encode(text, max_length=512, padding='max_length', return_tensors='pt') # ä½¿ç”¨æ›´å¤šå‚æ•°
# encodeä»…è¾“å‡ºå•è¯IDå¼ é‡ï¼Œencode_plusè¾“å‡ºåŒ…å«å•è¯IDå¼ é‡å’Œé™„åŠ å¼ é‡çš„å­—å…¸ã€‚
# {'input_ids': [101, 7592, 2088, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}
token_ids = tokenizer.encode_plus(text)
# æ‰¹é‡è½¬åŒ–
# {'input_ids': [[101, 7592, 2088, 999, 102], [101, 7592, 7733, 999, 102]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}
token_ids = tokenizer.batch_encode_plus([text])
```

#### å˜æ¢è¿‡ç¨‹

ã€2023-3-21ã€‘`string`ã€`tokens`ã€`ids` ä¸‰è€…è½¬æ¢
- `string` â†’ `tokens` **tokenize**(text: str, **kwargs)
- `tokens` â†’ `string` **convert_tokens_to_string**(tokens: List\[token])
- `tokens` â†’ `ids` **convert_tokens_to_ids**(tokens: List\[token])
- `ids` â†’ `tokens` **convert_ids_to_tokens**(ids: int or List\[int], skip_special_tokens=False)
- `string` â†’ `ids` **encode**(text, text_pair=None, add_special_tokens=True, padding=False, truncation=False, max_length=None, return_tensors=None)
- `ids` â†’ `string` **decode**(token_ids: List[int], skip_special_tokens=False, clean_up_tokenization_spaces=True)

tokenizerç”¨æ³•ï¼šencodeã€encode_plusã€batch_encode_plusç­‰ç­‰
- **encode_plus**:
  - encode_plus(text, text_pair=None, add_special_tokens=True, padding=False, truncation=False, max_length=None, stride=0, is_pretokenized=False, pad_to_multiple_of=None, return_tensors=None, return_token_type_ids=None, return_attention_mask=None, return_overflowing_tokens=False, return_special_tokens_mask=False, return_offsets_mapping=False, return_length=False)
- **batch_encode_plus**: è¾“å…¥ä¸º encode è¾“å…¥çš„ batchï¼Œå…¶å®ƒå‚æ•°ç›¸åŒã€‚æ³¨æ„ï¼Œplus æ˜¯è¿”å›ä¸€ä¸ªå­—å…¸ã€‚
- **batch_decode**: è¾“å…¥æ˜¯batch.

```py
#è¿™é‡Œä»¥bertæ¨¡å‹ä¸ºä¾‹ï¼Œä½¿ç”¨ä¸Šè¿°æåˆ°çš„å‡½æ•°

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
text = "It's a nice day today!"

#tokenize,#ä»…ç”¨äºåˆ†è¯
seg_words = tokenizer.tokenize(text)  
print("tokenizeåˆ†è¯ç»“æœï¼š\n",seg_words)

#convert_tokens_to_idsï¼Œå°†tokenè½¬åŒ–æˆidï¼Œåœ¨åˆ†è¯ä¹‹åã€‚
#convert_ids_to_tokens,å°†idè½¬åŒ–æˆtokenï¼Œé€šå¸¸ç”¨äºæ¨¡å‹é¢„æµ‹å‡ºç»“æœï¼ŒæŸ¥çœ‹æ—¶ä½¿ç”¨ã€‚
seg_word_id = tokenizer.convert_tokens_to_ids(seg_words)  
print("tokenize Id:\n",seg_word_id)

#encode,è¿›è¡Œåˆ†è¯å’Œtokenè½¬æ¢ï¼Œencode=tokenize+convert_tokens_to_ids
encode_text = tokenizer.encode(text)
print("encodeç»“æœï¼š\n",encode_text)

#encode_plus,åœ¨encodeçš„åŸºç¡€ä¹‹ä¸Šç”Ÿæˆinput_idsã€token_type_idsã€attention_mask
encode_plus_text = tokenizer.encode_plus(text)
print("encode_plusç»“æœï¼š\n",encode_plus_text)

#batch_encode_plus,åœ¨encode_plusçš„åŸºç¡€ä¹‹ä¸Šï¼Œèƒ½å¤Ÿæ‰¹é‡æ¢³ç†æ–‡æœ¬ã€‚
batch_encode_plus_text = tokenizer.batch_encode_plus([text,text])
print("batch_encode_plusç»“æœï¼š\n",batch_encode_plus_text)
```

[åŸæ–‡é“¾æ¥](https://blog.csdn.net/weixin_48030475/article/details/128688629)

#### å˜æ¢å›¾è§£

<div class="mxgraph" style="max-width:100%;border:1px solid transparent;" data-mxgraph="{&quot;highlight&quot;:&quot;#0000ff&quot;,&quot;nav&quot;:true,&quot;resize&quot;:true,&quot;toolbar&quot;:&quot;zoom layers tags lightbox&quot;,&quot;edit&quot;:&quot;_blank&quot;,&quot;xml&quot;:&quot;&lt;mxfile host=\&quot;app.diagrams.net\&quot; modified=\&quot;2023-03-21T09:35:14.611Z\&quot; agent=\&quot;5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\&quot; etag=\&quot;RBF3JRrHLDmxsSUVTnIQ\&quot; version=\&quot;21.0.7\&quot;&gt;&lt;diagram id=\&quot;Lw-1uFHNzwHmlxUDpAkU\&quot; name=\&quot;ç¬¬ 1 é¡µ\&quot;&gt;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&lt;/diagram&gt;&lt;/mxfile&gt;&quot;}"></div>
<script type="text/javascript" src="https://viewer.diagrams.net/js/viewer-static.min.js"></script>

### Modelï¼ˆBertModelï¼‰

å’ŒBERTæ¨¡å‹æœ‰å…³çš„ä»£ç ä¸»è¦å†™åœ¨`/models/bert/modeling_bert.py`ä¸­ï¼Œè¿™ä»½ä»£ç æœ‰1kå¤šè¡Œï¼ŒåŒ…å«BERTæ¨¡å‹çš„åŸºæœ¬ç»“æ„å’ŒåŸºäºå®ƒçš„å¾®è°ƒæ¨¡å‹ç­‰ã€‚ç»§æ‰¿è‡ªclass BertModel(BertPreTrainedModel)

BertModelä¸»è¦ä¸º transformer encoder ç»“æ„ï¼ŒåŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š
- `embeddings`ï¼Œå³BertEmbeddingsç±»çš„å®ä½“ï¼Œå¯¹åº”è¯åµŒå…¥ï¼›
- `encoder`ï¼Œå³BertEncoderç±»çš„å®ä½“ï¼›
- `pooler`ï¼Œ å³BertPoolerç±»çš„å®ä½“ï¼Œè¿™ä¸€éƒ¨åˆ†æ˜¯å¯é€‰çš„ã€‚

è¡¥å……ï¼š
- BertModelä¹Ÿå¯ä»¥é…ç½®ä¸ºDecoderï¼Œä¸è¿‡ä¸‹æ–‡ä¸­ä¸åŒ…å«å¯¹è¿™ä¸€éƒ¨åˆ†çš„è®¨è®ºã€‚

BertModelçš„å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­å„ä¸ªå‚æ•°çš„å«ä¹‰ä»¥åŠè¿”å›å€¼ï¼š

```python
def forward(
    self,
    input_ids=None,
    attention_mask=None,
    token_type_ids=None,
    position_ids=None,
    head_mask=None,
    inputs_embeds=None,
    encoder_hidden_states=None,
    encoder_attention_mask=None,
    past_key_values=None,
    use_cache=None,
    output_attentions=None,
    output_hidden_states=None,
    return_dict=None,
): ...
```

è¯´æ˜ï¼š
- `input_ids`ï¼šç»è¿‡ tokenizer åˆ†è¯åçš„`subword`å¯¹åº”çš„ä¸‹æ ‡åˆ—è¡¨ï¼›
- `attention_mask`ï¼šåœ¨self-attentionè¿‡ç¨‹ä¸­ï¼Œè¿™ä¸€å—maskç”¨äºæ ‡è®°`subword`æ‰€å¤„å¥å­å’Œ`padding`çš„åŒºåˆ«ï¼Œå°†paddingéƒ¨åˆ†å¡«å……ä¸º0ï¼›
- `token_type_ids`ï¼š æ ‡è®°subwordå½“å‰æ‰€å¤„å¥å­ï¼ˆç¬¬ä¸€å¥/ç¬¬äºŒå¥/paddingï¼‰ï¼›
- `position_ids`ï¼š æ ‡è®°å½“å‰è¯æ‰€åœ¨å¥å­çš„ä½ç½®ä¸‹æ ‡ï¼›
- `head_mask`ï¼š ç”¨äºå°†æŸäº›å±‚çš„æŸäº›æ³¨æ„åŠ›è®¡ç®—æ— æ•ˆåŒ–ï¼›
- `inputs_embeds`ï¼š å¦‚æœæä¾›äº†ï¼Œé‚£å°±ä¸éœ€è¦input_idsï¼Œè·¨è¿‡embedding lookupè¿‡ç¨‹ç›´æ¥ä½œä¸ºEmbeddingè¿›å…¥Encoderè®¡ç®—ï¼›
- `encoder_hidden_states`ï¼š è¿™ä¸€éƒ¨åˆ†åœ¨BertModelé…ç½®ä¸ºdecoderæ—¶èµ·ä½œç”¨ï¼Œå°†æ‰§è¡Œcross-attentionè€Œä¸æ˜¯self-attentionï¼›
- `encoder_attention_mask`ï¼š åŒä¸Šï¼Œåœ¨cross-attentionä¸­ç”¨äºæ ‡è®°encoderç«¯è¾“å…¥çš„paddingï¼›
- `past_key_values`ï¼šè¿™ä¸ªå‚æ•°è²Œä¼¼æ˜¯æŠŠé¢„å…ˆè®¡ç®—å¥½çš„K-Vä¹˜ç§¯ä¼ å…¥ï¼Œä»¥é™ä½cross-attentionçš„å¼€é”€ï¼ˆå› ä¸ºåŸæœ¬è¿™éƒ¨åˆ†æ˜¯é‡å¤è®¡ç®—ï¼‰ï¼›
- `use_cache`ï¼š å°†ä¿å­˜ä¸Šä¸€ä¸ªå‚æ•°å¹¶ä¼ å›ï¼ŒåŠ é€Ÿdecodingï¼›
- `output_attentions`ï¼šæ˜¯å¦è¿”å›ä¸­é—´æ¯å±‚çš„attentionè¾“å‡ºï¼›
- `output_hidden_states`ï¼šæ˜¯å¦è¿”å›ä¸­é—´æ¯å±‚çš„è¾“å‡ºï¼›
- `return_dict`ï¼šæ˜¯å¦æŒ‰é”®å€¼å¯¹çš„å½¢å¼ï¼ˆModelOutputç±»ï¼Œä¹Ÿå¯ä»¥å½“ä½œtupleç”¨ï¼‰è¿”å›è¾“å‡ºï¼Œé»˜è®¤ä¸ºçœŸã€‚

æ³¨æ„
- è¿™é‡Œçš„head_maskå¯¹æ³¨æ„åŠ›è®¡ç®—çš„æ— æ•ˆåŒ–ï¼Œå’Œä¸‹æ–‡æåˆ°çš„æ³¨æ„åŠ›å¤´å‰ªæä¸åŒï¼Œè€Œä»…ä»…æŠŠæŸäº›æ³¨æ„åŠ›çš„è®¡ç®—ç»“æœç»™ä¹˜ä»¥è¿™ä¸€ç³»æ•°ã€‚

è¿”å›å€¼ä¸ä½†åŒ…å«äº†encoderå’Œpoolerçš„è¾“å‡ºï¼Œä¹ŸåŒ…å«äº†å…¶ä»–æŒ‡å®šè¾“å‡ºçš„éƒ¨åˆ†ï¼ˆhidden_stateså’Œattentionç­‰ï¼Œè¿™ä¸€éƒ¨åˆ†åœ¨encoder_outputs\[1:]ï¼‰æ–¹ä¾¿å–ç”¨

BertModelè¿˜æœ‰ä»¥ä¸‹çš„æ–¹æ³•ï¼Œæ–¹ä¾¿BERTç©å®¶è¿›è¡Œå„ç§éªšæ“ä½œï¼š
- `get_input_embeddings`ï¼šæå–embeddingä¸­çš„word_embeddingså³è¯å‘é‡éƒ¨åˆ†ï¼›
- `set_input_embeddings`ï¼šä¸ºembeddingä¸­çš„word_embeddingsèµ‹å€¼ï¼›
- `_prune_heads`ï¼šæä¾›äº†å°†æ³¨æ„åŠ›å¤´å‰ªæçš„å‡½æ•°ï¼Œè¾“å…¥ä¸º {layer_num: list of heads to prune in this layer} çš„å­—å…¸ï¼Œå¯ä»¥å°†æŒ‡å®šå±‚çš„æŸäº›æ³¨æ„åŠ›å¤´å‰ªæã€‚

è¡¥å……ï¼š
- **å‰ªæ**æ˜¯ä¸€ä¸ªå¤æ‚çš„æ“ä½œï¼Œéœ€è¦å°†ä¿ç•™çš„æ³¨æ„åŠ›å¤´éƒ¨åˆ†çš„Wqã€Kqã€Vqå’Œæ‹¼æ¥åå…¨è¿æ¥éƒ¨åˆ†çš„æƒé‡æ‹·è´åˆ°ä¸€ä¸ªæ–°çš„è¾ƒå°çš„æƒé‡çŸ©é˜µï¼ˆæ³¨æ„å…ˆç¦æ­¢gradå†æ‹·è´ï¼‰ï¼Œå¹¶å®æ—¶è®°å½•è¢«å‰ªæ‰çš„å¤´ä»¥é˜²ä¸‹æ ‡å‡ºé”™ã€‚å…·ä½“å‚è€ƒBertAttentionéƒ¨åˆ†çš„prune_headsæ–¹æ³•ã€‚

#### BertEmbeddings

åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†æ±‚å’Œå¾—åˆ°ï¼š
- ![ç»“æ„å›¾](https://pic3.zhimg.com/80/v2-58b65365587f269bc76358016414dc26_720w.jpg)
- `word_embeddings`ï¼Œä¸Šæ–‡ä¸­`æ¬¡è¯`subwordå¯¹åº”çš„åµŒå…¥ã€‚
- `token_type_embeddings`ï¼Œç”¨äºè¡¨ç¤ºå½“å‰è¯æ‰€åœ¨çš„å¥å­ï¼Œè¾…åŠ©åŒºåˆ«å¥å­ä¸paddingã€å¥å­å¯¹é—´çš„å·®å¼‚ã€‚
- `position_embeddings`ï¼Œå¥å­ä¸­æ¯ä¸ªè¯çš„**ä½ç½®**åµŒå…¥ï¼Œç”¨äºåŒºåˆ«è¯çš„é¡ºåºã€‚å’Œtransformerè®ºæ–‡ä¸­çš„è®¾è®¡ä¸åŒï¼Œè¿™ä¸€å—æ˜¯è®­ç»ƒå‡ºæ¥çš„ï¼Œè€Œä¸æ˜¯é€šè¿‡Sinusoidalå‡½æ•°è®¡ç®—å¾—åˆ°çš„å›ºå®šåµŒå…¥ã€‚ä¸€èˆ¬è®¤ä¸ºè¿™ç§å®ç°ä¸åˆ©äºæ‹“å±•æ€§ï¼ˆéš¾ä»¥ç›´æ¥è¿ç§»åˆ°æ›´é•¿çš„å¥å­ä¸­ï¼‰ã€‚

ä¸‰ä¸ªembeddingä¸å¸¦æƒé‡ç›¸åŠ ï¼Œå¹¶é€šè¿‡ä¸€å±‚ LayerNorm+dropout åè¾“å‡ºï¼Œå…¶å¤§å°ä¸º $(batch_size, sequence_length, hidden_size)$ã€‚

è¡¥å……ï¼š
- ä¸ºä»€ä¹ˆè¦ç”¨LayerNorm+Dropoutå‘¢ï¼Ÿä¸ºä»€ä¹ˆè¦ç”¨LayerNormè€Œä¸æ˜¯BatchNormï¼Ÿå¯ä»¥å‚è€ƒä¸€ä¸ªä¸é”™çš„[å›ç­”](https://www.zhihu.com/question/395811291/answer/1260290120)

#### BertEncoder

åŒ…å«å¤šå±‚BertLayerï¼Œè¿™ä¸€å—æœ¬èº«æ²¡æœ‰ç‰¹åˆ«éœ€è¦è¯´æ˜çš„åœ°æ–¹ï¼Œä¸è¿‡æœ‰ä¸€ä¸ªç»†èŠ‚å€¼å¾—å‚è€ƒï¼š
- åˆ©ç”¨gradient checkpointingæŠ€æœ¯ä»¥é™ä½è®­ç»ƒæ—¶çš„æ˜¾å­˜å ç”¨ã€‚
- è¡¥å……ï¼šgradient checkpointingå³æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œé€šè¿‡å‡å°‘ä¿å­˜çš„è®¡ç®—å›¾èŠ‚ç‚¹å‹ç¼©æ¨¡å‹å ç”¨ç©ºé—´ï¼Œä½†æ˜¯åœ¨è®¡ç®—æ¢¯åº¦çš„æ—¶å€™éœ€è¦é‡æ–°è®¡ç®—æ²¡æœ‰å­˜å‚¨çš„å€¼ï¼Œå‚è€ƒè®ºæ–‡ã€ŠTraining Deep Nets with Sublinear Memory Costã€‹ï¼Œè¿‡ç¨‹å¦‚ä¸‹[ç¤ºæ„å›¾](https://pic2.zhimg.com/v2-24dfc50af29690e09dd5e8cc3319847d_b.webp)
- ![](https://pic2.zhimg.com/v2-24dfc50af29690e09dd5e8cc3319847d_b.webp)

åœ¨BertEncoderä¸­ï¼Œgradient checkpointæ˜¯é€šè¿‡torch.utils.checkpoint.checkpointå®ç°çš„ï¼Œä½¿ç”¨èµ·æ¥æ¯”è¾ƒæ–¹ä¾¿ï¼Œå¯ä»¥å‚è€ƒ[æ–‡æ¡£](https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/checkpoint.html)

#### BertLayer

è¿™ä¸€å±‚åŒ…è£…äº†BertAttentionå’ŒBertIntermediate+BertOutputï¼ˆå³Attentionåçš„FFNéƒ¨åˆ†ï¼‰ï¼Œä»¥åŠè¿™é‡Œç›´æ¥å¿½ç•¥çš„cross-attentionéƒ¨åˆ†ï¼ˆå°†BERTä½œä¸ºDecoderæ—¶æ¶‰åŠçš„éƒ¨åˆ†ï¼‰ã€‚

ç†è®ºä¸Šï¼Œè¿™é‡Œé¡ºåºè°ƒç”¨ä¸‰ä¸ªå­æ¨¡å—å°±å¯ä»¥ï¼Œæ²¡æœ‰ä»€ä¹ˆå€¼å¾—è¯´æ˜çš„åœ°æ–¹ã€‚

ç»†èŠ‚ï¼š
- apply_chunking_to_forwardå’Œfeed_forward_chunkäº†å—ï¼ˆä¸ºä»€ä¹ˆè¦æ•´è¿™ä¹ˆå¤æ‚ï¼Œç›´æ¥è°ƒç”¨å®ƒä¸é¦™å—ï¼Ÿ
- èŠ‚çº¦æ˜¾å­˜çš„æŠ€æœ¯â€”â€”åŒ…è£…äº†ä¸€ä¸ªåˆ‡åˆ†å°batchæˆ–è€…ä½ç»´æ•°æ“ä½œçš„åŠŸèƒ½ï¼šè¿™é‡Œå‚æ•°chunk_sizeå…¶å®å°±æ˜¯åˆ‡åˆ†çš„batchå¤§å°ï¼Œè€Œchunk_dimå°±æ˜¯ä¸€æ¬¡è®¡ç®—ç»´æ•°çš„å¤§å°ï¼Œæœ€åæ‹¼æ¥èµ·æ¥è¿”å›ã€‚
- ä¸è¿‡ï¼Œåœ¨é»˜è®¤æ“ä½œä¸­ä¸ä¼šç‰¹æ„è®¾ç½®è¿™ä¸¤ä¸ªå€¼ï¼ˆåœ¨æºä»£ç ä¸­é»˜è®¤ä¸º0å’Œ1ï¼‰ï¼Œæ‰€ä»¥ä¼šç›´æ¥ç­‰æ•ˆäºæ­£å¸¸çš„forwardè¿‡ç¨‹ã€‚

#### BertAttention

æœ¬ä»¥ä¸ºattentionçš„å®ç°å°±åœ¨è¿™é‡Œï¼Œæ²¡æƒ³åˆ°è¿˜è¦å†ä¸‹ä¸€å±‚â€¦â€¦å…¶ä¸­ï¼Œselfæˆå‘˜å°±æ˜¯å¤šå¤´æ³¨æ„åŠ›çš„å®ç°ï¼Œè€Œoutputæˆå‘˜å®ç°attentionåçš„å…¨è¿æ¥+dropout+residual+LayerNormä¸€ç³»åˆ—æ“ä½œã€‚å‡ºç°äº†ä¸Šæ–‡æåˆ°çš„å‰ªææ“ä½œï¼Œå³prune_headsæ–¹æ³•

class BertAttention(nn.Module)æ¦‚æ‹¬å¦‚ä¸‹ï¼š
- find_pruneable_heads_and_indicesæ˜¯å®šä½éœ€è¦å‰ªæ‰çš„headï¼Œä»¥åŠéœ€è¦ä¿ç•™çš„ç»´åº¦ä¸‹æ ‡indexï¼›
- prune_linear_layeråˆ™è´Ÿè´£å°†Wk/Wq/Wvæƒé‡çŸ©é˜µï¼ˆè¿åŒbiasï¼‰ä¸­æŒ‰ç…§indexä¿ç•™æ²¡æœ‰è¢«å‰ªæçš„ç»´åº¦åè½¬ç§»åˆ°æ–°çš„çŸ©é˜µã€‚

##### BertSelfAttention

é¢„è­¦ï¼šè¿™ä¸€å—å¯ä»¥è¯´æ˜¯æ¨¡å‹çš„æ ¸å¿ƒåŒºåŸŸï¼Œä¹Ÿæ˜¯å”¯ä¸€æ¶‰åŠåˆ°å…¬å¼çš„åœ°æ–¹ï¼Œæ‰€ä»¥å°†è´´å‡ºå¤§é‡ä»£ç ã€‚

class BertSelfAttention(nn.Module)

```python
class BertSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, "embedding_size"):
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (config.hidden_size, config.num_attention_heads)
            )

        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
        if self.position_embedding_type == "relative_key" or self.position_embedding_type == "relative_key_query":
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)

        self.is_decoder = config.is_decoder
```

- é™¤æ‰ç†Ÿæ‚‰çš„queryã€keyã€valueä¸‰ä¸ªæƒé‡å’Œä¸€ä¸ªdropoutï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ªè°œä¸€æ ·çš„position_embedding_typeï¼Œä»¥åŠdecoderæ ‡è®°ï¼ˆå½“ç„¶ï¼Œæˆ‘ä¸æ‰“ç®—ä»‹ç»cross-attentonéƒ¨åˆ†ï¼‰ï¼›
- æ³¨æ„ï¼Œhidden_sizeå’Œall_head_sizeåœ¨ä¸€å¼€å§‹æ˜¯ä¸€æ ·çš„ã€‚è‡³äºä¸ºä»€ä¹ˆè¦çœ‹èµ·æ¥å¤šæ­¤ä¸€ä¸¾åœ°è®¾ç½®è¿™ä¸€ä¸ªå˜é‡â€”â€”æ˜¾ç„¶æ˜¯å› ä¸ºä¸Šé¢é‚£ä¸ªå‰ªæå‡½æ•°ï¼Œå‰ªæ‰å‡ ä¸ªattention headä»¥åall_head_sizeè‡ªç„¶å°±å°äº†ï¼›
- hidden_sizeå¿…é¡»æ˜¯num_attention_headsçš„æ•´æ•°å€ï¼Œä»¥bert-baseä¸ºä¾‹ï¼Œæ¯ä¸ªattentionåŒ…å«12ä¸ªheadï¼Œhidden_sizeæ˜¯768ï¼Œæ‰€ä»¥æ¯ä¸ªheadå¤§å°å³attention_head_size=768/12=64ï¼›
- position_embedding_typeæ˜¯ä»€ä¹ˆï¼Ÿ

multi-head self-attentionçš„åŸºæœ¬å…¬å¼
- ![](https://pic4.zhimg.com/80/v2-0c1ffd5ec70918a7c6c42fc7aafd7b0b_720w.png)

æ³¨æ„åŠ›å¤´ï¼Œä¼—æ‰€å‘¨çŸ¥æ˜¯å¹¶è¡Œè®¡ç®—çš„ï¼Œæ‰€ä»¥ä¸Šé¢çš„queryã€keyã€valueä¸‰ä¸ªæƒé‡æ˜¯å”¯ä¸€çš„â€”â€”è¿™å¹¶ä¸æ˜¯æ‰€æœ‰headså…±äº«äº†æƒé‡ï¼Œè€Œæ˜¯â€œæ‹¼æ¥â€èµ·æ¥äº†ã€‚

è¡¥å……ï¼šåŸè®ºæ–‡ä¸­å¤šå¤´çš„ç†ç”±ä¸ºMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.è€Œå¦ä¸€ä¸ªæ¯”è¾ƒé è°±çš„[åˆ†æ](https://www.zhihu.com/question/341222779/answer/814111138)

forwardæ–¹æ³•

```python
def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        past_key_value=None,
        output_attentions=False,
    ):
        mixed_query_layer = self.query(hidden_states)

        # çœç•¥ä¸€éƒ¨åˆ†cross-attentionçš„è®¡ç®—
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        value_layer = self.transpose_for_scores(self.value(hidden_states))
        query_layer = self.transpose_for_scores(mixed_query_layer)

        # Take the dot product between "query" and "key" to get the raw attention scores.
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        # ...
```

- transpose_for_scoresç”¨æ¥æŠŠhidden_sizeæ‹†æˆå¤šä¸ªå¤´è¾“å‡ºçš„å½¢çŠ¶ï¼Œå¹¶ä¸”å°†ä¸­é—´ä¸¤ç»´è½¬ç½®ä»¥è¿›è¡ŒçŸ©é˜µç›¸ä¹˜ï¼›
- è¿™é‡Œkey_layer/value_layer/query_layerçš„å½¢çŠ¶ä¸ºï¼š(batch_size, num_attention_heads, sequence_length, attention_head_size)ï¼›
- è¿™é‡Œattention_scoresçš„å½¢çŠ¶ä¸ºï¼š(batch_size, num_attention_heads, sequence_length, sequence_length)ï¼Œç¬¦åˆå¤šä¸ªå¤´å•ç‹¬è®¡ç®—è·å¾—çš„attention mapå½¢çŠ¶ã€‚
- åˆ°è¿™é‡Œå®ç°äº†Kä¸Qç›¸ä¹˜ï¼Œè·å¾—raw attention scoresçš„éƒ¨åˆ†ï¼ŒæŒ‰å…¬å¼æ¥ä¸‹æ¥åº”è¯¥æ˜¯æŒ‰dkè¿›è¡Œscalingå¹¶åšsoftmaxçš„æ“ä½œã€‚å¥‡æ€ªçš„positional_embeddingï¼Œä»¥åŠä¸€å †çˆ±å› æ–¯å¦æ±‚å’Œ

ã€‚ã€‚ã€‚

get_extended_attention_maskè¿™ä¸ªå‡½æ•°æ˜¯åœ¨ä»€ä¹ˆæ—¶å€™è¢«è°ƒç”¨çš„å‘¢ï¼Ÿå’ŒBertModelæœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿ
- BertModelçš„ç»§æ‰¿ç»†èŠ‚äº†ï¼šBertModelç»§æ‰¿è‡ªBertPreTrainedModel ï¼Œåè€…ç»§æ‰¿è‡ªPreTrainedModelï¼Œè€ŒPreTrainedModelç»§æ‰¿è‡ª[nn.Module, ModuleUtilsMixin, GenerationMixin] ä¸‰ä¸ªåŸºç±»ã€‚â€”â€”å¥½å¤æ‚çš„å°è£…ï¼
- è¿™ä¹Ÿå°±æ˜¯è¯´ï¼Œ BertModelå¿…ç„¶åœ¨ä¸­é—´çš„æŸä¸ªæ­¥éª¤å¯¹åŸå§‹çš„attention_maskè°ƒç”¨äº†get_extended_attention_mask ï¼Œå¯¼è‡´attention_maskä»åŸå§‹çš„[1, 0]å˜ä¸º[0, -1e4]çš„å–å€¼ã€‚BertModelçš„å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­æ‰¾åˆ°äº†è¿™ä¸€è°ƒç”¨ï¼ˆç¬¬944è¡Œï¼‰
- é—®é¢˜è§£å†³äº†ï¼šè¿™ä¸€æ–¹æ³•ä¸ä½†å®ç°äº†æ”¹å˜maskçš„å€¼ï¼Œè¿˜å°†å…¶å¹¿æ’­ï¼ˆbroadcastï¼‰ä¸ºå¯ä»¥ç›´æ¥ä¸attention mapç›¸åŠ çš„å½¢çŠ¶ã€‚

ç»†èŠ‚æœ‰ï¼š
- æŒ‰ç…§æ¯ä¸ªå¤´çš„ç»´åº¦è¿›è¡Œç¼©æ”¾ï¼Œå¯¹äºbert-baseå°±æ˜¯64çš„å¹³æ–¹æ ¹å³8ï¼›
- attention_probsä¸ä½†åšäº†softmaxï¼Œè¿˜ç”¨äº†ä¸€æ¬¡dropoutï¼Œè¿™æ˜¯æ‹…å¿ƒattentionçŸ©é˜µå¤ªç¨ å¯†å—â€¦â€¦è¿™é‡Œä¹Ÿæåˆ°å¾ˆä¸å¯»å¸¸ï¼Œä½†æ˜¯åŸå§‹Transformerè®ºæ–‡å°±æ˜¯è¿™ä¹ˆåšçš„ï¼›
- head_maskå°±æ˜¯ä¹‹å‰æåˆ°çš„å¯¹å¤šå¤´è®¡ç®—çš„maskï¼Œå¦‚æœä¸è®¾ç½®é»˜è®¤æ˜¯å…¨1ï¼Œåœ¨è¿™é‡Œå°±ä¸ä¼šèµ·ä½œç”¨ï¼›
- context_layerå³attentionçŸ©é˜µä¸valueçŸ©é˜µçš„ä¹˜ç§¯ï¼ŒåŸå§‹çš„å¤§å°ä¸ºï¼š(batch_size, num_attention_heads, sequence_length, attention_head_size) ï¼›
- context_layerè¿›è¡Œè½¬ç½®å’Œviewæ“ä½œä»¥åï¼Œå½¢çŠ¶å°±æ¢å¤äº†(batch_size, sequence_length, hidden_size)ã€‚

#### BertSelfOutput

è¿™ä¸€å—æ“ä½œç•¥å¤šä½†ä¸å¤æ‚

```python
class BertSelfOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states
```

è¡¥å……ï¼šè¿™é‡Œåˆå‡ºç°äº†LayerNormå’ŒDropoutçš„ç»„åˆï¼Œåªä¸è¿‡è¿™é‡Œæ˜¯å…ˆDropoutï¼Œè¿›è¡Œæ®‹å·®è¿æ¥åå†è¿›è¡ŒLayerNormã€‚è‡³äºä¸ºä»€ä¹ˆè¦åšæ®‹å·®è¿æ¥ï¼Œæœ€ç›´æ¥çš„ç›®çš„å°±æ˜¯é™ä½ç½‘ç»œå±‚æ•°è¿‡æ·±å¸¦æ¥çš„è®­ç»ƒéš¾åº¦ï¼Œå¯¹åŸå§‹è¾“å…¥æ›´åŠ æ•æ„Ÿ

#### BertIntermediate

çœ‹å®Œäº†BertAttentionï¼Œåœ¨Attentionåé¢è¿˜æœ‰ä¸€ä¸ªå…¨è¿æ¥+æ¿€æ´»çš„æ“ä½œ

```python
class BertIntermediate(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        return hidden_states
```

- å…¨è¿æ¥åšäº†ä¸€ä¸ªæ‰©å±•ï¼Œä»¥bert-baseä¸ºä¾‹ï¼Œæ‰©å±•ç»´åº¦ä¸º3072ï¼Œæ˜¯åŸå§‹ç»´åº¦768çš„4å€ä¹‹å¤šï¼›
  - è¡¥å……ï¼šä¸ºä»€ä¹ˆè¦è¿‡ä¸€ä¸ªFFNï¼Ÿä¸çŸ¥é“â€¦â€¦è°·æ­Œæœ€è¿‘çš„[è®ºæ–‡](https://arxiv.org/abs/2103.03404)è²Œä¼¼è¯´æ˜åªæœ‰attentionçš„æ¨¡å‹ä»€ä¹ˆç”¨éƒ½æ²¡æœ‰
- æ¿€æ´»å‡½æ•°é»˜è®¤å®ç°ä¸ºgeluï¼ˆGaussian Error Linerar Units(GELUSï¼‰ï¼š ![å…¬å¼](https://www.zhihu.com/equation?tex=GELU%28x%29%3DxP%28X%3C%3Dx%29%3Dx%CE%A6%28x%29+) ï¼›å½“ç„¶ï¼Œå®ƒæ˜¯æ— æ³•ç›´æ¥è®¡ç®—çš„ï¼Œå¯ä»¥ç”¨ä¸€ä¸ªåŒ…å«tanhçš„è¡¨è¾¾å¼è¿›è¡Œè¿‘ä¼¼ï¼ˆç•¥ï¼‰ã€‚

ä¸ºä»€ä¹ˆåœ¨transformerä¸­è¦ç”¨è¿™ä¸ªæ¿€æ´»å‡½æ•°
- è¡¥å……ï¼šçœ‹äº†ä¸€äº›ç ”ç©¶ï¼Œåº”è¯¥æ˜¯è¯´GeLUæ¯”ReLUè¿™äº›è¡¨ç°éƒ½å¥½ï¼Œä»¥è‡³äºåç»­çš„è¯­è¨€æ¨¡å‹éƒ½æ²¿ç”¨äº†è¿™ä¸€æ¿€æ´»å‡½æ•°ã€‚

#### BertOutput

åœ¨è¿™é‡Œåˆæ˜¯ä¸€ä¸ªå…¨è¿æ¥+dropout+LayerNormï¼Œè¿˜æœ‰ä¸€ä¸ªæ®‹å·®è¿æ¥residual connect

```python
class BertOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor)
        return hidden_states
```

è¿™é‡Œçš„æ“ä½œå’ŒBertSelfOutputä¸èƒ½è¯´æ²¡æœ‰å…³ç³»ï¼Œåªèƒ½è¯´ä¸€æ¨¡ä¸€æ ·â€¦â€¦éå¸¸å®¹æ˜“æ··æ·†çš„ä¸¤ä¸ªç»„ä»¶ã€‚

### BertPooler

è¿™ä¸€å±‚åªæ˜¯ç®€å•åœ°å–å‡ºäº†å¥å­çš„ç¬¬ä¸€ä¸ªtokenï¼Œå³[CLS]å¯¹åº”çš„å‘é‡ï¼Œç„¶åè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚å’Œä¸€ä¸ªæ¿€æ´»å‡½æ•°åè¾“å‡º

```python
class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
```

### å°ç»“

- åœ¨HuggingFaceå®ç°çš„Bertæ¨¡å‹ä¸­ï¼Œä½¿ç”¨äº†å¤šç§èŠ‚çº¦æ˜¾å­˜çš„æŠ€æœ¯ï¼š
  - gradient checkpointï¼Œä¸ä¿ç•™å‰å‘ä¼ æ’­èŠ‚ç‚¹ï¼Œåªåœ¨ç”¨æ—¶è®¡ç®—ï¼›
  - apply_chunking_to_forwardï¼ŒæŒ‰å¤šä¸ªå°æ‰¹é‡å’Œä½ç»´åº¦è®¡ç®—FFNéƒ¨åˆ†ï¼›
- BertModelåŒ…å«å¤æ‚çš„å°è£…å’Œè¾ƒå¤šçš„ç»„ä»¶ã€‚ä»¥bert-baseä¸ºä¾‹ï¼Œä¸»è¦ç»„ä»¶å¦‚ä¸‹ï¼š
  - æ€»è®¡Dropoutå‡ºç°äº†1+(1+1+1)x12=37æ¬¡ï¼›
  - æ€»è®¡LayerNormå‡ºç°äº†1+(1+1)x12=25æ¬¡ï¼›
  - æ€»è®¡denseå…¨è¿æ¥å±‚å‡ºç°äº†(1+1+1)x12+1=37æ¬¡ï¼Œå¹¶ä¸æ˜¯æ¯ä¸ªdenseéƒ½é…äº†æ¿€æ´»å‡½æ•°
- BertModelæœ‰æå¤§çš„å‚æ•°é‡ã€‚ä»¥bert-baseä¸ºä¾‹ï¼Œå…¶å‚æ•°é‡ä¸º109Mï¼Œå…·ä½“è®¡ç®—è¿‡ç¨‹å¯ä»¥[å‚è€ƒ](https://zhuanlan.zhihu.com/p/144582114)

### BERT-based Models

åŸºäºBERTçš„æ¨¡å‹éƒ½å†™åœ¨/models/bert/modeling_bert.pyé‡Œé¢ï¼ŒåŒ…æ‹¬BERTé¢„è®­ç»ƒæ¨¡å‹å’ŒBERTåˆ†ç±»æ¨¡å‹ï¼ŒUMLå›¾å¦‚ä¸‹ï¼š
- ![](https://pic1.zhimg.com/80/v2-0e126f74d40d2db8bc133bc67f8055b4_720w.png)

BERTæ¨¡å‹ä¸€å›¾æµï¼ˆå»ºè®®ä¿å­˜åæ”¾å¤§æŸ¥çœ‹ï¼‰

é¦–å…ˆï¼Œä»¥ä¸‹æ‰€æœ‰çš„æ¨¡å‹éƒ½æ˜¯åŸºäºBertPreTrainedModelè¿™ä¸€æŠ½è±¡åŸºç±»çš„ï¼Œè€Œåè€…åˆ™åŸºäºä¸€ä¸ªæ›´å¤§çš„åŸºç±»PreTrainedModelã€‚è¿™é‡Œæˆ‘ä»¬å…³æ³¨BertPreTrainedModelçš„åŠŸèƒ½ï¼š
- ç”¨äºåˆå§‹åŒ–æ¨¡å‹æƒé‡ï¼ŒåŒæ—¶ç»´æŠ¤ç»§æ‰¿è‡ªPreTrainedModelçš„ä¸€äº›æ ‡è®°èº«ä»½æˆ–è€…åŠ è½½æ¨¡å‹æ—¶çš„ç±»å˜é‡ã€‚

#### BertForPreTraining

ä¼—æ‰€å‘¨çŸ¥ï¼ŒBERTé¢„è®­ç»ƒä»»åŠ¡åŒ…æ‹¬ä¸¤ä¸ªï¼š
- Masked Language Modelï¼ˆMLMï¼‰ï¼šåœ¨å¥å­ä¸­éšæœºç”¨[MASK]æ›¿æ¢ä¸€éƒ¨åˆ†å•è¯ï¼Œç„¶åå°†å¥å­ä¼ å…¥ BERT ä¸­ç¼–ç æ¯ä¸€ä¸ªå•è¯çš„ä¿¡æ¯ï¼Œæœ€ç»ˆç”¨[MASK]çš„ç¼–ç ä¿¡æ¯é¢„æµ‹è¯¥ä½ç½®çš„æ­£ç¡®å•è¯ï¼Œè¿™ä¸€ä»»åŠ¡æ—¨åœ¨è®­ç»ƒæ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡ç†è§£å•è¯çš„æ„æ€ï¼›
- Next Sentence Predictionï¼ˆNSPï¼‰ï¼šå°†å¥å­å¯¹Aå’ŒBè¾“å…¥BERTï¼Œä½¿ç”¨[CLS]çš„ç¼–ç ä¿¡æ¯è¿›è¡Œé¢„æµ‹Bæ˜¯å¦Açš„ä¸‹ä¸€å¥ï¼Œè¿™ä¸€ä»»åŠ¡æ—¨åœ¨è®­ç»ƒæ¨¡å‹ç†è§£é¢„æµ‹å¥å­é—´çš„å…³ç³»ã€‚

![](https://pic4.zhimg.com/80/v2-778b166945e69e7689cccfe7532e74e3_720w.jpg)

å¯¹åº”åˆ°ä»£ç ä¸­ï¼Œè¿™ä¸€èåˆä¸¤ä¸ªä»»åŠ¡çš„æ¨¡å‹å°±æ˜¯BertForPreTrainingã€‚ç•¥

è¿™ä»½ä»£ç é‡Œé¢ä¹ŸåŒ…å«äº†å¯¹äºåªæƒ³å¯¹å•ä¸ªç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒçš„BERTæ¨¡å‹ï¼ˆå…·ä½“ç»†èŠ‚ä¸ä½œå±•å¼€ï¼‰ï¼š
- BertForMaskedLMï¼šåªè¿›è¡ŒMLMä»»åŠ¡çš„é¢„è®­ç»ƒï¼›
  - åŸºäºBertOnlyMLMHeadï¼Œè€Œåè€…ä¹Ÿæ˜¯å¯¹BertLMPredictionHeadçš„å¦ä¸€å±‚å°è£…ï¼›
- BertLMHeadModelï¼šè¿™ä¸ªå’Œä¸Šä¸€ä¸ªçš„åŒºåˆ«åœ¨äºï¼Œè¿™ä¸€æ¨¡å‹æ˜¯ä½œä¸ºdecoderè¿è¡Œçš„ç‰ˆæœ¬ï¼›
  - åŒæ ·åŸºäºBertOnlyMLMHeadï¼›
- BertForNextSentencePredictionï¼šåªè¿›è¡ŒNSPä»»åŠ¡çš„é¢„è®­ç»ƒã€‚
  - åŸºäºBertOnlyNSPHeadï¼Œå†…å®¹å°±æ˜¯ä¸€ä¸ªçº¿æ€§å±‚â€¦â€¦

å„ç§Fine-tuneæ¨¡å‹ï¼ŒåŸºæœ¬éƒ½æ˜¯åˆ†ç±»ä»»åŠ¡ï¼š
- ![](https://pic1.zhimg.com/80/v2-d870cb6a4cc1b6f5f7f54cd9f563e468_720w.jpg)

#### BertForSequenceClassification

è¿™ä¸€æ¨¡å‹ç”¨äºå¥å­åˆ†ç±»ï¼ˆä¹Ÿå¯ä»¥æ˜¯å›å½’ï¼‰ä»»åŠ¡ï¼Œæ¯”å¦‚GLUE benchmarkçš„å„ä¸ªä»»åŠ¡ã€‚
- å¥å­åˆ†ç±»çš„è¾“å…¥ä¸ºå¥å­ï¼ˆå¯¹ï¼‰ï¼Œè¾“å‡ºä¸ºå•ä¸ªåˆ†ç±»æ ‡ç­¾ã€‚
ç»“æ„ä¸Šå¾ˆç®€å•ï¼Œå°±æ˜¯BertModelï¼ˆæœ‰poolingï¼‰è¿‡ä¸€ä¸ªdropoutåæ¥ä¸€ä¸ªçº¿æ€§å±‚è¾“å‡ºåˆ†ç±»

åœ¨å‰å‘ä¼ æ’­æ—¶ï¼Œå’Œä¸Šé¢é¢„è®­ç»ƒæ¨¡å‹ä¸€æ ·éœ€è¦ä¼ å…¥labelsè¾“å…¥ã€‚
- å¦‚æœåˆå§‹åŒ–çš„num_labels=1ï¼Œé‚£ä¹ˆå°±é»˜è®¤ä¸ºå›å½’ä»»åŠ¡ï¼Œä½¿ç”¨MSELossï¼›
- å¦åˆ™è®¤ä¸ºæ˜¯åˆ†ç±»ä»»åŠ¡ã€‚

#### BertForMultipleChoice

è¿™ä¸€æ¨¡å‹ç”¨äºå¤šé¡¹é€‰æ‹©ï¼Œå¦‚RocStories/SWAGä»»åŠ¡ã€‚
- å¤šé¡¹é€‰æ‹©ä»»åŠ¡çš„è¾“å…¥ä¸ºä¸€ç»„åˆ†æ¬¡è¾“å…¥çš„å¥å­ï¼Œè¾“å‡ºä¸ºé€‰æ‹©æŸä¸€å¥å­çš„å•ä¸ªæ ‡ç­¾ã€‚
ç»“æ„ä¸Šä¸å¥å­åˆ†ç±»ç›¸ä¼¼ï¼Œåªä¸è¿‡çº¿æ€§å±‚è¾“å‡ºç»´åº¦ä¸º1ï¼Œå³æ¯æ¬¡éœ€è¦å°†æ¯ä¸ªæ ·æœ¬çš„å¤šä¸ªå¥å­çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ä½œä¸ºæ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹åˆ†æ•°ã€‚
- å®é™…ä¸Šï¼Œå…·ä½“æ“ä½œæ—¶æ˜¯æŠŠæ¯ä¸ªbatchçš„å¤šä¸ªå¥å­ä¸€åŒæ”¾å…¥çš„ï¼Œæ‰€ä»¥ä¸€æ¬¡å¤„ç†çš„è¾“å…¥ä¸º[batch_size, num_choices]æ•°é‡çš„å¥å­ï¼Œå› æ­¤ç›¸åŒbatchå¤§å°æ—¶ï¼Œæ¯”å¥å­åˆ†ç±»ç­‰ä»»åŠ¡éœ€è¦æ›´å¤šçš„æ˜¾å­˜ï¼Œåœ¨è®­ç»ƒæ—¶éœ€è¦å°å¿ƒã€‚

#### BertForTokenClassification

è¿™ä¸€æ¨¡å‹ç”¨äºåºåˆ—æ ‡æ³¨ï¼ˆè¯åˆ†ç±»ï¼‰ï¼Œå¦‚NERä»»åŠ¡ã€‚
- åºåˆ—æ ‡æ³¨ä»»åŠ¡çš„è¾“å…¥ä¸ºå•ä¸ªå¥å­æ–‡æœ¬ï¼Œè¾“å‡ºä¸ºæ¯ä¸ªtokenå¯¹åº”çš„ç±»åˆ«æ ‡ç­¾ã€‚
ç”±äºéœ€è¦ç”¨åˆ°æ¯ä¸ªtokenå¯¹åº”çš„è¾“å‡ºè€Œä¸åªæ˜¯æŸå‡ ä¸ªï¼Œæ‰€ä»¥è¿™é‡Œçš„BertModelä¸ç”¨åŠ å…¥poolingå±‚ï¼›
- åŒæ—¶ï¼Œè¿™é‡Œå°†_keys_to_ignore_on_load_unexpectedè¿™ä¸€ä¸ªç±»å‚æ•°è®¾ç½®ä¸º[r"pooler"]ï¼Œä¹Ÿå°±æ˜¯åœ¨åŠ è½½æ¨¡å‹æ—¶å¯¹äºå‡ºç°ä¸éœ€è¦çš„æƒé‡ä¸å‘ç”ŸæŠ¥é”™ã€‚

#### BertForQuestionAnswering

è¿™ä¸€æ¨¡å‹ç”¨äºè§£å†³é—®ç­”ä»»åŠ¡ï¼Œä¾‹å¦‚SQuADä»»åŠ¡ã€‚
- é—®ç­”ä»»åŠ¡çš„è¾“å…¥ä¸ºé—®é¢˜+ï¼ˆå¯¹äºBERTåªèƒ½æ˜¯ä¸€ä¸ªï¼‰å›ç­”ç»„æˆçš„å¥å­å¯¹ï¼Œè¾“å‡ºä¸ºèµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®ç”¨äºæ ‡å‡ºå›ç­”ä¸­çš„å…·ä½“æ–‡æœ¬ã€‚
è¿™é‡Œéœ€è¦ä¸¤ä¸ªè¾“å‡ºï¼Œå³å¯¹èµ·å§‹ä½ç½®çš„é¢„æµ‹å’Œå¯¹ç»“æŸä½ç½®çš„é¢„æµ‹ï¼Œä¸¤ä¸ªè¾“å‡ºçš„é•¿åº¦éƒ½å’Œå¥å­é•¿åº¦ä¸€æ ·ï¼Œä»å…¶ä¸­æŒ‘å‡ºæœ€å¤§çš„é¢„æµ‹å€¼å¯¹åº”çš„ä¸‹æ ‡ä½œä¸ºé¢„æµ‹çš„ä½ç½®ã€‚
- å¯¹è¶…å‡ºå¥å­é•¿åº¦çš„éæ³•labelï¼Œä¼šå°†å…¶å‹ç¼©ï¼ˆtorch.clamp_ï¼‰åˆ°åˆç†èŒƒå›´ã€‚

ä½œä¸ºä¸€ä¸ªè¿Ÿåˆ°çš„è¡¥å……ï¼Œè¿™é‡Œç¨å¾®ä»‹ç»ä¸€ä¸‹ModelOutputè¿™ä¸ªç±»ã€‚å®ƒä½œä¸ºä¸Šè¿°å„ä¸ªæ¨¡å‹è¾“å‡ºåŒ…è£…çš„åŸºç±»ï¼ŒåŒæ—¶æ”¯æŒå­—å…¸å¼çš„å­˜å–å’Œä¸‹æ ‡é¡ºåºçš„è®¿é—®ï¼Œç»§æ‰¿è‡ªpythonåŸç”Ÿçš„OrderedDict ç±»ã€‚

### BERTè®­ç»ƒå’Œä¼˜åŒ–

#### Pre-Training

é¢„è®­ç»ƒé˜¶æ®µï¼Œé™¤äº†ä¼—æ‰€å‘¨çŸ¥çš„15%ã€80%maskæ¯”ä¾‹ï¼Œæœ‰ä¸€ä¸ªå€¼å¾—æ³¨æ„çš„åœ°æ–¹å°±æ˜¯å‚æ•°å…±äº«ã€‚

ä¸æ­¢BERTï¼Œæ‰€æœ‰huggingfaceå®ç°çš„PLMçš„word embeddingå’Œmasked language modelçš„é¢„æµ‹æƒé‡åœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­éƒ½æ˜¯å…±äº«çš„ï¼š

#### Fine-Tuning

å¾®è°ƒä¹Ÿå°±æ˜¯ä¸‹æ¸¸ä»»åŠ¡é˜¶æ®µï¼Œä¹Ÿæœ‰ä¸¤ä¸ªå€¼å¾—æ³¨æ„çš„åœ°æ–¹ã€‚

##### AdamW

é¦–å…ˆä»‹ç»ä¸€ä¸‹BERTçš„ä¼˜åŒ–å™¨ï¼šAdamWï¼ˆAdamWeightDecayOptimizerï¼‰ã€‚

è¿™ä¸€ä¼˜åŒ–å™¨æ¥è‡ªICLR 2017çš„Best Paperï¼šã€ŠFixing Weight Decay Regularization in Adamã€‹ä¸­æå‡ºçš„ä¸€ç§ç”¨äºä¿®å¤Adamçš„æƒé‡è¡°å‡é”™è¯¯çš„æ–°æ–¹æ³•ã€‚è®ºæ–‡æŒ‡å‡ºï¼ŒL2æ­£åˆ™åŒ–å’Œæƒé‡è¡°å‡åœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹å¹¶ä¸ç­‰ä»·ï¼Œåªåœ¨SGDä¼˜åŒ–çš„æƒ…å†µä¸‹æ˜¯ç­‰ä»·çš„ï¼›è€Œå¤§å¤šæ•°æ¡†æ¶ä¸­å¯¹äºAdam+L2æ­£åˆ™ä½¿ç”¨çš„æ˜¯æƒé‡è¡°å‡çš„æ–¹å¼ï¼Œä¸¤è€…ä¸èƒ½æ··ä¸ºä¸€è°ˆã€‚

##### Warmup

BERTçš„è®­ç»ƒä¸­å¦ä¸€ä¸ªç‰¹ç‚¹åœ¨äºWarmupï¼Œå…¶å«ä¹‰ä¸ºï¼š
- åœ¨è®­ç»ƒåˆæœŸä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆä»0å¼€å§‹ï¼‰ï¼Œåœ¨ä¸€å®šæ­¥æ•°ï¼ˆæ¯”å¦‚1000æ­¥ï¼‰å†…é€æ¸æé«˜åˆ°æ­£å¸¸å¤§å°ï¼ˆæ¯”å¦‚ä¸Šé¢çš„2e-5ï¼‰ï¼Œé¿å…æ¨¡å‹è¿‡æ—©è¿›å…¥å±€éƒ¨æœ€ä¼˜è€Œè¿‡æ‹Ÿåˆï¼›
- åœ¨è®­ç»ƒåæœŸå†æ…¢æ…¢å°†å­¦ä¹ ç‡é™ä½åˆ°0ï¼Œé¿å…åæœŸè®­ç»ƒè¿˜å‡ºç°è¾ƒå¤§çš„å‚æ•°å˜åŒ–ã€‚
åœ¨Huggingfaceçš„å®ç°ä¸­ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§warmupç­–ç•¥
- CONSTANTï¼šä¿æŒå›ºå®šå­¦ä¹ ç‡ä¸å˜ï¼›
- CONSTANT_WITH_WARMUPï¼šåœ¨æ¯ä¸€ä¸ªstepä¸­çº¿æ€§è°ƒæ•´å­¦ä¹ ç‡ï¼›
- LINEARï¼šä¸Šæ–‡æåˆ°çš„ä¸¤æ®µå¼è°ƒæ•´ï¼›
- COSINEï¼šå’Œä¸¤æ®µå¼è°ƒæ•´ç±»ä¼¼ï¼Œåªä¸è¿‡é‡‡ç”¨çš„æ˜¯ä¸‰è§’å‡½æ•°å¼çš„æ›²çº¿è°ƒæ•´ï¼›
- COSINE_WITH_RESTARTSï¼šè®­ç»ƒä¸­å°†ä¸Šé¢COSINEçš„è°ƒæ•´é‡å¤næ¬¡ï¼›
- POLYNOMIALï¼šæŒ‰æŒ‡æ•°æ›²çº¿è¿›è¡Œä¸¤æ®µå¼è°ƒæ•´ã€‚


### å…¥é—¨ä»£ç 

```python
import torch
from transformers import BertModel, BertTokenizer

# è°ƒç”¨bert-baseæ¨¡å‹ï¼ŒåŒæ—¶æ¨¡å‹çš„è¯å…¸ç»è¿‡å°å†™å¤„ç†
model_name = 'bert-base-uncased'
model_name = 'bert-base-chinese' # ä¸­æ–‡æ¨¡å‹
# ----------- åˆ†è¯å™¨ ------------
# è¯»å–æ¨¡å‹å¯¹åº”çš„tokenizer
tokenizer = BertTokenizer.from_pretrained(model_name) 
# ã€2023-2-22ã€‘é»˜è®¤ä¿å­˜è·¯å¾„ï¼š~/.cache/huggingface/hub/
tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir='./transformers/')	# cache_dirè¡¨ç¤ºå°†é¢„è®­ç»ƒæ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹ï¼Œè€Œä¸æ˜¯é»˜è®¤è·¯å¾„
# è·å–è¯è¡¨
vocab = tokenizer.get_vocab()
print("vocab: ", len(vocab))

# ----------- æ¨¡å‹ ------------
# è½½å…¥æ¨¡å‹
model = BertModel.from_pretrained(model_name)
# æœ¬åœ°ä¿å­˜
model = BertModel.from_pretrained(model_name, cache_dir='./transformers/')
# è¾“å‡ºéšå«å±‚
model = BertModel.from_pretrained('./model', output_hidden_states = True,)

# è·å–è¯å‘é‡çŸ©é˜µ
word_embedding = model.get_input_embeddings()
embed_weights = word_embedding.weight
print("embed_weights: ", embed_weights.shape, type(embed_weights))
# embed_weights: torch.Size([30522, 768]
# ----------- æµ‹è¯• ------------
# ï¼ˆ1ï¼‰å•è¡Œæ–‡æœ¬
input_text = "Here is some text to encode"
# é€šè¿‡tokenizeræŠŠæ–‡æœ¬å˜æˆ token_id
input_ids = tokenizer.encode(input_text, add_special_tokens=True)
# input_ids: [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]
input_ids = torch.tensor([input_ids])
# ä¸­æ–‡æµ‹è¯•
input_ids = torch.tensor(tokenizer.encode("é‡è§è¢«è€å¸ˆæé—®é—®é¢˜", add_special_tokens=True)).unsqueeze(0)	# å¢åŠ ä¸€ä¸ªç»´åº¦å› ä¸ºè¾“å…¥åˆ°Bertæ¨¡å‹ä¸­è¦æ±‚äºŒç»´(Batch_size, seq_len)
print("input_ids: ", input_ids)
output = model(input_ids=input_ids)
last_hidden_states_0 = output[0]
print("last_hidden_states_0.shape: ", last_hidden_states_0.shape)
last_hidden_states_1 = output[1]
print("last_hidden_states_1.shape: ", ast_hidden_states_1.shape)
# input_ids:  tensor([[ 101, 6878, 6224, 6158, 5439, 2360, 2990, 7309, 7309, 7579,  102]])
# last_hidden_states_0.shape: torch.Size([1, 11, 768]
# last_hidden_states_1.shape: torch.Size([1, 768]

# ï¼ˆ2ï¼‰pairæ–‡æœ¬å¯¹
text_a = "EU rejects German call to boycott British lamb ."
text_b = "This tokenizer inherits from :class: transformers.PreTrainedTokenizer"

tokens_encode = tokenizer.encode_plus(text=text, text_pair=text_b, max_length=20, truncation_strategy="longest_first", truncation=True)
print("tokens_encode: ", tokens_encode)
# tokens_encode:  {'input_ids': [2, 2898, 12170, 18, 548, 645, 20, 16617, 388, 8624, 3, 48, 20, 2853, 11907, 17569, 18, 37, 13, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
# è¾“å‡ºä»¥åˆ—è¡¨çš„å½¢å¼ä¿å­˜
# - input_idsçš„å†…å®¹ä¸encode()æ–¹æ³•è¿”å›çš„ç»“æœç›¸åŒï¼Œä¸ºtokenè½¬åŒ–ä¸ºidä¹‹åçš„è¡¨ç¤ºã€‚
# - token_type_idsçš„å†…å®¹è¡¨ç¤ºç”¨æ¥åŒºåˆ«ä¸¤ä¸ªæ–‡æœ¬ï¼Œä¸º0è¡¨ç¤ºç¬¬ä¸€ä¸ªæ–‡æœ¬ï¼Œä¸º1è¡¨ç¤ºç¬¬äºŒä¸ªæ–‡æœ¬ã€‚
# - attention_maskè¡¨ç¤ºæ–‡æœ¬paddingçš„éƒ¨åˆ†(è¿™é‡Œæ²¡æœ‰ï¼Œæ‰€æœ‰å…¨ä¸º1)ã€‚
# æ¯ä¸ªéƒ¨åˆ†åˆ†åˆ«å¯¹åº”äºBertModelçš„è¾“å…¥å‚æ•°ï¼Œä½¿ç”¨æ—¶å–å‡ºå¯¹åº”é”®å€¼çš„å†…å®¹è¾“å…¥åˆ°ç›¸åº”å‚æ•°å³å¯
# forward(input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None)[SOURCE]


# è·å¾—BERTæ¨¡å‹æœ€åä¸€ä¸ªéšå±‚ç»“æœ
with torch.no_grad():
    last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples

""" tensor([[[-0.0549,  0.1053, -0.1065,  ..., -0.3550,  0.0686,  0.6506],
         [-0.5759, -0.3650, -0.1383,  ..., -0.6782,  0.2092, -0.1639],
         [-0.1641, -0.5597,  0.0150,  ..., -0.1603, -0.1346,  0.6216],
         ...,
         [ 0.2448,  0.1254,  0.1587,  ..., -0.2749, -0.1163,  0.8809],
         [ 0.0481,  0.4950, -0.2827,  ..., -0.6097, -0.1212,  0.2527],
         [ 0.9046,  0.2137, -0.5897,  ...,  0.3040, -0.6172, -0.1950]]]) 
	shape: (1, 9, 768)     
"""
# ----------- é…ç½®æ–‡ä»¶ ------------
from transformers import BertConfig
# è·å–bertæ¨¡å‹ç»“æ„å‚æ•°
bert_config = BertConfig.from_pretrained('bert-base-uncased')
print(bert_config.get_config_dict('bert-base-uncased'))
# ({'architectures': ['BertForMaskedLM'], 'attention_probs_dropout_prob': 0.1, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'hidden_size': 768, 'initializer_range': 0.02, 'intermediate_size': 3072, 'layer_norm_eps': 1e-12, 'max_position_embeddings': 512, 'model_type': 'bert', 'num_attention_heads': 12, 'num_hidden_layers': 12, 'pad_token_id': 0, 'type_vocab_size': 2, 'vocab_size': 30522}, {})
# ----------- albertæ¨¡å‹ ------------
from transformers import AlbertTokenizer, AlbertModel
# albertæ¨¡å‹
tokenizer = AlbertTokenizer.from_pretrained("albert-base-v2", cache_dir="./transformers/")
model = AlbertModel.from_pretrained("albert-base-v2", cache_dir="transformers/")
# å¤šç§æ¨¡å‹ï¼Œå¦‚XLNetã€DistilBBETã€RoBERTaç­‰æ¨¡å‹éƒ½å¯ä»¥ä»¥åŒæ ·çš„æ–¹å¼è¿›è¡Œå¯¼

# ----------- å­¦ä¹ ç‡è®¾ç½® ------------
from transformers import AdaW, get_linear_schedule_with_warmup

warmup_steps = int(args.warmup_proportion * num_train_optimization_steps)	# å®šä¹‰warmupæ–¹å¼çš„æ­¥é•¿
    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)	# å®šä¹‰ä¼˜åŒ–å™¨
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_train_optimization_steps)		# æ›´æ–°å­¦ä¹ ç‡çš„æ–¹å¼

# ----------- tfæ¨¡å‹è®­ç»ƒ ------------
def data_incoming(path):
    x = []
    y = []
    with open(path, 'r') as f:
        for line in f.readlines():
            line = line.strip('\n')
            line = line.split('\t')
            x.append(line[0])
            y.append(line[1])
    df_row = pd.DataFrame([x, y], index=['text', 'label'])
    df_row = df_row.T
    df_label = pd.DataFrame({"label": ['YOUR_LABEL'], 'y': list(range(10))})
    output = pd.merge(df_row, df_label, on='label', how='left')
    return output

def convert_example_to_feature(review):
    return tokenizer.encode_plus(review,
                                 max_length=256,
                                 pad_tp_max_length=True,
                                 return_attention_mask=True,
                                 truncation=True
                                 )

def map_example_to_dict(input_ids, attention_mask, token_type_ids, label):
    return {
               "input_ids": input_ids,
               "token_type_ids": token_type_ids,
               "attention_mask": attention_mask,
           }, label

def encode_example(ds, limit=-1):
    input_ids_list = []
    token_type_ids_list = []
    attention_maks_list = []
    label_list = []
    if limit > 0:
        ds.take(limit)
    for index, row in ds.iterrows():
        review = row["text"]
        label = row['y']
        bert_input = convert_example_to_feature(review)
        input_ids_list.append(bert_input["input_ids"])
        token_type_ids_list.append(bert_input['token_type_ids'])
        attention_maks_list.append(bert_input['attention_maks'])
        label_list.append([label])
    return tf.data.Dataset.from_tensor_slices(
        (input_ids_list, token_type_ids_list, attention_maks_list, label_list)).map(map_example_to_dict)

train = data_incoming(data_path + 'train.tsv')
test = data_incoming(data_path + 'test.tsv')
train = encode_example(train).shuffle(100000).batch(100)
test = encode_example(test).batch(100)
model = TFBertForSequenceClassification(model_path, num_labels=num_labels)
optimizer = tf.keras.optimizers.Adam(1e-5)
model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))
model.fit(train, epochs=epoch, verbose=1, validation_data=test)

```

åŒ…æ‹¬importåœ¨å†…çš„ä¸åˆ°åè¡Œä»£ç ï¼Œæˆ‘ä»¬å°±å®ç°äº†è¯»å–ä¸€ä¸ªé¢„è®­ç»ƒè¿‡çš„BERTæ¨¡å‹ï¼Œæ¥encodeæˆ‘ä»¬æŒ‡å®šçš„ä¸€ä¸ªæ–‡æœ¬ï¼Œå¯¹æ–‡æœ¬çš„æ¯ä¸€ä¸ªtokenç”Ÿæˆ768ç»´çš„å‘é‡ã€‚å¦‚æœæ˜¯äºŒåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥å°±å¯ä»¥æŠŠç¬¬ä¸€ä¸ªtokenä¹Ÿå°±æ˜¯\[CLS]çš„768ç»´å‘é‡ï¼Œæ¥ä¸€ä¸ªlinearå±‚ï¼Œé¢„æµ‹å‡ºåˆ†ç±»çš„logitsï¼Œæˆ–è€…æ ¹æ®æ ‡ç­¾è¿›è¡Œè®­ç»ƒã€‚

**BERT configuration**

Transformersçš„æºç ï¼šè·¯å¾„ src/transformers ä¸‹æœ‰å¾ˆå¤šçš„pythonä»£ç æ–‡ä»¶ã€‚ä»¥ configuration å¼€å¤´çš„éƒ½æ˜¯å„ä¸ªæ¨¡å‹çš„é…ç½®ä»£ç ï¼Œæ¯”å¦‚ configuration_bert.pyï¼Œä¸»è¦æ˜¯ä¸€ä¸ªç»§æ‰¿è‡ª PretrainedConfig çš„ç±» BertConfigçš„å®šä¹‰ï¼Œä»¥åŠä¸åŒBERTæ¨¡å‹çš„configæ–‡ä»¶çš„ä¸‹è½½è·¯å¾„ï¼Œä¸‹æ–¹æ˜¾ç¤ºå‰ä¸‰ä¸ªã€‚
- bert-base-uncasedçš„æ¨¡å‹çš„é…ç½®ï¼Œå…¶ä¸­åŒ…æ‹¬dropout, hidden_size, num_hidden_layers, vocab_size ç­‰ç­‰ã€‚
- æ¯”å¦‚bert-base-uncasedçš„é…ç½®å®ƒæ˜¯12å±‚çš„ï¼Œè¯å…¸å¤§å°30522ç­‰ç­‰ï¼Œç”šè‡³å¯ä»¥åœ¨configé‡Œåˆ©ç”¨output_hidden_statesé…ç½®æ˜¯å¦è¾“å‡ºæ‰€æœ‰hidden_stateã€‚

```python
BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {
    "bert-base-uncased": "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json",
    "bert-large-uncased": "https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json",
    "bert-base-cased": "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json",
}
```

**BERT tokenization**

ä»¥tokenizationå¼€å¤´çš„éƒ½æ˜¯è·Ÿvocabæœ‰å…³çš„ä»£ç ï¼Œæ¯”å¦‚åœ¨ tokenization_bert.py ä¸­æœ‰å‡½æ•°å¦‚whitespace_tokenizeï¼Œè¿˜æœ‰ä¸åŒçš„tokenizerçš„ç±»ã€‚åŒæ—¶ä¹Ÿæœ‰å„ä¸ªæ¨¡å‹å¯¹åº”çš„vocab.txtã€‚ä»ç¬¬ä¸€ä¸ªé“¾æ¥è¿›å»å°±æ˜¯bert-base-uncasedçš„è¯å…¸ï¼Œè¿™é‡Œé¢æœ‰30522ä¸ªè¯ï¼Œå¯¹åº”ç€configé‡Œé¢çš„vocab_sizeã€‚
- å…¶ä¸­ï¼Œç¬¬0ä¸ªtokenæ˜¯\[pad]ï¼Œç¬¬101ä¸ªtokenæ˜¯\[CLS]ï¼Œç¬¬102ä¸ªtokenæ˜¯\[SEP]ï¼Œæ‰€ä»¥ä¹‹å‰encodeå¾—åˆ°çš„ [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102] ï¼Œå…¶å®tokenizeåconvertå‰çš„tokenå°±æ˜¯ [ '[ CLS]', 'here', 'is', 'some', 'text', 'to', 'en', '##code', '[ SEP]' ]ï¼Œç»è¿‡ä¹‹å‰BERTè®ºæ–‡çš„ä»‹ç»ï¼Œå¤§å®¶åº”è¯¥éƒ½æ¯”è¾ƒç†Ÿæ‚‰äº†ã€‚
- BERTçš„vocabé¢„ç•™äº†ä¸å°‘unused tokenï¼Œå¦‚æœæˆ‘ä»¬ä¼šåœ¨æ–‡æœ¬ä¸­ä½¿ç”¨ç‰¹æ®Šå­—ç¬¦ï¼Œåœ¨vocabä¸­æ²¡æœ‰ï¼Œè¿™æ—¶å€™å°±å¯ä»¥é€šè¿‡æ›¿æ¢vacabä¸­çš„unused tokenï¼Œå®ç°å¯¹æ–°çš„tokençš„embeddingè¿›è¡Œè®­ç»ƒã€‚

```python
PRETRAINED_VOCAB_FILES_MAP = {
    "vocab_file": {
        "bert-base-uncased": "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt",
        "bert-large-uncased": "https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt",
        "bert-base-cased": "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt",
    }
}
```

**BERT modeling**

ä»¥modelingå¼€å¤´çš„å°±æ˜¯æœ€å…³å¿ƒçš„æ¨¡å‹ä»£ç ï¼Œæ¯”å¦‚ modeling_bert.pyã€‚æ–‡ä»¶ä¸­æœ‰è®¸å¤šä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹ä»¥ä¾›ä¸‹è½½ï¼Œå¯ä»¥æŒ‰éœ€è·å–ã€‚

ä»£ç ä¸­å¯ä»¥é‡ç‚¹çœ‹**BertModel**ç±»ï¼Œå®ƒå°±æ˜¯BERTæ¨¡å‹çš„åŸºæœ¬ä»£ç , ç±»å®šä¹‰ä¸­ï¼Œç”±embeddingï¼Œencoderï¼Œpoolerç»„æˆï¼Œforwardæ—¶é¡ºåºç»è¿‡ä¸‰ä¸ªæ¨¡å—ï¼Œè¾“å‡ºoutputã€‚

```python
class BertModel(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.config = config

        self.embeddings = BertEmbeddings(config)
        self.encoder = BertEncoder(config)
        self.pooler = BertPooler(config)

        self.init_weights()
        
 def forward(
        self, input_ids=None, attention_mask=None, token_type_ids=None,
        position_ids=None, head_mask=None, inputs_embeds=None,
        encoder_hidden_states=None, encoder_attention_mask=None,
    ):
    """ çœç•¥éƒ¨åˆ†ä»£ç  """
    
        embedding_output = self.embeddings(
            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
        )
        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
        )
        sequence_output = encoder_outputs[0]
        pooled_output = self.pooler(sequence_output)

        outputs = (sequence_output, pooled_output,) + encoder_outputs[
            1:
        ]  # add hidden_states and attentions if they are here
        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)
```
BertEmbeddingsè¿™ä¸ªç±»ä¸­å¯ä»¥æ¸…æ¥šçš„çœ‹åˆ°ï¼Œembeddingç”±ä¸‰ç§embeddingç›¸åŠ å¾—åˆ°ï¼Œç»è¿‡layernorm å’Œ dropoutåè¾“å‡ºã€‚

```python
def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
        # any TensorFlow checkpoint file
        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        
def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None):
        """ çœç•¥ embeddingç”Ÿæˆè¿‡ç¨‹ """
        embeddings = inputs_embeds + position_embeddings + token_type_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
```

BertEncoderä¸»è¦å°†embeddingçš„è¾“å‡ºï¼Œé€ä¸ªç»è¿‡æ¯ä¸€å±‚Bertlayerçš„å¤„ç†ï¼Œå¾—åˆ°å„å±‚hidden_stateï¼Œå†æ ¹æ®configçš„å‚æ•°ï¼Œæ¥å†³å®šæœ€åæ˜¯å¦æ‰€æœ‰çš„hidden_stateéƒ½è¦è¾“å‡ºï¼ŒBertLayerçš„å†…å®¹å±•å¼€çš„è¯ï¼Œç¯‡å¹…è¿‡é•¿ï¼Œè¯»è€…æ„Ÿå…´è¶£å¯ä»¥è‡ªå·±ä¸€æ¢ç©¶ç«Ÿã€‚

```python
class BertEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.output_attentions = config.output_attentions
        self.output_hidden_states = config.output_hidden_states
        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])

    def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
    ):
        all_hidden_states = ()
        all_attentions = ()
        for i, layer_module in enumerate(self.layer):
            if self.output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_outputs = layer_module(
                hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask
            )
            hidden_states = layer_outputs[0]

            if self.output_attentions:
                all_attentions = all_attentions + (layer_outputs[1],)
        # Add last layer
        if self.output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        outputs = (hidden_states,)
        if self.output_hidden_states:
            outputs = outputs + (all_hidden_states,)
        if self.output_attentions:
            outputs = outputs + (all_attentions,)
        return outputs  # last-layer hidden state, (all hidden states), (all attentions)
```

Bertpooler å…¶å®å°±æ˜¯å°†BERTçš„\[CLS]çš„hidden_state å–å‡ºï¼Œç»è¿‡ä¸€å±‚DNNå’ŒTanhè®¡ç®—åè¾“å‡ºã€‚

```python
class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
```

åœ¨è¿™ä¸ªæ–‡ä»¶ä¸­è¿˜æœ‰ä¸Šè¿°åŸºç¡€çš„BertModelçš„è¿›ä¸€æ­¥çš„å˜åŒ–ï¼Œæ¯”å¦‚BertForMaskedLMï¼ŒBertForNextSentencePredictionè¿™äº›æ˜¯BertåŠ äº†é¢„è®­ç»ƒå¤´çš„æ¨¡å‹ï¼Œè¿˜æœ‰BertForSequenceClassificationï¼Œ BertForQuestionAnswering è¿™äº›åŠ ä¸Šäº†ç‰¹å®šä»»åŠ¡å¤´çš„æ¨¡å‹ã€‚

[Huggingfaceç®€ä»‹åŠBERTä»£ç æµ…æ](https://zhuanlan.zhihu.com/p/120315111)

### pipeline NLPå¿«é€Ÿåº”ç”¨

[å‚è€ƒæ–‡ç« ](https://blog.csdn.net/YangStudent/article/details/118879560)ï¼špipelineæ¶‰åŠå¤šä¸ªNLPä»»åŠ¡ï¼Œtransformersåº“ï¼Œpiplineå‡½æ•°
- åˆ†ç±»ï¼Œå®ä½“è¯†åˆ«ï¼Œç”Ÿæˆï¼Œé¢„æµ‹ï¼Œé—®ç­”ï¼Œæ‘˜è¦ï¼Œç¿»è¯‘ï¼Œç›¸ä¼¼åº¦ï¼Œè¿ç§»å­¦ä¹ ï¼Œé¢„è®­ç»ƒæ¨¡å‹ï¼Œtransformeræ¦‚å¿µ
- ç±»ä¼¼sklearnçš„pipelineæµæ°´çº¿æœºåˆ¶

```python
from transformers import pipeline
 
# 1. æƒ…æ„Ÿåˆ†ç±»
classfier1 = pipeline("sentiment-analysis")
print(classfier1("My wife is a beautiful girl"))
# [{'label': 'POSITIVE', 'score': 0.9998767971992493}]
 
# print(classfier1('I am pool', 'My PBL is beautiful, but I love it'))
# [{'label': 'NEGATIVE', 'score': 0.7211759090423584}, {'label': 'POSITIVE', 'score': 0.9998372197151184}]
 
classfier2  = pipeline("zero-shot-classification")
print(classfier2(
    "This a project about the Style transfer",
    candidate_labels = ['education', 'politics', 'business']
))
# {'sequence': 'This a project about the Style transfer', 'labels': ['business', 'education', 'politics'], 'scores': [0.673454225063324, 0.17288313806056976, 0.15366260707378387]}
 
# 2.æ–‡æœ¬ç”Ÿæˆ
generator1 = pipeline("text-generation") # é»˜è®¤çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹æ˜¯gpt2
print(generator1(
    "I owe 2300 yuan",
    max_length = 50, # æŒ‡å®šç”Ÿæˆå¥çš„å¤§å°
    num_return_sequence = 2, # æŒ‡å®šç”Ÿæˆçš„å¥å­ä¸ªæ•°
))
# [{'generated_text': "I owe 2300 yuan from the bank since it made me a few dollars but it's just so damn hard to pay. I'm on a two-yearly policy and the current rate I'm using has to be 100 yuan. So, I"}]
#
 
generator2 = pipeline("text-generation", model="distilgpt2") # æŒ‡å®šæ¨¡å‹ä¸ºdistilgpt2,è½»é‡çš„gpt2
print(generator2(
    "I owe 2300 yuan"
))
# [{'generated_text': 'I owe 2300 yuan to the country.â€'}]
 
# 3.é¢„æµ‹æ–‡æœ¬é®ç½©
unmasker = pipeline('fill-mask') # åŸºäºbert
print(unmasker('My favorite girl is <mask>'))
# top_kçš„å«ä¹‰æ˜¯è¿”å›æœ€æœ‰å¯èƒ½çš„ä¸¤ç§ç»“æœ
# [{'sequence': '<s>My favorite girl isâ€¦</s>', 'score': 0.035072073340415955, 'token': 1174, 'token_str': 'Ã¢Ä¢Â¦'}, {'sequence': '<s>My favorite girl is...</s>', 'score': 0.034020423889160156, 'token': 734, 'token_str': '...'}, {'sequence': '<s>My favorite girl is Barbie</s>', 'score': 0.01795039512217045, 'token': 31304, 'token_str': 'Ä Barbie'}, {'sequence': '<s>My favorite girl is Cinderella</s>', 'score': 0.011553746648132801, 'token': 34800, 'token_str': 'Ä Cinderella'}, {'sequence': '<s>My favorite girl is ______</s>', 'score': 0.010862686671316624, 'token': 47259, 'token_str': 'Ä ______'}]
 
# 4.å‘½åå®ä½“è¯†åˆ«ï¼Œè¯†åˆ«ä¸€å¥è¯ä¸­çš„ï¼Œå®ä½“ï¼Œå¦‚äººï¼Œç»„ç»‡ï¼Œæˆ–åœ°ç‚¹
ner = pipeline('ner', grouped_entities=True) # grouped_entities=True, å…è®¸ç›¸ä¼¼çš„å®ä½“åˆ†ç»„åˆ°åŒä¸€ä¸ªç»„å†…
print(ner("I'm working in CCNU , is a beautiful school , and I like Wollongong"))
# [{'entity_group': 'I-ORG', 'score': 0.9960816502571106, 'word': 'CCNU'}, {'entity_group': 'I-LOC', 'score': 0.9867993593215942, 'word': 'Wollongong'}]
 
 
# 5.æå–é—®é¢˜ç­”æ¡ˆ åœ¨contextä¸­æå–å‡ºquestionçš„ç­”æ¡ˆ
question_answer = pipeline('question-answering')
print(question_answer(
    question = 'Who are you?',
    context = 'I am XsY and good luck to see you',
))
# {'score': 0.6727198958396912, 'start': 5, 'end': 8, 'answer': 'XsY'}
 
# 6.æ–‡æœ¬æ‘˜è¦
summarizer = pipeline('summarization')
print(summarizer("""    America has changed dramatically during recent years. Not only has the number of 
    graduates in traditional engineering disciplines such as mechanical, civil, 
    electrical, chemical, and aeronautical engineering declined, but in most of 
    the premier American universities engineering curricula now concentrate on 
    and encourage largely the study of engineering science. As a result, there 
    are declining offerings in engineering subjects dealing with infrastructure, 
    the environment, and related issues, and greater concentration on high 
    technology subjects, largely supporting increasingly complex scientific 
    developments. While the latter is important, it should not be at the expense 
    of more traditional engineering.
    Rapidly developing economies such as China and India, as well as other 
    industrial countries in Europe and Asia, continue to encourage and advance 
    the teaching of engineering. Both China and India, respectively, graduate 
    six and eight times as many traditional engineers as does the United States. 
    Other industrial countries at minimum maintain their output, while America 
    suffers an increasingly serious decline in the number of engineering graduates 
    and a lack of well-educated engineers.
    """))
# [{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil, electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India, as well as other industrial countries, continue to encourage and advance the teaching of engineering .'}]
 
 
# 7.ç¿»è¯‘
translator = pipeline('translation', model='Helsinki-NLP/opus-mt-zh-en')
print(translator('æˆ‘æ˜¯çœŸçš„å¾ˆç©·ä¸è¦å†å‘æˆ‘äº†'))
# [{'translation_text': "I'm really poor. Don't lie to me again."}]
```



### æ¨¡å‹ä¿¡æ¯

[Transformersæ˜¯TensorFlow 2.0å’ŒPyTorchçš„æœ€æ–°è‡ªç„¶è¯­è¨€å¤„ç†åº“](https://pytorchchina.com/2020/02/20/transformers_1/)

æ¯ä¸ªæ¨¡å‹æ¶æ„çš„è¯¦ç»†ç¤ºä¾‹(Bertã€GPTã€GPT-2ã€Transformer-XLã€XLNetå’ŒXLM)å¯ä»¥åœ¨å®Œæ•´[æ–‡æ¡£](https://huggingface.co/transformers/)ä¸­æ‰¾åˆ°

```python
import torch
from transformers import *

# transformeræœ‰ä¸€ä¸ªç»Ÿä¸€çš„API
# æœ‰10ä¸ªTransformerç»“æ„å’Œ30ä¸ªé¢„è®­ç»ƒæƒé‡æ¨¡å‹ã€‚
#æ¨¡å‹|åˆ†è¯|é¢„è®­ç»ƒæƒé‡
MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),
          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),
          (GPT2Model,       GPT2Tokenizer,       'gpt2'),
          (CTRLModel,       CTRLTokenizer,       'ctrl'),
          (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),
          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),
          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),
          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-cased'),
          (RobertaModel,    RobertaTokenizer,    'roberta-base'),
          (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),
         ]

# è¦ä½¿ç”¨TensorFlow 2.0ç‰ˆæœ¬çš„æ¨¡å‹ï¼Œåªéœ€åœ¨ç±»åå‰é¢åŠ ä¸Šâ€œTFâ€ï¼Œä¾‹å¦‚ã€‚â€œTFRobertaModelâ€æ˜¯TF2.0ç‰ˆæœ¬çš„PyTorchæ¨¡å‹â€œRobertaModelâ€

# è®©æˆ‘ä»¬ç”¨æ¯ä¸ªæ¨¡å‹å°†ä¸€äº›æ–‡æœ¬ç¼–ç æˆéšè—çŠ¶æ€åºåˆ—:
for model_class, tokenizer_class, pretrained_weights in MODELS:
    # åŠ è½½pretrainedæ¨¡å‹/åˆ†è¯å™¨
    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
    model = model_class.from_pretrained(pretrained_weights)

    # ç¼–ç æ–‡æœ¬
    input_ids = torch.tensor([tokenizer.encode("Here is some text to encode", add_special_tokens=True)])  # æ·»åŠ ç‰¹æ®Šæ ‡è®°
    with torch.no_grad():
        last_hidden_states = model(input_ids)[0]  # æ¨¡å‹è¾“å‡ºæ˜¯å…ƒç»„

# æ¯ä¸ªæ¶æ„éƒ½æä¾›äº†å‡ ä¸ªç±»ï¼Œç”¨äºå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œè°ƒä¼˜ï¼Œä¾‹å¦‚ã€‚
BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,
                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]

# ä½“ç³»ç»“æ„çš„æ‰€æœ‰ç±»éƒ½å¯ä»¥ä»è¯¥ä½“ç³»ç»“æ„çš„é¢„è®­ç»ƒæƒé‡å¼€å§‹
#æ³¨æ„ï¼Œä¸ºå¾®è°ƒæ·»åŠ çš„é¢å¤–æƒé‡åªåœ¨éœ€è¦æ¥å—ä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒæ—¶åˆå§‹åŒ–

pretrained_weights = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(pretrained_weights)
for model_class in BERT_MODEL_CLASSES:
    # è½½å…¥æ¨¡å‹/åˆ†è¯å™¨
    model = model_class.from_pretrained(pretrained_weights)

    # æ¨¡å‹å¯ä»¥åœ¨æ¯ä¸€å±‚è¿”å›éšè—çŠ¶æ€å’Œå¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„æƒå€¼
    model = model_class.from_pretrained(pretrained_weights,
                                        output_hidden_states=True,
                                        output_attentions=True)
    input_ids = torch.tensor([tokenizer.encode("Let's see all hidden-states and attentions on this text")])
    all_hidden_states, all_attentions = model(input_ids)[-2:]

    #æ¨¡å‹ä¸Torchscriptå…¼å®¹
    model = model_class.from_pretrained(pretrained_weights, torchscript=True)
    traced_model = torch.jit.trace(model, (input_ids,))

    # æ¨¡å‹å’Œåˆ†è¯çš„ç®€å•åºåˆ—åŒ–
    model.save_pretrained('./directory/to/save/')  # ä¿å­˜
    model = model_class.from_pretrained('./directory/to/save/')  # é‡è½½
    tokenizer.save_pretrained('./directory/to/save/')  # ä¿å­˜
    tokenizer = BertTokenizer.from_pretrained('./directory/to/save/')  # é‡è½½
```

å¦‚ä½•ç”¨12è¡Œä»£ç è®­ç»ƒTensorFlow 2.0æ¨¡å‹,ç„¶ååŠ è½½åœ¨PyTorchå¿«é€Ÿæ£€éªŒ/æµ‹è¯•ã€‚

```python
import tensorflow as tf
import tensorflow_datasets
from transformers import *

# ä»é¢„è®­ç»ƒæ¨¡å‹/è¯æ±‡è¡¨ä¸­åŠ è½½æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ¨¡å‹
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')
data = tensorflow_datasets.load('glue/mrpc')

# å‡†å¤‡æ•°æ®é›†ä½œä¸ºtf.data.Datasetçš„å®ä¾‹
train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')
valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')
train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)
valid_dataset = valid_dataset.batch(64)

# å‡†å¤‡è®­ç»ƒ:ç¼–å†™tf.kerasæ¨¡å‹ä¸ä¼˜åŒ–ï¼ŒæŸå¤±å’Œå­¦ä¹ ç‡è°ƒåº¦
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# ç”¨tf.keras.Model.fitè¿›è¡Œæµ‹è¯•å’Œè¯„ä¼°
history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,
                    validation_data=valid_dataset, validation_steps=7)

# åœ¨PyTorchä¸­åŠ è½½TensorFlowæ¨¡å‹è¿›è¡Œæ£€æŸ¥
model.save_pretrained('./save/')
pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)

#è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å­¦ä¼šäº†è¿™ä¸ªä»»åŠ¡
sentence_0 = "This research was consistent with his findings."
sentence_1 = "His findings were compatible with this research."
sentence_2 = "His findings were not compatible with this research."
inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')
inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')

pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()
pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()

print("sentence_1 is", "a paraphrase" if pred_1 else "not a paraphrase", "of sentence_0")
print("sentence_2 is", "a paraphrase" if pred_2 else "not a paraphrase", "of sentence_0")
```


# ç»“æŸ