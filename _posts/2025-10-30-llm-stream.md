---
layout: post
title:  大模型流式输出与安全
date:   2025-10-30 12:00:00
categories: 大模型
tags: LLM 大模型 流式 安全 qwen 阿里巴巴
excerpt: 大模型流式输出方法，如何实施安全审核？
mathjax: true
permalink: /llm_stream
---

* content
{:toc}


# 大模型流式输出与安全

LLM **流式**输出（Stream）和**非流式**输出（Non-stream）是两种截然不同的数据交互模式，在用户体验、资源消耗和应用场景上存在显著差异。


## 流式输出

【2025-2-27】[大模型 API 调用中的流式输出与非流式输出全面对比：原理、场景与最佳实践](https://help.apiyi.com/stream-vs-nonstream-api-comparison.html)

批量（非流式）输出特点：
- **完整性**：返回的是经过完全处理和验证的完整响应
- **单次**传输：采用标准 HTTP 请求-响应模式，一次性传输所有数据
- 等待时间：用户需要等待整个生成过程完成（可能需要数秒甚至更长）

实时（流式）输出核心特点：
- **实时性**：模型生成一小段内容就立即传输，用户几乎可以实时看到生成过程
- **增量**传输：通过 SSE（Server-Sent Events）或 WebSocket 协议实现服务器到客户端的持续数据流
- **低感知延迟**：用户通常在 100ms 内就能看到首批内容，大幅降低等待感

流式输出与非流式输出

性能差异

| 性能指标   | 流式输出               | 非流式输出               |
|------------|------------------------|--------------------------|
| 首字节延迟 | 极低（通常 100ms 内）| 较高（需等待全部生成）|
| 总完成时间 | 与非流式相近或略长     | 与流式相近或略短         |
| 服务器负载 | 连接维护成本高         | 单次处理负载高但短暂     |
| 网络流量   | 略高（协议开销）| 略低（单次传输）|
| 客户端复杂度 | 较高（需处理流式数据） | 较低（简单的请求-响应）|
| 容错能力   | 较弱（中断风险高）| 较强（完整性保证）|

技术实现差异

| 维度       | 流式输出                     | 非流式输出                 |
|------------|------------------------------|----------------------------|
| 传输协议   | SSE/WebSocket（长连接）| HTTP/1.1（短连接）|
| 连接状态   | 保持长连接直到生成完成       | 请求发出后等待，完成后断开 |
| 数据格式   | 分块传输，每块包含增量内容   | JSON 格式的完整响应体      |
| 服务器资源 | 维持连接状态，内存占用较高   | 生成完毕即释放资源，更节省内存 |
| 网络要求   | 对网络稳定性要求高           | 对网络稳定性要求相对较低   |
| 错误处理   | 中间状态可能导致部分内容丢失 | 全量结果校验，容错性更强   |

代码里体现为 stream 参数值 true 还是 false ，但应用场景很不同

```py
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "讲一个关于人工智能的故事"}],
    stream=true  # 启用流式输出，在代码里就这么一个参数而已
    # stream=false  # 禁用流式输出（默认值）
)

for chunk in response:
    if chunk.choices.delta.content:
        print(chunk.choices.delta.content, end="", flush=True)
```


## 问题
 
LLM 响应延迟是影响用户体验的关键因素。

- 当LLM生成响应时间超过1秒时，用户操作流程就会被打断；
- 若超过10秒，用户往往会切换任务。

传统的"**生成后过滤**"安全防护模式面临严峻挑战。


## 流式安全审核案例


### 【2025-6-30】阿里云

【2025-6-30】[对语言模型流式输出文字进行文本审核](https://help.aliyun.com/document_detail/2642626.html)

当采用模型流式输出方式时，要拼接文字片段后作为文本审核服务的输入内容，但存在明显缺点：
- LLM 容易生成较长内容的文字，拼接全部输出文字后，内容长度有可能超过文本审核的输入限制；
- 用户端会更快看到模型生成内容片段，待全部内容输出时潜在风险可能长时间暴露给用户。

因此, 阿里云文本审核增强版提供了对模型流式输出文字进行**自动拼接并审核**的功能。

该功能既避免了由于内容过长导致文本审核输入限制，也显著降低潜在风险暴露给用户的时间。


### 【2025-7-10】LLM Guard

【2025-7-10】[LLM Guard项目中的流式输出安全防护技术解析](https://blog.gitcode.com/c48f7874fc6b7783d2d0afe5e7131a49.html)

llm-guard  源代码
- [The Security Toolkit for LLM Interactions](https://gitcode.com/gh_mirrors/llm/llm-guard)

LLM Guard 最初针对**完整生成内容**进行安全检查，这在实时交互场景中存在明显缺陷。典型的流式应用需要以token为单位逐步输出内容，传统防护方案会导致两种不良结果：
- 要么用户需要等待完整响应才能看到内容
- 要么系统需要在无防护状态下直接输出内容。

技术实现方案

项目团队经过探索，提出基于**异步**处理的流式防护方案。核心创新点包括：
- 逐token分析机制：系统能够实时分析每个生成的token，在极短时间内完成安全评估
- 并行处理架构：利用asyncio库实现防护逻辑与生成过程的并行执行
- 动态阻断能力：当检测到风险内容时，可以立即终止后续内容生成

未来可能的发展方向包括：
- 专用微型LLM作为协处理器，专门负责实时安全分析
- 基于生成对抗网络(GAN)的动态检测机制
- 硬件加速的实时内容过滤方案

这种流式安全防护技术的成熟，将为实时对话系统、代码自动补全等低延迟应用场景提供可靠的安全保障。


### 【2025-9-23】Qwen3Guard-Stream

【2025-9-23】[Qwen3Guard: 实时安全，逐词响应](https://qwen.ai/blog?id=f0bbad0677edf58ba93d80a1e12ce458f7a80548&from=research.research-list)

Qwen 家族中首款专为安全防护设计的护栏模型。

该模型基于强大的 Qwen3 基础架构打造，并针对安全分类任务进行了专项微调，旨在为人工智能交互提供精准、可靠的安全保障。

无论是用户输入的提示，还是模型生成的回复，Qwen3Guard 均可高效识别潜在风险，输出细粒度的风险等级与分类标签，助力实现更负责任的 AI 应用。

多项主流安全评测基准上，Qwen3Guard 表现卓越，稳居行业领先水平，全面覆盖英语、中文及多语言场景下的提示与回复安全检测任务。

<img width="1206" height="682" alt="image" src="https://github.com/user-attachments/assets/05dcb181-4627-4822-a390-13a92c7e3ffe" />

典型应用：
- （1）利用 Qwen3Guard-Gen 进行安全强化学习（Safety RL）：在不损害模型输出整体有用性的前提下，显著提升模型的内在安全性；
- （2）利用 Qwen3Guard-Stream 实现实时动态干预：无需重新训练模型，即可在生成过程中即时拦截风险内容，确保输出安全可控。

#### Qwen3Guard-Stream 工作流程详解

Qwen3Guard-Stream 工作流程：
- （1）**提示词**级安全预检
  - 用户输入的提示（Prompt）同步发送至大语言模型（LLM）与 Qwen3Guard-Stream。
  - 后者立即对提示内容进行安全评估，并输出对应的安全标签（如“安全”“争议性”“不安全”）。
  - 基于该评估结果，上层系统可智能决策：是允许对话继续进行，还是提前拦截以防范潜在风险。
- （2）**实时逐词**安全审核
  - 若对话获准继续，LLM 将开始逐词（Token-by-Token）流式生成回复。
  - 每个 Token 均会实时传递至 Qwen3Guard-Stream，由其即时判断当前内容的安全性。
  - 该机制实现了贯穿整个回复生成过程的细粒度、不间断内容审核，在不中断用户体验的前提下，动态识别并阻断潜在风险内容。

Qwen3Guard 提供两大专业版本，满足不同应用场景需求：
- Qwen3Guard-Gen（生成式版） 支持对完整用户输入与模型输出进行安全分类，适用于离线数据集的安全标注、过滤，亦可作为强化学习中基于安全性的奖励信号源，是构建高质量训练数据的理想工具。
- Qwen3Guard-Stream（流式检测版） 突破了传统的护栏模型架构，首次实现模型生成过程中的实时、流式安全检测，显著提升在线服务的安全响应效率与部署灵活性。

为适配多样化的部署环境与算力资源，两大版本均提供 0.6B、4B、8B 三种参数规模，兼顾性能与效率，满足从边缘设备到云端服务的全场景需求

Qwen3Guard 支持 119 种语言及方言，适用于全球部署与跨语言应用场景，并在各类语言中均能提供稳定、高质量的安全检测能力

#### 实时流式检测

Qwen3Guard-Stream 专为低延迟设计，可在模型逐词生成回复的过程中实时进行内容审核，确保安全性的同时不牺牲响应速度。

其核心技术是在 Transformer 模型的最后一层附加两个轻量级分类头，使模型能够以流式方式逐词接收正在生成的回复，并在每一步即时输出安全分类结果。

<img width="1203" height="458" alt="image" src="https://github.com/user-attachments/assets/2a40ffa9-0ec1-461e-a221-77043e1fa1f3" />


#### 三级风险等级分类

除传统的“安全”与“不安全”标签外，新增 “争议性” 标签，以支持根据不同应用场景灵活调整安全策略。

用户可根据实际需求，动态将“争议性”内容重新归类为“安全”或“不安全”，从而按需调节审核的严格程度。

如下方评估所示，现有护栏模型受限于二元标签体系，难以同时适配不同数据集的标准。而 Qwen3Guard 凭借三级风险分类设计，可在“严格模式”与“宽松模式”间灵活切换，在多个数据集上均保持稳健的高性能表现。

<img width="3754" height="1713" alt="image" src="https://github.com/user-attachments/assets/3feded33-84cf-4cb5-a028-db999a4bb005" />


# 结束
