---
layout: post
title:  端到端语音交互 Voice Interaction of End2End
date:   2024-11-08 10:00:00
categories: 大模型
tags: llm 对话系统 全双工 tts 语音 贾扬清
excerpt: 端到端语音交互方案汇总
mathjax: true
permalink: /voice_end
---

* content
{:toc}



# 端到端语音交互


端到端实时语音交互 LLM + TTS

## 总结

端到端的音频模型：
- 1、[hertz-dev](https://github.com/Standard-Intelligence/hertz-dev)
- 2、[mini-omni2](https://github.com/gpt-omni/mini-omni2)
- 3、[GLM-4-Voice](https://github.com/THUDM/GLM-4-Voice)
- 4、[moshi](https://moshi.chat) 
- 5、[Spiritlm](https://github.com/facebookresearch/spiritlm)


## 传统AI语音助手 

【2024-8-8】 [告别TTS！贾扬清领衔的 Lepton AI 推出实时语音交互](https://mp.weixin.qq.com/s/4mNd43wTiUbffSkRRAkYCA)

AI语音助手传统路子:
- 把问题往LLM（大语言模型）里一丢，等回话，再让TTS（文本转语音）上阵 ―― 这一连串动作，**听起来挺顺，实则很卡**。
- 跟AI聊天还得等它反应，就像给朋友发微信，结果他秒回了个“**正在输入……**”，急不急人？
  - 传统方法每个步骤都得排队，结果就是“首次音频时间”（TTFA） 拖长，对话流畅度直接打折。
- 分块和缓冲 是工程师们的噩梦。
  - 为了快那么一点点，系统得把**长句子**切成**小块**，到时候再像玩拼图一样拼起来。但这拼图可不是随便拼的，时间差一丁点，不是这边话音未落那边又响起来了，就是句子讲到一半突然卡壳，尴尬得能抠出三室一厅。
- 错误处理也是个大坑。文本和语音本来天生一对，结果被硬生生拆散了。万一哪边出了岔子，找起原因来就像大海捞针，用户体验？先放一边凉快吧。
- 馊主意: 把长句子拆成小段，一个个往TTS里送，想着这样能快点。
  - 结果 协调起来比登天还难，同步稍有不慎，**音频乱套**、**停顿**尴尬。

说好的流畅对话呢？最后还是让人直呼“带不动”。

手机上 Siri、小爱同学，问它问题需要花费几秒钟去检索

包括GPT4，切换到语音输出模式，还是有不小的延迟。这样一来就显得有些卡顿，等待AI回复的过程像是过了几千年，让人恨不得把脑袋伸进手机里让AI快点。

贾扬清创办的Lepton AI刚刚宣布，Lepton LLM API 已经支持实时语音交互了！

## 解决方案


### Ichigo

本地实时语音交互

[Ichigo](https://github.com/homebrewltd/ichigo) 是一个开放的、持续的研究实验，旨在扩展文本基础的大语言模型，使其具备原生的"听觉"能力。

一个开放数据、开放权重、在设备上运行的 Siri

### 全双工 LSLM

【2024-8-5】[全双工对话:大模型能边说边听了](https://mp.weixin.qq.com/s/ud0Zhy380vgTGQ7-t1S9VA)
- 上海交大开发出新模型 `LSLM`（Listening-while-Speaking Language Model），实现了真正的"**全双工对话**"。listening-while-speaking language model
- 论文 [Language Model Can Listen While Speaking](https://arxiv.org/abs/2408.02622)
- [Demo](https://ziyang.tech/LSLM/)

传统的AI对话模型都是"你一句我一句"的轮流模式。但LSLM不一样，它可以同时说话和听话。AI一边"嘴巴"不停，一边"耳朵"也没闲着

两个关键技术:
- 基于token的解码器TTS:负责生成语音
- 流式自监督学习编码器:实时处理音频输入

为了让"说"和"听"这两个通道更好地协同工作，探索了三种融合策略:
- 早期融合
- 中期融合
- 晚期融合
- ![](https://ziyang.tech/LSLM/pic/model-fusion.png)

最终，中期融合脱颖而出，在语音生成和实时交互之间取得了最佳平衡。
- "中期融合就像人类大脑处理信息的方式，既不会太早下结论，也不会反应太慢。这可能是未来对话AI的发展方向。"

两种实验场景:
- 基于命令的全双工模式
- 基于语音的全双工模式

结果显示，LSLM不仅能抗噪音，还能对各种指令保持敏感。

Full Duplex Modeling (FDM)
- ![](https://ziyang.tech/LSLM/pic/duplex.png)
- ![](https://ziyang.tech/LSLM/pic/model-model.png)


<iframe width="560" height="315" src="https://www.youtube.com/embed/vNV4ZhuUb8o?si=CqerpFw8_iscMBsD" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

虽然LSLM看起来很厉害，但距离真正的"自然对话"还有一段距离。

不过，这项研究确实为交互式语音对话系统的发展开辟了新路径，让AI在实际应用中更接地气。


### Lepton LLM API

【2024-8-8】 [告别TTS！贾扬清领衔的 Lepton AI 推出实时语音交互](https://mp.weixin.qq.com/s/4mNd43wTiUbffSkRRAkYCA)
- 官方 [API](https://www.lepton.ai/references/llm_models)

#### 技术原理

Lepton AI 直接把 `LLM` 和 `TTS` 合二为一。
- 传统系统里，文本和音频排队等处理；
- 这里文本和语音**并行处理**，速度嘎嘎快，首次音频时间（TTFA）直接缩水到十分之一，自然无比顺滑。

除了减少延迟外，Lepton AI 还引入用于简化和优化内容处理的高级机制，能**根据对话内容动态调整音频片段**。这样，对话不仅连贯，还超级自然，停顿、中断？不存在的！用户体验直接拉满！

这技术还超级百搭，跟那些开源的LLM模型都私下里串通好了。
- 比如Llama3.1系列，无论是8B、70B还是405B，都能跟Lepton AI的语音模式无缝对接。
- 开发者们可以随心所欲地挑选心仪的模型，再搭配上 Lepton AI 语音黑科技，创造出既个性又高效的应用，享受“私人订制”服务。

#### 效果

向AI提问题后，AI立即进行回答，几乎是秒回，而且还有不同音色任君选择。

根据测试，他们已经能做到让AI在在300ms内开始回答问题。

贾扬清 [Twitter 演示视频](https://x.com/LeptonAI/status/1820868523746312636)

### Mini-Omni

【2024-9-3】[开源版GPT-4o语音来袭，Mini-Omni开启实时语音对话](https://mp.weixin.qq.com/s/tdtEeJ0yRWKEs2TucWK9NA)

Mini-Omni, 更强大的实时语音对话AI模型开源

【2024-8-30】清华 gpt-omni 团队开发，语音助手界的一匹黑马，不仅能实现**实时**语音对话，还能同时生成**文本**和**音频**
- 模型下载:[mini-omni](https://hf.co/gpt-omni/mini-omni)
- 论文地址:[Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming](https://hf.co/papers/2408.16725)
- 代码仓库:[mini-omni](https://github.com/gpt-omni/mini-omni)

Mini-Omni有哪些特性呢？
- **实时**语音对话:这意味着你说话的同时，AI就能立即理解并回应，不再有明显的延迟。
- 同时生成**文本**和**音频**:这个功能简直太强大了！AI不仅能说，还能同步给出文字版本，对听力不好的朋友来说简直是福音。
- **流式**音频输出:这个技术确保了对话的流畅性，让整个交互过程更加自然。

Mini-Omni:语言模型在流式处理中的听、说、思考能力

Mini-Omni 是一个开源的多模态大型语言模型，能够在思考的同时进行听觉和对话。它具备实时的端到端语音输入和流式音频输出对话功能。
- [Qwen2](https://github.com/QwenLM/Qwen2) 作为 LLM 主干。
- [litGPT](https://github.com/Lightning-AI/litgpt) 用于训练和推理。
- [whisper](https://github.com/openai/whisper) 用于音频编码。
- [snac](https://github.com/hubertsiuzdak/snac) 用于音频解码。
- [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) 用于生成**合成**语音。
- [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) 和 [MOSS](https://github.com/OpenMOSS/MOSS/tree/main) 用于对齐。


功能特点
- ? 实时语音对话功能，无需额外的ASR或TTS模型。
- ? 边思考边对话，支持同时生成文本和音频。
- ? 支持流式音频输出。
- ? 提供“音频转文本”和“音频转音频”的批量推理，进一步提升性能。

模型结构
- ![](https://github.com/gpt-omni/mini-omni/raw/main/data/figures/frameworkv3.jpg)


安装

```sh
conda create -n omni python=3.10
conda activate omni

git clone https://github.com/gpt-omni/mini-omni.git
cd mini-omni
pip install -r requirements.txt
```

使用

```sh
# 启动服务器
conda activate omni
cd mini-omni

# 本地测试运行预设的音频样本和问题
python inference.py

# 启动服务
python3 server.py --ip '0.0.0.0' --port 60808
# 运行 Streamlit 演示
# 注意:本地运行 Streamlit 并安装 PyAudio。
pip install PyAudio==0.2.14
API_URL=http://0.0.0.0:60808/chat streamlit run webui/omni_streamlit.py
# 运行 Gradio 演示
API_URL=http://0.0.0.0:60808/chat python3 webui/omni_gradio.py
```


### Hertz-dev

【2024-11-5】[Hertz-dev： 首个开源的超低延迟的实时交互语音对话模型](https://mp.weixin.qq.com/s/9QSrPeaYLfODZ1BpPpEYSQ)

- 代码地址：[hertz-dev](https://github.com/Standard-Intelligence/hertz-dev)
- 体验地址：[hertz-dev](https://si.inc/hertz-dev/)

Hertz-dev 在 RTX 4090 上的理论延迟为 65 毫秒，实际平均延迟为 120 毫秒。这比世界上任何公共模型的延迟都低约 2 倍

模型能够以类似人类的方式互动的先决条件，而不是感觉像延迟、断断续续的电话通话。

作者目前正在训练更大、更先进的 Hertz 版本，它将使用缩放的基础模型配方和 RL 调整来大幅提高模型的原始功能和最终一致性。

Hertz-dev 是实时语音交互的一次探索，也是世界上最容易让研究人员进行微调和构建的对话音频模型。



# 结束