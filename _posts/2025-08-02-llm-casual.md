---
layout: post
title:  "大模型因果推断"
date:   2025-08-02 10:55:00
categories: 大模型
tags: 大模型 因果推理
excerpt: 大模型推理能力如何提升？引入因果推理。大模型+因果推断=？
author: 鹤啸九天
mathjax: true
permalink: /llm_casual
---

* content
{:toc}

# 大模型因果推断


## 因果推断

详见站内专题：[因果科学](casual)


## 大模型+因果

大语言模型 × 因果推断：谁在因果谁?

随着 ChatGPT、Claude、Gemini 等大语言模型（LLM）日益强大，学界开始探索其在因果推断 中的潜力与挑战：
- 一方面，想利用`因果推断`提升语言模型的**鲁棒性、解释性与可靠性**；
- 另一方面，借助 LLMs 强大的知识能力辅助`因果结构发现`、`反事实生成`与`干预决策`。

### 因果→LLM 

如何让LLM真正理解事物间的**因果联系**，而非仅仅是**模式匹配**，一直是行业面临的重大挑战。


### LLM → 因果

传统因果发现方法在面对数据扰动时，往往容易**过拟合**，表现近乎随机。

【2025-7-31】荷兰 莱顿大学计算机学院 [LAICS](https://liacs.leidenuniv.nl/) 
- 论文 [Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery](https://arxiv.org/pdf/2507.23488)

因果推断仍然是大型语言模型面临的基本挑战。

当前最好的推理模型能否稳健因果发现？

传统模型通常会因数据扰动而出现严重的过拟合和近乎随机的性能。

用 Corr2Cause 基准测试，研究了 OpenAI 的 o 系列和 DeepSeek-R 模型家族在因果发现中的表现
- 这些以推理优先的架构相比先前方法取得了显著性能提升。
- 为了充分利用这些优势，引入了受**树形思维**和**链式思维**方法启发的模块化上下文管道，在传统基线模型上实现了接近三倍的改进。

进一步通过分析推理链的长度和复杂度，并进行传统模型和推理模型的定性和定量比较，来探讨该管道的影响。

研究结果表明，尽管高级推理模型已经取得了显著的进步，但精心构建的上下文框架对于最大化这些模型的能力并为跨不同领域的因果发现提供通用蓝图至关重要。

这项研究核心：
- 像OpenAI的o系列和DeepSeek-R这类“推理优先”的LLM架构，在因果发现任务上展现出远超以往方法的原生优势。
- 这标志着LLM不再只是文本生成器，而是开始具备了更深层次的逻辑推理能力，能够理解“为什么”而不是“是什么”。

创新
- 借鉴了“思维之树”（Tree-of-Thoughts）和“思维链”（Chain-of-Thoughts）的思路，提出了一种模块化的上下文学习（in-context learning）流水线。
- 将复杂的**因果推理任务**分解成一系列可管理的步骤，让LLM能够逐步构建因果关系。

实验结果
- 将传统基线方法的性能提升了近三倍
- 这不仅仅是性能的提升，更是对LLM内在推理机制的深度挖掘和有效利用。

应用
- 在医疗领域，帮助医生更准确地诊断疾病的根本原因；
- 金融领域，揭示市场波动的深层驱动因素；
- 自动驾驶领域，提升车辆对复杂交通场景的理解和决策能力。

任何需要理解事物间因果联系的领域，都将因这项技术而受益。
	


## 综述

潜在方向
- 利用 LLM 进行因果结构建模
- 将因果推断方法嵌入 LLM 系统
- 基于 LLM 的因果发现与评估自动化


### LLM 因果结构建模

利用 LLM 进行因果结构建模

LLMs 可以辅助识别变量之间的因果关系，特别是在文本、知识图谱或非结构化数据中。

例如：
-	从文献中提取“X 导致 Y”的显性/隐性结构
-	利用多轮问答评估变量之间的干预关系

代表工作：
-	LLM 作为“因果判断者”（e.g., “Can language models infer causality?” NeurIPS 2022）：探索 LLM 在结构学习中的 prompt engineering 与 few-shot 表现。
-	CausalQA 数据集：训练 LLMs 在问答框架中识别干预与反事实关系。
	
### 嵌入 LLM

将因果推断方法嵌入 LLM 系统

目标: 让语言模型不仅“预测”，还能“解释+干预”：
- 将 因果图（causal graph） 用作 prompt 或 context，在生成时约束信息流
- 结合 do-calculus 理论框架，对复杂系统进行干预模拟（如政策模拟、医疗推荐）

代表方向：
-	CausalGPT / Counterfactual LLMs：将反事实建模机制整合入 decoder，使模型能够生成“如果…会怎样”的干预性语言。
-	Causal Chain-of-Thought：将因果图作为“思维链条”，嵌入语言模型的推理流程中。

### 因果发现/评估自动化

基于 LLM 的因果发现与评估自动化
-	自动审阅论文中的因果假设与方法（如 GRADE 框架）
-	将复杂的 RCT、IV、DID 设计算法转化为 prompt 可控的因果建模器


### 多智能体 CausalMACE

如何用“因果规划”，解决任务依赖难

在长周期、多步骤的协作任务中，传统单智能体面临**任务成功率随步骤长度快速衰减**，**错误级联**导致容错率极低等问题。
	
需要构建具备**全局规划**与**因果依赖管理**能力的分布式智能体框架，并在真实游戏中验证效能。
	
【2025-8-26】港科广和腾讯提出 CausalMACE 方法，将因果推理机制系统性地引入开放世界多智能体系统，为复杂任务协同提供了可扩展的工程化解决方案。
- [CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks]()

目前，该工作已中稿EMNLP 2025 Findings。

为了让一群AI像项目团队一样，既分工明确又能动态调整。论文提出“`全局因果任务图`”概念，让AI学会“如果-那么”的逻辑。
	
先搭地基再砌墙，先找食材再下锅。
	
`全局因果任务图`包含两个部分：
- **因果干预**模块：引入`平均处理效应` (ATE) 量化每条依赖边与游戏规则的一致性，自动剔除由大模型先验幻觉导致的错误依赖
- **负载感知调度**：基于DFS路径搜索与动态“繁忙率”指标，实现多智能体实时任务再分配
	
方法框架层面，CausalMACE 则包含“判断”、“规划”、“执行”三个环节。
- 1️⃣ Judger——“裁判”: 实时验证动作是否合法，并给出成败反馈，保证所有智能体在同一套游戏规则下行动。
- 2️⃣ Planner——“总工”
  - 先把复杂任务拆成若干“小工单”，一次性列清，再按游戏规则画一张“粗线条流程图”。
  - 之后，通过因果推理“精修”这张图，对每一条先后关系，让大模型回答“如果游戏规则变了，这条先后关系还成立吗？”
  - 如果，所有规则改变均不影响关系的成立，就删掉这条关系，避免AI做无用功。
  - 经过这轮“去伪存真”，得到一张干净、可执行的任务因果图。
- 3️⃣ Worker——“调度室”
  - 首先，用深度优先搜索把因果图拆成多条“生产线”，给每条生产线实时计算“繁忙指数”。其中，正在这条线上干活的AI越多、离起点越远，指数越高。
  - 接下来，让新来的AI自动加入指数最低的那条线，既避免扎堆，也减少等待。每完成一步，AI向Planner申请下一步任务，整个过程持续迭代。
	

在VillagerBench三项基准任务（建造、烹饪、密室逃脱）中，相较AgentVerse与VillagerAgent基线，任务完成率最高提升12%，效率提升最高达1.5倍。
	
代理工作量更加平衡，相同设置下最大增益达到13%。



## 挑战

⚠️ 挑战与未来方向
-	语义 ≠ 因果：文本中出现“因为”不代表真实因果，如何让 LLM 理解统计学层面的因果推断逻辑仍待突破。
-	缺乏可验证性：LLM 输出的“因果判断”如何在实证中被验证？
-	模型偏倚与稳健性：大模型自身可能携带错误的世界观，甚至强化 spurious correlation。



# 结束