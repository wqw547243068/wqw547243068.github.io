---
layout: post
title:  "大模型应用: 代码辅助"
date:   2025-02-25 19:25:00
categories: 大模型
tags: 大模型 代码生成 代码测试
excerpt: 大模型代码辅助专题
author: 鹤啸九天
mathjax: true
permalink: /llm_code
---

* content
{:toc}

# 大模型应用: 代码辅助


## 总结

【2024-9-6】 第三方观点 常见编码助手：阿里通义灵码，商汤小浣熊，智谱codegeex，讯飞iflycoder
- 反响最好是通义灵码…

【2025-2-25】 最强代码模型 Claude 3.7, Grok 3


## 代码生成概要


【2025-4-13】悉尼大学论文
- [From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future](https://arxiv.org/pdf/2408.02479)

summarise six key topics: `requirement engineering`, `code generation`, `autonomous decision-making`, `software design`, `test generation`, and `software maintenance`. 

SE:
1) **需求文档** Requirement Engineering and Documentation: Capturing, analyzing, and documenting software requirements, as well as generating user manuals and technical documentation.
2) **代码开发** Code Generation and Software Development: Automating code generation, assisting in the development lifecycle, refactoring code, and providing intelligent code recommendations.
3) **自学习和决策** Autonomous Learning and Decision Making: Highlighting the capabilities of LLM-based agents in autonomous learning, decision-making, and adaptive planning within SE contexts.
4) **软件设计评估** Software Design and Evaluation: Contributing to design processes, architecture validation, performance evaluation, and code quality assessment.
5) **软件测试** Software Test Generation: Generating, optimizing, and maintaining software tests, including unit tests, integration tests, and system tests.
6) **软件安全、维护** Software Security & Maintenance: Enhancing security protocols, facilitating maintenance tasks, and aiding in vulnerability detection and patching

We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. 

| Category | LLMs | LLM - based agents | Total |
| ---- | ---- | ---- | ---- |
| Requirement Engineering and Documentation | Requirement Classification and Extraction (4)<br>Requirement Generation and Description (6)<br>Requirements Satisfaction Assessment (1)<br>Requirement Verification (1)<br>Quality Evaluation (5)<br>Ambiguity Detection (2) | Generation of Semi - structured Documents (1)<br>Generate safety requirements (1)<br>Automatically generating use cases based on requirements (1)<br>Requirements Satisfaction Assessment (1)<br>Automated User Story Quality Enhancement (3) | 28 |
| Code Generation and software development | Code Generation Debugging (3)<br>Code Evaluation (2)<br>Implement HTTP server (1)<br>Enhancing Code Generation Capabilities (5)<br>Specialized Code Generation (3)<br>Human Feedback Preference Simulation (1) | Automating the Software Development Process (5)<br>Large - Scale Code and Document Generation (2)<br>Tool and External API Usage (4)<br>Multi - Agent Collaboration and Code Refine (6)<br>Improving Code Generation Quality (3) | 35 |
| Autonomous Learning and Decision Making | Multi - LLM Decision - Making (1)<br>Creativity Evaluation (1)<br>Self - Identify and Correct Code (1)<br>Judge Chatbot Response (1)<br>Mimics Human Scientific Debugging (1)<br>Deliberate Problem Solving(1) | Collaborative Decision - Making and Multi - Agent Systems (6)<br>Learning, Reasoning and Decision - Making (12)<br>Learning and Adaptation through Feedback (4)<br>Simulation and Evaluation of Human - like Behaviors (2) | 30 |
| Software Design and Evaluation | Creative Capabilities Evaluation (1)<br>Performance in SE Tasks (1)<br>Educational Utility and Assessment (1)<br>Efficiency Optimization (2) | Automation of Software Engineering Processes (3)<br>Enhancing Problem Solving and Reasoning (4)<br>Integration and Management of AI Models and Tools (3)<br>Performance and Efficiency Improvement (2)<br>Performance Assessment in Dynamic Environments (2) | 19 |
| Software Test Generation | Bug Reproduction and Debugging (2)<br>Security Test (2)<br>Test Coverage (3)<br>Test - Informed Code Generation (1)<br>Universal Fuzzing (1) | Multi - agent Collaborative Test Generation (3)<br>Autonomous Testing and Conversational Interfaces (3) | 15 |
| Software Security & Maintenance | Vulnerability Detection (7)<br>Vulnerability Repair (2)<br>Program Repair (5)<br>Code Generation (1)<br>Requirements Analysis (1)<br>Fuzzing (1)<br>Duplicate Entry (1)<br>Code Generation and Debugging (4)<br>Penetration Testing and Security Assessment (2)<br>Program Analysis and Debugging (1) | Autonomous Software Development and Maintenance (6)<br>Program Fault Localization (4)<br>Vulnerability Detection and Generation Testing (3)<br>Smart Contract Auditing and Repair (2)<br>Safety and Risk Analysis (2)<br>Adaptive and Communicative Agents (1) | 43 | 


## 最新进展


## 代码辅助工具


### AI Shell

AI Shell 将**自然语言**转换为**Shell命令**的CLI工具。受 GitHub Copilot X CLI 启发，但AI Shell是开源的，为所有人提供服务。

用户只需安装 AI Shell 并从OpenAI获取API密钥，便可使用该工具。
- [ai-shell](https://github.com/BuilderIO/ai-shell)

![](https://user-images.githubusercontent.com/844291/230413167-773845e7-4c9f-44a5-909c-02802b5e49f6.gif)



### Cursor

【2024-11-16】10几个人如何构建20多亿的cursor，Lex 对 Cursor 团队访谈
- 视频版 [Cursor CEO访谈](https://youtu.be/oFfVt3S51T4?si=pOPwdXxdALWLrcuw)
- 文章介绍 
  - 【2024-10-31】[Cursor：如何构建 AI Coding 最佳实践？](https://mp.weixin.qq.com/s/4gXqwmtTFny9QMuw1WVdRw)

AI coding 是模型推理能力增加之后的下一个竞争高地。
- Github Copilot 是第一个 LLM-driven 的消费级应用

除了模型厂商、AI Labs 之外，这个领域的参与者也有着 Cursor 这样的初创团队

作为一个 LLM-first IDE，Cursor 在今年迅速出圈
- 一方面: 底层模型 `Claude Sonnet 3.5` 模型 coding 能力提升带来的体验升级
- 另一方面: 团队在 AI Coding UI/UX 上的持续投入。

技术选型
- 刚开始用 Vim 做代码编辑。当时还没有 Neovim，只有 Vim 和一个终端。
- 2021 年 Copilot 发布时，由于 Copilot 只能在 VS Code 上使用，所以 Cursor 转用 VS Code 了。
- Copilot 和 VS Code 组合使用体验特别好，所以即便很喜欢 Vim，还是转向了 VS Code。
- 开发 Cursor 之前，VS Code 都是默认编辑器。

Cursor 是怎么做预测的？Cursor 延迟很低, Tab 健能做下一步动作预测（next action prediction）

背后的技术细节
- 训练了专门**MoE小模型**: 这些模型很依赖 pre-fill tokens
  - 这些模型面对的是非常长的 prompt，需要处理很多代码行，但是实际生成的 token 并不多。这种情况下使用**稀疏模型**（Sparse Model）就很合适，一种 MoE 模型。这个突破**显著**提高了模型处理长上下文时的性能。
- 基于**推测解码**（Speculative Decoding）构建了**推测编辑**（Speculative Edits）。

这两个因素是 Cursor 生成质量高、速度快的关键。

没有哪个模型能在所有方面的表现都比其他模型更好，包括速度、代码编辑能力、处理大量代码的能力、上下文长度和代码能力等等。不过，整体上表现最好的模型是 Sonnet，这也是共识。

大量和 prompt 相关的信息，包括文档、添加的文件和对话历史等。

问题：在 context window 有限的情况下，该如何筛选和组织这些信息？
- Cursor Priompt  渲染器把内容合理地排布在页面上，只需要告诉它想要什么，它就会帮你实现。
- 开发了 Priompt 内部系统， 借鉴现代网页开发的最佳实践。和固定版式的杂志排版不同，网站开发中会涉及到的一个情况是，不同设备的中信息的展示多少、格式等是动态变化的，而用户到底在哪里查看网站开发者事前并不知道，但无论终端怎么变，都要保证网站信息在不同设备上正常显示。AI 提示词工程也是类似，我们要做到无论 input 内容多大、怎么变，output 的格式都能正确展示。


团队创始成员 Aman Sanger （CEO）、Arvid Lunnemark（CTO）、Sualeh Asif（COO）和 Michael Truell（设计主管）详细分享了 Cursor 产品体验、infra、模型训练、数据安全等细节，以及对于 AI coding、AI Agent 的思考，通过这些分享也能了解 Cursor UI/UX 背后的理念。
- • **o1 不会干掉 Cursor**，AI Coding 领域才刚刚开始；
- • 围绕**代码预测、补齐**等各类任务 Cursor 还训练了一系列专门的小模型；
- • Cursor 正在试验一个叫做 `Shadow Space` 的产品概念，后台运行一个隐藏窗口让 AI 在不影响到开发者的操作的情况下进行 coding 任务；
- • 团队在 code base  indexing 上投入了大量精力，这个 indexing 系统会成为接下来其他代码任务可以展开的基础；
- • 未来编程会是自然语言和代码将共存，根据具体任务选择最有效的交互；
- • **AI 正在重塑编程体验**，提高效率的同时保持程序员的创造力和控制力；
- • Cursor 认为 Claude 3.5 Sonnet 综合实力更强，Sonnet 最强的地方在于能够很好地**理解开发者表述并不清晰的目标**，预测程序员接下来的操作、给出适当建议；
- • 即便是 SOTA 模型, 也**不擅长找 bug**，这会是 Cursor 的机会；
- • 当前代码任务基准测试并不能准确反映模型的真实能力，因为现实中的代码任务更加复杂多样，并且充满了模糊性和上下文依赖。Cursor 团队更倾向于通过真实用户的使用反馈来评估模型的性能；
- • 目前还没有人能很好地解决 models routing 问题，底座模型和自有模型之间可以初步实现模型切换，但如果是 GPT-4o、Claude sonnet 和 o1 之间的切换可能要更加复杂。


### MarsCode

【2024-6-27】[探索豆包 MarsCode：字节跳动的AI编程助手](https://zhuanlan.zhihu.com/p/705825268)

字节跳动推出的革命性工具——豆包 [MarsCode](https://www.marscode.cn/) ，免费的AI编程助手，旨在提升开发者的编码体验。

MarsCode不仅仅是一个编程工具，它是一个全方位的AI助手，集成了代码补全、生成、解释、优化、注释生成、单元测试生成、智能问答和问题修复等强大功能。它支持多种编程语言，并且可以无缝集成到Visual Studio Code和JetBrains等主流IDE中。

主要功能
- AI助手：提供代码补全、生成、优化、注释生成和解释。
- 智能问答: 唤起对话框后，你可以在输入框中输入你的问题，然后点击 发送 按钮或敲击回车键，豆包 MarsCode 编程助手将回答你的问题。你可以进行多轮问答，不断补充细节，从而使插件的回答更加准确。


### cline

Cline是一款功能强大且完全免费的AI编程工具，能够显著提高开发效率。

代码编程插件 [Cline](https://github.com/cline/cline)
- vs code 插件[地址](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)

![](https://pic2.zhimg.com/v2-221f39f89a39116bde7788d0524aa5ed_1440w.jpg)

VSCode中安装Cline插件：
- 打开VSCode
- 点击插件图标
- 搜索"Cline"并安装
- 安装完成后，左侧会出现小机器人图标

Cline支持配置多种API key，包括Open Router、Open AI和Ollama等。
- 也可使用 deepseek api

创建一个登录页面非常简单：
- 在Cline中输入任务描述："写一个好看的登录页面，使用HTML、JS、CSS"
- Cline会自动分析需求，并逐步生成HTML、CSS和JavaScript代码
- 最终生成的页面美观实用，完全符合预期

【2025-1-21】实践

任务：
> 生成一段html,js代码，实现功能：逐项卡片展示数组a里的项目，布局要求：第一行加粗展示“RedNote Slangs for TiktokRefugee”，其次再展示卡片，每行3个卡片，超过后另起一行，每个卡片浅蓝色背景，卡片是云朵颜色，3个字段字体颜色依次是红色、绿色、紫色，点击卡片后自动放大，要求简洁、美观、立体效果、配色好看；数据：国内常用网络用语，每个短语包含字段：中文短语、英文短语、英文解释；以 javascript list 输出, 示例 [['a', 'a', 'a']]；注意：只输出代码，不要解释

报错
> Command failed with exit code 1: powershell (Get-CimInstance -ClassName Win32_OperatingSystem).caption
'powershell' 

提交问题官方 [issue](https://github.com/cline/cline/issues/1334)

### Trae

【2025-1-20】[字节全新AI编程软件：Trae！免费无限量使用Claude](https://zhuanlan.zhihu.com/p/19573922437)

[Trae](https://www.trae.ai/) 是字节跳动推出的免费中文 AI IDE，通过 AI 技术提升开发效率。
- 支持原生中文，集成了 Claude 3.5 和 GPT-4o 等主流 AI 模型，完全免费使用。

Trae 主要功能: Builder 模式和 Chat 模式
- Builder 模式可帮助开发者从**零**开始构建项目
- Chat 模式支持对代码库或编程问题进行提问和优化。

Chat 模式
- 快捷键：使用 Cmd + i 或 Cmd + u 调用 Chat 功能。
- 交互方式：在对话框中输入问题或代码需求，Trae 会基于 AI 模型生成代码建议或解答。
- 代码更新：Trae 会显示原始代码和优化后的代码对比，开发者可以选择接受或拒绝。

Builder 模式
- 项目生成：通过简单描述（如“生成一个图片压缩工具”），Trae 可以自动生成项目代码。
- 交互执行：在生成过程中，Trae 可能会征求用户意见（如是否执行命令），需要手动确认。
- 代码预览与调试：Trae 提供 Webview 功能，可以直接在 IDE 内预览 Web 页面，方便前端开发。如果遇到错误，可以通过点击命令行中的“Add To Chat”按钮，将错误信息复制到 Chat 中，让 AI 帮助解决。
- 上下文引用：在 Chat 中可以引用代码块、文件、文件夹或整个项目。
- 命令行工具：支持在本地终端安装 Trae 的命令行工具。

注意事项：
- Trae 的 AI 功能目前不支持直接读取外网链接。
- 使用 Builder 生成项目时，建议提前手动创建虚拟环境（如 Python 的 venv 或 Conda），避免环境变量问题。

Trae 具备友好的交互设计，如代码预览、Webview 功能，以及强大的代码生成能力。

作为一款直接对标 Cursor 和 Windsurf 的全新 AI IDE，Trae 的目标不仅是与这些工具竞争，更是要弥补它们在中文开发者体验上的短板。现在 Trae IDE 的 Claude 3.5 和 GPT-4o 都是限时免费用

![](https://pic1.zhimg.com/v2-3935e23284e2f3c7368af9396a839084_1440w.jpg)


### Firebase

【2025-4-10】谷歌发布 AI 编程工具 [Firebase Studio](https://firebase.studio/) 一款基于云端、人工智能驱动的集成开发环境（IDE）
- lovable + cursor + replit + windsurf 合体应用
- 支持一键构建后端、前端和移动应用
- 支持 React、Next.js、Angular、Vue.js、Flutter、Android、Node.js、Java 和 Python Flask 等多种编程语言和框架，让开发者能快速上手，满足不同项目需求。

从生产到发布都在一个地方完成，除了网页还有安卓应用

应用发布后还带有数据监控能力

Firebase Studio 为缺乏编程经验的用户提供便利，这符合当前“`氛围编码`”（vibecoding）热潮。

与 Cursor AI 等竞品相比，Firebase Studio 优势:
- 不仅支持多种编程框架，还注重用户体验，通过直观的界面和强大的AI生成能力，为非技术用户提供更友好的开发体验。


特点
- Forget about infrastructure
  - 无服务器的 PaaS 层平台，使用者不需要关心服务器、网络甚至不需要去做运维，只需尽情使用就好。
- Make smart, data-driven decisions
  - 数据驱动决策，现在大家都有一个共识，决策应当依托于数据。通过 A/B test，数据会帮助使用者决定使用什么颜色的按钮、每天的广告频次、该用那一半的页面布局等等。
- 跨平台，良好的兼容性
  - 用户可能用苹果（iOS），安卓（Android），或别的应用，可能用C语言，或“ 调用 API ”的方式，Firebase 全都可以支持。
- 免费支持
  - 一则 Firebase Google 本身就是免费的，二则 WebEye 作为谷歌云的高级合作伙伴，除了谷歌的支持以外，WebEye 也会24小时随时提供支持。


## 代码生成


### 数据集


#### BigCodeBench

BigCodeBench: 继 HumanEval 之后的新一代代码生成基准测试
- [BigCodeBench: The Next Generation of HumanEval](https://huggingface.co/blog/leaderboard-bigcodebench)

BigCodeBench 包含1140个函数级任务，挑战 LLMs 遵循指令并将来自139个库的多个函数调用作为工具进行组合。

BigCodeBench 为每个任务提供了复杂的、面向用户的指令，包括清晰的功能描述、输入/输出格式、错误处理和已验证的交互示例。我们避免逐步的任务指令，相信有能力的 LLMs 应该能够从用户的角度以开放的方式理解和解决任务。我们通过测试用例验证特定功能。

![](https://github.com/bigcode-bench/bigcode-bench.github.io/blob/main/asset/tease.svg?raw=true)


### 网站设计


【2025-3-7】[Wegic](https://wegic.ai/)

提示语

```sh
设计一个科技公司主页，要求大气
主营业务：软件开发、人工智能、大模型、AIGC等
团队成员：王文，北京航空航天大学；管同学，北京交通大学博士；王小文，中国农业大学计算机硕士
过往项目：① 嵌入式设备开发 ② 视频直播软件开发 ③ 大模型对话助手
联系方式：公众号 廿面体，邮箱 wqw547243068@163.com
```


### 哪吒抽取系统


【2025-2-25】 实测: 大模型生成《哪吒2》人物抽签系统
- [公众号](https://mp.weixin.qq.com/s/jbbTPy_zkZIfRUH8wjkSGA)

总结（60分及格，低分不计入榜单）
- 功能完成度：DeepSeek V3＞Gemini-2 Flash＞豆包 1.5-Pro＞OpenAI o3-mini
- 页面美观度：DeepSeek V3＞DeepSeek R1满血版=DeepSeek R1联网版=Gemini-2 Flash＞豆包 1.5-Pro
- 数据准确度：DeepSeek R1满血版=豆包 1.5-Pro＞DeepSeek R1联网版=OpenAI o3-mini=Gemini-2 Flash＞GPT-4o-mini
- 自我认知：全部失败，只有DeepSeek能报出模型名（不过是GPT-4）

整体：DeepSeek V3强于DeepSeek R1=openai系列

![](https://pic1.zhimg.com/v2-2185f2f980835b7e02b5b4c27b9ec736_1440w.jpg)


### 聊天框

提示词

```sh
用前端代码写一个聊天对话页面，布局：第一行是标题 "ChatBot Demo"，背景是蓝天, 5朵边缘模糊的白云从左往右慢慢飘动，右上角是太阳，周围发出金光；标题下是 对话框，UI 仿照微信聊天框风格，聊天框居中，占页面 1/2，半透明效果，可滑动，立体效果，Bot/User两种角色分别使用不同logo；支持流式输出；左侧是两个滑动控制条，控制两个参数 temperature 取值范围 [0,1], top_p 取值范围 [0,1]; 对话框下方备注“LLM Web 示例<br>2025-03-01”
```



## NL2Code

【2023-5-30】[代码大模型综述：中科院和MSRA调研27个LLMs，并给出5个有趣挑战](https://mp.weixin.qq.com/s/t2SMftox6546E7kvRgQMnA)
- NL2Code: 将自然语言转换成可执行代码来提高开发人员的工作效率
- 中科院和微软亚洲研究院在 ACL 2023 国际顶会上发表的一篇综述：调研了 NL2Code 领域中的「27 个大型语言模型以及相关评价指标」，分析了「LLMs 的成功在于模型参数、数据质量和专家调优」，并指出了「NL2Code 领域研究的 5 个机遇挑战」，最后作者建立了一个分享[网站](https://nl2code.github.io)来跟踪 LLMs 在 NL2Code 任务上的最新进展。
- [Large Language Models Meet NL2Code: A Survey](https://arxiv.org/abs/2212.09420)


## 代码测试


随着软件系统的复杂性不断增加，软件测试的重要性越来越高，测试活动将影响开发人员的工作效率，产品的可靠性、稳定性和合规性，以及最终产品的运营效率。


### 智能测试

智能测试发展阶段
- 大模型出现之前，软件测试领域一直在探索“智能测试”，例如精准测试、通过各种传统算法生成用例、UI自动化测试等。
- 大模型出现后，智能测试层次不断提升，真正进入了“智能测试”新时代。

《大模型应用跟踪月报（2024年10月）》，从场景上看
- 相较于2024年上半年常见的知识助手、编码助手、智能客服等场景
- 大模型在销售赋能、软件测试、智能运维等场景的应用上升明显。
- ![](https://shaqiu-hub.oss-cn-hangzhou.aliyuncs.com/article/2676_W24ouczRC)

软件测试领域，**自动化测试脚本**成为继**测试用例生成**外又一个显著赋能企业质量和测试活动的重要场景。

从测试端到端工作量分布来看，**测试自动化**工作量占比较大，随着测试业务量的持续增大，对测试自动化的**及时性**和**自动化率**提出了更高要求，同时测试自动化程度高也会降低测试执行部分的工作量。

### 大模型能力

#### 优势

核心优势
- 自然语言理解能力
  - 从**非结构化**需求文档中提取测试需求和关键场景。
  - 自动识别需求中的**模糊或矛盾**之处，优化测试设计。
- **知识学习**与**推理**能力
  - 大模型的上下文推理能力强，复杂场景下生成高质量测试用例。
  - 基于现有知识，预测潜在缺陷位置，提升测试效率。
- **多语言**和**多平台**支持
  - 支持多种语言的测试脚本生成和转换（如将 Java 转为 Python 测试代码）。
  - 跨平台测试（如 Web 和移动端）中提供一致性支持。
- 数据**生成**与**分析**能力
  - 生成多样化测试数据，包括：边界值、随机值和异常值。
  - 高效分析测试结果并自动生成测试报告。

                        
参考：
- 【2025-1-13】[大模型在测试中的应用：开启智能化测试新时代](https://blog.csdn.net/tony2yy/article/details/145108116)

#### 不足

大模型存在的问题
- 模型的准确性与上下文理解
  - 特定领域的专业知识可能不足，需结合领域数据进行微调。
- 生成代码的可维护性
  - 自动化生成的代码质量不稳定，可能需要人工优化，RAG知识库等手段来提升质量。
- 测试流程集成
  - 将大模型能力高效集成到现有测试工具链中仍需探索。
- 数据隐私与安全
  - 生成测试数据或分析日志时，需确保敏感信息的脱敏处理。

                        
参考：
- 【2025-1-13】[大模型在测试中的应用：开启智能化测试新时代](https://blog.csdn.net/tony2yy/article/details/145108116)
- 【2024-06-19】[【AI大模型】在测试中的深度应用与实践案例](https://blog.csdn.net/rjdeng/article/details/139246321)


### 自动化场景

大模型为自动化测试脚本生成带来新方案
- 大模型可以编写自动化测试脚本，用于`单元`、`API`和`UI`功能性和非功能性检查及评估，但是可能需要其他平台或工具执行自动化测试脚本。

范围
- 测试用例生成
- 自动化脚本生成
- 缺陷预测
- 测试数据生成
- 等任务

### (1) 测试用例生成

通过解析需求文档，大模型可以生成覆盖不同场景和边界条件的测试用例

基本流程一致
- 收集全部产品需求和研发设计文档，输入到大模型，生成自动测试用例

区别
- 方案1：**原文**整体输入大模型
- 方案2：原文**摘要**后再给大模型
- 方案3：原文存入向量数据库，通过搜索相似内容，自动生成部分测试用例

3种方案使用场景不同，优缺点也可互补

| 方案 | 文档处理方式 |优点|缺点|适用场景|
| ---- | --- | ---- | ---- | ---- |
| 方案1 | **原文整体**|用例内容相对准确|不支持特大文档，容易超出token限制|**普通规模**需求及设计|
| 方案2 | **原文摘要**|摘要后无需担心token问题|用例**内容不准确**，大部分都概况|**特大规模**的需求及设计|
| 方案3 | **RAG**|用例内容更**聚焦**，无需担心token问题|部分用例|仅对需求及设计中**部分**生成用例| 

参考
- 【2024-10-07】[利用LangChain与大模型自动化生成测试用例](https://blog.csdn.net/2401_84495872/article/details/142739932)


#### streamlit


代码 [AITester](https://github.com/timshen/AITester)



#### LangChain

Langchain 测试用例生成方案


#### 用户登录模块

需求：
- 测试用户登录模块，包括正常登录、错误密码、账号锁定等场景。

代码示例

```py
from wenxin_api import TextGeneration
# 初始化大模型
model = TextGeneration(api_key="your_api_key")

# 输入需求描述
requirement = """
用户登录模块需要支持以下场景：
1. 正确的用户名和密码可以成功登录。
2. 错误的密码会提示登录失败。
3. 连续三次错误登录后，账号会被锁定。
"""

# 生成测试用例
response = model.generate_text(prompt=f"根据以下需求生成测试用例：\n{requirement}")
print(response["result"])
```

输出测试用例示例：
- 正确用户名 "test_user"，密码 "password123"，预期结果：登录成功。
- 用户名 "test_user"，密码 "wrong_password"，预期结果：提示登录失败。
- 连续输入错误密码三次后，预期结果：账号锁定。

解析：
- 通过模型生成的测试用例，涵盖了功能测试的核心场景，并能快速扩展至异常处理和边界条件测试。


GPT-4 生成测试用例示例：

依赖

```sh
pip install openai
pip install pytest
pip install requests
```

代码

```py
import openai

# 设置API密钥
openai.api_key = "YOUR_API_KEY"

def generate_test_cases(prompt):
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=500
    )
    return response.choices[0].text.strip()

# 定义测试用例生成的提示
prompt = """
Generate test cases for an e-commerce platform with the following features:
1. User Registration
2. User Login
3. Product Search
4. Add to Cart
5. Place Order
6. Payment

Please provide detailed test cases including steps, expected results, and any necessary data.
"""

# 生成测试用例
test_cases = generate_test_cases(prompt)
print(test_cases)
```


### (2) 测试数据生成


大模型能够根据场景需求，快速生成多样化的测试数据，包括边界值、异常值和随机值。

生成银行账户系统的测试数据

目标：
- 为账户余额字段生成不同类型的测试数据。

```py
data_requirement = """
生成用于测试银行账户系统的数据，包括：
1. 正常值：0 到 100 万之间的金额。
2. 边界值：负值、0、最大值。
3. 异常值：空值、非数字字符。
"""

response = model.generate_text(prompt=f"根据以下需求生成测试数据：\n{data_requirement}")
print(response["result"])
```

输出结果：

```sh
正常值：500, 10000, 999999
边界值：-1, 0, 1000000
异常值：None, "abc", 1.5e6
```

通过模型生成的数据多样性显著提高，能够有效覆盖更多测试场景。

### (3) 自动化脚本生成

大模型通过自然语言理解，将需求描述转化为可执行代码，极大地提高了测试脚本的开发效率。


#### 功能测试


用测试用例编写自动化测试脚本。

用pytest框架进行功能测试

```py
import requests

# 基础URL
BASE_URL = "http://example.com/api"

def test_user_registration():
    url = f"{BASE_URL}/register"
    data = {
        "username": "testuser",
        "email": "testuser@example.com",
        "password": "password123"
    }
    response = requests.post(url, json=data)
    assert response.status_code == 201
    assert response.json()["message"] == "User registered successfully."

def test_user_login():
    url = f"{BASE_URL}/login"
    data = {
        "email": "testuser@example.com",
        "password": "password123"
    }
    response = requests.post(url, json=data)
    assert response.status_code == 200
    assert "token" in response.json()

def test_product_search():
    url = f"{BASE_URL}/search"
    params = {"query": "laptop"}
    response = requests.get(url, params=params)
    assert response.status_code == 200
    assert len(response.json()["products"]) > 0

def test_add_to_cart():
    # 假设我们已经有一个有效的用户token
    token = "VALID_USER_TOKEN"
    url = f"{BASE_URL}/cart"
    headers = {"Authorization": f"Bearer {token}"}
    data = {"product_id": 1, "quantity": 1}
    response = requests.post(url, json=data, headers=headers)
    assert response.status_code == 200
    assert response.json()["message"] == "Product added to cart."

def test_place_order():
    # 假设我们已经有一个有效的用户token
    token = "VALID_USER_TOKEN"
    url = f"{BASE_URL}/order"
    headers = {"Authorization": f"Bearer {token}"}
    data = {"cart_id": 1, "payment_method": "credit_card"}
    response = requests.post(url, json=data, headers=headers)
    assert response.status_code == 200
    assert response.json()["message"] == "Order placed successfully."
```


#### 性能测试

大模型生成高并发用户请求，进行负载测试。

```py
import threading
import time

def perform_load_test(url, headers, data, num_requests):
    def send_request():
        response = requests.post(url, json=data, headers=headers)
        print(response.status_code, response.json())

    threads = []
    for _ in range(num_requests):
        thread = threading.Thread(target=send_request)
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

# 示例负载测试
url = f"{BASE_URL}/order"
headers = {"Authorization": "Bearer VALID_USER_TOKEN"}
data = {"cart_id": 1, "payment_method": "credit_card"}

# 模拟100个并发请求
perform_load_test(url, headers, data, num_requests=100)
```

#### UI 测试

“操控”浏览器进行自动化测试

大模型可直接将**需求描述**或**测试用例**转化为具体的执行动作
-  Selenium 或 Appium 自动化测试脚本。
-  Browser Use

##### Selenium

登录功能的 Selenium 测试脚本生成

需求：
- 对登录页面进行自动化测试，包括验证输入框和按钮的基本功能。

生成测试脚本

```py 
from wenxin_api import TextGeneration
 
# 输入测试需求
requirement = """
测试目标：验证登录页面基本功能。
1. 页面应包含用户名输入框、密码输入框和登录按钮。
2. 输入正确的用户名和密码后，应成功跳转到首页。
"""

# 生成 Selenium 脚本
response = model.generate_text(prompt=f"根据以下需求生成 Selenium 测试脚本：\n{requirement}")
print(response["result"])
```

输出: 

```py 
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
 
# 初始化 WebDriver
driver = webdriver.Chrome()
 
# 打开登录页面
driver.get("http://example.com/login")
 
# 验证页面元素
assert driver.find_element(By.ID, "username")
assert driver.find_element(By.ID, "password")
assert driver.find_element(By.ID, "loginButton")
 
# 输入用户名和密码
driver.find_element(By.ID, "username").send_keys("test_user")
driver.find_element(By.ID, "password").send_keys("password123")
 
# 点击登录按钮
driver.find_element(By.ID, "loginButton").click()
 
# 验证跳转到首页
assert "Homepage" in driver.title
 
driver.quit()
```

##### Playwright

Playwright 为现代 Web 应用提供可靠的端到端测试。

Playwright 是微软开发的 Web应用 的 **自动化测试框架** 。

selenium 相对于 Playwright 慢很多，因为
- Playwright 是**异步**实现，但 selenium **同步**，后一个操作必须等待前一个操作。
- selenium 由相应厂商提驱动，python+驱动执行相当自动化操作，缺点:如果浏览器驱动和浏览器版本不对应，selenium就会报错，而且时刻关注版本问题。
- Playwright 基于 Node.js 语言开发，不需要再重新下载一个浏览器驱动，相当于已经写好了，仅仅需要安装这个库即可

                        
原文链接：https://blog.csdn.net/ak_bingbing/article/details/135852038

任何浏览器 • 任何平台 • 一个 API
- 跨浏览器。 Playwright 支持所有现代渲染引擎，包括 Chromium、WebKit 和 Firefox。
- 跨平台。 在 Windows、Linux 和 macOS 上进行本地或 CI 测试，无头或有头。
- 跨语言。 在 TypeScript、JavaScript、Python、.NET、Java 中使用 Playwright API。
- 测试移动网络。 适用于 Android 的 Google Chrome 和 Mobile Safari 的原生移动模拟。 相同的渲染引擎可以在桌面和云端运行。


### (4) 静态代码分析

缺陷预测与静态代码分析

大模型通过学习历史代码和缺陷数据，能够预测可能的缺陷位置，并给出优化建议。

基于代码的缺陷预测 

目标：
- 分析一段 Python 代码，预测可能存在的安全漏洞。

示例

```py
code_snippet = """
def login(username, password):
    query = "SELECT * FROM users WHERE username = '{}' AND password = '{}'".format(username, password)
    execute_query(query)
"""
 
# 使用大模型分析代码
response = model.generate_text(prompt=f"分析以下代码并指出潜在的安全问题：\n{code_snippet}")
print(response["result"])
```

输出结果：

```sh
- 问题：代码存在 SQL 注入漏洞。
- 优化建议：使用参数化查询代替字符串拼接。
```

大模型结合知识库和推理能力，可以高效发现代码中的常见漏洞，提升代码质量。

### (5) 测试报告

测试报告自动化生成

大模型可根据测试结果生成详细的测试报告，包括: **问题统计**、**覆盖率分析**和**改进建议**。

自动生成测试报告

目标：
- 对测试结果进行分析，并生成适合管理层的测试总结。

```py
def analyze_test_results(results):
    prompt = f"""
Analyze the following test results and provide a summary report including the number of successful tests, failures, and any recommendations for improvement:

{results}
"""
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=500
    )
    return response.choices[0].text.strip()

# 示例测试结果
test_results = """
Test User Registration: Success
Test User Login: Success
Test Product Search: Success
Test Add to Cart: Failure (Product not found)
Test Place Order: Success
"""
test_results = """
通过的测试用例：90
失败的测试用例：10
覆盖率：85%
"""

# 分析测试结果
report = analyze_test_results(test_results)
print(report)

```

输出示例：

```sh
总测试用例数：100
通过率：90%
覆盖率分析：当前覆盖率为 85%，建议增加边界条件测试以提高覆盖率。
改进建议：关注失败用例涉及的模块，特别是登录和支付功能。
```

### (6) 系统集成

问题
- 如何将上述代码整合到一个`持续集成`（CI）/`持续交付`（CD）管道中
- 如何处理和报告测试结果

确保测试过程高效、自动化，并且易于维护。

详见
- 【2024-06-19】[【AI大模型】在测试中的深度应用与实践案例](https://blog.csdn.net/rjdeng/article/details/139246321)


### 测试工具





#### Shortest

[Shortest](https://shortest.com/) 一款开源 AI 测试框架，彻底改变了开发者**端到端测试**的方式。
- github [shortest](https://github.com/anti-work/shortest) 包含演示视频

开发者用简单易懂的**纯英语**编写**测试用例**。Shortest 将这些指令转换成可执行的测试代码。

Shortest 用 Anthropic 的 Claude API 进行准确的解释和执行。

该框架与 GitHub 无缝集成，并利用 Playwright 强大的测试引擎。

Shortest 提供更快、更直观的测试流程，减少了对大量编码知识的需求。

关键特性：
- 自然语言处理：Shortest 接受用日常英语书写的测试指令。无需学习复杂的测试语法或API。
- LLM驱动的测试执行：Anthropic Claude API 能够解释自然语言输入，并转换为可靠的可执行测试代码。
- Playwright 基础：Shortest 建立在 Playwright强大的测试引擎之上，确保测试执行的稳定性和可靠性。
- GitHub 集成：Shortest 与 GitHub无缝集成，方便管理测试套件和开发者之间的协作。
- 快速创建测试：开发者可以专注于描述测试场景，Shortest负责将其转换为可执行代码，从而加快测试开发速度。


#### Browser Use

网页应用的功能越来越丰富，交互性越来越强。从简单的**信息展示**页面到复杂的**在线办公系统**、**电商平台**，网页应用的测试难度呈指数级增长

传统自动化测试工具在复杂应用面前，显得力不从心。

Browser Use 打破了传统测试工具的局限性，能够快速、准确地模拟用户在浏览器中的各种操作，对网页应用进行全面、深入的测试

Browser Use 运用了一系列先进的**浏览器自动化技术**来实现对浏览器的操控。
- 通过调用浏览器的开发者工具接口，能够模拟用户的各种操作，如点击、输入、滚动等。
- 同时，它还可以监控浏览器的各种事件，如页面加载完成、元素出现或消失等。

信息
- Github：[browser-use](https://github.com/browser-use/browser-use)
- [官网](https://browser-use.com/)
- [操作文档](https://docs.browser-use.com/quickstart)
- 详见站内专题: [agent](agent)

Browser Use
- 将用户**测试需求**转化为语言模型能够理解的格式，发送给LLM进行处理。
- LLM 生成相应的**测试步骤**和**操作指令**
- 再将指令转化为实际的**浏览器操作**。

例如，测试一个复杂的**在线表单填写**功能
- 用户输入“填写所有必填字段并提交表单”
- LLM 分析表单中各个字段，并生成相应的填写内容和操作步骤
- Browser Use 则按照这些步骤，在浏览器中模拟用户的填写和提交操作。


Prompt 示例: 演示案例见 [走进Browser Use：领略AI赋能UI自动化测试的魔法魅力](https://mp.weixin.qq.com/s/Jsg4C6jTj7cMX4LTFfDckg)
- 用谷歌邮箱给我爸爸写一封信，感谢他所做的一切，并将文档保存为PDF
- 阅读我的简历并找到ML工作，将其保存到文件中，然后在新选项卡中开始申请，如果您需要帮助，请咨询我。
- 在kayak.com上查找 2024年12月25日至2025年2月2日从苏黎世飞往北京的航班。
- 查找具有cc-by-sa-4.0 license的模型，并按照在Hugging Face上的最多点赞数进行排序，将前5名保存到文件中。

代码 

```py
from langchain_openai import ChatOpenAI
from browser_use import Agent
import asyncio

llm = ChatOpenAI(model="gpt-4o")

async def main():
    agent = Agent(
        task="帮我查找2025年1月12日从巴厘岛飞往阿曼的单程航班，并返回最便宜的选项。",
        llm=llm,
    )
    result = await agent.run()
    print(result)

asyncio.run(main())
```

#### AutoMouser


【2025-1-17】[AutoMouser：AI Chrome扩展程序，实时跟踪用户的浏览器操作，自动生成自动化操作脚本](https://mp.weixin.qq.com/s/wWce-aQRajT2ZCV36TSUcA)
- GitHub 仓库：[AutoMouser](https://github.com/guoriyue/AutoMouser)
- 功能：实时跟踪用户交互行为，自动生成Selenium测试代码。
- 技术：基于OpenAI的GPT模型，支持多种XPath生成策略。
- 应用：适用于自动化测试脚本生成和用户交互行为记录。

AutoMouser是一款Chrome扩展程序，能够智能地跟踪用户的浏览器操作，如点击、拖动、悬停等，并将这些操作转化为结构清晰、易于维护的Python Selenium脚本。通过记录用户的交互行为，AutoMouser简化了自动化测试的创建过程，提高了测试效率。

AutoMouser 核心功能是借助OpenAI的GPT模型，将用户的浏览器操作自动转化为Selenium测试代码。这使得开发者和测试工程师能够快速生成自动化测试脚本，减少了手动编写测试脚本的时间和复杂性。

AutoMouser 主要功能
- 实时交互跟踪：能实时捕捉用户的浏览器操作，包括点击、输入、滚动等，精准地记录下用户在网页上的各种交互行为。
- 自动代码生成：借助OpenAI的GPT模型，将记录下来的用户操作自动转化为Selenium测试代码，生成Python Selenium脚本。
- 智能输入整合：对用户的输入操作进行智能整合，优化代码结构，使生成的测试脚本更加简洁、高效。
- 窗口大小变化检测：能检测浏览器窗口的大小变化，确保生成的测试代码能够适应不同的窗口尺寸。
- JSON动作日志导出：支持将用户的交互数据导出为JSON格式的动作日志文件，方便用户对原始数据进行查看、分析和进一步处理。
- 多种XPath生成策略：采用多种XPath生成策略，能更准确地定位网页元素，提高测试的准确性和可靠性。
- 代码结构优化：输出的Selenium测试代码结构清晰、整洁，易于阅读和理解，方便开发人员进行后续的开发和维护工作。


### 自动化测试应用

【2024-11-04】 [大模型在自动化测试的突破：蚂蚁、华为等头部企业应用实践](https://www.shaqiu.cn/article/J1na9WB7Y0Xp)

蚂蚁集团、中国邮储银行、科大讯飞、华为等4家企业自动化测试领域大模型应用实践

#### 案例1：支付宝小程序基于AI大模型的自动化测试实践

支付宝小程序在质量检测中挑战
- 传统监控**无法有效识别**业务问题。

蚂蚁集团利用AI大模型技术，开发**智能异常检测**和**链路测试**算法，实现自动化测试。

通过自然语言处理和多模态大模型提升识别业务异常和深度链路问题的准确性。

该实践不仅提高了检测效率，还通过智能动线预测模块，增强了用户体验。

完整内容：
- [支付宝小程序基于AI大模型的自动化测试实践]()

#### 案例2：中国邮储银行基于大模型的自动化测试脚本智能生成

传统自动化测试脚本编写需要测试人员具备一定编程能力，且耗时耗力，导致脚本编写人力成本大，质量参差不齐。

邮储银行`金牛座`自动化测试系统引入大模型技术，结合**知识库**、录制、**报文解析**、图像识别等技术，针对不同场景提供脚本智能生成功能：
- **单接口**脚本批量生成：主要用于单接口测试，提供了单接口脚本批量生成功能，有效解决了单接口测试脚本编写量级较大的问题；
- **多接口**组合场景脚本智能生成：主要用于测试业务流程，针对组合场景脚本，采用 录制+智能分析+辅助编写 方案智能生成脚本；
- **UI测试脚本**智能生成：借助大模型技术，可以根据简单的用例描述，直接自动生成测试脚本。


#### 案例3：科大讯飞基于大模型的自动化测试实践

`接口测试`脚本生成场景，科大讯飞将融合大模型能力，提供**智能用例生成**服务，当前平台主要针对HTTP协议进行接口的智能化测试。
- **智能用例生成**: 当新增接口用例或将接口定义导入时，会调用智能用例生成服务自动生成接口自动化用例；
- **UI自动化测试脚本生成**: 用例生成脚本模式，通过用例文本输入引导，结合其他输入数据，生成**UI自动化脚本**。经业务验证，在用例编写规范的情况下，UI自动化脚本生成的采纳率可以达到**50%**以上。

完整内容：
- [科大讯飞基于大模型的自动化测试实践]()

#### 案例4：华为基于LLM的测试自动化代码生成实践

华为选择大模型辅助测试**自动化代码生成**作为大模型在智能测试领域应用的突破点
- 首先, 用`SFT`调优方案，落地场景为**老特性防护网补齐**，但存在时间间隔导致无法写新特性；
- 然后，使用`RAG`方案实现分钟级**新特性编写**；
- 再次，进一步实现无需写**样例脚本**，直接通过 AW 生成。从整个方案迭代方向看，AI自动生成的比例越来越大。

截止2024年6月底，大模型辅助测试自动化代码生成的应用人数为近3k人，覆盖 60+产品，测试自动化生成的代码量 40+万 行。

完整内容：
- [华为基于LLM的测试自动化代码生成实践](https://www.shaqiu.cn/zhiku)


# 结束